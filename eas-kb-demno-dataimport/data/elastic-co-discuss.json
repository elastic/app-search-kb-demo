[
  {
    "id": "c11d8198-24ec-4d5a-83b7-1084f0670e57",
    "url": "https://discuss.elastic.co/t/about-the-beats-category/1067",
    "title": "About the Beats category",
    "category": [
      "Beats"
    ],
    "author": "Leslie_Hawthorn",
    "date": "November 10, 2018, 11:37pm June 2, 2015, 12:11pm March 4, 2016, 8:12pm July 5, 2017, 9:49pm",
    "body": "Any questions regarding Beats, forwarders and shippers for various types of data.",
    "website_area": "discuss"
  },
  {
    "id": "ac276dda-ea4f-4a12-a06a-cbcdbeb2a584",
    "url": "https://discuss.elastic.co/t/error-metricset-jolokia-jmx-mbean-properties-must-be-in-the-form-key-value/215778",
    "title": "ERROR: metricset jolokia.jmx: mbean properties must be in the form key=value",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "vmoragar",
    "date": "January 20, 2020, 4:10pm January 20, 2020, 4:34pm January 22, 2020, 9:04am January 22, 2020, 9:51am January 22, 2020, 3:51pm January 22, 2020, 3:54pm January 22, 2020, 3:55pm",
    "body": "Hi, I use metricbeat (jolokia) to get jmx information in Websphere. jmx.mappings: mbean: 'WebSphere:type=ThreadPool,name=WebContainer,*' attributes: attr: maximumSize field: threadpool.maximumSize The error is: 2020-01-20T17:00:57.768+0100 INFO module/wrapper.go:252 Error fetching data for metricset jolokia.jmx: mbean properties must be in the form key=value: WebSphere:type=ThreadPool,name= WebContainer,* But If you put this url: jolokia-war-1.6.2/read/WebSphere:type=ThreadPool,name=WebContainer,* the result is: {\"value\":{\"WebSphere:cell=Cell01,mbeanIdentifier=cells/Cell01/nodes/Node01/servers/server_01_01/server.xml#ThreadPool_1527689262778,name=WebContainer,node=Node01,platform=dynamicproxy,process=Fortuny_01_01,spec=1.0,type=ThreadPool,version=8.0.0.4\":{\"maximumSize\":50,\"name\":\"WebContainer\",\"inactivityTimeout\":60000,\"growable\":true,\"minimumSize\":50,\"stats\":{\"statistics\":[{\"lowWaterMark\":1,\"highWaterMark\":4,\"unit\":\"N/A\",\"current\":2,\"name\":\"ActiveCount\",\"upperBound\":0,\"wSImpl\":{\"enabled\":true,\"highWaterMark\":4,\"unit\":\"N/A\",\"mean\":1.245895201393063,\"statisticType\":7,\"name\":\"ActiveCount\",\"integral\":2.6420724E7,\"lastSampleTime\":1579519581782,\"startTime\":1579498375565,\"lowWaterMark\":1,\"dataInfo\":{\"aggregatable\":true,\"participation\":null,\"unit\":\"N/A\",\"level\":7,\"updateOnRequest\":false,\"dependency\":null,\"name\":\"ActiveCount\",\"statisticSet\":\"basic\",\"platform\":\"all\",\"submoduleName\":null,\"type\":5,\"id\":3,\"description\":\"The number of concurrently active threads.\",\"zosAggregatable\":true,\"category\":\"all\",\"comment\":\"The number of concurrently active threads\",\"resettable\":true},\"current\":2,\"id\":3,\"upperBound\":0,\"description\":\"The number of concurrently active threads.\",\"lowerBound\":0},\"description\":\"The number of concurrently active threads.\",\"lowerBound\":0,\"lastSampleTime\":1579519581782,\"startTime\":1579498375565},{\"lowWaterMark\":1,\"highWaterMark\":50,\"unit\":\"N/A\",\"current\":7,\"name\":\"PoolSize\",\"upperBound\":50,\"wSImpl\": .........",
    "website_area": "discuss"
  },
  {
    "id": "faebdb90-c92b-468b-a3d7-4e3f14449f68",
    "url": "https://discuss.elastic.co/t/very-low-index-rate-when-using-logtstash-and-or-kafka-redis/215781",
    "title": "Very low index rate when using logtstash and / or kafka/redis",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "MarcusCaepio",
    "date": "January 20, 2020, 4:45pm January 20, 2020, 5:08pm January 21, 2020, 7:58am January 21, 2020, 9:59am January 21, 2020, 10:04am January 21, 2020, 3:02pm January 21, 2020, 5:46pm January 22, 2020, 3:46pm",
    "body": "Hi all, Building a new cluster under 7.5.1. We are using the Filebeat Cisco Module to collect cisco asa logs. Firstly, we tried to store them in a Kafka Node and get them via Logstash into Elasticsearch. We saw very fast, that the index rate was not high enough to get all the logs. We have to index several thousands of messages per second. The Kafka Disk ran full, because it became more input than logstash could remove again. We tried many different combinations with logstash (1 Logstash host, 2 Logstash hosts), also on different locations (stand alone, together with the hot elastic nodes) etc. We also tried different CPUs, RAM, changed worker, batch size and so on. We tested really much for the last whole week. All in all, we never got an index rate higher than around ~7000k/s. Then we tested to push the logs directly into Elasticsearch by the as same as filebeat, which is also getting the logs. After tweaking the filebeat a little bit (more worker, queue mem etc.) we suddenly got an index rate up to 34.000/s. The average rate was around 12000/s for a single beat and we didn't have any lag in getting the logs anymore. Before that, we could see, hot the gap in time became bigger and bigger, because the events where indexed to slow. In conclusion we tried this combinations (where || is a Firewall) Elasticsearch<-Logstash(input kafka output elastic)->||Kafka<-Filebeat (output kafka) Elasticsearch<-Filebeat(input kafka output elastic)->||Kafka<-Filebeat (output kafka) Elasticsearch<-Logstash(input redis output elastic)->||Redis<-Filebeat (output redis) Elasticsearch<-Filebeat(input redis output elastic)->||Redis<-Filebeat (output redis) Elasticsearch||<-Filebeat (output elastic) The big question now is, where does the huge difference come from? Regards, Marcus",
    "website_area": "discuss"
  },
  {
    "id": "64f174f5-fe01-4d41-b6b1-f9f614439327",
    "url": "https://discuss.elastic.co/t/beats-configuration-help/216081",
    "title": "Beats configuration help",
    "category": [
      "Beats"
    ],
    "author": "Walter_Hiranpat",
    "date": "January 22, 2020, 2:47pm",
    "body": "I have deploy an elastic stack mostly using just kibana and elasticsearch (3 master, 5 data, 5 ingest nodes) version 7.5 . I have about 20 application servers and that I installed the beats on each of these servers. It seems to work for a while but after a week or so the beats would stop send beats data and store it locally. This cause the application servers to crash because it would consume majority of the disk drive (use default beats configuration with only modification to elasticsearch and kibana host url). I should also mention that the elasticsearch cluster, each individual host disk space still have enough disk space. I didn't seem to have this issue before on older edition. Does this have to do with how I configure the beats or is this due to something else? Thank you,",
    "website_area": "discuss"
  },
  {
    "id": "740dbcf9-a801-49b3-ba42-7d8aafcb7529",
    "url": "https://discuss.elastic.co/t/best-way-to-attach-delete-index-via-ilm/216078",
    "title": "Best way to attach delete index via ilm",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Jathin",
    "date": "January 22, 2020, 2:34pm",
    "body": "It seems each new filebeat version creates its own mapping template and ilm policy. i had to manually update ilm policy to add delete phase. but for each new version a new ilm policy is created with default sections (without delete phase) and then has to be updated manually. is there a more easier approach for this ?",
    "website_area": "discuss"
  },
  {
    "id": "ce943ca2-9955-42c6-9af2-59cd90cd0a4e",
    "url": "https://discuss.elastic.co/t/yum-install-fails/215936",
    "title": "Yum install fails",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Jathin",
    "date": "January 21, 2020, 6:10pm January 21, 2020, 6:12pm January 21, 2020, 6:34pm January 21, 2020, 6:37pm January 21, 2020, 6:42pm January 21, 2020, 9:33pm January 22, 2020, 2:21pm January 22, 2020, 2:23pm",
    "body": "yum install filebeat-7.5.* Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile Resolving Dependencies --> Running transaction check ---> Package filebeat.x86_64 0:7.5.1-1 will be updated ---> Package filebeat.x86_64 0:7.5.2-1 will be an update --> Finished Dependency Resolution Dependencies Resolved ======================================================================================================================================================================================= Package Arch Version Repository Size ======================================================================================================================================================================================= Updating: filebeat x86_64 7.5.2-1 elastic-7.x 23 M Transaction Summary ======================================================================================================================================================================================= Upgrade 1 Package Total download size: 23 M Is this ok [y/d/N]: y Downloading packages: Delta RPMs disabled because /usr/bin/applydeltarpm not installed. filebeat-7.5.2-x86_64.rpm FAILED https://artifacts.elastic.co/packages/7.x/yum/7.5.2/filebeat-7.5.2-x86_64.rpm: [Errno 14] HTTPS Error 404 - Not Found ] 0.0 B/s | 0 B --:--:-- ETA Trying other mirror. To address this issue please refer to the below wiki article https://wiki.centos.org/yum-errors If above article doesn't help to resolve this issue please use https://bugs.centos.org/. Error downloading packages: filebeat-7.5.2-1.x86_64: [Errno 256] No more mirrors to try. it seems yum metadata is updated with new release but i cant install latest as the contents are not released.. this is true for other elasticsearch and other elk binaries installed in similar fashion",
    "website_area": "discuss"
  },
  {
    "id": "7415c2b5-b793-4b02-8476-afa03059d749",
    "url": "https://discuss.elastic.co/t/metricbeat-oracle-module/215097",
    "title": "Metricbeat Oracle module",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "January 15, 2020, 3:17pm January 20, 2020, 12:16pm January 21, 2020, 3:33am January 22, 2020, 12:25pm January 22, 2020, 1:10pm January 22, 2020, 1:40pm January 22, 2020, 2:05pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "44f83409-8749-4af1-b451-8d64021a1c08",
    "url": "https://discuss.elastic.co/t/filter-instances/214372",
    "title": "Filter instances",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Samerd",
    "date": "January 9, 2020, 1:50pm January 22, 2020, 12:29pm",
    "body": "Hi, i want to monitor specific instance only called 'logz_io': Can we use filter instances : i found this example , but it is not working : - module: aws period: 10s metricsets: - ec2 default_region: eu-central-1 access_key_id: 'xxxxxx' secret_access_key: 'xxx+xxxxx' tags_filter: - key: \"Name\" value: \"logz_io\"",
    "website_area": "discuss"
  },
  {
    "id": "2b7cdfef-aba1-49be-a442-359010c49aea",
    "url": "https://discuss.elastic.co/t/metricbeat-7-5-1-monitors-windows-processes/215999",
    "title": "Metricbeat 7.5.1 monitors Windows Processes",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Marcel_Palme",
    "date": "January 22, 2020, 6:56am January 22, 2020, 12:20pm",
    "body": "Hi, now i want to monitor all windows processes on a Host. I configure system.yml als follow: Module: system module: system metricsets: process processes: '.*' but i get not all Processes that i see unter the Task Manager. Can mi everybody say why?",
    "website_area": "discuss"
  },
  {
    "id": "974e41cc-a887-4794-8e4c-0403b3fd5a12",
    "url": "https://discuss.elastic.co/t/filebeat-not-sending-logs-to-elasticsearch-when-pipeline-is-configured-in-filebeat-yml/216049",
    "title": "Filebeat not sending logs to elasticsearch when pipeline is configured in filebeat.yml",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "ramkms6666",
    "date": "January 22, 2020, 11:53am",
    "body": "hi, I have scenario trying to upload product logs using Filebeat to Elasticsearch. I have created ingest pipeline for converting log date to event date. Below is the ingest pipeline processor, PUT _ingest/pipeline/mylogs { \"description\" : \"Extracting date from log line\", \"processors\" : [ { \"date\" : { \"field\" : \"log.timestamp\", \"target_field\" : \"@timestamp\", \"formats\" : [ \"yyyy-MM-dd HH:mm:ss.SSSS\", \"ISO8601\" ] } } ] } here is the configuration filebeat.yml pointing to pipeline output.elasticsearch: hosts: [\"127.0.0.1:9200\"] pipeline: \"mylogs\" (sorry about the formatting, post doesn't allow me to put next line, but above yaml file is indexed propertly), the moment I add pipeline to filebeat.yml, logs are not being indexed at elastic. Everytime I restart Filebeat, this is what I see in file beat logs. `2020-01-22T17:03:09.151+0530 INFO log/harvester.go:251 Harvester started for file: somepath\\logfile-2020-01-21y - Copy (3).log 2020-01-22T17:03:09.152+0530 INFO log/harvester.go:251 Harvester started for file: somepath\\logfile-2020-01-21y.log 2020-01-22T17:03:09.153+0530 INFO log/harvester.go:251 Harvester started for file: somepath\\logfile-2020-01-21y - Copy (2) - Copy - Copy.log 2020-01-22T17:03:09.153+0530 INFO log/harvester.go:251 Harvester started for file: somepath\\logfile-2020-01-21y - Copy (2) - Copy.log 2020-01-22T17:03:09.175+0530 INFO log/harvester.go:251 Harvester started for file: somepath\\logfile-2020-01-21y - Copy (2).log 2020-01-22T17:03:09.187+0530 INFO log/harvester.go:251 Harvester started for file: somepath\\logfile-2020-01-21y - Copy (3) - Copy.log 2020-01-22T17:03:38.352+0530 INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":828,\"time\":{\"ms\":78}},\"total\":{\"ticks\":1609,\"time\":{\"ms\":141},\"value\":1609},\"user\":{\"ticks\":781,\"time\":{\"ms\":63}}},\"handles\":{\"open\":276},\"info\":{\"ephemeral_id\":\"02b57ff2-f216-4886-8c42-0e4c9916b8b7\",\"uptime\":{\"ms\":960239}},\"memstats\":{\"gc_next\":13469456,\"memory_alloc\":9853264,\"memory_total\":80321576,\"rss\":8192},\"runtime\":{\"goroutines\":55}},\"filebeat\":{\"events\":{\"added\":6,\"done\":6},\"harvester\":{\"open_files\":6,\"running\":6,\"started\":6}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":0,\"filtered\":6,\"total\":6}}},\"registrar\":{\"states\":{\"current\":2369,\"update\":6},\"writes\":{\"success\":6,\"total\":6}}}}}` Not sure what is blocking the logs from pipline into index, any help greatly appreciated. we can't use logstash as this is going to add one more tool for maintenance, so please provide solution without logstash. Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "2a0df04e-95bf-40aa-ae86-773319cb33d8",
    "url": "https://discuss.elastic.co/t/no-logs-shipped-from-kubernetes-filebeat/215899",
    "title": "No logs shipped from Kubernetes Filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 21, 2020, 2:45pm January 21, 2020, 4:42pm January 21, 2020, 6:03pm January 21, 2020, 6:22pm January 22, 2020, 12:17pm January 22, 2020, 11:48am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "3a652b48-ff99-434f-9d8d-210fb40f71da",
    "url": "https://discuss.elastic.co/t/send-aws-metadata-alongwith-logs-from-filebeat-to-kafka/216022",
    "title": "Send AWS Metadata alongwith logs from filebeat to kafka",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "krish0608",
    "date": "January 22, 2020, 9:46am",
    "body": "Is there any way through which i can send Aws metadata like instance id,Region,AccountId etc. This is my use case can you please help me out.",
    "website_area": "discuss"
  },
  {
    "id": "90aa3258-f030-4b00-8f7e-67d301f50ec8",
    "url": "https://discuss.elastic.co/t/actual-memory-usage-on-the-suse-12-sp4-operating-system-is-inaccurate/214516",
    "title": "Actual memory usage on the SUSE 12 SP4 operating system is inaccurate",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "wangtanxu",
    "date": "January 10, 2020, 1:52am January 22, 2020, 9:15am",
    "body": "KiB Mem : 16260192 total, 195472 free, 15297300 used KiB Swap: 16257532 total, 8238932 free, 18600 used. 8257532 cache My actual memory usage is 15297300 / 16260192 = 0.94, right? but the value that metricbeat collects is (16260192 - 8257532) / 16260192 = 0.49. I guess it's supposed to subtract buffer/cache of memory, but it's actually subtracting the cache of swap.",
    "website_area": "discuss"
  },
  {
    "id": "8c9c4ce4-d52d-4599-ac61-4d8ee0f175e2",
    "url": "https://discuss.elastic.co/t/monitor-api-in-heartbeat/216006",
    "title": "Monitor API in heartbeat",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "ollebil",
    "date": "January 22, 2020, 7:59am",
    "body": "I'm trying to monitor an API that is behind Azure API management using heartbeat, and I need to submit the API key. This works: \"curl https://api.blabla.com/service/v1/product/apicall\" -H \"Ocp-Apim-Subscription-Key: 1234567890\" -i -s\" My heartbeat config is: ... check.request: method: \"GET\" headers: \"'Ocp-Apim-Subscription-Key': '1234567890'\" ... but all i get is \"\"statusCode\": 401, \"message\": \"Access denied due to missing subscription key\" What am I missing?",
    "website_area": "discuss"
  },
  {
    "id": "b720eb75-67c5-4fe6-ac36-dd434355aff7",
    "url": "https://discuss.elastic.co/t/failed-to-connect-to-backoff-filebeat-on-server-b-and-elk-on-local-machine/215970",
    "title": "Failed to connect to backoff, Filebeat on Server B and ELK on local machine",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Mehak_Bhargava",
    "date": "January 22, 2020, 1:43am January 22, 2020, 3:20am",
    "body": "Filebeat is running on server B which read logs and pushes to ELK logstash on local Machine A. I want to use filebeat to send real time log files from the server, and point it to my local machine where Elasticsearch, logstahs, kibana is running. But When I run Filebeat on the Server B, I get this error. In services, I tried to see if FIlebeat started but it doesnt show up there too. 2020-01-21T16:59:59.149-0800 ERROR pipeline/output.go:100 Failed to connect to backoff(async(tcp://192.168.40.53:5044)): dial tcp 192.168.40.53:5044: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. 2020-01-21T16:59:59.149-0800 INFO pipeline/output.go:93 Attempting to reconnect to backoff(async(tcp://192.168.40.53:5044)) with 1 reconnect attempt(s) 2020-01-21T17:00:04.005-0800 INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":78,\"time\":{\"ms\":78}},\"total\":{\"ticks\":406,\"time\":{\"ms\":406},\"value\":406},\"user\":{\"ticks\":328,\"time\":{\"ms\":328}}},\"handles\":{\"open\":206},\"info\":{\"ephemeral_id\":\"0a360c35-31e5-4e4a-9a67-1e6cca6c02fa\",\"uptime\":{\"ms\":30186}},\"memstats\":{\"gc_next\":29718816,\"memory_alloc\":21262304,\"memory_total\":42124048,\"rss\":49532928},\"runtime\":{\"goroutines\":30}},\"filebeat\":{\"events\":{\"active\":5504,\"added\":5506,\"done\":2},\"harvester\":{\"open_files\":1,\"running\":1,\"started\":1}},\"libbeat\":{\"config\":{\"module\":{\"running\":0},\"reloads\":1},\"output\":{\"type\":\"logstash\"},\"pipeline\":{\"clients\":1,\"events\":{\"active\":4117,\"filtered\":1389,\"published\":4116,\"retry\":2048,\"total\":5506}}},\"registrar\":{\"states\":{\"current\":1,\"update\":2},\"writes\":{\"success\":2,\"total\":2}},\"system\":{\"cpu\":{\"cores\":8}}}}} 2020-01-21T17:00:22.194-0800 ERROR pipeline/output.go:100 Failed to connect to backoff(async(tcp://192.168.40.53:5044)): dial tcp 192.168.40.53:5044: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.",
    "website_area": "discuss"
  },
  {
    "id": "5bc0fbeb-e324-4886-a61e-9b834ae638d1",
    "url": "https://discuss.elastic.co/t/unable-to-generatecustombeat-with-git-bash-on-windows/215974",
    "title": "Unable to GenerateCustomBeat with Git Bash on Windows",
    "category": [
      "Beats"
    ],
    "author": "Mint-M",
    "date": "January 22, 2020, 3:05am",
    "body": "I have followed all steps of the official guide bellow to generate a custom beat. https://www.elastic.co/guide/en/beats/devguide/current/newbeat-generate.html#newbeat-generate It works on CentOS8 ,however I met a few problems on Windows. Git 2.25.0 ,Python 2.7.17 ,go 1.13.5 and virtualenv 16.7.9 were installed on this Windows laptop. I fixed the first problem of the wrong directory format myself by editing beats\\generator\\common\\beatgen\\setup\\setup.go. Line 100: \"git archive --remote \"+beatPath+\" HEAD | tar -x --exclude=x-pack -C \"+vendorPath) should be replaced with \"git archive --remote '\"+beatPath+\"' HEAD | tar -x --exclude=x-pack -C \"+vendorPath). $ mage GenerateCustomBeat 2020/01/22 10:10:04 Found Elastic Beats dir at D:\\go\\src\\github.com\\elastic\\beats Enter the beat name [examplebeat]: Enter your github name [your-github-name]: Enter the beat path [github.com/your-github-name/examplebeat]: Enter your full name [Firstname Lastname]: Enter the beat type [beat]: fatal: sent error to the client: git upload-archive: archiver died with error remote: fatal: 'D:gosrcgithub.comelasticbeats' does not appear to be a git repository remote: git upload-archive: archiver died with error tar: This does not look like a tar archive tar: Exiting with failure status due to previous errors Error: error running git archive: running \"sh -c git archive --remote D:\\go\\src\\github.com\\elastic\\beats HEAD | tar -x --exclude=x-pack -C vendor\\github.com\\elastic\\beats\" failed with exit code 2 Nevertheless, it caused some loops after running 'mage GenerateCustomBeat' again. The output kept showing \"Found Elastic Beats dir at C:\\Users\\beats\\go\\src\\github.com\\your-github-name\\examplebeat\\vendor\\github.com\\elastic\\beats\". Unfortunately, I didn't find any useful message which could help resolve this problem. It can also be reproduced on a fresh installed Windows computer. beats@DESKTOP-T5E96U8 MINGW64 ~/go/src/github.com/elastic/beats (master) $ mage -debug GenerateCustomBeat DEBUG: 10:30:28.377736 getting all non-mage files in . DEBUG: 10:30:28.555476 found non-mage files DEBUG: 10:30:28.555476 getting all files plus mage files DEBUG: 10:30:28.886081 time to scan for Magefiles: 508.3452ms DEBUG: 10:30:28.886081 found magefiles: magefile.go DEBUG: 10:30:29.033354 output exe is C:\\Users\\beats\\magefile\\e3fea17945c6cebfb332d4aa149fcb540de3c7f3.exe DEBUG: 10:30:29.164922 go build cache exists, will ignore any compiled binary DEBUG: 10:30:29.164922 parsing files DEBUG: 10:30:29.166134 found target AddLicenseHeaders DEBUG: 10:30:29.166134 found target CheckLicenseHeaders DEBUG: 10:30:29.166134 found target DumpVariables DEBUG: 10:30:29.166134 found target Fmt DEBUG: 10:30:29.166134 found target GenerateCustomBeat DEBUG: 10:30:29.166134 found target PackageBeatDashboards DEBUG: 10:30:29.166134 time parse Magefiles: 1.1523ms DEBUG: 10:30:29.166241 Creating mainfile at mage_output_file.go DEBUG: 10:30:29.166442 writing new file at mage_output_file.go DEBUG: 10:30:29.169910 compiling to C:\\Users\\beats\\magefile\\e3fea17945c6cebfb332d4aa149fcb540de3c7f3.exe DEBUG: 10:30:29.169910 compiling using gocmd: go DEBUG: 10:30:29.517677 running go build -o C:\\Users\\beats\\magefile\\e3fea17945c6cebfb332d4aa149fcb540de3c7f3.exe magefile.go mage_output_file.go DEBUG: 10:30:30.660657 time to compile Magefile: 1.1290491s DEBUG: 10:30:30.660804 running binary C:\\Users\\beats\\magefile\\e3fea17945c6cebfb332d4aa149fcb540de3c7f3.exe DEBUG: 10:30:30.660804 running magefile with mage vars: MAGEFILE_DEBUG=1 MAGEFILE_GOCMD=go 2020/01/22 10:30:30 Found Elastic Beats dir at C:\\Users\\beats\\go\\src\\github.com\\elastic\\beats Enter the beat name [examplebeat]: Enter your github name [your-github-name]: Enter the beat path [github.com/your-github-name/examplebeat]: Enter your full name [Firstname Lastname]: Enter the beat type [beat]: You have uncommited changes in elastic/beats. Running CopyVendor running in dev mode, elastic/beats will be copied into the vendor directory with cp DEBUG: 10:31:05.434840 getting all non-mage files in . DEBUG: 10:31:06.824772 found non-mage files main.go DEBUG: 10:31:06.824772 marked file as non-mage: \"main.go\" DEBUG: 10:31:06.824772 getting all files plus mage files DEBUG: 10:31:07.556151 time to scan for Magefiles: 2.1213114s DEBUG: 10:31:07.556151 found magefiles: magefile.go DEBUG: 10:31:07.708800 output exe is C:\\Users\\beats\\magefile\\7f339464e19330127d1e4d10c963fc90c91870d6.exe DEBUG: 10:31:07.858847 go build cache exists, will ignore any compiled binary DEBUG: 10:31:07.858847 parsing files DEBUG: 10:31:07.858847 found target Build DEBUG: 10:31:07.858847 found target BuildGoDaemon DEBUG: 10:31:07.858847 found target Check DEBUG: 10:31:07.858847 found target Clean DEBUG: 10:31:07.858847 found target Config DEBUG: 10:31:07.858847 found target CrossBuild DEBUG: 10:31:07.858847 found target Fields DEBUG: 10:31:07.858847 found target Fmt DEBUG: 10:31:07.858847 found target GolangCrossBuild DEBUG: 10:31:07.858847 found target Package DEBUG: 10:31:07.858847 found target Test DEBUG: 10:31:07.858847 found target Update DEBUG: 10:31:07.858847 found target VendorUpdate DEBUG: 10:31:07.858847 time parse Magefiles: 0s DEBUG: 10:31:07.858847 Creating mainfile at mage_output_file.go DEBUG: 10:31:07.859787 writing new file at mage_output_file.go DEBUG: 10:31:07.862983 compiling to C:\\Users\\beats\\magefile\\7f339464e19330127d1e4d10c963fc90c91870d6.exe DEBUG: 10:31:07.862983 compiling using gocmd: go DEBUG: 10:31:08.232689 running go build -o C:\\Users\\beats\\magefile\\7f339464e19330127d1e4d10c963fc90c91870d6.exe magefile.go mage_output_file.go DEBUG: 10:31:09.383308 time to compile Magefile: 1.1342371s DEBUG: 10:31:09.383640 running binary C:\\Users\\beats\\magefile\\7f339464e19330127d1e4d10c963fc90c91870d6.exe DEBUG: 10:31:09.383640 running magefile with mage vars: MAGEFILE_DEBUG=1 MAGEFILE_GOCMD=go MAGEFILE_DEBUG=1 MAGEFILE_GOCMD=go 2020/01/22 10:31:09 Found Elastic Beats dir at C:\\Users\\beats\\go\\src\\github.com\\your-github-name\\examplebeat\\vendor\\github.com\\elastic\\beats DEBUG: 10:31:09.849543 getting all non-mage files in . DEBUG: 10:31:10.568216 found non-mage files main.go DEBUG: 10:31:10.568216 marked file as non-mage: \"main.go\" DEBUG: 10:31:10.568216 getting all files plus mage files DEBUG: 10:31:11.293979 time to scan for Magefiles: 1.444436s DEBUG: 10:31:11.293979 found magefiles: magefile.go DEBUG: 10:31:11.461685 output exe is C:\\Users\\beats\\magefile\\7f339464e19330127d1e4d10c963fc90c91870d6.exe DEBUG: 10:31:11.615653 go build cache exists, will ignore any compiled binary DEBUG: 10:31:11.615653 parsing files DEBUG: 10:31:11.615653 found target Build DEBUG: 10:31:11.615653 found target BuildGoDaemon DEBUG: 10:31:11.615653 found target Check DEBUG: 10:31:11.615653 found target Clean DEBUG: 10:31:11.615653 found target Config DEBUG: 10:31:11.615653 found target CrossBuild DEBUG: 10:31:11.615653 found target Fields DEBUG: 10:31:11.615653 found target Fmt DEBUG: 10:31:11.615653 found target GolangCrossBuild DEBUG: 10:31:11.615653 found target Package DEBUG: 10:31:11.615653 found target Test DEBUG: 10:31:11.615653 found target Update DEBUG: 10:31:11.615653 found target VendorUpdate DEBUG: 10:31:11.615653 time parse Magefiles: 0s DEBUG: 10:31:11.615653 Creating mainfile at mage_output_file.go DEBUG: 10:31:11.616593 writing new file at mage_output_file.go DEBUG: 10:31:11.618614 compiling to C:\\Users\\beats\\magefile\\7f339464e19330127d1e4d10c963fc90c91870d6.exe DEBUG: 10:31:11.618614 compiling using gocmd: go DEBUG: 10:31:11.959351 running go build -o C:\\Users\\beats\\magefile\\7f339464e19330127d1e4d10c963fc90c91870d6.exe magefile.go mage_output_file.go DEBUG: 10:31:13.051396 time to compile Magefile: 1.0751484s DEBUG: 10:31:13.051396 running binary C:\\Users\\beats\\magefile\\7f339464e19330127d1e4d10c963fc90c91870d6.exe DEBUG: 10:31:13.051396 running magefile with mage vars: MAGEFILE_DEBUG=1 MAGEFILE_GOCMD=go MAGEFILE_DEBUG=1 MAGEFILE_GOCMD=go",
    "website_area": "discuss"
  },
  {
    "id": "360e4769-4635-4144-9a14-092e474f1169",
    "url": "https://discuss.elastic.co/t/parse-json-from-selected-pods-only/215969",
    "title": "Parse json from selected pods only",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 22, 2020, 1:16am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c9ec374b-f484-4a35-acfb-8c35b2bf4db0",
    "url": "https://discuss.elastic.co/t/could-not-load-template-couldnt-load-template-couldnt-load-json-error-400-bad-request/215961",
    "title": "Could not load template: couldn't load template: couldn't load json. Error: 400 Bad Request",
    "category": [
      "Beats",
      "Community Beats"
    ],
    "author": "Bjen_Shah",
    "date": "January 21, 2020, 10:33pm",
    "body": "2020-01-17T09:49:26-06:00 INFO Connected to Elasticsearch version 7.5.0 2020-01-17T09:49:26-06:00 INFO Trying to load template for client: http://0.0.0.0:9200 2020-01-17T09:49:26-06:00 ERR Connecting error publishing events (retrying): Connection marked as failed because the onConnect callback failed: Could not load template: couldn't load template: couldn't load json. Error: 400 Bad Request I know it seems old template; I have updated from default to _doc still I get errors, can I get a few suggestion to make compatible with new version 7.5.x. Please. When I have used this following templates : github.com hartfordfive/cloudflarebeat/blob/master/cloudflarebeat.template.json { \"mappings\": { \"_default_\": { \"_all\": { \"norms\": false }, \"_meta\": { \"version\": \"6.0.0-alpha1\" }, \"dynamic_templates\": [ { \"fields\": { \"mapping\": { \"ignore_above\": 1024, \"type\": \"keyword\" }, \"match_mapping_type\": \"string\", \"match\": \"*\" } } This file has been truncated. show original",
    "website_area": "discuss"
  },
  {
    "id": "dba262c0-ba97-488e-a4ef-6f3359926ff2",
    "url": "https://discuss.elastic.co/t/new-module-fails-system-pipeline-reload-test/215793",
    "title": "New Module Fails System Pipeline Reload Test",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "seaseao",
    "date": "January 20, 2020, 8:05pm January 21, 2020, 10:29pm",
    "body": "Hi Everyone, I'm working on creating a new module for Filebeat to capture Greenplum command logs. After creating a module and fileset and adding in a first pipeline, the test_reload_writes_pipeline test fails with error code -15. I am unsure of what that error code means. An awkward thing about my pipeline is that it has double quotes which I use a slash to cancel, I was able to run the system tests fine on a commit where the pipeline was a copy of the postgres pipeline, which makes me believe that something about how I wrote my pipeline.json file is causing the problem. Here is my pipeline { \"description\": \"Pipeline for parsing Greenplum logs.\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"ignore_missing\": true, \"patterns\": [ \"^%{DATETIME:greenplum.log.timestamp},,,%{WORD:greenplum.log.process_id},th-%{WORD:greenplum.log.thread_id},,,%{DATETIME:greenplum.log.timestamp},%{NUMBER:greenplum.log.transaction_id},,,%{WORD:greenplum.log.gp_segment.type}-%{WORD:greenplum.log.gp_segment.id},,,,,\"%{WORD:greenplum.log.level}\",\"%{NUMBER:greenplum.log.state_code}\",\"%{GREEDYDATA:greenplum.log.message}\",,,,,,,%{NUMBER:greenplum.log.cursor_position},,\"%{GREEDYDATA:greenplum.log.file.name}\",%{NUMBER:greenplum.log.file.line},\" ], \"pattern_definitions\": { \"DATETIME\": \"[-0-9]+ %{TIME} %{WORD:event.timezone}\", \"GREEDYDATA\": \"(.|\\n|\\t)\", \"GREENPLUM_DB_NAME\": \"[a-zA-Z0-9_]+[a-zA-Z0-9_\\$]\", \"GREENPLUM_QUERY_STEP\": \"%{WORD:greenplum.log.query_step}(?: | %{WORD:greenplum.log.query_name})?\" } } }, { \"date\": { \"field\": \"greenplum.log.timestamp\", \"target_field\": \"@timestamp\", \"formats\": [ \"yyyy-MM-dd HH:mm:ss.SSS zz\", \"yyyy-MM-dd HH:mm:ss zz\" ] } }, { \"script\": { \"lang\": \"painless\", \"source\": \"ctx.event.duration = Math.round(ctx.temp.duration * params.scale)\", \"params\": { \"scale\": 1000000 }, \"if\": \"ctx.temp?.duration != null\" } }, { \"remove\": { \"field\": \"temp.duration\", \"ignore_missing\": true } } ], \"on_failure\": [ { \"set\": { \"field\": \"error.message\", \"value\": \"{{ _ingest.on_failure_message }}\" } } ] } I have forked the beats repo in case anyone wants to see something else in the project structure. Please let me know if you have any ideas about what I need to fix to pass the system test. Thanks, Christian",
    "website_area": "discuss"
  },
  {
    "id": "9c802695-0872-4792-84e8-0f621e7cbe43",
    "url": "https://discuss.elastic.co/t/custom-beat-install-information-and-help/215958",
    "title": "Custom beat install information and help",
    "category": [
      "Beats",
      "Beats Developers"
    ],
    "author": "kenrowland",
    "date": "January 21, 2020, 10:05pm",
    "body": "I am developing a beat for our cluster platform. I believe I am going to create a metricset and module. However, the documentation does not cover installation on the target system. Can someone please point me to where I can find more information? I also developed my own beat based on metricbeat. I see the installation packages that are generated in the build directory, however when I install it, the module is not installed. Installation of beats based on metric beat is also not covered in the documentation. Any help is appreciated. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "96b9cd14-7aa2-47ee-9a00-4f5d3eb07039",
    "url": "https://discuss.elastic.co/t/filebeat-7-4v-does-not-pick-up-the-index-templates-provided-in-filebeat-yml/214982",
    "title": "Filebeat 7.4v does not pick up the index templates provided in filebeat.yml",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Sherlock_H",
    "date": "January 14, 2020, 11:20am January 14, 2020, 4:55pm January 16, 2020, 5:24am January 21, 2020, 8:06pm",
    "body": "Hi Team, I have recently upgraded filebeat to 7.4v . I have setup.ilm.enabled: true in filebeat.yml but it does not seem to pick up the index template that i have provided in filebeat.yml, and instead it uses the default index template and index pattern. What configuration should I change or add to make filebeat pickup the templates and patterns that I have provided? Any help in this regard is appreciated.",
    "website_area": "discuss"
  },
  {
    "id": "1bf6b4a5-3c74-4de7-a666-7d9ddb6f8339",
    "url": "https://discuss.elastic.co/t/keeping-track-of-changes-to-host-configuration-object-in-metricbeat/215394",
    "title": "Keeping Track of changes to host configuration object in Metricbeat",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "January 17, 2020, 8:45am January 21, 2020, 1:58pm January 21, 2020, 7:35pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5de8830b-0bd2-4cf6-8ee9-c7adad21de47",
    "url": "https://discuss.elastic.co/t/auditbeat-not-is-connecting-to-elasticksearch/215891",
    "title": "Auditbeat not is connecting to Elasticksearch",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "Adrian_Martinez_Doca",
    "date": "January 21, 2020, 1:53pm January 21, 2020, 6:40pm January 21, 2020, 6:39pm",
    "body": "Hi to all, i am configuring auditbeat in some servers, i could configure in one server correctly and i can see info in Kibana, but when i configure the other servers i have the same message error. auditbeat: 2020-01-21T10:50:58.845-0300#011INFO#011template/load.go:88#011Template auditbeat-7.4.0 already exists and will not be overwritten. the entire log sequence is this Jan 21 10:50:10 server_test1 auditbeat: 2020-01-21T10:50:10.270-0300#011INFO#011[index-management]#011idxmgmt/std.go:289#011Loaded index template. Jan 21 10:50:38 server_test1 auditbeat: 2020-01-21T10:50:38.725-0300#011INFO#011[monitoring]#011log/log.go:145#011Non-zero metrics in the last 30s#011{\"monitoring\": {\"metrics\": {\"auditd\":{\"received_msgs\":1116},\"beat\":{\"cpu\":{\"system\":{\"ticks\":6660,\"time\":{\"ms\":341}},\"total\":{\"ticks\":29860,\"time\":{\"ms\":1602},\"value\":29860},\"user\":{\"ticks\":23200,\"time\":{\"ms\":1261}}},\"handles\":{\"limit\":{\"hard\":4096,\"soft\":1024},\"open\":115},\"info\":{\"ephemeral_id\":\"a2920d1c-325b-414d-b9cf-62b7f2d8fb0f\",\"uptime\":{\"ms\":452920}},\"memstats\":{\"gc_next\":97531600,\"memory_alloc\":79333552,\"memory_total\":715694776,\"rss\":8192},\"runtime\":{\"goroutines\":49}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"output\":{\"read\":{\"bytes\":1990},\"write\":{\"bytes\":924}},\"pipeline\":{\"clients\":3,\"events\":{\"active\":4119,\"retry\":32}}},\"system\":{\"load\":{\"1\":0.14,\"15\":0.22,\"5\":0.29,\"norm\":{\"1\":0.035,\"15\":0.055,\"5\":0.0725}}}}}} Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.812-0300#011ERROR#011pipeline/output.go:100#011Failed to connect to backoff(elasticsearch(http://10.24.35.50:9200)): Connection marked as failed because the onConnect callback failed: resource 'auditbeat-7.4.0' exists, but it is not an alias Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.813-0300#011INFO#011pipeline/output.go:93#011Attempting to reconnect to backoff(elasticsearch(http://10.24.35.50:9200)) with 14 reconnect attempt(s) Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.813-0300#011INFO#011[publisher]#011pipeline/retry.go:189#011retryer: send unwait-signal to consumer Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.813-0300#011INFO#011[publisher]#011pipeline/retry.go:191#011 done Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.813-0300#011INFO#011[publisher]#011pipeline/retry.go:166#011retryer: send wait signal to consumer Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.813-0300#011INFO#011[publisher]#011pipeline/retry.go:168#011 done Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.814-0300#011INFO#011elasticsearch/client.go:743#011Attempting to connect to Elasticsearch version 7.4.0 Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.842-0300#011INFO#011[index-management]#011idxmgmt/std.go:252#011Auto ILM enable success. Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.843-0300#011INFO#011[index-management.ilm]#011ilm/std.go:134#011do not generate ilm policy: exists=true, overwrite=false Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.843-0300#011INFO#011[index-management]#011idxmgmt/std.go:265#011ILM policy successfully loaded. Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.843-0300#011INFO#011[index-management]#011idxmgmt/std.go:394#011Set setup.template.name to '{auditbeat-7.4.0 {now/d}-000001}' as ILM is enabled. Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.843-0300#011INFO#011[index-management]#011idxmgmt/std.go:399#011Set setup.template.pattern to 'auditbeat-7.4.0-*' as ILM is enabled. Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.843-0300#011INFO#011[index-management]#011idxmgmt/std.go:433#011Set settings.index.lifecycle.rollover_alias in template to {auditbeat-7.4.0 {now/d}-000001} as ILM is enabled. Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.843-0300#011INFO#011[index-management]#011idxmgmt/std.go:437#011Set settings.index.lifecycle.name in template to {auditbeat-7.4.0 {\"policy\":{\"phases\":{\"hot\":{\"actions\":{\"rollover\":{\"max_age\":\"30d\",\"max_size\":\"50gb\"}}}}}}} as ILM is enabled. Jan 21 10:50:58 server_test1 auditbeat: 2020-01-21T10:50:58.845-0300#011INFO#011template/load.go:88#011Template auditbeat-7.4.0 already exists and will not be overwritten. Please can you help me to know why this is happened? Regards",
    "website_area": "discuss"
  },
  {
    "id": "113639a6-8323-4168-ae42-83ac8ca615cb",
    "url": "https://discuss.elastic.co/t/capture-all-raw-traffic/215937",
    "title": "Capture All RAW traffic?",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "Matt_Vasquez",
    "date": "January 21, 2020, 6:23pm",
    "body": "Is there a way for packetbeat to capture all raw packet data? I don't want to decode or specify protocols, just want all the raw packets as I would expect in wireshark..",
    "website_area": "discuss"
  },
  {
    "id": "96a1db96-0d35-42a9-87f6-f9c0926322bd",
    "url": "https://discuss.elastic.co/t/elastic-cloud-output-on-shared-non-trusted-environments/215855",
    "title": "Elastic Cloud output on shared / non-trusted environments",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "gerard1",
    "date": "January 21, 2020, 9:55am January 21, 2020, 5:05pm January 21, 2020, 5:51pm January 21, 2020, 6:02pm",
    "body": "Imagine the following scenario: You want to deploy Filebeat using Elastic Cloud in some servers that are managed by a 3rd party company. Is there any way to do that without using the Elastic Cloud global credentials? As those credentials are the same for accessing Kibana / ElasticSearch, that 3rd part company could use them to access all the other servers logs.",
    "website_area": "discuss"
  },
  {
    "id": "51798e4c-e013-4b63-a15d-f621a457b266",
    "url": "https://discuss.elastic.co/t/multiple-multiline-blocks-in-same-filebeat-yml-filebeats/215922",
    "title": "Multiple 'Multiline' blocks in same filebeat.yml - Filebeats",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "klang",
    "date": "January 21, 2020, 5:12pm January 21, 2020, 5:37pm January 21, 2020, 5:49pm",
    "body": "I'm looking to understand if I may have more than 1 multiline.pattern defined in a filebeat configuration of which these multiline configurations would be against the same log file. When I run something similar to the below, none of the patterns work but if comment out all but one of them, the single multiline.pattern works. type: log tags: [\"someAppTag\"] paths: - /var/log/someapp/app.log # Merge multiple authentication lines into their own doc multiline: pattern: '^\\*\\*\\* Received from ' negate: true match: after # Merge App restart lines into their own doc multiline: pattern: 'app received sigterm' negate: true match: after flush_pattern: 'app is now running!' The application log file I'm trying to document has several different unique messages to merge but I may only get one multiline.pattern working at a time. All of these entries are generic $timestamp.message lines spread across numerous lines and unreadable in Kibana without merging their multiline content into a single document. Any thoughts?",
    "website_area": "discuss"
  },
  {
    "id": "9fb00d22-9187-4d8e-8e81-813084048137",
    "url": "https://discuss.elastic.co/t/is-there-a-way-to-specify-the-nic-for-output/215913",
    "title": "Is there a way to specify the NIC for output?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "austin0918",
    "date": "January 21, 2020, 4:00pm January 21, 2020, 4:23pm January 21, 2020, 4:32pm",
    "body": "Hi There, I have two NICs on a Linux server. Is there a way to specify which NIC to use by Filebeat for logstash output?",
    "website_area": "discuss"
  },
  {
    "id": "29d7274b-9b86-4dda-9c9d-39530f33450f",
    "url": "https://discuss.elastic.co/t/mapping-definition-for-name-has-unsupported-parameters-path-docker-container-image/215904",
    "title": "Mapping definition for [name] has unsupported parameters: [path : docker.container.image]",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "realyn_elastic",
    "date": "January 21, 2020, 3:22pm",
    "body": "Hello team, I am facing the following issue whenever I try to roll over the winlogbeat index. I checked my document, and couldn't find any field related to docker at all. I am using default template mapping that is being auto-created by winlogbeat. { \"reason\": \"Failed to parse mapping [_doc]: Mapping definition for [name] has unsupported parameters: [path : docker.container.image]\", \"caused_by\": { \"type\": \"mapper_parsing_exception\", \"reason\": \"Mapping definition for [name] has unsupported parameters: [path : docker.container.image]\" } Winlogbeat Version: 7.4.1 Elasticsearch Version: 7.5.1 Thanks",
    "website_area": "discuss"
  },
  {
    "id": "a6f996cd-e404-4aee-8114-4edd31b23a18",
    "url": "https://discuss.elastic.co/t/generating-metricbeat-index-pattern/214682",
    "title": "Generating metricbeat index pattern",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "mladen",
    "date": "January 10, 2020, 10:53pm January 11, 2020, 8:32pm January 21, 2020, 1:59pm",
    "body": "Hello, following beats developer guide I mange to install go. My goal is to change default metricbeat index pattern. I clone git repo, run make command & make update. Command make update created fields.yml file. I add my custom field to fields.yml and run again make update command. After this action I released that fields.yml file is rewritten. Developer guide mention if you want to change index pattern just run make update command. So what I am doing wrong? BR, Mladen",
    "website_area": "discuss"
  },
  {
    "id": "70486ce3-c2f5-472f-b409-83c12035ec2b",
    "url": "https://discuss.elastic.co/t/rabbitmq-redeliveries-metrics/215527",
    "title": "RabbitMQ redeliveries metrics?",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "IamaBase",
    "date": "January 17, 2020, 9:50pm January 21, 2020, 12:55pm",
    "body": "I notice that these used to be provided in the exchange metricset: https://github.com/elastic/beats/pull/6607/files/34b28f231537933ee1e0296393f6e726bc6d060d But it appears they are not included as of 7.5.1. Is there a way to implement these?",
    "website_area": "discuss"
  },
  {
    "id": "3efd3432-f1b3-4f68-98cb-46c58a1f34f5",
    "url": "https://discuss.elastic.co/t/metricbeat-http-module-mapping-error-cant-merge-a-non-object-mapping-http-json-namespace-classes-with-an-object-mapping-http-json-namespace-classes/215322",
    "title": "Metricbeat Http Module mapping error (Can't merge a non object mapping [http.json_namespace.classes] with an object mapping [http.json_namespace.classes] )",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "fnkbz",
    "date": "January 16, 2020, 1:53pm January 20, 2020, 7:57am January 21, 2020, 12:53pm",
    "body": "I am running elasticsearch 7.5, kibana 7.5 and Metricbeat 7.5. I have enabled metricbeat's http module to get data from a spring boot app. Here is my metricbeat.yml http module snippet #--------------------------------- HTTP Module --------------------------------- - module: http metricsets: [\"json\"] period: 60s hosts: [\"localhost:9998\"] namespace: \"json_namespace\" path: \"/metrics\" method: \"GET\" I get this error when it tries to fetch data from the http module [2020-01-16T13:28:11,059][DEBUG][o.e.a.b.TransportShardBulkAction] [elasticstage1] [metricbeat-2020.01.16-000001][0] failed to execute bulk item (create) index {[metricbeat][_doc][wQiKrm8BkKjJO7a7Nkwx], source[n/a, actual length: [5.9kb], max length: 2kb]} java.lang.IllegalArgumentException: Can't merge a non object mapping [http.json_namespace.classes] with an object mapping [http.json_namespace.classes] at org.elasticsearch.index.mapper.ObjectMapper.merge(ObjectMapper.java:439) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.mapper.ObjectMapper.merge(ObjectMapper.java:47) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.mapper.DocumentParser.createDynamicUpdate(DocumentParser.java:237) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:83) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:267) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:776) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:753) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShard.applyIndexOperationOnPrimary(IndexShard.java:725) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.bulk.TransportShardBulkAction.executeBulkItemRequest(TransportShardBulkAction.java:258) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.bulk.TransportShardBulkAction$2.doRun(TransportShardBulkAction.java:161) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnPrimary(TransportShardBulkAction.java:193) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:118) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:79) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryShardReference.perform(TransportReplicationAction.java:917) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.ReplicationOperation.execute(ReplicationOperation.java:108) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.runWithPrimaryShardReference(TransportReplicationAction.java:394) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.lambda$doRun$0(TransportReplicationAction.java:316) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShard.lambda$wrapPrimaryOperationPermitListener$21(IndexShard.java:2752) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.ActionListener$3.onResponse(ActionListener.java:113) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShardOperationPermits.acquire(IndexShardOperationPermits.java:285) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShardOperationPermits.acquire(IndexShardOperationPermits.java:237) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShard.acquirePrimaryOperationPermit(IndexShard.java:2726) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction.acquirePrimaryOperationPermit(TransportReplicationAction.java:858) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.doRun(TransportReplicationAction.java:312) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction.handlePrimaryRequest(TransportReplicationAction.java:275) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler$1.doRun(SecurityServerTransportInterceptor.java:257) [x-pack-security-7.5.0.jar:7.5.0] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:315) [x-pack-security-7.5.0.jar:7.5.0] at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:63) [elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.transport.TransportService$7.doRun(TransportService.java:752) [elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:773) [elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.5.0.jar:7.5.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_222] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_222] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_222] [2020-01-16T13:29:11,062][DEBUG][o.e.a.b.TransportShardBulkAction] [elasticstage1] [metricbeat-2020.01.16-000001][0] failed to execute bulk item (create) index {[metricbeat][_doc][9wiLrm8BkKjJO7a7IF6R], source[n/a, actual length: [5.9kb], max length: 2kb]} java.lang.IllegalArgumentException: mapper [http.json_namespace.classes] of different type, current_type [long], merged_type [ObjectMapper] at org.elasticsearch.index.mapper.FieldMapper.doMerge(FieldMapper.java:352) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.mapper.NumberFieldMapper.doMerge(NumberFieldMapper.java:1081) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.mapper.FieldMapper.merge(FieldMapper.java:339) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.mapper.FieldMapper.merge(FieldMapper.java:55) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.mapper.DocumentParser.createDynamicUpdate(DocumentParser.java:237) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:83) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:267) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:776) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:753) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShard.applyIndexOperationOnPrimary(IndexShard.java:725) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.bulk.TransportShardBulkAction.executeBulkItemRequest(TransportShardBulkAction.java:258) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.bulk.TransportShardBulkAction$2.doRun(TransportShardBulkAction.java:161) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnPrimary(TransportShardBulkAction.java:193) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:118) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:79) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryShardReference.perform(TransportReplicationAction.java:917) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.ReplicationOperation.execute(ReplicationOperation.java:108) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.runWithPrimaryShardReference(TransportReplicationAction.java:394) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.lambda$doRun$0(TransportReplicationAction.java:316) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShard.lambda$wrapPrimaryOperationPermitListener$21(IndexShard.java:2752) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.ActionListener$3.onResponse(ActionListener.java:113) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShardOperationPermits.acquire(IndexShardOperationPermits.java:285) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShardOperationPermits.acquire(IndexShardOperationPermits.java:237) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.index.shard.IndexShard.acquirePrimaryOperationPermit(IndexShard.java:2726) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction.acquirePrimaryOperationPermit(TransportReplicationAction.java:858) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.doRun(TransportReplicationAction.java:312) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.action.support.replication.TransportReplicationAction.handlePrimaryRequest(TransportReplicationAction.java:275) ~[elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler$1.doRun(SecurityServerTransportInterceptor.java:257) [x-pack-security-7.5.0.jar:7.5.0] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:315) [x-pack-security-7.5.0.jar:7.5.0] at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:63) [elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.transport.TransportService$7.doRun(TransportService.java:752) [elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:773) [elasticsearch-7.5.0.jar:7.5.0] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.5.0.jar:7.5.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_222] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_222] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_222] From the above stacktrace it seems like it tries to create the same fields with a different type. I didnt mess with anything else like fields.yml. Any help?",
    "website_area": "discuss"
  },
  {
    "id": "e5ddfd10-3d48-4bac-84d2-506e406bc78a",
    "url": "https://discuss.elastic.co/t/functionbeat-kinesis-filter-pattern-and-elastic-id/215867",
    "title": "Functionbeat kinesis filter_pattern and elastic id",
    "category": [
      "Beats",
      "Functionbeat"
    ],
    "author": "nmesmeric",
    "date": "January 21, 2020, 11:10am",
    "body": "Hey, I have set up functionbeat to send events from Amazon Kinesis to our Elasticsearch cluster. I have two issues that i wondered if anyone could assist with. I have added a string as a filter_pattern, however it doesn't seem to work and I still just receive all events unfiltered. How does it work i just want to receive events where \"eventid\" = \"1.21.1\" for instance. How does the filter pattern work? Also is it possible to specify using functionbeat which field to use as the elastic id field? Thanks for your help with this Cheers, Felix",
    "website_area": "discuss"
  },
  {
    "id": "88177d16-d692-41a5-b232-c28340bdfbc6",
    "url": "https://discuss.elastic.co/t/time-zone-in-cef-fb-module/215188",
    "title": "Time Zone in CEF FB Module",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "culprit",
    "date": "January 15, 2020, 6:33pm January 16, 2020, 3:42pm January 16, 2020, 5:34pm January 16, 2020, 5:42pm January 16, 2020, 5:56pm January 16, 2020, 6:02pm January 16, 2020, 6:06pm January 16, 2020, 9:35pm January 16, 2020, 9:30pm January 16, 2020, 9:54pm January 17, 2020, 6:25am January 17, 2020, 4:21pm January 17, 2020, 10:35pm January 21, 2020, 9:41am",
    "body": "I am new to all this, so bear with me if i use the wrong terminology. Working on getting Fortigate logs into ES. Currently the path the events take are: Fortgate -> FortiAnalyzer (forwarded in CEF format) -> FileBeat(CEF Module) -> Logstash -> ES Data is flowing fine, except the time in the events forwarded from the FortiAnalyzer are in EST (UTC -5), and somewhere during ingestion, it gets tagged with a UTC timestamp. This causes the CEF events in Kibana to look like they happened 5 hours earlier. Can I edit a pipeline json file somewhere so the conversion to UTC time for the timestamp does not occur, or is there another way of accomplishing the task? NOTE: Everything here is based on ES v7.5.1 basic license with everything running on a single node.",
    "website_area": "discuss"
  },
  {
    "id": "43c6d955-6188-4ee3-add5-8b6924e89705",
    "url": "https://discuss.elastic.co/t/limitnofile-parameter-65k-for-elasticsearch-for-rhel/215837",
    "title": "LimitNOFILE Parameter 65K for ElasticSearch . for RHEL",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "kprasann",
    "date": "January 21, 2020, 8:32am",
    "body": "Hi, Is it ok to set 65k as LimitNOFile in RHEL as root user? If there are performance issues with 65K File handles, how can that be mitigated? Can the file handle processes be killed? Any monitoring tool that can be used to improve performance? Thanks, P",
    "website_area": "discuss"
  },
  {
    "id": "637739bf-6c00-4eb6-b1f6-1feb5ee86962",
    "url": "https://discuss.elastic.co/t/using-filebeat-pibeat-easybeat-w-suricata-on-raspberry-pi-3-for-bitnami-elk/215819",
    "title": "Using Filebeat/Pibeat/Easybeat w/ Suricata on Raspberry Pi 3 for Bitnami ELK",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "gorgymorg",
    "date": "January 21, 2020, 4:44am",
    "body": "Hi, I have been reading over forum posts and readmes for trying to use filebeat and offshoots to integrate logs from Suricata running on a Raspberry Pi and ship them to a Bitnami ELK VM I have running on my computer - I have looked at the following options: GitHub dam90/pibeats Contribute to dam90/pibeats development by creating an account on GitHub. GitHub josh-thurston/easyBEATS Beats for Raspberry Pi / ARM. Contribute to josh-thurston/easyBEATS development by creating an account on GitHub. But am having difficulty knowing where to begin and how to get them to work with each other. When I try to edit filebeat.yml and filebeat.reference.yml, they do not have the sections described here - https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-configuration.html I cannot find the inputs like in the example: filebeat.inputs: type: log enabled: true paths: /var/log/*.log #- c:\\programdata\\elasticsearch\\logs* When I ran ./easyBEATS-7.3.2_arm I got the following which seems to indicate it didn't install right: System Update... Tue 21 Jan 01:32:51 GMT 2020 -> System Update Complete Creating Go Workspace directories... -> making /root/go... -> Go Workspace directories created Checking for python-pip git... -> python-pip git is installed Checking for virtualenv... -> virtualenv is installed Checking for Make... -> Make is installed Checking for GCC... -> GCC is installed Checking for Go... -> Go is installed Getting Beats files from Elastic repo on github... package github.com/elastic/beats: cannot download, $GOPATH not set. For more details see: go help gopath ./easyBEATS-7.3.2_arm: line 75: cd: beats: No such file or directory Checking out Beats... fatal: Not a git repository (or any of the parent directories): .git fatal: Not a git repository (or any of the parent directories): .git Temporarily enabling swap space Setting up swapspace version 1, size = 2 GiB (2147479552 bytes) no label, UUID=c468269a-c38c-4894-8fe3-5e41d6ea923f /swapfile swap swap defaults 0 0 NAME TYPE SIZE USED PRIO /var/swap file 100M 100M -2 /swapfile file 2G 0B -3 ---- Filebeat ---- ./easyBEATS-7.3.2_arm: line 87: cd: /root/go/src/github.com/elastic/beats/filebeat: No such file or directory Creating Filebeat... can't load package: package .: no buildable Go source files in /root/go/src/github.com/elastic make: *** No targets specified and no makefile found. Stop. Filebeat created Creating Filebeat directories... -> making /usr/share/filebeat... -> making /etc/filebeat... -> making /var/log/filebeat... -> making /var/lib/filebeat... Moving Filebeat... mv: cannot stat '/root/go/src/github.com/elastic/beats/filebeat/filebeat': No such file or directory mv: cannot stat '/root/go/src/github.com/elastic/beats/filebeat/module': No such file or directory cp: cannot stat '/root/go/src/github.com/elastic/beats/filebeat/filebeat.reference.yml': No such file or directory mv: cannot stat '/root/go/src/github.com/elastic/beats/filebeat/modules.d/': No such file or directory cp: cannot stat '/root/go/src/github.com/elastic/beats/filebeat/filebeat.yml': No such file or directory cp: cannot stat '/root/beats_arm/filebeat_files/fields.yml': No such file or directory cp: cannot stat '/root/beats_arm/filebeat_files/LICENSE.txt': No such file or directory cp: cannot stat '/root/beats_arm/filebeat_files/NOTICE.txt': No such file or directory cp: cannot stat '/root/beats_arm/filebeat_files/filebeat.service': No such file or directory Failed to enable unit: File filebeat.service: No such file or directory ---- Metricbeat ---- ./easyBEATS-7.3.2_arm: line 148: cd: /root/go/src/github.com/elastic/beats/metricbeat: No such file or directory Creating Metricbeat... can't load package: package .: no buildable Go source files in /root/go/src/github.com/elastic make: *** No targets specified and no makefile found. Stop. Metricbeat created Creating Metricbeat directories... -> making /usr/share/metricbeat... -> making /etc/metricbeat... -> making /var/log/metricbeat... -> making /var/lib/metricbeat... mv: cannot stat '/root/go/src/github.com/elastic/beats/metricbeat/metricbeat': No such file or directory mv: cannot stat '/root/go/src/github.com/elastic/beats/metricbeat/module': No such file or directory mv: cannot stat '/root/go/src/github.com/elastic/beats/metricbeat/modules.d/': No such file or directory cp: cannot stat '/root/go/src/github.com/elastic/beats/metricbeat/metricbeat.yml': No such file or directory cp: cannot stat '/root/go/src/github.com/elastic/beats/metricbeat/metricbeat.reference.yml': No such file or directory cp: cannot stat '/root/beats_arm/metricbeat_files/fields.yml': No such file or directory cp: cannot stat '/root/beats_arm/metricbeat_files/LICENSE.txt': No such file or directory cp: cannot stat '/root/beats_arm/metricbeat_files/NOTICE.txt': No such file or directory cp: cannot stat '/root/beats_arm/metricbeat_files/metricbeat.service': No such file or directory Failed to enable unit: File metricbeat.service: No such file or directory ---- Packetbeat ---- Reading package lists... Done Building dependency tree Reading state information... Done libpcap-dev is already the newest version (1.8.1-3). 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. ./easyBEATS-7.3.2_arm: line 209: cd: /root/go/src/github.com/elastic/beats/packetbeat: No such file or directory Creating Packetbeat... can't load package: package .: no buildable Go source files in /root/go/src/github.com/elastic make: *** No targets specified and no makefile found. Stop. Packetbeat created Creating Packetbeat directories... -> making /usr/share/packetbeat... -> making /etc/packetbeat... -> making /var/log/packetbeat... -> making /var/lib/packetbeat... mv: cannot stat '/root/go/src/github.com/elastic/beats/packetbeat/packetbeat': No such file or directory cp: cannot stat '/root/go/src/github.com/elastic/beats/packetbeat/_meta/kibana': No such file or directory cp: cannot stat '/root/go/src/github.com/elastic/beats/packetbeat/packetbeat.reference.yml': No such file or directory cp: cannot stat '/root/go/src/github.com/elastic/beats/packetbeat/packetbeat.yml': No such file or directory cp: cannot stat '/root/beats_arm/packetbeat_files/fields.yml': No such file or directory cp: cannot stat '/root/beats_arm/packetbeat_files/LICENSE.txt': No such file or directory cp: cannot stat '/root/beats_arm/packetbeat_files/NOTICE.txt': No such file or directory cp: cannot stat '/root/beats_arm/packetbeat_files/packetbeat.service': No such file or directory Failed to enable unit: File packetbeat.service: No such file or directory ---- Auditbeat ---- ./easyBEATS-7.3.2_arm: line 268: cd: /root/go/src/github.com/elastic/beats/auditbeat: No such file or directory Creating Auditbeat... can't load package: package .: no buildable Go source files in /root/go/src/github.com/elastic make: *** No targets specified and no makefile found. Stop. Auditbeat created Creating Auditbeat directories... -> making /usr/share/auditbeat... -> making /etc/auditbeat... I therefore cannot run Filebeat, and I can't tell which ports Bitnami is using for the various components of the ELK stack. If anyone has suggestions on how to begin with using Easybeat/Pibeat for configuring the setup I described I would be very grateful. I would try SELKS but I am afraid a Raspberry Pi 3 wouldn't have the RAM or storage for it and haven't seen much written about running it on a Pi - https://github.com/StamusNetworks/SELKS/wiki/First-time-setup",
    "website_area": "discuss"
  },
  {
    "id": "bb372b52-b326-40a9-b202-910ae8dff9e1",
    "url": "https://discuss.elastic.co/t/using-custom-pipeline-for-existing-filebeat-module/213270",
    "title": "Using custom pipeline for existing filebeat module",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 28, 2019, 11:02pm January 21, 2020, 12:31am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a5c1308b-6ec4-45eb-a863-7c4b0e00f305",
    "url": "https://discuss.elastic.co/t/filebeat-cisco-module-listening-on-other-than-localhost/212944",
    "title": "Filebeat + cisco module - listening on other than localhost",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 26, 2019, 2:40pm January 20, 2020, 9:50pm January 20, 2020, 10:18pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "691b3743-a443-4bda-bee5-dfca6888f1d5",
    "url": "https://discuss.elastic.co/t/sysmon-dns-query-missing-status-codes/213396",
    "title": "Sysmon DNS Query missing Status codes",
    "category": [
      "Beats"
    ],
    "author": "Nicholas_Penning",
    "date": "December 31, 2019, 12:06am January 20, 2020, 2:49pm January 20, 2020, 6:22pm January 20, 2020, 6:44pm",
    "body": "Hello, We are ingesting some SysMon (Version 10.41) DNS logs via WinLogbeat 7.4.0 and have found that some status codes are not either getting translated with the sysmon.js or are not in the list. I will focus on the event log that is in the list. The status is 9560 which should be displaying as DNS_ERROR_INVALID_NAME_CHAR but instead the logs in Kibana are showing 9560. From Microsoft (https://docs.microsoft.com/en-us/windows/win32/debug/system-error-codes--9000-11999-) DNS_ERROR_INVALID_NAME_CHAR 9560 (0x2558) DNS name contains an invalid character. You can test this by trying to ping www.google,com You see that the status code is in fact 9560 in the Windows event log but for some reason the sysmon.js doesn't clean up like most of the other DNS queries. Any ideas? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "95b4779f-51d6-45aa-91ed-3c1d87451321",
    "url": "https://discuss.elastic.co/t/sysmon-dns-logs-dns-answers-types/213471",
    "title": "SysMon DNS Logs - dns.answers - Types",
    "category": [
      "Beats"
    ],
    "author": "Nicholas_Penning",
    "date": "December 31, 2019, 10:49pm January 20, 2020, 2:46pm January 20, 2020, 6:17pm",
    "body": "Hello, We are currently leveraging SysMon DNS logs and would like to have the capability to search/aggregrate on the types from the dns.answers that WinLogBeat ships to Elastic. Currently, the dns.answers field contains: { \"data\": \"elasticsearch.trainingrocket.com\", \"type\": \"CNAME\" }, { \"data\": \"d1bzcgvkzhwrpe.cloudfront.net\", \"type\": \"CNAME\" }, { \"data\": \"13.227.45.18\", \"type\": \"A\" }, { \"data\": \"13.227.45.48\", \"type\": \"A\" }, { \"data\": \"13.227.45.74\", \"type\": \"A\" }, { \"data\": \"13.227.45.36\", \"type\": \"A\" } It would be great if this was parsed similarly to the dns.resolved_ip field to show something like: dns.resolved.type or dns.answers.type to contain: CNAME, A It doesn't seem that this is the case as noted on github (https://github.com/elastic/beats/pull/12960): winlog.event_data.QueryResults -> dns.answers.data , dns.answers.type Is this a bug or to be expected?",
    "website_area": "discuss"
  },
  {
    "id": "d8aee55d-3f22-44a3-9c9b-aa4b304a0fa9",
    "url": "https://discuss.elastic.co/t/help-with-processors-in-filebeat-modules/215711",
    "title": "Help with Processors in filebeat modules",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Jose_E",
    "date": "January 20, 2020, 3:05pm January 20, 2020, 4:14pm January 20, 2020, 5:26pm January 20, 2020, 5:51pm",
    "body": "Hi, I'm having a lot of issues trying to figure out how to filter out log lines before they are indexed. After failing using \"exclude_lines\" for a couple of times, I quickly moved to the use of processors. The main constraint I have is that I am using Saltstack to apply the configuration and therefore I'm trying to use as few nested clauses as possible. Here's the code I'm using: /etc/filebeat/modules.d/apache.yml - module: apache access: enabled: True processors: - drop_event.when.and: - equals.http.response.code: 302 - equals.source.ip: \"172.21.205.252\" - equals.http.request.method: \"HEAD\" /etc/filebeat/modules.d/system.yml - module: system syslog: enabled: True processors: - drop_event.when.contains.message: lxc-container-default-with-nfs auth: enabled: True These are the messages I am trying to get rid of: [apache][access] 172.21.205.252 - \"HEAD / HTTP/1.0\" 302 undefined [7580482.483661] audit: type=1400 audit(1579509159.591:12505): apparmor=\"DENIED\" operation=\"mount\" info=\"failed flags match\" error=-13 profile=\"lxc-container-default-with-nfs\" name=\"/\" pid=7990 comm=\"(imedated)\" flags=\"rw, rslave I believe the issue might be in taking too much advantage of the YAML format and joining all commands, but I need some reassurance and hopefully some other ideas. Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "e8834906-0b06-48fd-a219-0d434e25277e",
    "url": "https://discuss.elastic.co/t/syslog-filebeat-input-how-to-get-sender-ip-address/214809",
    "title": "Syslog filebeat input, how to get sender IP address?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "emilie",
    "date": "January 13, 2020, 11:12am January 13, 2020, 7:38pm January 14, 2020, 10:06am January 14, 2020, 7:00pm January 17, 2020, 2:18pm January 15, 2020, 4:07pm January 15, 2020, 4:50pm January 20, 2020, 5:03pm",
    "body": "Hello, I'm using filebeat to send syslog input to a kafka server (it works wonderfully, thank you). But I'm wondering: how can I add the IP from the machine that is sending its syslog input in my logs? (I'm aware of processors like add_host_metada but I need the IP from the machine filebeat is receiving from)",
    "website_area": "discuss"
  },
  {
    "id": "710503a3-7177-4655-bb10-a7d1e09a3d7b",
    "url": "https://discuss.elastic.co/t/not-able-to-filter-from-filebeat/215770",
    "title": "Not able to filter from filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "surya1",
    "date": "January 20, 2020, 3:49pm January 20, 2020, 4:45pm",
    "body": "Hi , I have a log a file that has 10-12 lines per paragraph and this same pattern repeats. From this log file I need to select only 4-5 lines . Tried to use include lines but in elastic search i am not getting these as a single filtered message. instead it is coming as multiple messages. Eg: This is my line#1 This is my line#2 Yes line number3 .. .. This is my lastline. Here I have to get only line number 3 and the last line. Added include lines with corresponding regular expressions and it is working. But the issue is elastic search shows \"Yes line number3\" and \"This is my lastline.\" as two separate lines/indexes. How can I get it as a single message like: \"Yes line number3 This is my lastline.\"",
    "website_area": "discuss"
  },
  {
    "id": "73d9a577-05e4-47aa-91ff-8d9d0114568f",
    "url": "https://discuss.elastic.co/t/error-reading-netapp-evtx-file-with-winlogbeat/215780",
    "title": "Error Reading Netapp evtx file with winlogbeat",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "Anabella_Cristaldi",
    "date": "January 20, 2020, 4:37pm",
    "body": "Hi, I'm trying to read evtx files from Netapp logs using the following https://www.elastic.co/guide/en/beats/winlogbeat/current/reading-from-evtx.html I'm able to read the events but when parsing the version field I get an error because the version field is not integer. It appears to be related to https://github.com/elastic/beats/pull/10629, but it is not clear to me if it was fixed or not Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "95c24c78-f0a9-4385-a65b-83225df4e07c",
    "url": "https://discuss.elastic.co/t/unable-to-insert-log-data-using-filebeat/215458",
    "title": "Unable to insert log data using filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "NileshG",
    "date": "January 17, 2020, 1:00pm January 17, 2020, 7:28pm January 20, 2020, 11:50am",
    "body": "Hi, I am trying to insert log data using filebeat in an ElascticSearch. The index is created but the data differ than the actual one. My Log is 2020-01-17T17:13:43.218+0530 INFO crawler/crawler.go:106 Loading and starting Inputs completed. Enabled inputs: 1 2020-01-17T17:13:43.218+0530 INFO cfgfile/reload.go:171 Config reloader started 2020-01-17T17:13:43.218+0530 INFO cfgfile/reload.go:226 Loading of config files completed. 2020-01-17T17:13:43.229+0530 INFO log/harvester.go:251 Harvester started for file: C:\\Users\\Nilesh.Gunjkar\\Desktop\\ElasticSearch\\LogsData\\Book3.xlsx 2020-01-17T17:13:46.182+0530 INFO add_cloud_metadata/add_cloud_metadata.go:89 add_cloud_metadata: hosting provider type not detected. My Config File is ###################### Filebeat Configuration Example ######################### This file is an example configuration file highlighting only the most common options. The filebeat.reference.yml file from the same directory contains all the supported options with more comments. You can use it as a reference. You can find the full configuration reference here: https://www.elastic.co/guide/en/beats/filebeat/index.html For more available modules and options, please see the filebeat.reference.yml sample configuration file. #=========================== Filebeat inputs ============================= filebeat.inputs: Each - is an input. Most options can be set at the input level, so you can use different inputs for various configurations. Below are the input specific configurations. type: log Change to true to enable this input configuration. enabled: true Paths that should be crawled and fetched. Glob based paths. paths: D:\\Desktop\\ElasticSearch\\LogsData* #- c:\\programdata\\elasticsearch\\logs* Exclude lines. A list of regular expressions to match. It drops the lines that are matching any regular expression from the list. #exclude_lines: ['^DBG'] Include lines. A list of regular expressions to match. It exports the lines that are matching any regular expression from the list. #include_lines: ['^ERR', '^WARN'] Exclude files. A list of regular expressions to match. Filebeat drops the files that are matching any regular expression from the list. By default, no files are dropped. #exclude_files: ['.gz$'] #================================ Outputs ===================================== Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: Array of hosts to connect to. hosts: [\"elk.dev.echdev1.com\"] Optional protocol and basic auth credentials. #protocol: \"http\" #username: \"\" #password: \"\"",
    "website_area": "discuss"
  },
  {
    "id": "f6366cf2-f19c-4a76-b267-b141a4c2e06c",
    "url": "https://discuss.elastic.co/t/filebeat-configuration-in-main-config-or-seperate/215724",
    "title": "Filebeat configuration in main config or seperate?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "hlamsc",
    "date": "January 20, 2020, 11:49am",
    "body": "Hello there, we want to use the \"close_renamed: true\" option for our filebeat. Im currently asking myself if I could set it \"globally\" in the filebeat.yml (so every config in conf.d is affected) or do I have to set it seperatly in every config file in conf.d? Thanks and best regards",
    "website_area": "discuss"
  },
  {
    "id": "77c19c14-1ec6-48a3-8659-9e2047fef9a6",
    "url": "https://discuss.elastic.co/t/need-explanations-with-filebeats-ilm/214629",
    "title": "Need explanations with filebeats & ILM",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 10, 2020, 4:02pm January 16, 2020, 11:12pm January 20, 2020, 10:55am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ef8b5d3b-0aff-434f-9965-150b71026e0c",
    "url": "https://discuss.elastic.co/t/file-beat-stops-time-to-time-in-rhel-environment/213855",
    "title": "File beat stops time to time in RHEL environment",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "ruranga",
    "date": "January 6, 2020, 9:06am January 7, 2020, 6:29am January 20, 2020, 10:31am",
    "body": "Hi, I have installed filbeat in RHEL syslog server and sending the logs to ELK stack at another server, By the way filebeat in the syslog server getting stopped time to time. When I check the logs following found 2020-01-06T07:33:11.233Z INFO beater/filebeat.go:443 Stopping filebeat 2020-01-06T07:33:11.292Z INFO crawler/crawler.go:139 Stopping Crawler 2020-01-06T07:33:11.430Z INFO crawler/crawler.go:149 Stopping 1 inputs 2020-01-06T07:33:11.430Z INFO input/input.go:149 input ticker stopped 2020-01-06T07:33:11.430Z INFO cfgfile/reload.go:229 Dynamic config reloader stopped 2020-01-06T07:33:11.430Z INFO input/input.go:167 Stopping Input: 6167974775168247055 2020-01-06T07:33:11.431Z INFO log/harvester.go:272 Reader was closed: /logs/DC1-Proxy-GW3/dc1-proxy-gw3_proxy.log. Closing. 2020-01-06T07:33:11.431Z INFO crawler/crawler.go:165 Crawler stopped 2020-01-06T07:33:11.431Z INFO registrar/registrar.go:367 Stopping Registrar 2020-01-06T07:33:11.431Z INFO registrar/registrar.go:293 Ending Registrar 2020-01-06T07:33:11.440Z INFO [monitoring] log/log.go:153 Total non-zero metrics {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":66700,\"time\":{\"ms\":66703}},\"total\":{\"ticks\":496080,\"time\":{\"ms\":496085},\"value\":496080},\"user\":{\"ticks\":429380,\"time\":{\"ms\":429382}}},\"handles\":{\"limit\":{\"hard\":4096,\"soft\":1024},\"open\":7},\"info\":{\"ephemeral_id\":\"6246d6ce-83f1-4636-836e-4946c73d8f84\",\"uptime\":{\"ms\":8381436}},\"memstats\":{\"gc_next\":16522000,\"memory_alloc\":8437704,\"memory_total\":34548886184,\"rss\":35966976},\"runtime\":{\"goroutines\":14}},\"filebeat\":{\"events\":{\"active\":386,\"added\":4189851,\"done\":4189465},\"harvester\":{\"closed\":3,\"open_files\":0,\"running\":0,\"started\":3},\"input\":{\"log\":{\"files\":{\"truncated\":2}}}},\"libbeat\":{\"config\":{\"module\":{\"running\":0},\"reloads\":1},\"output\":{\"events\":{\"acked\":4189459,\"batches\":8489,\"total\":4189459},\"read\":{\"bytes\":50934},\"type\":\"logstash\",\"write\":{\"bytes\":259049178}},\"pipeline\":{\"clients\":0,\"events\":{\"active\":386,\"filtered\":6,\"published\":4189845,\"retry\":2048,\"total\":4189851},\"queue\":{\"acked\":4189459}}},\"registrar\":{\"states\":{\"current\":2,\"update\":4189465},\"writes\":{\"success\":8484,\"total\":8484}},\"system\":{\"cpu\":{\"cores\":8},\"load\":{\"1\":1.96,\"15\":1.15,\"5\":1.36,\"norm\":{\"1\":0.245,\"15\":0.1438,\"5\":0.17}}}}}} 2020-01-06T07:33:11.441Z INFO [monitoring] log/log.go:154 Uptime: 2h19m41.441602094s 2020-01-06T07:33:11.441Z INFO [monitoring] log/log.go:131 Stopping metrics logging. 2020-01-06T07:33:11.441Z INFO instance/beat.go:435 filebeat stopped. Can someone pls help! Thanks in advance! KInd regards",
    "website_area": "discuss"
  },
  {
    "id": "e32d858e-4897-4d72-97a2-dd1d6ad62561",
    "url": "https://discuss.elastic.co/t/filebeat-docker-installation-connection-refused/215607",
    "title": "Filebeat docker installation connection refused",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "NZHawk",
    "date": "January 19, 2020, 7:33am January 19, 2020, 3:53pm",
    "body": "So I've just embarked on the windy ELK road and as a training ground I thought I'd look into the ease of Docker deployments before I get under the hood and drill down on config specifics. I've installed an ELK docker stack and it works fine. The trouble I'm having is when I come to install filebeats I get a connection refused. I'm clearly needing to pass the username elastic and default password \"changeme\" to the docker run setup but for the life of me I can't seem to get this parsed: root@elk:/home/elk# docker run docker.elastic.co/beats/filebeat:7.5.1 setup -E setup.kibana.host=0.0.0.0:5601 -E output.elasticsearch.hosts=[\"0.0.0.0:9200\"] username=\"elastic\" password=\"changeme\" -v Exiting: Couldn't connect to any of the configured Elasticsearch hosts. Errors: [Error connection to Elasticsearch http://0.0.0.0:9200: Get http://0.0.0.0:9200: dial tcp 0.0.0.0:9200: connect: connection refused] If anyone could give me a pointer I'd be most appreciative. I'm sure the answer is staring me in the face but I've gone a little blind.",
    "website_area": "discuss"
  },
  {
    "id": "0d71d198-0285-40f8-b033-77777240c2f8",
    "url": "https://discuss.elastic.co/t/need-help-with-filebeat-settings-to-separate-logs-in-logstash-for-grok-parser/215157",
    "title": "Need help with filebeat settings to separate logs in logstash for grok parser",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "T4iga",
    "date": "January 15, 2020, 2:41pm January 15, 2020, 8:25pm January 15, 2020, 11:32pm January 16, 2020, 8:59am January 16, 2020, 9:06am January 19, 2020, 2:36am",
    "body": "I have an environment with a log-server and a second server runnig an ELK stack. The log-server writes logs to multiple directories on disk. I use filebeat to send those logs to the ELK server. I wrote a grok parsers to be used to parse the logs from one specific directory on the log-server but not the others. I want to apply the parser to the approriate logs only. In the logstash config i am planning to use if statements to choose how to parse the logs like the following: filter { if [type] =~ \"syslog\" { grok { match => { \"message\" => \" ... if [type] =~ \"notSyslog\" { grok { match => { \"message\" => \" ... To set the type field I use a filebeat config with different which adds different entries in the \"type\" field depending on the folder the log is coming from. It looks as follows: filebeat.inputs: - type: log enabled: true paths: - path/*.log fields: type: myType1 fields_under_root: true - type: log enabled: true paths: - path2/*.log fields: type: myType2 fields_under_root: true ... I got filebeat to successfully restart with this config and i can see some traffic between both servers using TCPdump but I can not find any logs with kibana. I create an index \"*\" which should enable me to find the logs if they were injected at all, but I can't find them. First Question: Is the way I am planning on doing this the correct way? Do you see any errors which I need to correct? Do i need to use fields_under_root: true or false?",
    "website_area": "discuss"
  },
  {
    "id": "ec9b63f2-dd98-4fba-948a-2a05d4146cad",
    "url": "https://discuss.elastic.co/t/config-template-processors-section-shadows-custom-processors-in-custom-beats/215578",
    "title": "Config template \"processors\" section shadows custom processors in custom beats",
    "category": [
      "Beats",
      "Community Beats"
    ],
    "author": "chris-counteractive",
    "date": "January 18, 2020, 10:31pm",
    "body": "When you build a custom beat, the build process creates your config file by concatenating your custom _meta/beat.yml and libbeat's _meta/config.yml.tmpl. When it does this, any custom processors you've defined in _meta/config.yml.tmpl are shadowed by the second processors section containing add_host_metadata: ~ and - add_cloud_metadata: ~. This leaves the user with the inconvenient task of either removing that second processors section after installation, or merging them with the important processors up top, which they really shouldn't have to do. This happens in beats/dev-tools/mage/config.go, and there doesn't appear to be any configuration that'll suppress the inclusion of that second processors section, as the merge is just a simple file concat and the .tmpl file conditionals don't have any option for leaving it out entirely. My custom beat (o365beat) includes processors in its config to do some important things, and we don't want to put it on the users to fix this issue. We're considering removing the second section later in the build, which will be tricky because we won't have any control of future changes to that file. Would you be open to a pull request that conditionally excludes the entire template processors section from libbeat's _meta/config.yml.tmpl? I couldn't find any issues/PRs referencing this behavior. See this o365beat issue for more details.",
    "website_area": "discuss"
  },
  {
    "id": "93862757-2cd3-47e6-a0e5-b4bc4ebaf4e6",
    "url": "https://discuss.elastic.co/t/rpm-names-do-not-match-their-contents-recreating-issue/203195",
    "title": "RPM names do not match their contents - recreating issue",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "akshatsharma",
    "date": "October 11, 2019, 9:15am October 11, 2019, 3:13pm November 4, 2019, 11:40am December 2, 2019, 7:00am December 13, 2019, 12:03pm December 16, 2019, 6:21am December 16, 2019, 2:37pm December 22, 2019, 10:25pm January 8, 2020, 5:57am January 17, 2020, 6:10am January 18, 2020, 4:30pm",
    "body": "Creating new issue as the previous issue(RPM names do not match their contents) was closed. Do we have any update on this? Best Regards, Akshat",
    "website_area": "discuss"
  },
  {
    "id": "c9764448-e646-4b01-b060-ca8e6dc73802",
    "url": "https://discuss.elastic.co/t/filebeat-failed-to-publish-events-client-is-not-connected/215531",
    "title": "Filebeat failed to publish events: client is not connected",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "rnakam01",
    "date": "January 17, 2020, 11:15pm",
    "body": "executing sudo /opt/elkstack/filebeat/filebeat -e -c /opt/elkstack/filebeat/filebeat.yml -d \"publish\" Consistently get 2020-01-17T15:07:18.857-0800 ERROR logstash/async.go:256 Failed to publish events caused by: lumberjack protocol error 2020-01-17T15:07:18.883-0800 ERROR logstash/async.go:256 Failed to publish events caused by: client is not connected 2020-01-17T15:07:20.011-0800 ERROR pipeline/output.go:121 Failed to publish events: client is not connected This my logstash.conf.... input { beats { port => 5044 } } output { elasticsearch { hosts => [\"http://uinf9815p:9300\"] index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\" } }",
    "website_area": "discuss"
  },
  {
    "id": "d811a36b-4bdb-441e-91ff-c0ae88b14a76",
    "url": "https://discuss.elastic.co/t/metricbeat-autodiscover-redis-module-no-data/214670",
    "title": "Metricbeat Autodiscover Redis Module No Data",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "pooch",
    "date": "January 10, 2020, 9:11pm January 13, 2020, 5:44pm January 13, 2020, 6:18pm January 13, 2020, 6:20pm January 13, 2020, 6:30pm January 13, 2020, 6:32pm January 13, 2020, 6:37pm January 13, 2020, 6:38pm January 13, 2020, 6:42pm January 13, 2020, 6:50pm January 13, 2020, 6:51pm January 17, 2020, 8:38pm",
    "body": "For some reason I cannot pick up redis data using autodiscover for metricbeat. Auto discover for nginx is working just fine. I am running out of troubleshooting ideas. k8s version 1.14.6 metricbeats conf snip metricbeat.autodiscover: providers: - type: kubernetes host: ${HOSTNAME} in_cluster: true templates: - condition: contains: kubernetes.container.image: nginx config: - module: nginx metricsets: [\"stubstatus\"] enable: true period: 10s hosts: [\"${data.host}:18080\"] server_status_path: \"nginx_status\" - condition: contains: kubernetes.container.image: redis config: - module: redis metricsets: [\"info\", \"keyspace\"] period: 10s hosts: [\"${data.host}:6379\"] Some strange log entries from metricbeat, may not be related... E0110 21:09:57.785127 1 reflector.go:342] github.com/elastic/beats/libbeat/common/kubernetes/watcher.go:235: expected type *v1beta1.ReplicaSet, but watch event object had type *v1.ReplicaSet",
    "website_area": "discuss"
  },
  {
    "id": "3f9d60d9-c831-4c28-991a-a9b1061b21eb",
    "url": "https://discuss.elastic.co/t/google-cloud-module-extracted-fields/215216",
    "title": "Google Cloud Module - Extracted Fields",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "daniel_a",
    "date": "January 15, 2020, 9:23pm January 17, 2020, 6:44pm January 17, 2020, 6:44pm",
    "body": "Is the insertId field omitted in filebeat by Google Cloud module while extracting fields? The GCP flow logs have the insertId field in the initial logs, but it doesn't seem to be translated/parsed by the filebeat module. It will be hard to connect these two the same logs (in Kibana/ES and in GCP) together without an ID.",
    "website_area": "discuss"
  },
  {
    "id": "dffc9e04-f848-4b7b-b49b-ccea7597b2ed",
    "url": "https://discuss.elastic.co/t/segmentation-violation-and-other-errors-with-go-1-13-6-beats-7-5-1/215208",
    "title": "Segmentation violation and other errors with Go 1.13.6 - Beats 7.5.1",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "Rishi_Misra",
    "date": "January 15, 2020, 8:56pm January 15, 2020, 11:01pm January 15, 2020, 11:18pm January 16, 2020, 2:29am January 16, 2020, 8:35pm January 16, 2020, 10:33pm January 16, 2020, 10:33pm January 17, 2020, 3:23pm January 17, 2020, 3:26pm",
    "body": "Hi there, I am in the process of porting Beats on s390x architecture and noticed that there are multiple segmentation violation when running test cases (Most of the beats - for example Filebeat, Packetbeat, Heartbeat has this error). This also happens on x86 arch (Linux 2a4cae7a4792 4.15.0-29-generic #31-Ubuntu SMP Tue Jul 17 15:39:52 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux) /============================================/ ... Testing Heartbeat go test -i github.com/elastic/beats/heartbeat github.com/elastic/beats/heartbeat/autodiscover github.com/elastic/beats/heartbeat/autodiscover/builder/hints github.com/elastic/beats/heartbeat/beater github.com/elastic/beats/heartbeat/cmd github.com/elastic/beats/heartbeat/config github.com/elastic/beats/heartbeat/eventext github.com/elastic/beats/heartbeat/hbtest github.com/elastic/beats/heartbeat/include github.com/elastic/beats/heartbeat/look github.com/elastic/beats/heartbeat/monitors github.com/elastic/beats/heartbeat/monitors/active/dialchain github.com/elastic/beats/heartbeat/monitors/active/http github.com/elastic/beats/heartbeat/monitors/active/icmp github.com/elastic/beats/heartbeat/monitors/active/tcp github.com/elastic/beats/heartbeat/monitors/defaults github.com/elastic/beats/heartbeat/monitors/jobs github.com/elastic/beats/heartbeat/monitors/wrappers github.com/elastic/beats/heartbeat/reason github.com/elastic/beats/heartbeat/scheduler github.com/elastic/beats/heartbeat/scheduler/schedule github.com/elastic/beats/heartbeat/scheduler/schedule/cron github.com/elastic/beats/heartbeat/scripts/mage github.com/elastic/beats/heartbeat/watcher go test github.com/elastic/beats/heartbeat github.com/elastic/beats/heartbeat/autodiscover github.com/elastic/beats/heartbeat/autodiscover/builder/hints github.com/elastic/beats/heartbeat/beater github.com/elastic/beats/heartbeat/cmd github.com/elastic/beats/heartbeat/config github.com/elastic/beats/heartbeat/eventext github.com/elastic/beats/heartbeat/hbtest github.com/elastic/beats/heartbeat/include github.com/elastic/beats/heartbeat/look github.com/elastic/beats/heartbeat/monitors github.com/elastic/beats/heartbeat/monitors/active/dialchain github.com/elastic/beats/heartbeat/monitors/active/http github.com/elastic/beats/heartbeat/monitors/active/icmp github.com/elastic/beats/heartbeat/monitors/active/tcp github.com/elastic/beats/heartbeat/monitors/defaults github.com/elastic/beats/heartbeat/monitors/jobs github.com/elastic/beats/heartbeat/monitors/wrappers github.com/elastic/beats/heartbeat/reason github.com/elastic/beats/heartbeat/scheduler github.com/elastic/beats/heartbeat/scheduler/schedule github.com/elastic/beats/heartbeat/scheduler/schedule/cron github.com/elastic/beats/heartbeat/scripts/mage github.com/elastic/beats/heartbeat/watcher panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x202aff6] ... FAIL: Test JSON response with simple straight-forward comparisons [with body='{\"foo\": 3}', comparison={'foo': 3}] Traceback (most recent call last): File \"//src/github.com/elastic/beats/heartbeat/build/python-env/local/lib/python2.7/site-packages/parameterized/parameterized.py\", line 392, in standalone_func return func(*(a + p.args), **p.kwargs) File \"/src/github.com/elastic/beats/heartbeat/tests/system/test_monitor.py\", line 165, in test_json_simple_comparisons proc.check_kill_and_wait() File \"/src/github.com/elastic/beats/heartbeat/tests/system/../../../libbeat/tests/system/beat/beat.py\", line 104, in check_kill_and_wait return self.check_wait(exit_code=exit_code) File \"/src/github.com/elastic/beats/heartbeat/tests/system/../../../libbeat/tests/system/beat/beat.py\", line 93, in check_wait exit_code, actual_exit_code) AssertionError: Expected exit code to be 0, but it was 2 /============================================/ Upon changing Go version to 1.12.9 everything seems to work pretty well (at least most of the test cases pass). I wonder if these are known issues. Thanks for any guidance.",
    "website_area": "discuss"
  },
  {
    "id": "e128b7c9-add6-4e4c-96f3-3991a71c6b55",
    "url": "https://discuss.elastic.co/t/filebeat-does-not-work-while-using-cloud-id-and-cloud-auth/215167",
    "title": "Filebeat does not work while using cloud.id and cloud.auth",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "newKibanaUser",
    "date": "January 15, 2020, 4:32pm January 15, 2020, 4:43pm January 15, 2020, 4:54pm January 15, 2020, 5:09pm January 15, 2020, 6:37pm January 15, 2020, 7:14pm January 15, 2020, 7:45pm January 15, 2020, 7:52pm January 15, 2020, 8:13pm January 15, 2020, 9:21pm January 15, 2020, 9:23pm January 16, 2020, 4:43pm January 16, 2020, 5:59pm January 16, 2020, 7:43pm January 16, 2020, 7:42pm January 17, 2020, 2:48pm",
    "body": "Hello - I am trying to use cloud.id and cloud.auth in my file beat configuration file. But that seems not working. This Works perfectly without using cloud.id and cloud_auth. output.elasticsearch: hosts: [\"https://4d90d9c2814c429xxxxxxxxxx:99999\"] protocol: https ssl: null ignoreversion: true username: \"xxxxxxx\" password: \"yyyyyyy\" This does not work while using cloud.id and cloud_auth, getting error like below. 401 Unauthorized: {\"error\":{\"root_cause\":[{\"type\":\"security_exception\",\"reason\":\"action [cluster:monitor/main] requires authentication\",\"header\":{\"WWW-Authenticate\":[\"Bearer realm=\"securit....... output.elasticsearch: hosts: [\"https://4d90d9c2814c429xxxxxxxxxx:99999\"] ssl: null ignorversion: true cloud.id: \"temps:am9uxxxxxxxxxxxxxxxxxxxxxx\" cloud.auth: \"xxxxxxxx:yyyyyyyyy\" Can someone help? Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "12a6746f-e0fe-4993-9bc1-ce6791befb35",
    "url": "https://discuss.elastic.co/t/heartbeat-kubernetes-deployment-manifests-and-icmp-dashboards/214847",
    "title": "Heartbeat Kubernetes deployment manifests and icmp dashboards",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "akrzos",
    "date": "January 13, 2020, 2:01pm January 15, 2020, 11:07pm January 16, 2020, 12:46am January 16, 2020, 4:06am January 16, 2020, 2:19pm January 16, 2020, 3:44pm January 16, 2020, 7:01pm January 16, 2020, 7:26pm January 17, 2020, 9:55am January 17, 2020, 2:27pm",
    "body": "Is there a heartbeat kubernetes deployment manifest that is available? (I only see auditbeat/filebeat/metricbeat yamls) Ideally this would show heartbeat best practices and or possible deployments in kubernetes. Also are they any pre-built icmp dashboards or suggestions on how to best visually show when one host is pinging poorly compared to others?",
    "website_area": "discuss"
  },
  {
    "id": "0c1bd829-58f0-4ac7-ac94-bca3ad03a085",
    "url": "https://discuss.elastic.co/t/filebeat-azure-module-issue/215297",
    "title": "Filebeat Azure Module Issue",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 16, 2020, 11:07am January 16, 2020, 8:54pm January 16, 2020, 9:19pm January 17, 2020, 10:08am January 17, 2020, 12:37pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "da911eb2-6dbc-4b77-8197-2058fa68f079",
    "url": "https://discuss.elastic.co/t/filebeat-script/215402",
    "title": "Filebeat script",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "fang5826",
    "date": "January 17, 2020, 3:01am January 17, 2020, 8:33am",
    "body": "In filebeat's script, how to create a new event and make it take effect, just like new_event_block.call() in logstash's ruby plug-in",
    "website_area": "discuss"
  },
  {
    "id": "b67bb262-9707-4bbc-a06c-33483ea885df",
    "url": "https://discuss.elastic.co/t/filebeat-kafka-out-plugin-error-when-configuring-multiple-topics-using-regex/215264",
    "title": "Filebeat kafka out plugin error when configuring multiple topics using regex",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "iamabug",
    "date": "January 16, 2020, 8:00am January 16, 2020, 8:13pm January 16, 2020, 10:11pm",
    "body": "I am trying to output file to different topic using regex, but the following config does not work: filebeat.inputs: - type: log enabled: true paths: - ~/1.log output.kafka: enabled: true hosts: [\"localhost:9092\"] topic: \"default\" codec.format: string: '%{[message]}' topics: - topic: \"type1\" when.regexp.message: \"\\[type1\\].*\" - topic: \"type2\" when.regexp.message: \"\\[type2\\].*\" the error message is Exiting: error loading config file: yaml: line 20: found unknown escape character OS: Mac filebeat version: 6.5, 7.5 Kafka version : 2.3.1",
    "website_area": "discuss"
  },
  {
    "id": "e883fc4e-44ca-46d6-b725-32be2d4bd440",
    "url": "https://discuss.elastic.co/t/multiline-message-with-repeated-header/215324",
    "title": "Multiline message with repeated header",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "widhalmt",
    "date": "January 16, 2020, 2:07pm January 16, 2020, 8:21pm",
    "body": "Hi, I have a rather tricky situation. Multiline messages are sent through some transportation service which adds its own header. Due to policy (and technical) issues it's not possible to install Filebeat on the originating system. What Filebeat reads looks like this: 12:34, service1, 124, This is a single line message, extradata123 12:36, service1, 138, This is a, extradata145 12:36, service1, 138, multline message, extradata145 So the multiline message is packed between other columns so I can't use the usual multiline solutions in Filebeat. Please mind that the other columns are identical in these multline messages. There is some sort of id or hash (represented by 124 and 138 in my example). Before I discovered that there's this \"special\" form of multiline message I just used the csv Filter in Logstash and all was fine and cozy. Now I found these multiline messages and honestly I don't know that to do. The data is written to a file which I collect with a bash script which pipes the data into filebeat. (I know, there are other ways but there are other reasons why I have to use the script). I'm just giving this information because I wouldn't mind solving this in a bash/python/ruby/assembler tool before piping it into filebeat. Maybe you could give me a hint how to solve it? (Hint: Beating the developers of the originating tool until they send proper multiline messages might be fun but is not an option) Cheers, Thomas",
    "website_area": "discuss"
  },
  {
    "id": "c596de2b-8e47-4d1b-983c-b7518d33c5a4",
    "url": "https://discuss.elastic.co/t/metricbeat-and-pipelines/215306",
    "title": "MetricBeat and Pipelines?",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Vafa_Ronaghi",
    "date": "January 16, 2020, 11:36am January 16, 2020, 7:35pm",
    "body": "Is it possible to use multiple pipelines for Metricbeat like in Logstash ? I am triyng to collect metrics from multiple PostgreSQL IInstances on same Machine. Each Instance belongs to one Application. I dont want to start multiple Instances of Metricbeat. Any Idea ? Many thanks in advance Vafa",
    "website_area": "discuss"
  },
  {
    "id": "570dff12-286e-454c-a3c8-8505f2995ec4",
    "url": "https://discuss.elastic.co/t/filebeat-to-kafka-ssl-with-jks/213595",
    "title": "Filebeat to Kafka SSL (with jks)",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "nisham",
    "date": "January 2, 2020, 3:33pm January 16, 2020, 6:58pm",
    "body": "Hello, I have setup a Kafka server with SSL enabled using the jks file. I'm able to read and write from the Kafka topic using Logstash by providing the JKS files in the logstash config file. I need to write data from Filebeat to Kafka over the SSL using jks (but in documentation there is no option for jks). I tried with converting jks to cert.pem and key.pem but its not working. Could someone help me on this ? Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "38587f1d-ea3b-4c01-94c8-4c5dd9789657",
    "url": "https://discuss.elastic.co/t/filebeats-via-docker-refusing-to-decode-json/215261",
    "title": "Filebeats via docker refusing to decode JSON",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 16, 2020, 11:46am January 16, 2020, 4:39pm January 16, 2020, 6:03pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5ce6c808-9d69-4b98-a90d-bf8293e2a6fc",
    "url": "https://discuss.elastic.co/t/connection-marked-as-failed-because-the-onconnect-callback-failed-resource-filebeat-7-5-1-exists-but-it-is-not-an-alias/214253",
    "title": "Connection marked as failed because the onConnect callback failed: resource 'filebeat-7.5.1' exists, but it is not an alias\"",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Flaviu",
    "date": "January 8, 2020, 2:38pm January 9, 2020, 8:33pm January 16, 2020, 5:26pm",
    "body": "Hello, I have just installed a new server with filebeat 7.5.1 and for some reason, after I restart the filebeat or the server I get this error: Connection marked as failed because the onConnect callback failed: resource 'filebeat-7.5.1' exists, but it is not an alias\" if I go in kibana and delete the \"filebeat-7.5.1\" index I start to see logs coming in in the kibana/elasticsearch server, and the above error disappears from the filebeat error logs. If I restart the filebeat process or the server altogether the above error starts to appear again and the logs are not pus anymore in kibana/elasticsearch. Te repair this I just need to go back to kibana and delete the index. I have some more servers with a different version of filebeat and I don't have this issue. Am I doing something wrong? Thank you,",
    "website_area": "discuss"
  },
  {
    "id": "bcc6eb85-525c-43d7-b84e-6883930c57b3",
    "url": "https://discuss.elastic.co/t/geoip-enrichment-stops-es-from-receiving-data-from-packetbeat/214955",
    "title": "geoIP enrichment stops ES from receiving data from packetbeat",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "jsu",
    "date": "January 14, 2020, 1:09pm January 14, 2020, 7:39pm January 16, 2020, 9:42am January 16, 2020, 3:01pm January 16, 2020, 3:01pm",
    "body": "Hey there, I'm currently running Packetbeat on a linux box and it sends directly data to an Elasticsearch cluster. Everything runs perfectly fine, except when I want to add geoIP info following this documentation. Indeed, as soon as I update the config file, I don't receive any data anymore. Events continue to be emitted, but nothing appears in the dashboard [Packetbeat] Overview ECS anymore. Fail rates climbs to 3/s and open handles climb to 10/s. The weird thing is that when I test the config with sudo packetbeat test config -c packetbeat.yml -e , it says that the config is ok. And when I launch it as a service, it runs fine, but I still don't receive any data. As soon as I take away the pipeline in the config file and restart the service, everything runs fine again and I receive all the data. Do you have any idea what's happening and how I could fix this ? In case you need it, here is the config file: packetbeat.interfaces.device: any packetbeat.flows: timeout: 30s period: 10s packetbeat.protocols: - type: icmp enabled: true - type: amqp ports: [5672] - type: cassandra ports: [9042] - type: dhcpv4 ports: [67, 68] - type: dns ports: [53] - type: http ports: [80, 8080, 8000, 5000, 8002] - type: memcache ports: [11211] - type: mysql ports: [3306,3307] - type: pgsql ports: [5432] - type: redis ports: [6379] - type: thrift ports: [9090] - type: mongodb ports: [27017] - type: nfs ports: [2049] - type: tls ports: - 443 # HTTPS - 993 # IMAPS - 995 # POP3S - 5223 # XMPP over SSL - 8443 - 8883 # Secure MQTT - 9243 # Elasticsearch name: \"box_name\" setup.template.pattern: \"packetbeat*\" setup.template.overwrite: false setup.template.settings: index.number_of_shards: 2 index.number_of_replicas: 1 index.routing.allocation.require.temp: \"hot\" setup.ilm.enabled: true setup.ilm.rollover_alias: \"packetbeat\" setup.ilm.pattern: \"000001\" setup.ilm.policy_name: \"hotwarm_policy\" tags: [\"my_tags\"] setup.kibana: host: \"kibana_ip:5601\" username: \"${KIB_USER}\" password: \"${KIB_PWD}\" output.elasticsearch: hosts: [\"elasticsearch_node1_ip:9200\",\"elasticsearch_node2_ip:9200\"] username: \"${ES_USER}\" password: \"${ES_PWD}\" pipeline: geoip-info processors: - add_host_metadata: netinfo.enabled: true Geo: name: box_name location: xxxxx continent_name: xxxxx country_iso_code: xxxxx region_name: xxxxx region_iso_code: xxxxx city_name: xxxxx - add_locale: ~ - add_cloud_metadata: ~ - add_fields: when.network.source.ip: private fields: source.geo.location: lat: xxxxx lon: xxxxx source.geo.continent_name: xxxxx source.geo.region_iso_code: xxxxx source.geo.country_iso_code: xxxxx source.geo.region_name: xxxxx source.geo.name: box_name target: '' - add_fields: when.network.destination.ip: private fields: source.geo.location: lat: xxxxx lon: xxxxx source.geo.continent_name: xxxxx source.geo.region_iso_code: xxxxx source.geo.country_iso_code: xxxxx source.geo.region_name: xxxxx source.geo.name: xxxxx target: '' monitoring.enabled: true and here is the pipeline in the elasticsearch cluster: \"geoip-info\" : { \"description\" : \"Add geoip info\", \"processors\" : [ { \"geoip\" : { \"field\" : \"client.ip\", \"target_field\" : \"client.geo\", \"ignore_missing\" : true } }, { \"geoip\" : { \"field\" : \"source.ip\", \"target_field\" : \"source.geo\", \"ignore_missing\" : true } }, { \"geoip\" : { \"field\" : \"destination.ip\", \"target_field\" : \"destination.geo\", \"ignore_missing\" : true } }, { \"geoip\" : { \"field\" : \"server.ip\", \"target_field\" : \"server.geo\", \"ignore_missing\" : true } }, { \"geoip\" : { \"field\" : \"host.ip\", \"target_field\" : \"host.geo\", \"ignore_missing\" : true } } ] } Thanks in advance for your help ! Cheers, jsu",
    "website_area": "discuss"
  },
  {
    "id": "8ea94555-2386-41d9-a51f-6290b7c787fe",
    "url": "https://discuss.elastic.co/t/nginx-file-format-incorrect-output/215113",
    "title": "Nginx file_format incorrect output",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "marbro15",
    "date": "January 15, 2020, 10:33am January 15, 2020, 10:46am January 15, 2020, 11:24am January 15, 2020, 12:41pm January 15, 2020, 12:58pm January 15, 2020, 1:03pm January 16, 2020, 10:25am January 16, 2020, 10:52am January 16, 2020, 12:59pm",
    "body": "Hello, I have a question about the nginx log_format. Currently we have the problem, that the source.address in filebeat shows address and ip in one field. For example: \"source\": { \"address\": \"www.domain.tld 123.123.123.123\" }, This is the config of our nginx file_format (nginx.conf): log_format filebeat '$http_host $remote_addr - $remote_user [$time_local] ' '\"$request\" $status $bytes_sent ' '\"$http_referer\" \"$http_user_agent\" \"$request_time\"'; We are using the newest nginx filebeat module (master branch) from https://github.com/elastic/beats/blob/master/filebeat/module/nginx/access/ingest/default.json We want to split the ip into the field source.ip. Maybe someone can help me? Thank you! Regards!",
    "website_area": "discuss"
  },
  {
    "id": "a53e41e3-06b4-4a08-b91b-0eb921b67187",
    "url": "https://discuss.elastic.co/t/beats-privileges-documentation/214190",
    "title": "Beats privileges documentation",
    "category": [
      "Beats"
    ],
    "author": "jsu",
    "date": "January 8, 2020, 10:24am January 8, 2020, 6:16pm January 9, 2020, 4:29pm January 9, 2020, 7:13pm January 16, 2020, 2:06am January 16, 2020, 9:20am",
    "body": "Hi there, I had a problem that I managed to resolve but it seems then that the documentation isn't completely accurate. Let me know if I should post this somewhere else. So, when configuring Beats agents to send data to a cluster with security activated, I followed this documentation and configured a user having a \"writer role\" with the privileges create_doc and view_index_metadata for all the metricbeat-*, filebeat-* and other *beat-* as specified in the doc. The Beats were able to connect to the cluster but no data was coming in. The solution was to actually give this role the create_doc and view_index_metadata privileges for all indices * and the data began flowing. As far as I know, it's not mentioned anywhere in the doc that privileges need to be applied to more than the specific *beat-* indices. Is it indeed a problem with the documentation ? ps : my indices do have the regular patterns *beat-*. pps : I linked the doc for Metricbeat, but it's the same for others (here for Auditbeat)",
    "website_area": "discuss"
  },
  {
    "id": "7bf92d79-e5ca-435e-9503-331988947ed0",
    "url": "https://discuss.elastic.co/t/filebeat-sends-packets-with-0-leangth/215265",
    "title": "Filebeat sends packets with 0 leangth",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "ruranga",
    "date": "January 16, 2020, 8:35am",
    "body": "Hi, I'm sending logs to ELK server using filebeat running in the syslog server. I dont get any update in KIbana. Then I check the packets (using tcpdump) in the syslog server (where filebeat runs), all packets are with 0 length. Then I check with Output.console pretty: true and observed that file beat reads the log files without any issue. What is the possible issue? This was worked very fine few days ago! My filebeat version 7.5.1 Following are the filebeat logs which seems normal to me 2020-01-16T08:27:30.707Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":420,\"time\":{\"ms\":4}},\"total\":{\"ticks\":1560,\"time\":{\"ms\":10},\"value\":1560},\"user\":{\"ticks\":1140,\"time\":{\"ms\":6}}},\"handles\":{\"limit\":{\"hard\":4096,\"soft\":1024},\"open\":8},\"info\":{\"ephemeral_id\":\"ac23bda1-059b-431a-9e4e-7c892be68c6f\",\"uptime\":{\"ms\":1410103}},\"memstats\":{\"gc_next\":21941408,\"memory_alloc\":11657280,\"memory_total\":49431000},\"runtime\":{\"goroutines\":27}},\"filebeat\":{\"harvester\":{\"open_files\":1,\"running\":1}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"output\":{\"read\":{\"bytes\":36}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":4117}}},\"registrar\":{\"states\":{\"current\":1}},\"system\":{\"load\":{\"1\":1.02,\"15\":0.97,\"5\":1.01,\"norm\":{\"1\":0.1275,\"15\":0.1213,\"5\":0.1263}}}}}} 2020-01-16T08:28:00.707Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":420,\"time\":{\"ms\":3}},\"total\":{\"ticks\":1570,\"time\":{\"ms\":9},\"value\":1570},\"user\":{\"ticks\":1150,\"time\":{\"ms\":6}}},\"handles\":{\"limit\":{\"hard\":4096,\"soft\":1024},\"open\":8},\"info\":{\"ephemeral_id\":\"ac23bda1-059b-431a-9e4e-7c892be68c6f\",\"uptime\":{\"ms\":1440104}},\"memstats\":{\"gc_next\":21941408,\"memory_alloc\":12034544,\"memory_total\":49808264},\"runtime\":{\"goroutines\":27}},\"filebeat\":{\"harvester\":{\"open_files\":1,\"running\":1}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"output\":{\"read\":{\"bytes\":36}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":4117}}},\"registrar\":{\"states\":{\"current\":1}},\"system\":{\"load\":{\"1\":1.01,\"15\":0.97,\"5\":1.01,\"norm\":{\"1\":0.1263,\"15\":0.1213,\"5\":0.1263}}}}}} 2020-01-16T08:28:30.707Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":420,\"time\":{\"ms\":3}},\"total\":{\"ticks\":1610,\"time\":{\"ms\":42},\"value\":1610},\"user\":{\"ticks\":1190,\"time\":{\"ms\":39}}},\"handles\":{\"limit\":{\"hard\":4096,\"soft\":1024},\"open\":8},\"info\":{\"ephemeral_id\":\"ac23bda1-059b-431a-9e4e-7c892be68c6f\",\"uptime\":{\"ms\":1470104}},\"memstats\":{\"gc_next\":21941120,\"memory_alloc\":11139208,\"memory_total\":50092696},\"runtime\":{\"goroutines\":27}},\"filebeat\":{\"harvester\":{\"open_files\":1,\"running\":1}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"output\":{\"read\":{\"bytes\":36}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":4117}}},\"registrar\":{\"states\":{\"current\":1}},\"system\":{\"load\":{\"1\":0.95,\"15\":0.97,\"5\":0.99,\"norm\":{\"1\":0.1188,\"15\":0.1213,\"5\":0.1238}}}}}} Disclaimer : Thanks in advance! KR",
    "website_area": "discuss"
  },
  {
    "id": "c9239097-0ffb-4424-ab62-190400eea842",
    "url": "https://discuss.elastic.co/t/incorrect-filebeat-nginx-mappings-with-kubernetes-deployment/215046",
    "title": "Incorrect filebeat nginx mappings with kubernetes deployment",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "laurentiusoica",
    "date": "January 14, 2020, 9:05pm January 16, 2020, 6:50am",
    "body": "While using the k8s manifest from here https://raw.githubusercontent.com/elastic/beats/7.5/deploy/kubernetes/filebeat-kubernetes.yaml The nginx module mappings looks like these: \"nginx\" : { \"properties\" : { \"access\" : { \"properties\" : { \"geoip\" : { \"type\" : \"object\" }, \"user_agent\" : { \"type\" : \"object\" } } }, \"error\" : { \"properties\" : { \"connection_id\" : { \"type\" : \"long\" } } } } }, I was expecting to get the mappings as defined in the fields.yaml file.",
    "website_area": "discuss"
  },
  {
    "id": "a90c8a1a-b386-4957-b4e6-b40daff65d49",
    "url": "https://discuss.elastic.co/t/filebeat-cannot-find-registry-file-var-lib-filebeat-registry-filebeat/214997",
    "title": "Filebeat cannot find registry file (/var/lib/filebeat/registry/filebeat)",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "ntran",
    "date": "January 14, 2020, 1:34pm January 14, 2020, 7:35pm January 15, 2020, 6:23am January 15, 2020, 6:30am January 15, 2020, 4:09pm January 16, 2020, 6:36am",
    "body": "Hi all, I am getting this error message when I run filebeat -e: Writing of registry returned error: rename /var/lib/filebeat/registry/filebeat/data.json.new /var/lib/filebeat/registry/filebeat/data.json: no such file or directory I do not have a /var/lib/filebeat/registry/filebeat/data.json.new file, however, I do have /var/lib/filebeat/registry/filebeat/data.json Filebeat is still harvesting files, however, how does it know it has finished harvesting the file if it cannot find the registry file? [root@elastic-nfs01 ~]# ls -lh /var/lib/filebeat/registry/filebeat -rw-------. 1 root root 38K Jan 14 13:23 data.json -rwxrwxrwx. 1 root root 16 Jul 24 07:17 meta.json Filebeat.yml #=========================== Filebeat inputs ============================= filebeat.inputs: - type: log ignore_older: 48h clean_inactive: 72h clean_removed: true filebeat.registry.path: /var/lib/filebeat/registry # Change to true to enable this input configuration. # enabled: false paths: - /data/elastic-backup/logs/elastic-*/elastic-01_server.json - type: log # Change to true to enable this input configuration. # enabled: false paths: - /data/cluster-elastic-backup/logs/cluster-elastic*/cluster-cluster-01_server.json ignore_older: 48h clean_inactive: 72h clean_removed: true filebeat.registry.path: /var/lib/filebeat/registry #============================= Filebeat modules =============================== filebeat.config.modules: path: /etc/filebeat/modules.d/*.yml reload.enabled: false #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 1 #============================== Kibana ===================================== setup.kibana: host: \"https://XX.XX.XX.XX:5601\" ssl.verification_mode: \"none\" #================================ Outputs ===================================== #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"XX.XX.XX.XX:9200\"] # Optional protocol and basic auth credentials. protocol: \"https\" username: \"elastic\" password: \"changeme\" ssl.verification_mode: \"none\" bulk_max_size: 50 compression_level: 1 worker: 2 #================================ Processors ===================================== # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] Elasticsearch module > # Module: elasticsearch > # Docs: https://www.elastic.co/guide/en/beats/filebeat/7.2/filebeat-module-elasticsearch.html > - module: elasticsearch > # Server log > server: > enabled: false > # Set custom paths for the log files. If left empty, > # Filebeat will choose the paths depending on your OS. > var.paths: > - /data/elastic-backup/logs/elastic-hotdata01/*server.json > # Convert the timestamp to UTC. Requires Elasticsearch >= 6.1. > var.convert_timezone: true > audit: > enabled: true > # Set custom paths for the log files. If left empty, > # Filebeat will choose the paths depending on your OS. > var.paths: > - /data/elastic-backup/logs/elastic-warmdata03/elastic-01_audit*.json > - /data/elastic-backup/logs/elastic-hotdata01/elastic-01_audit*.log > - > # Convert the timestamp to UTC. Requires Elasticsearch >= 6.1. > var.convert_timezone: true",
    "website_area": "discuss"
  },
  {
    "id": "e8ddea2c-f512-4c3b-8012-2013f270d859",
    "url": "https://discuss.elastic.co/t/account-name-not-showing-in-windows-event-id-4732/213177",
    "title": "Account name not showing in windows event_id 4732",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "mancharagopan",
    "date": "December 27, 2019, 10:46am January 3, 2020, 3:25am January 3, 2020, 4:12am January 13, 2020, 10:11pm January 13, 2020, 10:12pm January 16, 2020, 3:56am",
    "body": "I am trying to track users added to Administrators group. But in the event viewer log shows local username and group, but in the event which i am receiving has only the SID. I check the Friendly View and XML View, both are same. Is there any way to resolve this issue? I am using ELK Stack and Beats agent to ship the logs. This is the Friendly view, Log Name: Security Source: Microsoft-Windows-Security-Auditing Date: 12/27/2019 3:25:02 PM Event ID: 4732 Task Category: Security Group Management Level: Information Keywords: Audit Success User: N/A Computer: pms-primary Description: A member was added to a security-enabled local group. Subject: Security ID: PMS-PRIMARY\\Administrator Account Name: Administrator Account Domain: PMS-PRIMARY Logon ID: 0x1bc355 Member: Security ID: PMS-PRIMARY\\test Account Name: - Group: Security ID: BUILTIN\\Administrators Group Name: Administrators Group Domain: Builtin Additional Information: Privileges: - Event Xml: <Event xmlns=\"http://schemas.microsoft.com/win/2004/08/events/event\"> <System> <Provider Name=\"Microsoft-Windows-Security-Auditing\" Guid=\"{54849625-5478-4994-A5BA-3E3B0328C30D}\" /> <EventID>4732</EventID> <Version>0</Version> <Level>0</Level> <Task>13826</Task> <Opcode>0</Opcode> <Keywords>0x8020000000000000</Keywords> <TimeCreated SystemTime=\"2019-12-27T09:55:02.995947100Z\" /> <EventRecordID>480532626</EventRecordID> <Correlation /> <Execution ProcessID=\"556\" ThreadID=\"5760\" /> <Channel>Security</Channel> <Computer>pms-primary</Computer> <Security /> </System> <EventData> <Data Name=\"MemberName\">-</Data> <Data Name=\"MemberSid\">S-1-5-21-1340285219-1682968722-2481256013-1046</Data> <Data Name=\"TargetUserName\">Administrators</Data> <Data Name=\"TargetDomainName\">Builtin</Data> <Data Name=\"TargetSid\">S-1-5-32-544</Data> <Data Name=\"SubjectUserSid\">S-1-5-21-1340285219-1682968722-2481256013-500</Data> <Data Name=\"SubjectUserName\">Administrator</Data> <Data Name=\"SubjectDomainName\">PMS-PRIMARY</Data> <Data Name=\"SubjectLogonId\">0x1bc355</Data> <Data Name=\"PrivilegeList\">-</Data> </EventData> </Event>",
    "website_area": "discuss"
  },
  {
    "id": "f4631dc2-09d0-4e5e-bc26-b32765e69ae7",
    "url": "https://discuss.elastic.co/t/suricata-logs-to-filebeat-to-kafka-topics-by-event-type/215179",
    "title": "Suricata logs to Filebeat to Kafka topics, by event-type",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "driekhof",
    "date": "January 15, 2020, 5:24pm January 15, 2020, 11:00pm January 15, 2020, 11:01pm",
    "body": "Hello, new to Filebeat. I have a simplified case working: Suricata to Filebeat to Kafka if I hard code one kafka topic name in filebeat.yml. But I'd like to make Filebeat dynamically route events to different topics based on their Suricata event type. I've taken several wild guesses, but I don't really understand how dynamic filebeat fields work, or what I need to do to set them up. Here's one attempt after enabling the suricata module: topic: 'suricata-%{[fields.suricata.eve.event_type]}' The filebeat log just gives me this: 2020-01-14T23:44:49.550Z INFO kafka/log.go:53 kafka message: Initializing new client 2020-01-14T23:44:49.551Z INFO kafka/log.go:53 kafka message: Successfully initialized new client 2020-01-14T23:44:49.551Z INFO pipeline/output.go:105 Connection to kafka(somehost:9092) established 2020-01-14T23:44:49.551Z ERROR kafka/client.go:144 Dropping event: no topic could be selected 2020-01-14T23:44:49.551Z ERROR kafka/client.go:144 Dropping event: no topic could be selected etc..... Is this kind of thing possible with filebeat, and if so how? An example would be ideal.",
    "website_area": "discuss"
  },
  {
    "id": "ab521686-9e3d-47a2-a5e1-af06ccf5bcd8",
    "url": "https://discuss.elastic.co/t/question-regarding-customizing-fields-yml/215176",
    "title": "Question regarding Customizing fields.yml",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "john_eapen",
    "date": "January 15, 2020, 4:57pm January 15, 2020, 10:03pm",
    "body": "Hi there, Just wanted to know what the general guidelines/best practices are regarding fields.yml which is used for creating index template. The out-of-box fields.yml has hundreds of fields and we are not even using most of it. 1- Is it a good practice to remove the unwanted fields and actually add our own custom fields into fields.yml ?. 2- Can the out-of-box fields.yml cause performance issue since there are so many fields. Does ES internally really care about the unused fields. (fields with no data).? 3- If we do customize, then are \"setup.template.name|pattern|fields\" the correct config properties to set. Thanks John",
    "website_area": "discuss"
  },
  {
    "id": "f582ee57-becc-45c6-a884-32d7f521b346",
    "url": "https://discuss.elastic.co/t/metricbeat-on-mysql-docker/215045",
    "title": "Metricbeat on MySQL docker",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "January 14, 2020, 8:43pm January 15, 2020, 1:01am January 15, 2020, 1:09pm January 15, 2020, 4:50pm January 15, 2020, 7:32pm January 15, 2020, 8:25pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b9450792-e47c-43e3-a8cc-214973058dfe",
    "url": "https://discuss.elastic.co/t/drop-event-when-not-or-not-working-please-help/214843",
    "title": "Drop_event.when.not.or: - not working, Please help",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "GuessMyName",
    "date": "January 13, 2020, 1:46pm January 15, 2020, 7:06pm",
    "body": "Hi Guys, For some reason... Winlogbeat is grabbing all security logs not just the one listed below, any idea what I'm missing ?, Running 7.5.1 Thx! winlogbeat.event_logs: name: Security processors: drop_event.when.not.or: equals.winlog.event_id: 4754 equals.winlog.event_id: 4755 equals.winlog.event_id: 4757 equals.winlog.event_id: 4758 equals.winlog.event_id: 4764 equals.winlog.event_id: 4740 equals.winlog.event_id: 4728 equals.winlog.event_id: 4732 equals.winlog.event_id: 4756 equals.winlog.event_id: 4735 equals.winlog.event_id: 4724 equals.winlog.event_id: 4625 equals.winlog.event_id: 4648 equals.winlog.event_id: 1102 equals.winlog.event_id: 4624 equals.winlog.event_id: 5038 equals.winlog.event_id: 6281 equals.winlog.event_id: 4727 equals.winlog.event_id: 4729 equals.winlog.event_id: 4730 equals.winlog.event_id: 4731 equals.winlog.event_id: 4733 equals.winlog.event_id: 4734 equals.winlog.event_id: 4737 #ignore_older: 72h",
    "website_area": "discuss"
  },
  {
    "id": "e2f3a8e1-68d5-493e-b296-c11ca70f833c",
    "url": "https://discuss.elastic.co/t/filebeat-7-5-1-missing-a-step-for-custom-index-names-config/215044",
    "title": "Filebeat 7.5.1 Missing a step for custom index names config?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "cgutshall",
    "date": "January 14, 2020, 9:52pm January 15, 2020, 3:50pm January 15, 2020, 3:51pm",
    "body": "I have the following set it my filebeat.yml config file, which should allow me to see dev-* index in kibana. However these are still showing as filebeat-*. Is there something Im missing based on the docs and the reference file on install I should have everything set correctly. filebeat.inputs: - type: log enabled: true paths: - /var/log/*.log filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false output.elasticsearch: hosts: [\"my_es_host_ip:9200\"] index: \"dev-%{[agent.version]}-%{+yyyy.MM.dd}\" setup.template.enabled: true setup.template.name: \"dev-%{[agent.version]}\" setup.template.pattern: \"dev-%{[agent.version]}-*\" Thanks",
    "website_area": "discuss"
  },
  {
    "id": "fcc0ca4d-fe48-49d8-a303-df9f67f3029c",
    "url": "https://discuss.elastic.co/t/memory-issues-long-running-instances/215136",
    "title": "Memory issues (long running instances)",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "YvorL",
    "date": "January 15, 2020, 1:08pm",
    "body": "Hi, I've a lot of Filebeat instances (v7.2.1) running on separate containers and noticed that longer an instance is running, the more memory (RSS) it uses. While a restart helps and the memory usage drops (e.g., from 380MB to 15MB after ~80 days of continuous running) I don't see that as a viable option. Is there anything I missed during searching for various FB memory leak threads? AFAIK, there isn't any option to limit the memory usage of the process right? Is there any workaround other than restarting the process? If not, I'm concerned that during the restart and the initial reading of the input configurations (5 minutes for me and it's set to that for good reason) it might miss files that were rotated (close_renamed). Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "cef58c75-9aa7-4a53-9763-9d3fea4ae86c",
    "url": "https://discuss.elastic.co/t/auditbeat-only-sending-metrics-not-events/215121",
    "title": "Auditbeat only sending metrics, not events",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "blastic",
    "date": "January 15, 2020, 11:03am",
    "body": "Hello. I'm having the following problems: I'm having machines with Debian 9.11 that run auditd (for managing the audit-rules) and auditbeat (version 7.5.1, for shipping the events off to logstash). Even though the configuration for auditbeat is working and I see metrics sent to logstash (both in the journal of auditbeat as well as in logstash), I don't see any audit-events being processed. My auditbeat.yml is this: --- name: web01 fields_under_root: false queue: mem: events: 4096 flush: min_events: 0 timeout: 0s logging: level: info selectors: :undef to_syslog: false to_eventlog: false json: false to_files: true files: path: \"/var/log/auditbeat\" name: auditbeat keepfiles: 7 rotateeverybytes: 10485760 permissions: '0600' metrics: enabled: true period: 30s output: logstash: hosts: - log01:5000 - log02:5000 - log03:5000 ssl: enabled: true certificate_authorities: - \"/etc/ssl/certs/validcert.pem\" auditbeat: modules: module: auditd enabled: true The only interesting message I see in the logs is: [auditd] auditd/audit_linux.go:216 No audit_rules were specified. Which is correct, I did not specify any rules. But auditbeat seems to acknowledge that there are rules present, according to auditbeat show auditd-rules. Could anyone provide some leads on how to solve this? Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "1fe2c6bf-256c-46c2-9be3-e39a6f536f34",
    "url": "https://discuss.elastic.co/t/failed-to-import-dashboard-failed-to-load-directory/215079",
    "title": "Failed to import dashboard: Failed to load directory",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "Minh_Ti_n_Tr_n",
    "date": "January 15, 2020, 6:29am",
    "body": "I got problem when setup winlogbeat Preformatted textPS C:\\Program Files\\Winlogbeat> .\\winlogbeat.exe setup Index setup finished. Loading dashboards (Kibana must be running and reachable) Exiting: Failed to import dashboard: Failed to load directory C:\\Program Files\\Winlogbeat\\kibana/7/dashboard: error loading C:\\Program Files\\Winlogbeat\\kibana\\7\\dashboard\\Winlogbeat-overview.json: returned 403 to import file: . Response: {\"statusCode\":403,\"error\":\"Forbidden\",\"message\":\"Unable to bulk_create dashboard,visualization\"}Preformatted text My Winlogbeat conf: ###################### Winlogbeat Configuration Example ######################## # This file is an example configuration file highlighting only the most common # options. The winlogbeat.reference.yml file from the same directory contains # all the supported options with more comments. You can use it as a reference. # # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/winlogbeat/index.html #======================= Winlogbeat specific options =========================== # event_logs specifies a list of event logs to monitor as well as any # accompanying options. The YAML data type of event_logs is a list of # dictionaries. # # The supported keys are name (required), tags, fields, fields_under_root, # forwarded, ignore_older, level, event_id, provider, and include_xml. Please # visit the documentation for the complete details of each option. # https://go.es.io/WinlogbeatConfig winlogbeat.event_logs: - name: Application ignore_older: 72h - name: System - name: Security processors: - script: lang: javascript id: security file: ${path.home}/module/security/config/winlogbeat-security.js - name: Microsoft-Windows-Sysmon/Operational processors: - script: lang: javascript id: sysmon file: ${path.home}/module/sysmon/config/winlogbeat-sysmon.js #==================== Elasticsearch template settings ========================== setup.template.settings: index.number_of_shards: 1 #index.codec: best_compression #_source.enabled: false #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. #name: # The tags of the shipper are included in their own field with each # transaction published. #tags: [\"service-X\", \"web-tier\"] # Optional fields that you can specify to add additional information to the # output. #fields: # env: staging #============================== Dashboards ===================================== # These settings control loading the sample dashboards to the Kibana index. Loading # the dashboards is disabled by default and can be enabled either by setting the # options here or by using the `setup` command. #setup.dashboards.enabled: false # The URL from where to download the dashboards archive. By default this URL # has a value which is computed based on the Beat name and version. For released # versions, this URL points to the dashboard archive on the artifacts.elastic.co # website. #setup.dashboards.url: #============================== Kibana ===================================== # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. # This requires a Kibana endpoint configuration. setup.kibana: # Kibana Host # Scheme and port can be left out and will be set to the default (http and 5601) # In case you specify and additional path, the scheme is required: http://localhost:5601/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 host: \"https://192.168.1.17:443\" ssl.verification_mode: none username: \"kibana\" password: \"cw@kibana2020\" # Kibana Space ID # ID of the Kibana Space into which the dashboards should be loaded. By default, # the Default Space will be used. #space.id: #============================= Elastic Cloud ================================== # These settings simplify using Winlogbeat with the Elastic Cloud (https://cloud.elastic.co/). # The cloud.id setting overwrites the `output.elasticsearch.hosts` and # `setup.kibana.host` options. # You can find the `cloud.id` in the Elastic Cloud web UI. #cloud.id: # The cloud.auth setting overwrites the `output.elasticsearch.username` and # `output.elasticsearch.password` settings. The format is `<user>:<pass>`. #cloud.auth: #================================ Outputs ===================================== # Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"https://192.168.1.17:9200\"] #ssl.certificate_authorities: [\"/etc/elasticsearch/cert/ca.crt\"] #ssl.certificate: \"/etc/elasticsearch/cert/node-0.crt\" #ssl.key: \"/etc/elasticsearch/cert/node-0.key\" # Optional protocol and basic auth credentials. ssl.verification_mode: none protocol: \"https\" username: \"elastic\" password: \"cw@elastic2020\" #----------------------------- Logstash output -------------------------------- #output.logstash: # The Logstash hosts #hosts: [\"localhost:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" #================================ Processors ===================================== # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] #============================== X-Pack Monitoring =============================== # winlogbeat can export internal metrics to a central Elasticsearch monitoring # cluster. This requires xpack monitoring to be enabled in Elasticsearch. The # reporting is disabled by default. # Set to true to enable the monitoring reporter. #monitoring.enabled: false # Sets the UUID of the Elasticsearch cluster under which monitoring data for this # Winlogbeat instance will appear in the Stack Monitoring UI. If output.elasticsearch # is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch. #monitoring.cluster_uuid: # Uncomment to send the metrics to Elasticsearch. Most settings from the # Elasticsearch output are accepted here as well. # Note that the settings should point to your Elasticsearch *monitoring* cluster. # Any setting that is not set is automatically inherited from the Elasticsearch # output configuration, so if you have the Elasticsearch output configured such # that it is pointing to your Elasticsearch monitoring cluster, you can simply # uncomment the following line. #monitoring.elasticsearch: #================================= Migration ================================== # This allows to enable 6.7 migration aliases #migration.6_to_7.enabled: true Any idea to solve this problem?? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "4b411670-7f0f-487e-8b96-03564728e713",
    "url": "https://discuss.elastic.co/t/about-the-filebeat-input/215065",
    "title": "About the filebeat input",
    "category": [
      "Beats"
    ],
    "author": "eunnam",
    "date": "January 15, 2020, 2:36am",
    "body": "Hello i'm using filebeat in aws ec2 and filebeat input type is s3. filebeat.inputs: - type: s3 queue_url: \"aws sqs queue url\" If aws sqs queue is received message of aws s3 create, filebeat get file of gz format using s3 url in aws sqs queue message. we can use multiline in filebeat? Thank you for reply.",
    "website_area": "discuss"
  },
  {
    "id": "b9990f38-6d53-403c-9fac-a81df025679d",
    "url": "https://discuss.elastic.co/t/bug-report-filebeat-cant-finish-if-the-log-file-isn-t-ending-with-line-terminator-like-n/214835",
    "title": "[bug report]: filebeat can't finish ,if the log file isnt ending with line terminator(like \\n)",
    "category": [
      "Beats"
    ],
    "author": "chenyahui",
    "date": "January 13, 2020, 1:26pm January 14, 2020, 7:12pm January 14, 2020, 2:15am January 14, 2020, 7:13pm January 15, 2020, 2:17am",
    "body": "if a log file isnt ending with line terminator(like \\n), the last line can't be read by harvester. the harvester of the file will not finished filebeat version [7.5]",
    "website_area": "discuss"
  },
  {
    "id": "be84e02b-97f0-4c6f-8501-38d403c66bd2",
    "url": "https://discuss.elastic.co/t/send-filebeat-logs-through-logstash-to-different-indexes/214753",
    "title": "Send filebeat logs through logstash to different indexes",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Immac",
    "date": "January 13, 2020, 4:05am January 14, 2020, 5:47pm January 14, 2020, 1:20am January 14, 2020, 5:47pm January 14, 2020, 5:47pm",
    "body": "Hi Folks, Besides the standard modules( system, auditd etc), I have to send a custom logs from one server to elasticsearch. I am thinking of sending the logs to logstash first so that I can do some grok processing for this custom logs How can I differentiate the logs( by adding tags etc) so that default modules log go to one index whereas other logs go to different indexes Looking for some guidance. Thanks in advance Immac",
    "website_area": "discuss"
  },
  {
    "id": "dc9af539-7a00-4e55-9d31-f49ccbb21f0a",
    "url": "https://discuss.elastic.co/t/filebeat-7-5-suricata-module-and-elastic-cloud-ingest-pipeline-issue/215017",
    "title": "Filebeat 7.5 - Suricata module and Elastic cloud ingest pipeline issue",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "flxdan",
    "date": "January 14, 2020, 4:32pm",
    "body": "Running Ubuntu 18.04 running Suricata 5.0.1 - I've tried to use the filebeat (7.5.0 and 7.5.1) Suricata module on Elastic cloud but running setup on filebeat keeps giving me the following error: 2020-01-14T15:25:00.230Z INFO elasticsearch/client.go:753 Attempting to connect to Elasticsearch version 7.5.0 2020-01-14T15:25:00.285Z ERROR fileset/setup.go:75 Error loading pipeline: 1 error: Error loading pipeline for fileset suricata/eve: couldn't load pipeline: couldn't load json. Error: 500 Internal Server Error: {\"error\":{\"root_cause\":[{\"type\":\"circuit_breaking_exception\",\"reason\":\"[script] Too many dynamic script compilations within, max: [75/5m]; please use indexed, or scripts with parameters instead; this limit can be changed by the [script.max_compilations_rate] setting\",\"bytes_wanted\":0,\"bytes_limit\":0,\"durability\":\"TRANSIENT\"}],\"type\":\"general_script_exception\",\"reason\":\"Failed to compile inline script [long getOrZero(def map, def key) {\\n if (map!=null && map[key]!=null) {\\n return map[key];\\n }\\n return 0;\\n} def network=ctx['network'], source=ctx['source'], dest=ctx['destination']; def sp=getOrZero(source,'packets'), sb=getOrZero(source,'bytes'), dp=getOrZero(dest,'packets'), db=getOrZero(dest,'bytes'); if (sb+db+sp+dp > 0) {\\n if (network == null) {\\n network=new HashMap();\\n ctx['network']=network;\\n }\\n if (sb+db > 0) {\\n network['bytes'] = sb+db;\\n }\\n if(sp+dp>0) {\\n network['packets'] = sp+dp;\\n }\\n}\\n] using lang [painless]\",\"processor_type\":\"script\",\"caused_by\":{\"type\":\"circuit_breaking_exception\",\"reason\":\"[script] Too many dynamic script compilations within, max: [75/5m]; please use indexed, or scripts with parameters instead; this limit can be changed by the [script.max_compilations_rate] setting\",\"bytes_wanted\":0,\"bytes_limit\":0,\"durability\":\"TRANSIENT\"},\"suppressed\":[{\"type\":\"general_script_exception\",\"reason\":\"Failed to compile inline script [Instant ins(def d) {\\n try {\\n return Instant.parse(d);\\n } catch(Exception e) {\\n return null;\\n }\\n} def ev = ctx['event']; if (ev != null) {\\n def start = ins(ev['start']);\\n def end = ins(ev['end']);\\n if (start != null && end != null && !start.isAfter(end)) {\\n ev['duration'] = Duration.between(start,end).toNanos();\\n }\\n}\\n] using lang [painless]\",\"processor_type\":\"script\",\"caused_by\":{\"type\":\"circuit_breaking_exception\",\"reason\":\"[script] Too many dynamic script compilations within, max: [75/5m]; please use indexed, or scripts with parameters instead; this limit can be changed by the [script.max_compilations_rate] setting\",\"bytes_wanted\":0,\"bytes_limit\":0,\"durability\":\"TRANSIENT\"}},{\"type\":\"general_script_exception\",\"reason\":\"Failed to compile inline script [def t = ctx.suricata?.eve?.event_type; if (t == \"stats\") {\\n ctx['event']['kind'] = \"metric\";\\n} else if (t == \"alert\") {\\n ctx['event']['kind'] = \"alert\";\\n ctx['event']['category'] = \"network_traffic\";\\n} else {\\n ctx['event']['kind'] = \"event\";\\n ctx['event']['category'] = \"network_traffic\";\\n}\\n] using lang [painless]\",\"processor_type\":\"script\",\"caused_by\":{\"type\":\"circuit_breaking_exception\",\"reason\":\"[script] Too many dynamic script compilations within, max: [75/5m]; please use indexed, or scripts with parameters instead; this limit can be changed by the [script.max_compilations_rate] setting\",\"bytes_wanted\":0,\"bytes_limit\":0,\"durability\":\"TRANSIENT\"}}]},\"status\":500}. Response body: {\"error\":{\"root_cause\":[{\"type\":\"circuit_breaking_exception\",\"reason\":\"[script] Too many dynamic script compilations within, max: [75/5m]; please use indexed, or scripts with parameters instead; this limit can be changed by the [script.max_compilations_rate] setting\",\"bytes_wanted\":0,\"bytes_limit\":0,\"durability\":\"TRANSIENT\"}],\"type\":\"general_script_exception\",\"reason\":\"Failed to compile inline script [long getOrZero(def map, def key) {\\n if (map!=null && map[key]!=null) {\\n return map[key];\\n }\\n return 0;\\n} def network=ctx['network'], source=ctx['source'], dest=ctx['destination']; def sp=getOrZero(source,'packets'), sb=getOrZero(source,'bytes'), dp=getOrZero(dest,'packets'), db=getOrZero(dest,'bytes'); if (sb+db+sp+dp > 0) {\\n if (network == null) {\\n network=new HashMap();\\n ctx['network']=network;\\n }\\n if (sb+db > 0) {\\n network['bytes'] = sb+db;\\n }\\n if(sp+dp>0) {\\n network['packets'] = sp+dp;\\n }\\n}\\n] using lang [painless]\",\"processor_type\":\"script\",\"caused_by\":{\"type\":\"circuit_breaking_exception\",\"reason\":\"[script] Too many dynamic script compilations within, max: [75/5m]; please use indexed, or scripts with parameters instead; this limit can be changed by the [script.max_compilations_rate] setting\",\"bytes_wanted\":0,\"bytes_limit\":0,\"durability\":\"TRANSIENT\"},\"suppressed\":[{\"type\":\"general_script_exception\",\"reason\":\"Failed to compile inline script [Instant ins(def d) {\\n try {\\n return Instant.parse(d);\\n } catch(Exception e) {\\n return null;\\n }\\n} def ev = ctx['event']; if (ev != null) {\\n def start = ins(ev['start']);\\n def end = ins(ev['end']);\\n if (start != null && end != null && !start.isAfter(end)) {\\n ev['duration'] = Duration.between(start,end).toNanos();\\n }\\n}\\n] using lang [painless]\",\"processor_type\":\"script\",\"caused_by\":{\"type\":\"circuit_breaking_exception\",\"reason\":\"[script] Too many dynamic script compilations within, max: [75/5m]; please use indexed, or scripts with parameters instead; this limit can be changed by the [script.max_compilations_rate] setting\",\"bytes_wanted\":0,\"bytes_limit\":0,\"durability\":\"TRANSIENT\"}},{\"type\":\"general_script_exception\",\"reason\":\"Failed to compile inline script [def t = ctx.suricata?.eve?.event_type; if (t == \"stats\") {\\n ctx['event']['kind'] = \"metric\";\\n} else if (t == \"alert\") {\\n ctx['event']['kind'] = \"alert\";\\n ctx['event']['category'] = \"network_traffic\";\\n} else {\\n ctx['event']['kind'] = \"event\";\\n ctx['event']['category'] = \"network_traffic\";\\n}\\n] using lang [painless]\",\"processor_type\":\"script\",\"caused_by\":{\"type\":\"circuit_breaking_exception\",\"reason\":\"[script] Too many dynamic script compilations within, max: [75/5m]; please use indexed, or scripts with parameters instead; this limit can be changed by the [script.max_compilations_rate] setting\",\"bytes_wanted\":0,\"bytes_limit\":0,\"durability\":\"TRANSIENT\"}}]},\"status\":500} 2020-01-14T15:25:00.285Z INFO cfgfile/reload.go:264 Loading of config files completed Any ideas?",
    "website_area": "discuss"
  },
  {
    "id": "c5b0e293-7dec-4d98-a0fd-39d916619711",
    "url": "https://discuss.elastic.co/t/netflow-protocol-version-0-not-supported/214971",
    "title": "Netflow protocol version 0 not supported",
    "category": [
      "Beats"
    ],
    "author": "w33ha-zxc",
    "date": "January 14, 2020, 10:14am January 14, 2020, 3:58pm",
    "body": "Have been trying to ingest Netflow Data from a Fortigate Firewall through Filebeat, but upon running filebeat -e gets me to the following warning - netflow/input.go:240 Error parsing NetFlow packet of length 220 from 192.168.1.10:4216: netflow protocol version 0 not supported Below excerpt from my config file - module: netflow log: enabled: true var: netflow_host: 192.168.0.154 netflow_port: 2055 protocols: [ v9 ]",
    "website_area": "discuss"
  },
  {
    "id": "c6e74ab4-2fa6-4f15-9f5f-42af295df2d1",
    "url": "https://discuss.elastic.co/t/url-category/214983",
    "title": "URL category",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "ely_bainto",
    "date": "January 14, 2020, 11:21am",
    "body": "Is there any way to categorize url.domain? (e.g. sports, entertainment, etc.)",
    "website_area": "discuss"
  },
  {
    "id": "f0d979c1-236d-4f1f-99f5-32838002830d",
    "url": "https://discuss.elastic.co/t/winlogbeat-not-picking-up-all-application-logs/214978",
    "title": "Winlogbeat not picking up all Application Logs",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "ted1621",
    "date": "January 14, 2020, 11:02am",
    "body": "Under the Windows Application Logs I have a subfolder called DigitalPersona. I need to ship those logs to Kibana. Right now my winlogbeat.yml is configured as such: winlogbeat.event_logs: name: Application ignore_older: 72h What do I need to add to this in order for winlogbeat to pick up the DigitalPersona logs?",
    "website_area": "discuss"
  },
  {
    "id": "eeac7e62-0b44-4868-8e55-a2eb2f1168b1",
    "url": "https://discuss.elastic.co/t/cisco-module-arbitrary-parse-error-with-nearly-identical-messages/214884",
    "title": "Cisco Module arbitrary parse error with nearly identical messages",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 13, 2020, 5:53pm January 13, 2020, 8:19pm January 14, 2020, 8:51am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e710bb4c-6b48-4612-ad44-1fbe675def30",
    "url": "https://discuss.elastic.co/t/credentials-variables-blocking-beats-from-running-as-services-on-windows/214943",
    "title": "Credentials' variables blocking beats from running as services on Windows",
    "category": [
      "Beats"
    ],
    "author": "jsu",
    "date": "January 14, 2020, 8:11am",
    "body": "Hey there, So, the problem doesn't appear on linux boxes, but only windows ones : win server 2012 and win10. I had the (almost) exact same problem with Winlogbeat, Metricbeat and Filebeat. Following the documentation concerning how to configure the output with security activated and how to do it securely with a keystore, I added the following lines to the config file: output.elasticsearch: hosts: [\"myEShost:9200\"] username: \"${ES_USER}\" password: \"${ES_PWD}\" Then I actually store those variables in the keystore of the beat. Trying to Start-Service [beat_name] doesn't work, it gives the following error: Windows could not start the winlogbeat service on Local Computer. Error 1053: The service did not respond to the start or control request in a timely fashion. But if I try to run the beats in the foreground with winlogbeat -e -d \"*\", there's no error and everything runs fine for winlogbeat and filebeat. Metricbeat also seems to run fine with no error, but it stops sending data after a few minutes (still, without any error and seeming to run fine). If I replace the variables in the config file by the actual usernames and passwords, like this: output.elasticsearch: hosts: [\"myEShost:9200\"] username: \"actual_username\" password: \"actual_password\" I can run all the Beats as Services and there's no problem at all. Am I missing something in the documentation ? Or am I doing something else wrong ? Looking forward to hearing from you as I really don't know where to go from here. Thanks in advance ! Cheers, jsu",
    "website_area": "discuss"
  },
  {
    "id": "62e994a1-5fef-4957-b054-4ee3ba1738ac",
    "url": "https://discuss.elastic.co/t/how-to-fetch-all-the-fields-in-decode-json-fields-instead-of-specifying-all-the-fields-inside/213447",
    "title": "How to fetch all the fields in decode_json_fields instead of specifying all the fields inside",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Viswanath_Lekshman",
    "date": "December 31, 2019, 12:12pm January 3, 2020, 7:06pm January 10, 2020, 11:14pm January 14, 2020, 8:05am",
    "body": "When i'm using decode_json_fields in Filebeat. I'm unable to specify all fields in a json Below is my sample json {\"message\":{\"name\": \"Viswanath\",\"cancel\":1,\"description\":\"Hey\"},\"level\":\"info\"} When i use the below configuration processors: decode_json_fields: fields: [message] process_array: false max_depth: 1 target: \"\" overwrite_keys: false add_error_key: true json value of \"message\" appears as string in elasticsearch index. (Logstash is not filtering any data, bypassing for now) Is there any way to pass the data inside \"message\" key as json without name of the fields individually",
    "website_area": "discuss"
  },
  {
    "id": "f43ed5b1-fbde-4371-9172-ddc287c4941f",
    "url": "https://discuss.elastic.co/t/winlogbeat-service-wont-start-server-2016/211374",
    "title": "Winlogbeat service won't start - Server 2016",
    "category": [
      "Beats"
    ],
    "author": "uklipse",
    "date": "December 10, 2019, 8:47pm December 11, 2019, 3:05pm December 11, 2019, 3:52pm December 11, 2019, 3:57pm December 11, 2019, 4:16pm December 11, 2019, 4:18pm December 11, 2019, 4:31pm December 12, 2019, 10:22am December 12, 2019, 4:42pm January 2, 2020, 8:15pm",
    "body": "I'm trying to install winlogbeat on a Server 2016 host but the service won't start. I've tested the config file using: winlogbeat.exe test config and it comes back OK. I've tried running it in the foreground using winlogbeat.exe -c winlogbeat.yml -e -v -d \"*\" and get some results in Kibana. The same install files are used on Win10 machines and they install and services starts correctly. We use an older version of winlogbeat but I've downloaded the newest one (7.5) and get the same result of service not starting. Anything else I can try or is there something different with Server 2016?",
    "website_area": "discuss"
  },
  {
    "id": "02ec624d-c3cb-41ad-a037-b6c1369dce61",
    "url": "https://discuss.elastic.co/t/multiline-codec-with-timestamp-in-between-log-message/214691",
    "title": "Multiline codec with Timestamp in between log message",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Saravana37",
    "date": "January 11, 2020, 6:32am January 14, 2020, 7:16am January 14, 2020, 4:59am January 14, 2020, 6:34am January 14, 2020, 7:16am",
    "body": "Hi , My Sample looks as follows. ObjMgrBusCompLog Create 4 000010c25c361790:0 2020-01-11 05:58:14 Begin: construct BusComp \"Employee\" at 18d25dc8 ObjMgrBusCompLog Create 4 000010c25c361790:0 2020-01-11 05:58:14 End: construct BusComp \"Employee\" at 18d25dc8 ObjMgrSqlLog Detail 4 000010c25c361790:0 2020-01-11 05:58:14 SELECT statement with ID: 193DF750 SELECT T6.CONFLICT_ID, T6.LAST_UPD, T6.CREATED, T6.LAST_UPD_BY, T6.CREATED_BY, T6.MODIFICATION_NUM, T6.ROW_ID, T6.ROW_ID, T14.AGENT_FLG, T12.CTI_ACD_USERID, T12.X_MARKET, T12.X_PREF_BRKDN_CNTRY, T12.X_PREF_CASE_TYPE, T12.X_PREF_COUNTRY, T12.X_PREF_CUST_LANGUAGE, T2.NAME, T12.X_PREF_PDQ_GROUP, T12.X_PREF_SERVICE_TYPE, T13.CURR_PRD_OBJ_NAME, T14.EMP_FLG, T8.EMPLOYEE_TYPE_CD, T10.SCHED_ENGINE_CD, T14.FAX_PH_NUM, T14.FST_NAME, T14.LAST_NAME, T12.LOGIN, T14.MEMBER_FLG, T14.MID_NAME, T14.NEW_USER_RESP_NAME, T6.NAME, T14.BU_ID, T3.BU_ID, T14.PR_SYNC_USER_ID, T14.PR_PER_ADDR_ID, T14.PR_HELD_POSTN_ID, T14.PR_RESP_ID, T14.PR_POSTN_ID, T14.PROVIDER_FLG, T9.OWN_INST_ID, T9.INTEGRATION_ID, T8.SALARY_GRADE_ID, T8.SALARY_PLAN_ID, T10.TIME_ZONE_ID, T8.CUR_WRK_SHFT_ID, T14.PR_DEPT_OU_ID, T8.SYNC_SUCCESS_FLG, T14.TIMEZONE_ID, T7.NAME, T12.X_IWS_USERNAME, T12.X_IWS_FLAG, T5.NAME, T11.LOGIN, T1.NAME, T12.ROW_ID, T12.PAR_ROW_ID, T12.MODIFICATION_NUM, T12.CREATED_BY, T12.LAST_UPD_BY, T12.CREATED, T12.LAST_UPD, T12.CONFLICT_ID, T12.PAR_ROW_ID, T14.ROW_ID, T14.PAR_ROW_ID, T14.MODIFICATION_NUM, T14.CREATED_BY, T14.LAST_UPD_BY, T14.CREATED, T14.LAST_UPD, T14.CONFLICT_ID, T14.PAR_ROW_ID, T8.ROW_ID, T8.PAR_ROW_ID, T8.MODIFICATION_NUM, T8.CREATED_BY, T8.LAST_UPD_BY, T8.CREATED, T8.LAST_UPD, T8.CONFLICT_ID, T8.PAR_ROW_ID, T9.ROW_ID, T9.PAR_ROW_ID, T9.MODIFICATION_NUM, T9.CREATED_BY, T9.LAST_UPD_BY, T9.CREATED, T9.LAST_UPD, T9.CONFLICT_ID, T9.PAR_ROW_ID, T4.ROW_ID, T1.ROW_ID FROM SIEBEL.S_RESP T1, SIEBEL.S_LANG T2, SIEBEL.S_POSTN T3, SIEBEL.S_PARTY T4, SIEBEL.S_TIMEZONE_LANG T5, SIEBEL.S_PARTY T6, SIEBEL.S_TIMEZONE T7, SIEBEL.S_EMP_PER T8, SIEBEL.S_CONTACT_SS T9, SIEBEL.S_SRV_REGN T10, SIEBEL.S_USER T11, SIEBEL.S_USER T12, SIEBEL.S_BU T13, SIEBEL.S_CONTACT T14 WHERE T14.PR_HELD_POSTN_ID = T3.PAR_ROW_ID (+) AND T3.BU_ID = T13.ROW_ID (+) AND T12.X_PREF_CUST_LANGUAGE = T2.LANG_CD (+) AND T8.SRV_REGN_ID = T10.ROW_ID (+) AND T14.TIMEZONE_ID = T7.ROW_ID (+) AND T7.ROW_ID = T5.PAR_ROW_ID (+) AND T5.LANG_ID (+) = :1 AND T6.ROW_ID = T12.PAR_ROW_ID AND T6.ROW_ID = T14.PAR_ROW_ID AND T6.ROW_ID = T8.PAR_ROW_ID AND T6.ROW_ID = T9.PAR_ROW_ID (+) AND T14.PR_SYNC_USER_ID = T4.ROW_ID (+) AND T14.PR_SYNC_USER_ID = T11.PAR_ROW_ID (+) AND T14.PR_RESP_ID = T1.ROW_ID (+) AND (T14.EMP_FLG = 'Y') AND (T6.ROW_ID = :2) ORDER BY T14.EMP_FLG, T14.LAST_NAME, T14.FST_NAME ObjMgrSqlLog Detail 4 000010c25c361790:0 2020-01-11 05:58:14 Bind variable 1: ENU ObjMgrSqlLog Detail 4 000010c25c361790:0 2020-01-11 05:58:14 Bind variable 2: 1-1IXAY8D ObjMgrSqlLog Detail 4 000010c25c361790:0 2020-01-11 05:58:14 ***** SQL Statement Prepare Time for SQL Cursor with ID 193DF750: 0.000 seconds ***** ` I want to tell filebeat that where ever it finds a Timestamp in the message (in middle of message) , should consider the whole line as one single event . For example below multiline should consider as single event. ObjMgrSqlLog Detail 4 000010c25c361790:0 2020-01-11 05:58:14 SELECT statement with ID: 193DF750` SELECT T6.CONFLICT_ID, T6.LAST_UPD, T6.CREATED, T6.LAST_UPD_BY, T6.CREATED_BY, T6.MODIFICATION_NUM, T6.ROW_ID, T6.ROW_ID, T14.AGENT_FLG, T12.CTI_ACD_USERID, T12.X_MARKET, T12.X_PREF_BRKDN_CNTRY, T12.X_PREF_CASE_TYPE, T12.X_PREF_COUNTRY, T12.X_PREF_CUST_LANGUAGE, T2.NAME, T12.X_PREF_PDQ_GROUP, T12.X_PREF_SERVICE_TYPE, T13.CURR_PRD_OBJ_NAME, T14.EMP_FLG, T8.EMPLOYEE_TYPE_CD, T10.SCHED_ENGINE_CD, T14.FAX_PH_NUM, T14.FST_NAME, T14.LAST_NAME, T12.LOGIN, T14.MEMBER_FLG, T14.MID_NAME, T14.NEW_USER_RESP_NAME, T6.NAME, T14.BU_ID, T3.BU_ID, T14.PR_SYNC_USER_ID, T14.PR_PER_ADDR_ID, T14.PR_HELD_POSTN_ID, T14.PR_RESP_ID, T14.PR_POSTN_ID, T14.PROVIDER_FLG, T9.OWN_INST_ID, T9.INTEGRATION_ID, T8.SALARY_GRADE_ID, T8.SALARY_PLAN_ID, T10.TIME_ZONE_ID, T8.CUR_WRK_SHFT_ID, T14.PR_DEPT_OU_ID, T8.SYNC_SUCCESS_FLG, T14.TIMEZONE_ID, T7.NAME, T12.X_IWS_USERNAME, T12.X_IWS_FLAG, T5.NAME, T11.LOGIN, T1.NAME, T12.ROW_ID, T12.PAR_ROW_ID, T12.MODIFICATION_NUM, T12.CREATED_BY, T12.LAST_UPD_BY, T12.CREATED, T12.LAST_UPD, T12.CONFLICT_ID, T12.PAR_ROW_ID, T14.ROW_ID, T14.PAR_ROW_ID, T14.MODIFICATION_NUM, T14.CREATED_BY, T14.LAST_UPD_BY, T14.CREATED, T14.LAST_UPD, T14.CONFLICT_ID, T14.PAR_ROW_ID, T8.ROW_ID, T8.PAR_ROW_ID, T8.MODIFICATION_NUM, T8.CREATED_BY, T8.LAST_UPD_BY, T8.CREATED, T8.LAST_UPD, T8.CONFLICT_ID, T8.PAR_ROW_ID, T9.ROW_ID, T9.PAR_ROW_ID, T9.MODIFICATION_NUM, T9.CREATED_BY, T9.LAST_UPD_BY, T9.CREATED, T9.LAST_UPD, T9.CONFLICT_ID, T9.PAR_ROW_ID, T4.ROW_ID, T1.ROW_ID FROM SIEBEL.S_RESP T1, SIEBEL.S_LANG T2, SIEBEL.S_POSTN T3, SIEBEL.S_PARTY T4, SIEBEL.S_TIMEZONE_LANG T5, SIEBEL.S_PARTY T6, SIEBEL.S_TIMEZONE T7, SIEBEL.S_EMP_PER T8, SIEBEL.S_CONTACT_SS T9, SIEBEL.S_SRV_REGN T10, SIEBEL.S_USER T11, SIEBEL.S_USER T12, SIEBEL.S_BU T13, SIEBEL.S_CONTACT T14 WHERE T14.PR_HELD_POSTN_ID = T3.PAR_ROW_ID (+) AND T3.BU_ID = T13.ROW_ID (+) AND T12.X_PREF_CUST_LANGUAGE = T2.LANG_CD (+) AND T8.SRV_REGN_ID = T10.ROW_ID (+) AND T14.TIMEZONE_ID = T7.ROW_ID (+) AND T7.ROW_ID = T5.PAR_ROW_ID (+) AND T5.LANG_ID (+) = :1 AND T6.ROW_ID = T12.PAR_ROW_ID AND T6.ROW_ID = T14.PAR_ROW_ID AND T6.ROW_ID = T8.PAR_ROW_ID AND T6.ROW_ID = T9.PAR_ROW_ID (+) AND T14.PR_SYNC_USER_ID = T4.ROW_ID (+) AND T14.PR_SYNC_USER_ID = T11.PAR_ROW_ID (+) AND T14.PR_RESP_ID = T1.ROW_ID (+) AND (T14.EMP_FLG = 'Y') AND (T6.ROW_ID = :2) ORDER BY T14.EMP_FLG, T14.LAST_NAME, T14.FST_NAME I can see examples for Timestamp in Other forums as start date of Time stamp ^ Timestamp , But in My log time stamp is in middle , How can i specify this ? Any suggestions please ?",
    "website_area": "discuss"
  },
  {
    "id": "fa33e5e0-1fbf-4dd2-86ba-22ec96fff42d",
    "url": "https://discuss.elastic.co/t/wrong-timestamp-not-timezone-issue-with-system-module/214092",
    "title": "Wrong timestamp (NOT timezone issue) with System module",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "thomas.whc",
    "date": "January 7, 2020, 3:47pm January 13, 2020, 10:50pm January 9, 2020, 9:23pm January 13, 2020, 10:50pm",
    "body": "Hi team and thank you for your work. I am very new to ES and I am trying to build a simple stack with 1 ES, 1 KB and 2 filebeats (launched with binaries) all v7.5. I am not using LogStash but the prepacked modules of Filebeat. Things were going well before new year but as I come back to the office, I notice some new documents are indexed for the end of the year of 2020. After investigation, I can see that only System events are concerned. In fact, any event in the rotated log files (for example I have /var/log/messages-20191229) that has not been indexed, is now indexed as in 2020. As an example I have in this file: Dec 22 04:34:59 my-system kernel: Firewall: *UDP_IN Blocked* IN=eth0 OUT= MAC=xx:...:xx SRC=xxx.xxx.xxx.xxx DST=xxx.xxx.xxx.xxx LEN=1100 TOS=0x00 PREC=0x00 TTL=42 ID=54955 DF PROTO=UDP SPT=60569 DPT=443 LEN=1080 and in Kibana: { \"_index\": \"filebeat-7.5.1-2020.01.06-000001\", \"_type\": \"_doc\", \"_id\": \"ceeSgG8BDy804je_mJfq\", \"_version\": 1, \"_score\": null, \"_source\": { \"agent\": { \"hostname\": \"my-system.tld\", \"name\": \"my-system\", \"id\": \"d77c284b-dfd8-4d26-ab82-1fad8bc6ce6f\", \"type\": \"filebeat\", \"ephemeral_id\": \"0f8e7686-b861-4b60-8855-5b4ddcd6c5f6\", \"version\": \"7.5.1\" }, \"process\": { \"name\": \"kernel\" }, \"log\": { \"file\": { \"path\": \"/var/log/messages-20191229\" }, \"offset\": 1499167 }, \"fileset\": { \"name\": \"syslog\" }, \"message\": \"Firewall: *UDP_IN Blocked* IN=eth0 OUT= MAC=xx:...:xx SRC=xxx.xxx.xxx.xxx DST=xxx.xxx.xxx.xxx LEN=1100 TOS=0x00 PREC=0x00 TTL=42 ID=54955 DF PROTO=UDP SPT=60569 DPT=443 LEN=1080 \", \"input\": { \"type\": \"log\" }, \"@timestamp\": \"2020-12-22T04:34:59.000-05:00\", \"system\": { \"syslog\": {} }, \"ecs\": { \"version\": \"1.1.0\" }, \"service\": { \"type\": \"system\" }, \"host\": { \"hostname\": \"my-system\", \"os\": { \"kernel\": \"3.10.0-962.3.2.lve1.5.26.7.el7.x86_64\", \"name\": \"CloudLinux\", \"family\": \"\", \"version\": \"7.7 (Valery Bykovsky)\", \"platform\": \"cloudlinux\" }, \"containerized\": false, \"name\": \"my-system\", \"id\": \"a195f221b91edcc1ff7319c05c12b888\", \"architecture\": \"x86_64\" }, \"event\": { \"timezone\": \"-05:00\", \"module\": \"system\", \"dataset\": \"system.syslog\" } }, \"fields\": { \"suricata.eve.timestamp\": [ \"2020-12-22T09:34:59.000Z\" ], \"@timestamp\": [ \"2020-12-22T09:34:59.000Z\" ] }, \"sort\": [ 1608629699000 ] } I made some tests and reload the content of any rotated log file before 2020 by doing: delete documents with timestamp greater than today in ES delete any pipeline set file offset to 0 in data/registry/filebeat/data.json for appropriate files ...but still, all of these files' content (messages-201912* and secure-201912*) is indexed as end of year 2020. In short, as long as I indexed their content in 2019 everything was going well, but now we are in 2020 I can't index them without having wrong timestamp. Please forgive my poor english Any help appreciated. Thomas",
    "website_area": "discuss"
  },
  {
    "id": "64ccfb4f-894a-4632-a282-c8a2927e2d28",
    "url": "https://discuss.elastic.co/t/metricbeat-to-collect-metrics-from-kerberised-kafka-sslca-configuration/214491",
    "title": "Metricbeat to collect metrics from Kerberised Kafka + SSLCa configuration",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "lskaa",
    "date": "January 9, 2020, 9:03pm January 10, 2020, 12:20pm January 10, 2020, 12:28pm January 12, 2020, 7:29pm January 13, 2020, 8:48am January 13, 2020, 9:07pm",
    "body": "Hi All, May I know if anyone has experience to configure Metricbeat to collect metrics from Kerberised Kafka? Here is my kafka.yml. I also add Read and Describe operations to my Kafka instances for a user stats. module: kafka metricsets: partition consumergroup period: 10s hosts: broker1:9092 broker2:9092 broker3:9092 client_id: stats According to the log, it connected to the brokers seemingly. 2020-01-10T09:28:57.157+1300 INFO kafka/log.go:53 Connected to broker at broker1:9092 (unregistered) 2020-01-10T09:28:57.390+1300 INFO kafka/log.go:53 Closed connection to broker broker1:9092 However, it failed to fetch data for the partition and the consumergroup. 2020-01-10T09:28:57.672+1300 INFO module/wrapper.go:252 Error fetching data for metricset kafka.partition: error in connect: failed to query metadata: EOF 2020-01-10T09:28:57.672+1300 INFO module/wrapper.go:252 Error fetching data for metricset kafka.consumergroup: error in connect: failed to query metadata: EOF Thank you",
    "website_area": "discuss"
  },
  {
    "id": "0b390dac-43b8-4bcf-94b3-95bffd3e74d3",
    "url": "https://discuss.elastic.co/t/filbeat-with-docker-compose/214728",
    "title": "Filbeat with docker compose",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Haritz_Saiz",
    "date": "January 12, 2020, 12:27pm January 13, 2020, 6:38pm",
    "body": "Hello everyone I am trying to send the generated logs by cowrie to logstash: Here is my docker compose: version: '3' volumes: cowrie-etc: driver: local cowrie-var: driver: local services: cowrie: container_name: cowrie restart: always build: context: . dockerfile: ./cowrie/Dockerfile ports: - \"2222:2222\" - \"2223:2223\" volumes: - cowrie-etc:/cowrie/cowrie-git/etc - cowrie-var:/cowrie/cowrie-git/var/log/cowrie filebeat: image: docker.elastic.co/beats/filebeat:7.5.1 container_name: filebeat user: root volumes: - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro - cowrie-var:/data/cowrie:ro - /home/ubuntu/demo/:/data/cowrie:ro environment: - LOGSTASH_HOST='${LOGSTASH_HOST}' command: [\"--strict.perms=false\"] networks: - docker_elk networks: docker_elk: driver: bridge And this is my filebeat.yml config file: filebeat.inputs: - type: log enabled: true paths: - /data/cowrie/*.log - /data/cowrie/*.json* output: #logstash: # hosts: [\"${LOGSTASH_HOST}\"] # bulk_max_size: 1024 # username: \"elastic\" # password: \"password\" console: pretty: true When I run the containers, filebeat doesnt seam to work as it doesnt print any log. Dont have any clue of why it is not working. The container is up. Hope anyone can help me.",
    "website_area": "discuss"
  },
  {
    "id": "777bdc29-c23f-4e42-ba8f-01d258fcfe1d",
    "url": "https://discuss.elastic.co/t/heartbeat-indexing-not-utc/214706",
    "title": "Heartbeat indexing not UTC",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "richard_N",
    "date": "January 11, 2020, 3:33pm January 13, 2020, 6:18pm",
    "body": "Logstash indexes UTC, im +5hours UTC so at 8pm every night a new index is started for the next day. Im also using Grafana which is UTC, after 8 it's looking for the next day pattern but the problem is heartbeat isn't UTC by default when indexing straight to elastic. How do I get heartbeat to write UTC and roll the indices like logstash does?",
    "website_area": "discuss"
  },
  {
    "id": "608642bb-eabc-4892-bb96-e8dd1bca6a9d",
    "url": "https://discuss.elastic.co/t/can-i-use-iam-role-as-credentials/214369",
    "title": "Can i use IAM role as credentials?",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Samerd",
    "date": "January 9, 2020, 8:32am January 13, 2020, 6:02pm",
    "body": "Hi , Can we use IAM role instead of AWS credentials (without Access key and Secret Key that we use in metricbeat config file and without AWS token) ? thanks",
    "website_area": "discuss"
  },
  {
    "id": "5ddbe0d0-3fdc-40a6-90e8-2a097deb5a30",
    "url": "https://discuss.elastic.co/t/topic/214824",
    "title": "  ,   ,  ",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "castorlilasfroid",
    "date": "January 13, 2020, 12:06pm",
    "body": " .  ELK    Active Directory.   -   4624. ,   1      4624      winlog.event_data.LogonGuid,      .   ,     winlog.event_data.LogonGuid  .   ,     : \"aggs\" : { \"logins\" : { \"terms\" : { \"field\" : \"winlog.event_data.LogonGuid\" } } }",
    "website_area": "discuss"
  },
  {
    "id": "040fdcf1-58f5-438c-94c3-6846e811d849",
    "url": "https://discuss.elastic.co/t/filebeats-ilm-permission-error/214617",
    "title": "Filebeats ILM permission error",
    "category": [
      "Beats"
    ],
    "author": "",
    "date": "January 10, 2020, 3:11pm January 13, 2020, 8:39am January 13, 2020, 9:27am January 13, 2020, 10:01am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a8d88668-f5b6-4203-ba0d-e026cb35894b",
    "url": "https://discuss.elastic.co/t/error-while-forwarding-logs-from-one-machine-to-another-machine/214734",
    "title": "Error while forwarding logs from one machine to another machine",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 12, 2020, 4:08pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7a8e16a2-d8b8-491c-845b-63ce32ec6042",
    "url": "https://discuss.elastic.co/t/config-not-working-injecting-all-logs/214714",
    "title": "Config not working, Injecting all logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "GuessMyName",
    "date": "January 11, 2020, 10:18pm",
    "body": "Hi Guys, For some reason... Winlogbeat is grabbing all security logs not just the one listed below, any idea what I'm missing ?, Running 7.5.1 Thx! winlogbeat.event_logs: name: Security processors: drop_event.when.not.or: equals.winlog.event_id: 4754 equals.winlog.event_id: 4755 equals.winlog.event_id: 4757 equals.winlog.event_id: 4758 equals.winlog.event_id: 4764 equals.winlog.event_id: 4740 equals.winlog.event_id: 4728 equals.winlog.event_id: 4732 equals.winlog.event_id: 4756 equals.winlog.event_id: 4735 equals.winlog.event_id: 4724 equals.winlog.event_id: 4625 equals.winlog.event_id: 4648 equals.winlog.event_id: 1102 equals.winlog.event_id: 4624 equals.winlog.event_id: 5038 equals.winlog.event_id: 6281 equals.winlog.event_id: 4727 equals.winlog.event_id: 4729 equals.winlog.event_id: 4730 equals.winlog.event_id: 4731 equals.winlog.event_id: 4733 equals.winlog.event_id: 4734 equals.winlog.event_id: 4737 #ignore_older: 72h",
    "website_area": "discuss"
  },
  {
    "id": "98ec8b90-cb2f-4efb-94de-a00b2331d3aa",
    "url": "https://discuss.elastic.co/t/filebeat-autodiscover-seems-to-cause-terraform-kubernetes-based-deploy-issues/214667",
    "title": "Filebeat.autodiscover seems to cause terraform/kubernetes based deploy issues",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 10, 2020, 9:18pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6e2a3f39-4e6b-4de3-a7cd-92bd9662072e",
    "url": "https://discuss.elastic.co/t/is-there-a-way-to-merge-data-collected-from-apm-and-packetbeat/214656",
    "title": "Is there a way to merge data collected from APM and Packetbeat",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "Kartheek_Mannepalli",
    "date": "January 10, 2020, 7:32pm",
    "body": "If a transaction took longer our goal is to find out which step caused the slowness. Thanks to APM and RUM we have a good view of DB and http requests in the timeline view but it does not show any network related information. The image below is an example where I know the network was slow and the timeline view does show the total time the request spent on the network but it does give a breakdown of information. So is it possible to merge network data collected from packetbeat with the APM transaction data? I know its kind of merging two different sets of data together but I wanted to see if there is a possibility that this could be done. image1355857 104 KB",
    "website_area": "discuss"
  },
  {
    "id": "d908824a-19b2-44dd-9d60-a055e384df34",
    "url": "https://discuss.elastic.co/t/omit-non-json-lines-in-log-files-while-still-allowing-json-parsing/214319",
    "title": "Omit non-json lines in log files while still allowing json parsing",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Gregory_Zimmers",
    "date": "January 9, 2020, 12:29am January 9, 2020, 8:43pm January 10, 2020, 7:25pm",
    "body": "I've found that I'm only able to apply ['^{'] this pattern to include_lines when I omit json parsing from the yml. After reading the documentation, it implies the line_filtering is done after parsing, so if that's the case, how do I filter lines that I don't want included? Examples below This works filebeat.inputs: - type: log enabled: true paths: - /applogs/*.log* include_lines: ['^{'] # json.message_key: \"level\" # json.keys_under_root: true # json.overwrite_keys: true processors: - add_fields: fields: kibanaspace: \"specific-space\" This does not. filebeat.inputs: - type: log enabled: true paths: - /applogs/*.log* include_lines: ['^{'] json.message_key: \"level\" json.keys_under_root: true json.overwrite_keys: true processors: - add_fields: fields: kibanaspace: \"specific-space\" given an input log file of {\"level\": \"INFO\", \"workerId\": \"5cced6bb522d-10-139641205327616\", \"traceId\": null, \"message\": \"Metrics blah\", \"datetime\": \"20-01-09 00:17:10:446539\"} {\"level\": \"INFO\", \"workerId\": \"5cced6bb522d-10-139641205327616\", \"traceId\": null, \"message\": \"Metrics blah\", \"datetime\": \"20-01-09 00:17:10:446539\"} This is a 3rd party log that I dont want captured Another unstructured log I want filtered out",
    "website_area": "discuss"
  },
  {
    "id": "3f8edf11-c51f-4b7a-b6e2-b6b94af549b6",
    "url": "https://discuss.elastic.co/t/multiple-container-in-single-filebeat-autodiscover/214646",
    "title": "Multiple container in single filebeat.autodiscover",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "rguptarg",
    "date": "January 10, 2020, 5:52pm",
    "body": "HI, I have configured filebeat.autodiscover for docker container log analysis, I have 8 docker containers and I want to add few containers in filebeat, below code is working fine for one image , filebeat.autodiscover: providers: - type: docker templates: - condition: equals.docker.container.image: ucc/header-api:0.9 config: - type: log paths: - /VMWNODE/docker/containers/${data.docker.container.id}/*.log fields: environment: Development fields_under_root: true multiline.pattern: '^[[:space:]]+|]$' multiline.match: after I have tried below few conditions but it's not working, so can you please suggest correct systex :- as mentioned in \"Multiple conditions with autodiscover & docker containers\" condition.and: contains: docker.container.image: ucc contains: docker.container.image: swaggerapi condition.or: - contains.docker.container.name: \"apache\" - contains.docker.container.name: \"nginx\" please suggest correct code to monitor multiple container",
    "website_area": "discuss"
  },
  {
    "id": "3c7f3912-6703-4cf9-bb07-de2bfefa6f06",
    "url": "https://discuss.elastic.co/t/filebeat-failed-to-start-exiting-error-loading-config-file-yaml-line-31-did-not-find-expected-key/214632",
    "title": "Filebeat failed to start: Exiting: error loading config file: yaml: line 31: did not find expected key",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "lucc",
    "date": "January 10, 2020, 4:21pm",
    "body": "hi, I have an issue to start filebeat...Error message is: failed to start: Exiting: error loading config file: yaml: line 31: did not find expected key filebeat version 7.5.1 (amd64), libbeat 7.5.1 yml file below Thanks for your help ###################### Filebeat Configuration Example ######################### This file is an example configuration file highlighting only the most common options. The filebeat.reference.yml file from the same directory contains all the supported options with more comments. You can use it as a reference. You can find the full configuration reference here: https://www.elastic.co/guide/en/beats/filebeat/index.html For more available modules and options, please see the filebeat.reference.yml sample configuration file. #=========================== Filebeat inputs ============================= filebeat.inputs: Each - is an input. Most options can be set at the input level, so you can use different inputs for various configurations. Below are the input specific configurations. #- type: log - type: udp max_message_size: 10MiB host: \"0.0.0.0:514\" fields: logzio_codec: plain token: cCNykNugBZfvXWmznbTxQUAjZZOxHzFz type: checkpoint fields_under_root: true encoding: utf-8 ignore_older: 3h Change to true to enable this input configuration. enabled: true Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/*.log #- c:\\programdata\\elasticsearch\\logs* filebeat.registry.path: /var/lib/filebeat processors: - rename: fields: - from: \"agent\" to: \"beat_agent\" ignore_missing: true - rename: fields: - from: \"log.file.path\" to: \"source\" ignore_missing: true Exclude lines. A list of regular expressions to match. It drops the lines that are matching any regular expression from the list. #exclude_lines: ['^DBG'] Include lines. A list of regular expressions to match. It exports the lines that are matching any regular expression from the list. #include_lines: ['^ERR', '^WARN'] Exclude files. A list of regular expressions to match. Filebeat drops the files that are matching any regular expression from the list. By default, no files are dropped. #exclude_files: ['.gz$'] Optional additional fields. These fields can be freely picked to add additional information to the crawled log files for filtering #fields: level: debug review: 1 Multiline options Multiline can be used for log messages spanning multiple lines. This is common for Java Stack Traces or C-Line Continuation The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ #multiline.pattern: ^[ Defines if the pattern set under pattern should be negated or not. Default is false. #multiline.negate: false Match can be set to \"after\" or \"before\". It is used to define if lines should be append to a pattern that was (not) matched before or after or as long as a pattern is not matched based on negate. Note: After is the equivalent to previous and before is the equivalent to to next in Logstash #multiline.match: after #============================= Filebeat modules =============================== filebeat.config.modules: Glob pattern for configuration loading path: ${path.config}/modules.d/*.yml Set to true to enable config reloading reload.enabled: false Period on which files under path should be checked for changes #reload.period: 10s #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 1 #index.codec: best_compression #_source.enabled: false #================================ General ===================================== The name of the shipper that publishes the network data. It can be used to group all the transactions sent by a single shipper in the web interface. #name: The tags of the shipper are included in their own field with each transaction published. #tags: [\"service-X\", \"web-tier\"] Optional fields that you can specify to add additional information to the output. #fields: env: staging #============================== Dashboards ===================================== These settings control loading the sample dashboards to the Kibana index. Loading the dashboards is disabled by default and can be enabled either by setting the options here or by using the setup command. #setup.dashboards.enabled: false The URL from where to download the dashboards archive. By default this URL has a value which is computed based on the Beat name and version. For released versions, this URL points to the dashboard archive on the artifacts.elastic.co website. #setup.dashboards.url: #============================== Kibana ===================================== Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. This requires a Kibana endpoint configuration. setup.kibana: Kibana Host Scheme and port can be left out and will be set to the default (http and 5601) In case you specify and additional path, the scheme is required: http://localhost:5601/path IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 #host: \"localhost:5601\" Kibana Space ID ID of the Kibana Space into which the dashboards should be loaded. By default, the Default Space will be used. #space.id: #============================= Elastic Cloud ================================== These settings simplify using Filebeat with the Elastic Cloud (https://cloud.elastic.co/). The cloud.id setting overwrites the output.elasticsearch.hosts and setup.kibana.host options. You can find the cloud.id in the Elastic Cloud web UI. #cloud.id: The cloud.auth setting overwrites the output.elasticsearch.username and output.elasticsearch.password settings. The format is <user>:<pass>. #cloud.auth: #================================ Outputs ===================================== Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: Array of hosts to connect to. hosts: [\"localhost:9200\"] Optional protocol and basic auth credentials. #protocol: \"https\" #username: \"elastic\" #password: \"changeme\" #----------------------------- Logstash output -------------------------------- output.logstash: The Logstash hosts hosts: [\"listener-eu.logz.io:5015\"] ssl: certificate_authorities:['/etc/pki/tls/certs/COMODORSADomainValidationSecureServerCA.crt'] Optional SSL. By default is off. List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" #================================ Processors ===================================== Configure processors to enhance or manipulate events generated by the beat. processors: add_host_metadata: ~ add_cloud_metadata: ~ add_docker_metadata: ~ add_kubernetes_metadata: ~ #================================ Logging ===================================== Sets log level. The default log level is info. Available log levels are: error, warning, info, debug #logging.level: debug At debug level, you can selectively enable logging only for some components. To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", \"publish\", \"service\". #logging.selectors: [\"*\"] #============================== X-Pack Monitoring =============================== filebeat can export internal metrics to a central Elasticsearch monitoring cluster. This requires xpack monitoring to be enabled in Elasticsearch. The reporting is disabled by default. Set to true to enable the monitoring reporter. #monitoring.enabled: false Sets the UUID of the Elasticsearch cluster under which monitoring data for this Filebeat instance will appear in the Stack Monitoring UI. If output.elasticsearch is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch. #monitoring.cluster_uuid: Uncomment to send the metrics to Elasticsearch. Most settings from the Elasticsearch output are accepted here as well. Note that the settings should point to your Elasticsearch monitoring cluster. Any setting that is not set is automatically inherited from the Elasticsearch output configuration, so if you have the Elasticsearch output configured such that it is pointing to your Elasticsearch monitoring cluster, you can simply uncomment the following line. #monitoring.elasticsearch: #================================= Migration ================================== This allows to enable 6.7 migration aliases #migration.6_to_7.enabled: true t",
    "website_area": "discuss"
  },
  {
    "id": "e317bc2f-b448-445f-8a42-f8dc274542fc",
    "url": "https://discuss.elastic.co/t/exceptions-in-filebeat-logs/214589",
    "title": "Exceptions in filebeat logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "emilio",
    "date": "January 10, 2020, 11:07am",
    "body": "Dear all, What is wrong with filebeat version 7.4.2 config? ######################## Filebeat Configuration ############################ #========================== Modules configuration ============================= filebeat.modules: #----------------------------- Elasticsearch Module ----------------------------- - module: elasticsearch # Server log server: enabled: true gc: enabled: true audit: enabled: true slowlog: enabled: true deprecation: enabled: true #--------------------------------- Kafka Module --------------------------------- - module: kafka log: enabled: false #-------------------------------- Kibana Module -------------------------------- - module: kibana log: enabled: true #------------------------------- Logstash Module ------------------------------- - module: logstash log: enabled: true #=========================== Filebeat inputs ============================= filebeat.inputs: #------------------------------ Log input -------------------------------- - type: log enabled: true paths: - /home/gunner/Gogunner/logs/log4j_someapp.log fields: type: someapp application: someapp fields_under_root: true encoding: utf-8 - type: log enabled: true paths: - /home/gunner/Gogunner/logs/log4j_someapp_synch.log fields: type: someapp_synch_log application: someapp fields_under_root: true multiline.pattern: ^ multiline.negate: true multiline.match: after exclude_files: ['\\.gz$','\\.bz2$','\\.swp$'] fields_under_root: true scan_frequency: 5 ### Multiline options multiline.max_lines: 500 multiline.timeout: 10s close_inactive: 5m close_renamed: true close_removed: true clean_removed: true close_timeout: 30 # Defines if inputs is enabled enabled: true #------------------------------ Docker input -------------------------------- enabled: false #================================ General ====================================== name: 'gunner3_shipper' tags: [\"gunner3_shipper\", \"gunner3_logs\"] fields_under_root: false #-------------------------- Elasticsearch output ------------------------------- output.elasticsearch: # Boolean flag to enable or disable the output module. enabled: true hosts: [\"zz.zz.zzz.zz:9200\"] worker: 3 index: \"logs-%{+YYYY.MM.dd}\" timeout: 90 #----------------------------- Logstash output --------------------------------- output.logstash: # Boolean flag to enable or disable the output module. enabled: false # The Logstash hosts hosts: [\"yy.yy.yyy.yyy:54\"] # Number of workers per Logstash host. worker: 3 # Optional index name. The default index name is set to filebeat # in all lowercase. index: \"logs-%{+YYYY.MM.dd}\" #================================Data========================================= path.home: /usr/share/filebeat/bin path.config: /etc/filebeat/ path.data: /etc/filebeat/data path.logs: /var/log/filebeat #============================== Template ===================================== # Set to false to disable template loading. setup.template.enabled: true # Template name. By default the template name is \"filebeat-%{[beat.version]}\" # The template name and pattern has to be set in case the elasticsearch index pattern is modified. setup.template.name: \"filebeat-%{[beat.version]}\" setup.template.pattern: \"filebeat-%{[beat.version]}-*\" # Path to fields.yml file to generate the template setup.template.fields: \"${path.config}/fields.yml\" # Elasticsearch template settings setup.template.settings: index: number_of_shards: 1 codec: best_compression number_of_routing_shards: 30 #============================== Kibana ===================================== # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. # This requires a Kibana endpoint configuration. setup.kibana: kibana host: 'http://xx.xxx.xx:5601' #================================ Logging ====================================== # Configure the path where the logs are written. The default is the logs directory # under the home path (the binary location). # The name of the files where the logs are written to. # Configure log file size limit. If limit is reached, log file will be # automatically rotated # rotateeverybytes: 10485760 # = 10MB # Number of rotated log files to keep. Oldest files will be deleted first. logging.level: debug logging.to_files: true logging.files: path: /var/log/filebeat name: filebeat keepfiles: 7 permissions: 0644 rotateeverybytes: 10485760 # = 10MB I receive these kind of exceptions: 020-01-10T09:43:04.250+0400 WARN elasticsearch/client.go:535 Cannot index event publisher.Event{Content:beat.Event{Timestamp:time.Time{wall:0xbf7e2199cdc9cf0c, ext:22086279746, loc:(*time.Location)(0x4de6580)}, Meta:common.MapStr(nil), Fields:common.MapStr{\"agent\":common.MapStr{\"ephemeral_id\":\"81cdc692-d844-46f8-9470-88baa292d3b9\", \"hostname\":\"zzz.com\", \"id\":\"4f8513df-d84e-44f1-a4b5-92739d27db39\", \"name\":\"gunner3_shipper\", \"type\":\"filebeat\", \"version\":\"7.4.2\"}, \"application\":\"someapp\", \"ecs\":common.MapStr{\"version\":\"1.1.0\"}, \"host\":common.MapStr{\"name\":\"runner3_shipper\"}, \"input\":common.MapStr{\"type\":\"log\"}, \"log\":common.MapStr{\"file\":common.MapStr{\"path\":\"/home/runner/JavaRunner/logs/log4j_app.log\"}, \"offset\":6939560}, \"message\":\" 10 Jan 2020 09:43:02,395 INFO EasySignPrepaidSubscription : Starting to assign lifecycle offer\", \"tags\":[]string{\"runner3_shipper\", \"gunner3_logs\"}, \"type\":\"asan_imza_log\"}, Private:file.State{Id:\"\", Finished:false, Fileinfo:(*os.fileStat)(0xc00066c1a0), Source:\"/home/runner/JavaRunner/logs/log4j_someapp.log\", Offset:6939657, Timestamp:time.Time{wall:0xbf7e21944ca8f2b3, ext:67348994, loc:(*time.Location)(0x4de6580)}, TTL:-1, Type:\"log\", Meta:map[string]string(nil), FileStateOS:file.StateOS{Inode:0x62251, Device:0xfd02}}, TimeSeries:false}, Flags:0x1} (status=400): {\"type\":\"mapper_parsing_exception\",\"reason\":\"Failed to parse mapping [_default_]: Root mapping definition has unsupported parameters: [_all : {norms=false}]\",\"caused_by\":{\"type\":\"mapper_parsing_exception\",\"reason\": \"Root mapping definition has unsupported parameters: [_all : {norms=false}]\"}}",
    "website_area": "discuss"
  },
  {
    "id": "0e67bdf6-0e0d-433f-94ee-78f7e1893541",
    "url": "https://discuss.elastic.co/t/error-in-fetching-the-logs-filebeat-7-3-2/214168",
    "title": "Error in fetching the logs Filebeat 7.3.2",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Anwesha_Pokra",
    "date": "January 8, 2020, 7:15am January 9, 2020, 7:29pm January 10, 2020, 6:13am January 10, 2020, 6:15am January 10, 2020, 10:13am",
    "body": "Hi, I am trying to pass the pod logs to elasticsearch using filebeat, but logs are not appearing in the filebeat pod log. I am deploying the filebeat as kubernetes pod. kubernetes version - 1.12.7 docker version - 1.18 filebeat version - 7.3.2 elastic version -7.3.2 kibana version - 7.3.2 Below is the filebeat-kubernetes.yaml file configuration - apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: elk labels: k8s-app: filebeat data: filebeat.yml: |- filebeat.inputs: - type: container paths: - /var/log/containers/.log processors: - add_kubernetes_metadata: host: {NODE_NAME} matchers: - logs_path: logs_path: \"/var/log/containers/\" client_inactivity_timeout: 5m multiline.pattern: '^[0-9]{2}:[0-9]{2}' multiline.negate: true multiline.match: after # To enable hints based autodiscover, remove `filebeat.inputs` configuration and uncomment this: #filebeat.autodiscover: # providers: # - type: kubernetes # host: {NODE_NAME} # hints.enabled: true # hints.default_config: # type: container # paths: # - /var/log/containers/${data.kubernetes.container.id}.log processors: - add_cloud_metadata: - add_host_metadata: cloud.id: ${ELASTIC_CLOUD_ID} cloud.auth: ${ELASTIC_CLOUD_AUTH} output.elasticsearch: hosts: ['${ELASTICSEARCH_HOST:10.9.0.131}:${ELASTICSEARCH_PORT:9200}'] apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: elk labels: k8s-app: filebeat spec: selector: matchLabels: k8s-app: filebeat template: metadata: labels: k8s-app: filebeat spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 hostNetwork: true dnsPolicy: ClusterFirstWithHostNet containers: - name: filebeat image: docker.elastic.co/beats/filebeat:7.4.0 args: [ \"-c\", \"/etc/filebeat.yml\", \"-e\", ] env: - name: ELASTICSEARCH_HOST value: 10.9.0.131 - name: ELASTICSEARCH_PORT value: \"9200\" - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName securityContext: runAsUser: 0 # If using Red Hat OpenShift uncomment this: #privileged: true resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true - name: varlog mountPath: /var/log readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: varlog hostPath: path: /var/log # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: filebeat subjects: kind: ServiceAccount name: filebeat namespace: elk roleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: filebeat labels: k8s-app: filebeat rules: apiGroups: [\"\"] # \"\" indicates the core API group resources: namespaces pods verbs: get watch list apiVersion: v1 kind: ServiceAccount metadata: name: filebeat namespace: elk labels: k8s-app: filebeat",
    "website_area": "discuss"
  },
  {
    "id": "61f2017b-82d1-4496-81df-7b664f19910c",
    "url": "https://discuss.elastic.co/t/cant-get-logs-from-some-pods/214570",
    "title": "Can't get logs from some pods",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "ilanssari",
    "date": "January 10, 2020, 9:10am",
    "body": "Hello dear community. I have an issue that i can't get logs of some pods on our Kubernetes cluster, here is how we collect logs : filebeat -> logstash -> elasticsearch filebeat is deployed on each node on the cluster 1. logstash and elasticsearch are deployed on cluster 2. here is our config : filebeat : --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: kube-system labels: k8s-app: filebeat data: filebeat.yml: |- filebeat.config: inputs: reload.enabled: true modules: path: ${path.config}/modules.d/*.yml # Reload module configs as they change: reload.enabled: true filebeat.autodiscover: providers: - type: kubernetes hints.enabled: true filebeat.modules: - module: system output.logstash: hosts: ['${LOGSTASH_HOST}:${LOGSTASH_PORT}'] index: \"ds-ci-cluster\" setup.template: name: \"ds-ci-cluster\" pattern: \"ds-ci-cluster-*\" --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-inputs namespace: kube-system labels: k8s-app: filebeat data: kubernetes.yml: |- - type: docker containers.ids: - \"*\" processors: - add_kubernetes_metadata: in_cluster: true --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: filebeat-ds-ci namespace: kube-system labels: k8s-app: filebeat spec: template: metadata: labels: k8s-app: filebeat spec: tolerations: - operator: Exists serviceAccountName: filebeat terminationGracePeriodSeconds: 30 containers: - name: filebeat image: OURIMAGESREGISTRY/filebeat-oss:6.8.1 args: [ \"-c\", \"/etc/filebeat.yml\", \"-e\", ] env: - name: LOGSTASH_HOST value: \"100.107.237.232\" - name: LOGSTASH_PORT value: \"30003\" securityContext: runAsUser: 0 resources: limits: memory: 1500Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: inputs mountPath: /usr/share/filebeat/inputs.d readOnly: true - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true - name: varlog mountPath: /var/log readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: varlog hostPath: path: /var/log - name: inputs configMap: defaultMode: 0600 name: filebeat-inputs # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: filebeat subjects: - kind: ServiceAccount name: filebeat namespace: kube-system roleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: filebeat labels: k8s-app: filebeat rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: - namespaces - pods verbs: - get - watch - list --- apiVersion: v1 kind: ServiceAccount metadata: name: filebeat namespace: kube-system labels: k8s-app: filebeat --- logstash: --- apiVersion: v1 kind: ConfigMap metadata: namespace: logging name: logstash-jvm-config data: jvm.options: | ## JVM configuration # Xms represents the initial size of total heap space # Xmx represents the maximum size of total heap space -Xms31g -Xmx31g ################################################################ ## Expert settings ################################################################ ## ## All settings below this section are considered ## expert settings. Don't tamper with them unless ## you understand what you are doing ## ################################################################ -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djruby.compile.invokedynamic=true -Djruby.jit.threshold=0 -XX:+HeapDumpOnOutOfMemoryError -Djava.security.egd=file:/dev/urandom --- apiVersion: v1 kind: ConfigMap metadata: namespace: logging name: logstash-conf data: filebeat.conf: |- input { beats { port => 5044 } } filter { if [fields][log_type] == \"atlassian-confluence\" { grok { match => { \"message\" => \"%{TIMESTAMP_ISO8601:timestamp:date} %{LOGLEVEL:loglevel} %{GREEDYDATA:logmessage}\" } } date { match => [ \"timestamp\", \"yyyy-MM-dd HH:mm:ss,SSS\" ] target => \"@timestamp\" } } if [fields][log_type] == \"jenkins\" { grok { match => { \"source\" => \"/var/jenkins_home/jobs/%{DATA:job_name}/builds/%{DATA:build_number}/log\" } } } if [fields][type] == \"oil-audit\" { grok { match => { \"message\" => \"%{DATA:compartment} %{GREEDYDATA}\" } } mutate { gsub => [\"message\", \"(\\S+=)\", \", \\1\"] } mutate { gsub => [\"message\", \"(\\S+=\\S+) (\\S+) (\\S+) (UTC)\", \"\\1T\\2\"] } kv { include_keys => [\"CompartmentName\", \"CompartmentId\", \"TenantId\", \"EventId\", \"EventName\", \"EventType\", \"EventSource\", \"RequestAction\", \"UserName\", \"EventTime\", \"ResponseTime\", \"ResponseStatus\", \"RequestOrigin\", \"RequestAction\", \"RequestAgent\",\"ResponsePayload\"] field_split => \" , \" trim_key => \" \" trim_value => \" \" } } if [fields][log_type] == \"atlassian-jira\" { grok { match => { \"message\" => \"%{TIMESTAMP_ISO8601:timestamp:date} %{GREEDYDATA:logprefix} %{LOGLEVEL:loglevel} %{GREEDYDATA:logmessage}\" } } date { match => [ \"timestamp\", \"yyyy-MM-dd HH:mm:ss,SSS\" ] target => \"@timestamp\" } } if [fields][log_type] == \"atlassian-bitbucket\" { grok { match => { \"message\" => \"%{TIMESTAMP_ISO8601:timestamp:date} %{LOGLEVEL:loglevel} %{GREEDYDATA:logmessage}\" } } date { match => [ \"timestamp\", \"yyyy-MM-dd HH:mm:ss,SSS\" ] target => \"@timestamp\" } } if [fields][log_type] == \"atlassian-bitbucket-access\" { grok { match => { \"message\" => \"^%{IP:clientip}(,%{IP:proxyip})?(,%{IP:serverip})? \\| %{NOTSPACE:protocol} \\| %{NOTSPACE:requestid} \\| %{NOTSPACE:user} \\| %{TIMESTAMP_ISO8601:timestamp} \\| \\\"?%{DATA:action}\\\"? \\| (%{QS:ref_url}|-) (%{QS:agent} )?\\| (%{INT:response}|-) \\| (%{INT:bytes_read}|-) \\| (%{INT:bytes_written}|-) \\| (%{DATA:labels}|-) \\| (%{INT:response_time}|-) \\| (%{NOTSPACE:sessionid}|-) \\|\" } } date { match => [ \"timestamp\", \"yyyy-MM-dd HH:mm:ss,SSS\" ] target => \"@timestamp\" } } } output { elasticsearch { hosts => \"elasticsearch.logging:9200\" index => \"%{[@metadata][beat]}-%{+YYYY.MM}\" sniffing => false } } http.conf: |- input { http { port => 8080 } } filter { grok { match => ['[headers][request_path]', '/%{DATA:request_path}/'] } mutate { remove_field => [\"headers\"] } } output { elasticsearch { hosts => \"elasticsearch.logging:9200\" index => \"%{request_path}-%{+YYYY.MM}\" } stdout { codec => rubydebug {metadata => true} } } pipelines.yml: |- - pipeline.id: filebeat path.config: /usr/share/logstash/pipeline/filebeat.conf pipeline.workers: 20 pipeline.batch.size: 2000 - pipeline.id: http path.config: /usr/share/logstash/pipeline/http.conf pipeline.workers: 8 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: logging name: logstash spec: replicas: 1 template: metadata: labels: app: logstash spec: containers: - image: OURIMAGESREGISTRY/logstash-oss:6.6.1 name: logstash ports: - name: logstash containerPort: 5044 protocol: TCP - name: logstash-http containerPort: 8080 protocol: TCP - name: logstash-ep containerPort: 9600 protocol: TCP volumeMounts: - name: pipelines mountPath: /usr/share/logstash/config/pipelines.yml subPath: pipelines.yml readOnly: false - name: logstash-conf mountPath: /usr/share/logstash/pipeline readOnly: false - name: logstash-jvm-config mountPath: /usr/share/logstash/config/jvm.options subPath: jvm.options volumes: - name: logstash-jvm-config configMap: name: logstash-jvm-config items: - key: jvm.options path: jvm.options - name: logstash-conf configMap: name: logstash-conf items: - key: filebeat.conf path: filebeat.conf - key: http.conf path: http.conf - name: pipelines configMap: name: logstash-conf items: - key: pipelines.yml path: pipelines.yml --- apiVersion: v1 kind: Service metadata: namespace: logging name: logstash labels: app: logstash spec: type: NodePort selector: app: logstash ports: - name: logstash port: 5044 nodePort: 30003 protocol: TCP - name: logstash-http port: 8080 nodePort: 30004 protocol: TCP - name: logstash-ep port: 9600 nodePort: 30005 protocol: TCP what's weird is that in same namespace some logs get collected from a pod and not collected from another one in the same namespace. Sorry for the long post and i appreciate your help guys if you need more config/explanations please ping me. Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "38ea875a-25af-4b23-b3ea-470fe2e1e490",
    "url": "https://discuss.elastic.co/t/using-the-vault-to-store-the-credentials-secrets-of-beats/214531",
    "title": "Using the Vault to store the credentials/secrets of beats",
    "category": [
      "Beats"
    ],
    "author": "mruthyu",
    "date": "January 10, 2020, 5:20am",
    "body": "Would like to know whether the beats secrets can be stored in Vault. I know about the supported keystore approach, (https://www.elastic.co/guide/en/beats/filebeat/current/keystore.html), but would like to know whether beats supports the vault service available on AZURE and AWS.",
    "website_area": "discuss"
  },
  {
    "id": "8295497a-fcab-46a5-80bf-d3ba01f6fbd1",
    "url": "https://discuss.elastic.co/t/docker-logs-are-not-coming/214436",
    "title": "Docker logs are not coming",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "rguptarg",
    "date": "January 9, 2020, 1:35pm January 9, 2020, 6:13pm January 10, 2020, 5:16am",
    "body": "Hi Team, I am trying to configure Docker logs forwarding to ELK through filebeat. I have followed (https://www.elastic.co/blog/enrich-docker-logs-with-filebeat). I can see docker logs on kibana but without log message and no docker related info. please suggest if any other configuration required. filebeat yml (filebeat-7.3.2-1.x86_64) :- filebeat.inputs: type: log enabled: true paths: /VMWNODE/docker/containers//.log json.message_key: log json.keys_under_root: true processors: add_docker_metadata: host: \"unix:///var/run/docker.sock\" docker version :- Docker version 19.03.1, build 74b1e89 Kibana JSON { \"_index\": \"filebeat-7.3.2-2020.01.09-000001\", \"_type\": \"_doc\", \"_id\": \"2ad5im8BisIPkQkKIfp5\", \"_version\": 1, \"_score\": null, \"_source\": { \"@timestamp\": \"2020-01-09T13:23:10.607Z\", \"ecs\": { \"version\": \"1.0.1\" }, \"log\": { \"offset\": 68782907, \"file\": { \"path\": \"/VMWNODE/docker/containers/2e65d4a4f8f6f337877736b21195de83c4663009a876faace614509f66761f9e/2e65d4a4f8f6f337877736b21195de83c4663009a876faace614509f66761f9e-json.log\" } }, \"stream\": \"stdout\", \"time\": \"2020-01-08T21:22:58.353467812Z\", \"input\": { \"type\": \"log\" }, \"container\": { \"id\": \"2e65d4a4f8f6f337877736b21195de83c4663009a876faace614509f66761f9e-json.log\" }, \"host\": { \"hostname\": \"N1PBBL-EXPA0159\", \"architecture\": \"x86_64\", \"name\": \"N1PBBL-EXPA0159\", \"os\": { \"codename\": \"Maipo\", \"platform\": \"rhel\", \"version\": \"7.6 (Maipo)\", \"family\": \"redhat\", \"name\": \"Red Hat Enterprise Linux Server\", \"kernel\": \"3.10.0-957.21.2.el7.x86_64\" }, \"id\": \"f462e1d352fd42739bea16c0ed67b21e\", \"containerized\": false }, \"agent\": { \"ephemeral_id\": \"67dddafd-cf42-43c8-b727-1ba847c60430\", \"hostname\": \"N1PBBL-EXPA0159\", \"id\": \"65528822-8256-4813-9f44-dc4cfbd47a22\", \"version\": \"7.3.2\", \"type\": \"filebeat\" } }, \"fields\": { \"@timestamp\": [ \"2020-01-09T13:23:10.607Z\" ], \"suricata.eve.timestamp\": [ \"2020-01-09T13:23:10.607Z\" ] }, \"sort\": [ 1578576190607 ] }",
    "website_area": "discuss"
  },
  {
    "id": "c68d212f-97a2-4814-a575-279bcca0db1a",
    "url": "https://discuss.elastic.co/t/kibana-can-not-show-all-the-filebeat/214410",
    "title": "Kibana can not show all the filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 9, 2020, 10:13am January 10, 2020, 3:13am January 10, 2020, 3:22am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "af15d4fe-57b7-4486-9c46-888f1f4d1146",
    "url": "https://discuss.elastic.co/t/filebeat-unable-to-parse-date-with-timezone/214498",
    "title": "Filebeat unable to parse date with timezone",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "harddaynight",
    "date": "January 9, 2020, 10:11pm",
    "body": "Hello Elastic Team, Can you, please, help me to find out why my filebeat can't parse a date with timezone properly? Sample of data: {\"date\": \"2020-01-07 05:00:00+03:00\", some other JSON fields} The message I get in logs: \"reason\":\"failed to parse date field [2020-01-09 16:30:00+03:00] with format [yyyy-MM-dd HH:mm:ssZ]\",\"caused_by\":{\"type\":\"date_time_parse_exception\",\"reason\":\"Text '2020-01-09 16:30:00+03:00' could not be parsed at index 19\"}}} I have the following mapping for my index: PUT /scrapper-000001 { \"settings\": { \"index\": { \"number_of_shards\": 3, \"number_of_replicas\": 1 } }, \"aliases\": { \"scrapper\": {\"is_write_index\": true} }, \"mappings\": { \"properties\": { \"date\": {\"type\":\"date\", \"format\":\"yyyy-MM-dd HH:mm:ssZ\"} } } } Everything looks legit and works fine till I add a timezone.",
    "website_area": "discuss"
  },
  {
    "id": "f1af8706-2f77-423d-9cdd-0def734f3a42",
    "url": "https://discuss.elastic.co/t/about-the-filebeat-type-docker/213868",
    "title": "About the Filebeat type: docker",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 6, 2020, 9:54am January 9, 2020, 8:17pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "90d185c7-ae06-4070-b5d6-a5a7459d9cb3",
    "url": "https://discuss.elastic.co/t/custom-module-config/214003",
    "title": "Custom Module Config",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "dawiro",
    "date": "January 7, 2020, 1:22pm January 9, 2020, 6:17pm",
    "body": "Hi, Each filebeat module comes with a default prospector config. How are we meant to customise/extend it without editing the config files under /usr/share/filebeat/module? Thx D",
    "website_area": "discuss"
  },
  {
    "id": "91cd9684-4241-4221-a21f-d96a11fb92bb",
    "url": "https://discuss.elastic.co/t/using-auditd-along-with-auditbeat/214342",
    "title": "Using auditd along with auditbeat",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "akki2208",
    "date": "January 9, 2020, 5:00am January 9, 2020, 4:25pm",
    "body": "I want to run audit beat along with auditd. is there any option? or can we write auditbeat logs in some file?",
    "website_area": "discuss"
  },
  {
    "id": "315d6476-38a2-4f32-a931-52173a282fab",
    "url": "https://discuss.elastic.co/t/filebeat-for-greenplum-module-or-fileset/214463",
    "title": "Filebeat for Greenplum: Module or Fileset?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 9, 2020, 4:20pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "578aa028-f8ef-47f6-b424-a3053e5fd896",
    "url": "https://discuss.elastic.co/t/winlogbeat-dont-collect-some-event/213584",
    "title": "Winlogbeat don't collect some event",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "ghl1l0",
    "date": "January 2, 2020, 2:23pm January 3, 2020, 3:30am January 3, 2020, 11:01am January 9, 2020, 3:55pm",
    "body": "I had install winlogbeat in windows 10 until now i received all log include secrutiy application ... except \"Security Group Management\" log",
    "website_area": "discuss"
  },
  {
    "id": "1c8713ae-3cf9-4f7f-a01c-f32665d0984b",
    "url": "https://discuss.elastic.co/t/system-module-packages-error/214448",
    "title": "System module - packages error",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "samoz83",
    "date": "January 9, 2020, 2:46pm",
    "body": "Seem to be getting the below error from the packages module on our 18.04 machines meaning we get no stats on packages installed etc. failed to get packages: error getting DEB packages: error converting 25G to int: strconv.ParseUint: parsing \"25G\": invalid syntax Not sure what is causing it?",
    "website_area": "discuss"
  },
  {
    "id": "d6ff9e85-8ecd-41c4-9530-1438bd840ab2",
    "url": "https://discuss.elastic.co/t/heartbeat-kubernetes-autodiscover/214435",
    "title": "Heartbeat Kubernetes Autodiscover",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "akrzos",
    "date": "January 9, 2020, 1:29pm",
    "body": "I am trying to have heartbeat deployed in kubernetes as a daemonset in which each heartbeat daemon icmp pings all other nodes in a cluster. I was planning on doing this with the kubernetes autodiscover provider however I have been unsuccessful in getting it to recognize and ping more than just the pod on the same node. Can someone point me at a configuration that the provider recognizes all nodes in a cluster rather than just the node the daemon is running on. My heartbeat configuration resembles this: fields: site.name: testcluster fields_under_root: true heartbeat.config.monitors: # Directory + glob pattern to search for configuration files path: /usr/share/heartbeat/monitors.d/*.yml # If enabled, heartbeat will periodically check the config.monitors path for changes reload.enabled: true # How often to check for changes reload.period: 15s logging.json: true heartbeat.autodiscover: providers: - type: kubernetes include_labels: \"k8s-app\" # host: ${NODE_NAME} # namespace: test-heartbeat templates: - condition: contains: kubernetes.labels.k8s-app: heartbeat config: - type: icmp hosts: [\"${data.kubernetes.node.name}\"] schedule: '*/5 * * * * * *' mode: all ipv4: true timeout: 16s wait: 1s",
    "website_area": "discuss"
  },
  {
    "id": "591c61e9-a295-446e-94b2-8d92000c9851",
    "url": "https://discuss.elastic.co/t/metricbeat-beats-under-stack-monitoring-missing/214390",
    "title": "Metricbeat - \"Beats\" under Stack Monitoring missing",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "January 9, 2020, 1:26pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "cb4c12e0-97c7-42cf-b2f1-8d5086812d53",
    "url": "https://discuss.elastic.co/t/how-to-setup-hyper-v-with-winlogbeat/212807",
    "title": "How to setup Hyper V with winlogbeat",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "tahseen_fatima",
    "date": "December 23, 2019, 6:01am January 3, 2020, 3:18am January 9, 2020, 11:41am",
    "body": "Hey, I want to monitor hyper v events using winlogbeat. How to do that, can someone please help. Thanks, Tahseen.",
    "website_area": "discuss"
  },
  {
    "id": "9594627a-f510-4e07-9025-15fa795ad6d7",
    "url": "https://discuss.elastic.co/t/metrics-on-specific-region/214371",
    "title": "Metrics on specific region",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Samerd",
    "date": "January 9, 2020, 8:35am",
    "body": "Hi , I'm trying to choose specific region to specify the instances which includes in the chosen region only . i used this section , but it doesn't work: - module: aws period: 10s metricsets: - ec2 default_region: eu-central-1 regions: - eu-central-1 thanks",
    "website_area": "discuss"
  },
  {
    "id": "05b7a015-d4ef-4a02-af93-8e0c3c61d9e5",
    "url": "https://discuss.elastic.co/t/check-database-availability/213835",
    "title": "Check database availability",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "radd",
    "date": "January 6, 2020, 6:57am January 7, 2020, 9:34pm January 8, 2020, 12:59pm January 8, 2020, 3:03pm January 8, 2020, 6:20pm January 8, 2020, 9:06pm January 9, 2020, 8:00am",
    "body": "Hi, I am new in ELK and I have one question regarding monitoring oracle DB availability. Please advice me, because I wish, with your help and knowledge, to find the best solution. I have read that I can monitor for Oracle DB Connection (checking listener availability), using Heartbeat. But here I want to monitor database availability (periodically logon with one specific user for this purpose on Oracle database), not only to check listener availability. Please advice me, what will be the best way to do it. Thanks in advance!",
    "website_area": "discuss"
  },
  {
    "id": "ab0dfb73-7d76-4446-b4bb-93d932017a4f",
    "url": "https://discuss.elastic.co/t/date-parser-using-ingest-node/214341",
    "title": "Date parser using ingest node",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Praveen_V",
    "date": "January 9, 2020, 4:47am",
    "body": "Hi All - Am trying to chop a message and convert the string to Date using Ingest node , however able to get the date string from the message .. but the format is string in Elastic. Any help here would be much appreciated [ Am not using Logstash ] Original Message : 2019-12-24 09:43 +00:00: Serving page: /signin event_created is extracted successfully as : 2019-12-24 09:43 JSON curl -X PUT \"localhost:9200/_ingest/pipeline/fields_extraction?pretty\" -H 'Content-Type: application/json' -d' { \"description\" : \"parsing the input log to fields\", \"processors\" : [ { \"dissect\": { \"field\": \"message\", \"pattern\" : \"%{event_created} +%{extracted_timezone}: %{log_output}\", \"ignore_failure\" : true } }, { \"date\": { \"field\" : \"event_created\", \"target_field\" : \"event_created\", \"formats\" : [\"yyyy-MM-dd hh:mm\"], \"timezone\" : \"London\", \"ignore_failure\" : true } } ] }'",
    "website_area": "discuss"
  },
  {
    "id": "f8e1a520-5263-4cfe-8f6b-a348f06eb4fd",
    "url": "https://discuss.elastic.co/t/create-a-new-index-everyday-with-filebeat/214163",
    "title": "Create a new index everyday with filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "darshanky",
    "date": "January 8, 2020, 6:35am January 8, 2020, 9:04pm January 8, 2020, 11:44pm",
    "body": "Hello Folks, I am new to Elasticstack and trying to get my hear around. I have setup the single node elastic stack which runs the Kibana as well. I have a client that has file beat installed and sends logs to elasticsearh. The setup works perfectly fine. I have apache module enabled in the file beat and I can see the index is created with name filebeat-7.5.1-2020.01.08-000001 I would like to have a new index created everyday based on the timestamp from the apache logs. I have gone through some elastic stack documentation and blogs to achieve this. I was unable to create a new index everyday based on this criteria. I am stuck with this from two days. We are evaluating elastic stack for our upcoming projects and have not been able to set it up as per the POC requirement. Can you please point me to a documentation or elastic community post that explains the steps required to set it up.",
    "website_area": "discuss"
  },
  {
    "id": "9b1716c2-ea48-410f-a55c-b0e16220d432",
    "url": "https://discuss.elastic.co/t/filebeat-logstash-module-not-working-with-json-processor/213728",
    "title": "Filebeat logstash module not working with json processor",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 3, 2020, 6:51pm January 8, 2020, 8:43pm January 8, 2020, 9:54pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7168ba82-99c0-492b-ac9e-4c5aa1f8c441",
    "url": "https://discuss.elastic.co/t/scaling-filebeat-over-containers/213772",
    "title": "Scaling filebeat over containers",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "fizem",
    "date": "January 4, 2020, 9:39am January 8, 2020, 8:56pm",
    "body": "Hi, Im looking for the appropriate way to monitor applicative logs produced nginx, tomcat, springboot embedded in docker with filebeat and ELK. In the container strategy, a container should be use for only one purpose one nginx per container one tomcat per container meaning we cant have an additional filebeat within a nginx or tomcat container. Over what I have read over Internet, we could have the following setup: a volume dedicated for storing logs a nginx container which mount the dedicated logs volume a tomcat / springboot container which mount the dedicated logs volume a filebeat container also mounting the dedicated logs volume This works fine but when it comes to scale out nginx and springboot container, it is a little bit more complex for me Which pattern should I use to push my logs using filebeat to logstash if I have the following configuration: several nginx containers in load balancing with the same configuration (logs configuration is the same: same path) several springboot rest api containers behing nginx containers with the same configuration (logs configuration is the same:same path) Should I create one volume by set of nginx + springboot rest api and add a filebeat container ? Should I create a global log volume shared by all my containers and have a different log filename by container (having the name of the container in the filename of the logs?) and having only one filebeat container ? In the second proposal, how to scale filebeat ? Is there another way to do that ? Many thanks for your help.",
    "website_area": "discuss"
  },
  {
    "id": "e012606b-dd35-4125-bdf1-8b89863b6173",
    "url": "https://discuss.elastic.co/t/filebeat-log-folder-is-not-created-anymore/214269",
    "title": "Filebeat log folder is not created anymore",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Flaviu",
    "date": "January 8, 2020, 4:00pm January 8, 2020, 5:00pm",
    "body": "I have installed filebeat 7.5.1 on a new server, and I realized that the folder in which I want filebeat to save his own logs is not created anymore. This was working in the previous version. I am doing something wrong or it is just a change of behaviour in the way filebeat is working? I am configuring the log folder in filebeat.yml like this: logging.level: warning logging.to_files: true logging.to_syslog: false logging.metrics.period: 30s logging.files: path: /var/log/filebeat name: filebeat.log rotateeverybytes: 10485760 # = 500MB keepfiles: 7 permissions: 0644 logging.json: true",
    "website_area": "discuss"
  },
  {
    "id": "77d516d6-58aa-43fd-8eea-c458bf32af42",
    "url": "https://discuss.elastic.co/t/create-index-with-filebeat/211964",
    "title": "Create Index with filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "shrikantgulia",
    "date": "December 16, 2019, 9:12am December 16, 2019, 10:45am December 16, 2019, 10:49am December 19, 2019, 5:39am December 19, 2019, 5:54am December 19, 2019, 6:03am December 19, 2019, 7:13am December 19, 2019, 7:17am December 19, 2019, 7:22am December 19, 2019, 7:24am December 23, 2019, 7:38am December 30, 2019, 7:50am January 8, 2020, 3:46pm January 8, 2020, 4:09pm",
    "body": "Hello , I want to create a index and push data directly to elasticsearch with filebeat but i am not able to create a index its showing me the error can someone help me out. Exiting: setup.template.name and setup.template.pattern have to be set if index name is modified",
    "website_area": "discuss"
  },
  {
    "id": "a8234115-c712-47e3-acfc-e3cceaf270a0",
    "url": "https://discuss.elastic.co/t/cannot-see-data-from-metricbeat-data-in-elk/214037",
    "title": "Cannot see data from Metricbeat data in ELK",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Henry_Navarro",
    "date": "January 7, 2020, 11:29am January 8, 2020, 3:26pm",
    "body": "Hi everyone, I am having the I saw a similar question in this link. But I cannot understand the solution and the topic is currently closed. It seems metricbeat is working but I cannot see the data in ELK. I did what jsoriano suggests and it seems everything is working good: Sin ttulo460529 64.3 KB Please, can you help me?",
    "website_area": "discuss"
  },
  {
    "id": "c0bcae49-ee92-481a-9317-6c398f3b2af0",
    "url": "https://discuss.elastic.co/t/can-metric-beat-collect-remotely-from-systems/214246",
    "title": "Can metric beat collect remotely from systems",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "liorg2",
    "date": "January 8, 2020, 2:30pm",
    "body": "can metric beat collect remotely from systems? for example sql server metricbeat thanks",
    "website_area": "discuss"
  },
  {
    "id": "f702bd96-32fe-47aa-8c6b-af679f3f4594",
    "url": "https://discuss.elastic.co/t/missing-value-for-server-uptime-in-module-apache/214237",
    "title": "Missing value for server_uptime in module apache",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "mtudisco",
    "date": "January 8, 2020, 1:22pm",
    "body": "Hi, I'm using metricbeat 7.5 with apache module and Apache/2.4.6 on CentOS 7. Everythings works fine except for the fact that in the dashboad in the graph Uptime [Metricbeat Apache] ECS the metric server_uptime is always empty. I have looked in the index information and no document contains that field. What might be missing? thanks",
    "website_area": "discuss"
  },
  {
    "id": "43742cb2-b35f-4ebb-8064-3aafb26d227b",
    "url": "https://discuss.elastic.co/t/trouble-debugging-metricbeat-connection-issues/212931",
    "title": "Trouble debugging MetricBeat connection issues",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Christiaan",
    "date": "December 24, 2019, 8:51am January 3, 2020, 4:47pm January 8, 2020, 12:52pm",
    "body": "Hi all, I'm running an ECK on a Kubernetes cluster and am using FileBeat and MetricBeat to send monitoring information to Elastic. For the most part, things are running smoothly, except for the MetricBeat Deployment, which is not providing any output data (the DaemonSet works fine). I've confirmed that the KubeStateMetrics pod is up and running and gathering data as expected, so I assume that either MetricBeat has trouble connecting to KubeStateMetrics or to Elastic. The logs for the MetricBeat pod shows no connection errors however. In hopes of isolating the issue, I've changed the 'Host' properties of the Metricbeat deployment for both the Elastic and Kube-State-Metrics services to non-existent endpoints and changed the auth credentials, to see if that would throw any useful errors. I have tried (in separate steps): Invalid host for Kube-State-Metrics Invalid host for Elastic Invalid username/password for Elastic Invalid certificate for Elastic In all cases, no errors are shown at all. Activating 'debug' logging also yields no useful information. The Metricbeat logs simply shows the 'Non-zero metrics collected' INFO messages. Seemingly it does not matter whether I provide valid or invalid hosts or credentials. This strikes me as unexpected behavior, and makes me think I'm overlooking something. Can someone verify that under normal circumstances, connection errors should be shown? And are there further steps I can take to debug/isolate the problem? Deployment config: apiVersion: v1 kind: ConfigMap metadata: name: metricbeat-deployment-config namespace: elastic labels: k8s-app: metricbeat data: metricbeat.yml: |- metricbeat.config.modules: # Reload module configs as they change: reload.enabled: false processors: - add_cloud_metadata: - add_kubernetes_metadata: in_cluster: true setup.ilm.enabled: false output.elasticsearch: hosts: ['https://elastic-es-http:9200'] ssl.certificate_authorities: [\"/usr/share/elastic/certs/ca.crt\"] ssl.certificate: '/usr/share/elastic/certs/tls.crt' ssl.key: '/usr/share/elastic/certs/tls.key' username: '{username}' password: \"{password}\" --- apiVersion: v1 kind: ConfigMap metadata: name: metricbeat-deployment-modules namespace: elastic labels: k8s-app: metricbeat data: # This module requires `kube-state-metrics` up and running under `kube-system` namespace kubernetes.yml: |- - module: kubernetes labels.dedot: true annotations.dedot: true metricsets: - state_node - state_deployment - state_replicaset - state_pod - state_container - state_statefulset # Uncomment this to get k8s events: - event period: 10s hosts: [\"kube-state-metrics.kube-system.svc.cluster.local:8080\"] add_metadata: true in_cluster: true enabled: true --- # Deploy singleton instance in the whole cluster for some unique data sources, like kube-state-metrics apiVersion: apps/v1beta1 kind: Deployment metadata: name: metricbeat namespace: elastic labels: k8s-app: metricbeat spec: template: metadata: creationTimestamp: ~ labels: k8s-app: metricbeat spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: agentpool operator: In values: - elastic containers: - args: - \"-c\" - /etc/metricbeat.yml - \"-e\" image: \"docker.elastic.co/beats/metricbeat-oss:7.5.0\" imagePullPolicy: IfNotPresent name: metricbeat resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi securityContext: runAsUser: 0 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /usr/share/elastic/certs/ name: elastic-internal-http-certificates readOnly: true - mountPath: /etc/metricbeat.yml name: config readOnly: true subPath: metricbeat.yml - mountPath: /usr/share/metricbeat/modules.d name: modules readOnly: true dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: metricbeat serviceAccountName: metricbeat terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: restriction operator: Equal value: elastic volumes: - configMap: defaultMode: 384 name: metricbeat-deployment-config name: config - name: elastic-internal-http-certificates secret: defaultMode: 420 optional: false secretName: elastic-es-http-certs-internal - configMap: defaultMode: 384 name: metricbeat-deployment-modules name: modules",
    "website_area": "discuss"
  },
  {
    "id": "f735fd4f-5f71-411d-8949-89265eb06deb",
    "url": "https://discuss.elastic.co/t/sending-log4j-logs-in-xml-format-to-elasticsearch-using-filebeat/214199",
    "title": "Sending Log4j logs(in XML format) to Elasticsearch using Filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "JY_DT",
    "date": "January 8, 2020, 9:38am January 8, 2020, 11:15am",
    "body": "Hello, I need to send log files generated using Log4j on client machines to Elasticsearch installed on a server. I'm using Filebeat, but not Logstash. Is there any other plugin, etc required, or can it just be done using Filebeat itself? Thanks, Jy",
    "website_area": "discuss"
  },
  {
    "id": "e248c4a0-a05a-4de6-814c-a14a82a54e2f",
    "url": "https://discuss.elastic.co/t/filebeat-to-logstash-failed-to-publish-events-caused-by-read-tcp-192-168-155-177-55376-47-102-46-68wsarecv-an-existing-connection-was-forcibly-closed-by-the-remote-host/213657",
    "title": "Filebeat to logstash: Failed to publish events caused by: read tcp 192.168.155.177:55376->47.102.46.68:5045:wsarecv:An existing connection was forcibly closed by the remote host",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "cheninnocent",
    "date": "January 3, 2020, 9:05am January 3, 2020, 6:56pm January 6, 2020, 8:39am January 6, 2020, 8:42am January 7, 2020, 1:44am January 7, 2020, 6:24am January 8, 2020, 8:03am",
    "body": "github.com/elastic/beats filebeat to logstash: Failed to publish events caused by: read tcp 192.168.155.177:55376->47.102.46.68:5045:wsarecv:An existing connection was forcibly closed by the remote host opened 02:01AM - 03 Jan 20 UTC closed 02:58AM - 03 Jan 20 UTC cheninnocent computer A is ok. computer B is failed when log is to much. filebeat log filebeat to logstash: Failed to publish events caused by:... As I said above, the above bug appears on several computers, but is normal on others. Even logstash only have input and output. The above bug still appears on several computers.",
    "website_area": "discuss"
  },
  {
    "id": "535542e1-2c42-48f4-802e-41e57d66d6c4",
    "url": "https://discuss.elastic.co/t/packet-beat-error-you-dont-have-permission-to-capture-on-that-device/213587",
    "title": "Packet Beat error : You don't have permission to capture on that device",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "Vamsi_Vutukuri",
    "date": "January 8, 2020, 6:37am",
    "body": "Hi, i'm using elastic stack of version 7.5.1 with x-pack installed and can't able to run packetbeat and i'm getting the following error : Please help me solve it. Exiting: Sniffer main loop failed: Error starting sniffer: any: You don't have permission to capture on that device (socket: Operation not permitted) Exiting: Sniffer main loop failed: Error starting sniffer: any: You don't have permission to capture on that device (socket: Operation not permitted)",
    "website_area": "discuss"
  },
  {
    "id": "0d69ef87-6545-4c0b-af9a-9c364473d637",
    "url": "https://discuss.elastic.co/t/bug-with-filebeat-inlucde-lines/214138",
    "title": "Bug with filebeat inlucde_lines?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "RunningSmurf",
    "date": "January 7, 2020, 10:05pm January 7, 2020, 10:13pm",
    "body": "Hello, I have been trying to use the include_lines option in the filebeat.yml file to read a SQL Server log file. Note that I am not using the mssql module of FileBeat; I just have an input of type log and point it to my SQL Server log. I cannot get the inlude_lines to work, after trying multiple variations and doing some extensive online research. I create a dummy log file and update the lines manually. I point my filebeat to that dummy log. The include_lines option works perfectly fine. Has anyone run into this issue? In the mssql.yml file, I do not see an option to use include_lines, and the documentation does not mention it. Would anyone have a proven methodology for filtering SQL Server lines from logs? Thanks! Sincerely, RS",
    "website_area": "discuss"
  },
  {
    "id": "fa2a4b68-ba54-40bd-aa96-f80d8ef6f8a7",
    "url": "https://discuss.elastic.co/t/metricbeat-microsoft-sql-server-custom-port/214121",
    "title": "MetricBeat Microsoft SQL Server Custom Port",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Anji_Reddy",
    "date": "January 7, 2020, 7:47pm",
    "body": "We are trying to configure Metricbeat for Microsoft SQL server with custom port 31433. How do we configured mssql.yml file like below. This does not work. module: mssql metricsets: - \"transaction_log\" - \"performance\" hosts: [\"sqlserver://10.x.x.x:31433\"] username : xxxxx password: xxxx",
    "website_area": "discuss"
  },
  {
    "id": "fab041f5-c42d-42ca-8a53-6e82970ff324",
    "url": "https://discuss.elastic.co/t/unable-to-view-analytics-and-api-logs-is-filebeat-running-well/214075",
    "title": "Unable to view Analytics and API Logs : Is Filebeat running well?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "The-Big-K",
    "date": "January 7, 2020, 2:53pm January 7, 2020, 4:09pm",
    "body": "Very new to ES ecosystem and enjoying the learning process. I've got ES and App-Search (Self-Hosted) running on Ubuntu. The only problem now is that I cannot see API logs and Analytics in my app-search dashboard. Apparantly, filebeat was not running. I therefore installed filebeat using curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.5.1-amd64.deb sudo dpkg -i filebeat-7.5.1-amd64.deb added elasticsearch log path to filebeat.yml file in /etc/filebeat/ # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/elasticsearch/* #- c:\\programdata\\elasticsearch\\logs\\* then ran following commands - sudo systemctl enable filebeat sudo systemctl start filebeat systemctl status filebeat outputs the following Jan 07 14:42:09 my-server filebeat[13734]: 2020-01-07T14:42:09.866Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":30,\"tim which makes me think that filebeat is running successfully; and tracking. But I do not see anything in my app-search dashboard. Am I missing any step? Please help.",
    "website_area": "discuss"
  },
  {
    "id": "11d58dd7-0c27-4726-96e0-2881b2bffd64",
    "url": "https://discuss.elastic.co/t/metricbeat-dont-collect-kubernetes-volume-metrics-anymore/214094",
    "title": "Metricbeat don't collect kubernetes volume metrics anymore",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Chaya56",
    "date": "January 7, 2020, 3:51pm",
    "body": "Hello, I'm currently using Metricbeat to collect data from kubernetes volumes. It's a managed kubernetes hosted by OVH. it was working great until they installed a new standalone version of their plugin: cinder.csi.openstack.org. Since this, the kubelet API https://${node}:10250/stats/summary don't give anymore information about PersistentVolumeClaim. And that was the endpoint used by metricbeat to collect volume metrics. Regarding this issues: https://github.com/kubernetes/kubernetes/issues/68522 it seems this API is gradually abandoned. Do we have any alternative, to continue to get volume metrics with metricbeats ?",
    "website_area": "discuss"
  },
  {
    "id": "880f0773-0f94-4c45-823b-bc9ab7ab6554",
    "url": "https://discuss.elastic.co/t/minikube-not-adding-add-kubernetes-metadata/214046",
    "title": "Minikube not adding add_kubernetes_metadata",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "vaclav",
    "date": "January 7, 2020, 12:29pm",
    "body": "Hello, I am trying to run auditbeat on minikube, but kubernetes metadata are not added to events. Do you please know what should be correct configuration ? here is my config: --- apiVersion: v1 kind: ConfigMap metadata: name: auditbeat-config namespace: kube-system labels: k8s-app: auditbeat data: auditbeat.yml: |- auditbeat.modules: - module: auditd audit_rules: | -w /etc/passwd -p wa -k identity -a always,exit -F arch=b32 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit=-EPERM -k access -a always,exit -F arch=b64 -S execve,execveat -k exec processors: - add_kubernetes_metadata: host: ${NODE_NAME} output.file: path: \"/var/auditbeat/logs\" filename: auditbeat rotate_every_kb: 20000 number_of_files: 2 permissions: 0644 --- # Deploy a auditbeat instance per node for node metrics retrieval apiVersion: apps/v1 kind: DaemonSet metadata: name: auditbeat namespace: kube-system labels: k8s-app: auditbeat spec: selector: matchLabels: k8s-app: auditbeat template: metadata: labels: k8s-app: auditbeat spec: serviceAccountName: auditbeat terminationGracePeriodSeconds: 30 hostNetwork: true dnsPolicy: ClusterFirstWithHostNet hostPID: true containers: - name: auditbeat image: docker.elastic.co/beats/auditbeat:7.5.1 args: [ \"-c\", \"/etc/auditbeat.yml\", \"-e\", \"-d\", \"*\", ] env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName securityContext: runAsUser: 0 capabilities: add: [\"AUDIT_CONTROL\", \"AUDIT_READ\"] resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/auditbeat.yml readOnly: true subPath: auditbeat.yml volumes: - name: config configMap: defaultMode: 0600 name: auditbeat-config --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: auditbeat subjects: - kind: ServiceAccount name: auditbeat namespace: kube-system roleRef: kind: ClusterRole name: auditbeat apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: auditbeat labels: k8s-app: auditbeat rules: - apiGroups: [\"\"] resources: - nodes - namespaces - pods - events verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: v1 kind: ServiceAccount metadata: name: auditbeat namespace: kube-system labels: k8s-app: auditbeat --- in debug log I can see that add_kubernetes_metadata processor is generated, pods are discovered and than no metadata added 2020-01-07T10:11:02.593Z INFO add_kubernetes_metadata/kubernetes.go:68 add_kubernetes_metadata: kubernetes env detected, with version: v1.17.0 2020-01-07T10:11:02.593Z INFO kubernetes/util.go:79 kubernetes: Using node minikube provided in the config 2020-01-07T10:11:02.593Z DEBUG [kubernetes] add_kubernetes_metadata/kubernetes.go:138 Initializing a new Kubernetes watcher using host: minikube 2020-01-07T10:11:02.696Z DEBUG [kubernetes] kubernetes/watcher.go:241 cache sync done 2020-01-07T10:11:02.697Z DEBUG [processors] processors/processor.go:101 Generated new processors: add_kubernetes_metadata 2020-01-07T10:11:02.703Z DEBUG [kubernetes] add_kubernetes_metadata/kubernetes.go:156 add_kubernetes_metadata: adding pod: kube-system/storage-provisioner 2020-01-07T10:11:02.704Z DEBUG [kubernetes] add_kubernetes_metadata/kubernetes.go:156 add_kubernetes_metadata: adding pod: kube-system/coredns-6955765f44-2dpv8 2020-01-07T10:11:02.706Z DEBUG [kubernetes] add_kubernetes_metadata/kubernetes.go:156 add_kubernetes_metadata: adding pod: kube-system/etcd-minikube 2020-01-07T10:11:02.717Z DEBUG [kubernetes] add_kubernetes_metadata/kubernetes.go:156 add_kubernetes_metadata: adding pod: kube-system/kube-addon-manager-minikube 2020-01-07T10:11:02.717Z DEBUG [kubernetes] add_kubernetes_metadata/kubernetes.go:156 add_kubernetes_metadata: adding pod: kube-system/kube-controller-manager-minikube 2020-01-07T10:11:02.718Z DEBUG [kubernetes] add_kubernetes_metadata/kubernetes.go:156 add_kubernetes_metadata: adding pod: kube-system/kube-scheduler-minikube 2020-01-07T10:11:02.718Z DEBUG [kubernetes] add_kubernetes_metadata/kubernetes.go:156 add_kubernetes_metadata: adding pod: kube-system/auditbeat-zflhb 2020-01-07T10:11:34.841Z DEBUG [processors] processing/processors.go:186 Publish event: { \"@timestamp\": \"2020-01-07T10:11:34.833Z\", \"@metadata\": { \"beat\": \"auditbeat\", \"type\": \"_doc\", \"version\": \"7.5.1\" }, \"service\": { \"type\": \"auditd\" }, \"ecs\": { \"version\": \"1.1.0\" }, \"host\": { \"name\": \"auditbeat-zflhb\" }, \"user\": { \"saved\": { \"id\": \"0\", \"name\": \"root\", \"group\": { \"id\": \"0\", \"name\": \"root\" } }, \"name\": \"root\", \"effective\": { \"name\": \"root\", \"id\": \"0\", \"group\": { \"id\": \"0\", \"name\": \"root\" } }, \"selinux\": { \"user\": \"kernel\" }, \"filesystem\": { \"group\": { \"id\": \"0\", \"name\": \"root\" }, \"id\": \"0\", \"name\": \"root\" }, \"id\": \"0\", \"group\": { \"id\": \"0\", \"name\": \"root\" } }, \"tags\": [ \"exec\" ], \"file\": { \"path\": \"/usr/bin/cat\", \"gid\": \"0\", \"device\": \"00:00\", \"owner\": \"root\", \"selinux\": { \"user\": \"unlabeled\" }, \"group\": \"root\", \"inode\": \"5759839\", \"mode\": \"0755\", \"uid\": \"0\" }, \"auditd\": { \"paths\": [ { \"dev\": \"00:e4\", \"nametype\": \"NORMAL\", \"ogid\": \"0\", \"ouid\": \"0\", \"rdev\": \"00:00\", \"cap_fi\": \"0000000000000000\", \"name\": \"/usr/bin/cat\", \"cap_fe\": \"0\", \"mode\": \"0100755\", \"obj_user\": \"unlabeled\", \"cap_fver\": \"0\", \"inode\": \"5759839\", \"item\": \"0\", \"cap_fp\": \"0000000000000000\" }, { \"dev\": \"00:e4\", \"item\": \"1\", \"ogid\": \"0\", \"inode\": \"5761686\", \"name\": \"/lib64/ld-linux-x86-64.so.2\", \"nametype\": \"NORMAL\", \"obj_user\": \"unlabeled\", \"mode\": \"0100755\", \"rdev\": \"00:00\", \"cap_fe\": \"0\", \"cap_fi\": \"0000000000000000\", \"cap_fp\": \"0000000000000000\", \"cap_fver\": \"0\", \"ouid\": \"0\" } ], \"message_type\": \"syscall\", \"sequence\": 180, \"result\": \"success\", \"data\": { \"pi\": \"00000020e80425fb\", \"old_pp\": \"00000020e80425fb\", \"fe\": \"0\", \"old_pe\": \"00000020e80425fb\", \"arch\": \"x86_64\", \"pe\": \"00000020e80425fb\", \"syscall\": \"execve\", \"tty\": \"pts0\", \"old_pa\": \"0000000000000000\", \"old_pi\": \"00000020e80425fb\", \"argc\": \"2\", \"exit\": \"0\", \"a3\": \"0\", \"a1\": \"c00000dd60\", \"a0\": \"c00007af10\", \"a2\": \"c0000e27e0\", \"pp\": \"00000020e80425fb\", \"pa\": \"0000000000000000\", \"fi\": \"0000000000000000\", \"fp\": \"0000000000000000\", \"fver\": \"0\" }, \"summary\": { \"actor\": { \"secondary\": \"root\", \"primary\": \"unset\" }, \"object\": { \"primary\": \"/usr/bin/cat\", \"type\": \"file\" }, \"how\": \"/usr/bin/cat\" } }, \"agent\": { \"ephemeral_id\": \"a30166dd-3759-4f24-ad74-7680ded60640\", \"hostname\": \"auditbeat-zflhb\", \"id\": \"9113ca40-a989-4bda-99fa-974d37fad1e3\", \"version\": \"7.5.1\", \"type\": \"auditbeat\" }, \"event\": { \"category\": \"audit-rule\", \"action\": \"executed\", \"outcome\": \"success\", \"module\": \"auditd\" }, \"process\": { \"name\": \"cat\", \"executable\": \"/usr/bin/cat\", \"working_directory\": \"/usr/share/auditbeat\", \"args\": [ \"cat\", \"fffff\" ], \"pid\": 16629, \"ppid\": 16620, \"title\": \"cat fffff\" } } there are only add_cloud_metadata in auditbeat deployment for kubernetes https://github.com/elastic/beats/blob/master/deploy/kubernetes/auditbeat-kubernetes.yaml thank you",
    "website_area": "discuss"
  },
  {
    "id": "9a38db7f-97d3-41cf-b019-20538d133363",
    "url": "https://discuss.elastic.co/t/monitiring-elasticsearch-cluster/213992",
    "title": "Monitiring elasticsearch cluster",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "kiran_kumar1",
    "date": "January 7, 2020, 6:48am January 7, 2020, 9:02am January 7, 2020, 9:21am",
    "body": "HI Team, Getting below issue while conncting from monitoring cluster to production cluster by using metricbeat. issue:more than one namespace configured accessing config,could you please suggest on this. Thank you Kiran",
    "website_area": "discuss"
  },
  {
    "id": "269d9bbb-60bb-4646-ae32-28172aadd275",
    "url": "https://discuss.elastic.co/t/issues-pushing-kubernetes-ingress-nginx-logs-using-filebeat-deamonset-pods/206923",
    "title": "Issues pushing kubernetes ingress-nginx logs using filebeat deamonset pods",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Siva_Krishna",
    "date": "November 7, 2019, 10:41am November 7, 2019, 3:40pm November 11, 2019, 12:42pm November 11, 2019, 1:12pm November 11, 2019, 1:52pm November 11, 2019, 3:42pm November 13, 2019, 7:19am November 13, 2019, 8:15am November 13, 2019, 10:21am November 13, 2019, 4:37pm November 14, 2019, 9:11am November 14, 2019, 9:26am November 18, 2019, 3:00pm November 18, 2019, 3:58pm November 19, 2019, 3:07pm November 27, 2019, 2:33pm November 28, 2019, 9:10am November 28, 2019, 1:07pm December 23, 2019, 9:12am January 7, 2020, 8:40am",
    "body": "Hello all, We're facing issues pushing kubernetes ingress-nginx logs using filebeat deamonset pods. using filebeat v6.2.4, 6.3.0, & 6.4.0. please let us know if this is the correct configs to push only ingress-nginx pods logs to ELK, not all namespace pods. filebeat.yml: filebeat.config: prospectors: # Mounted `filebeat-prospectors` configmap: path: ${path.config}/prospectors.d/*.yml # Reload prospectors configs as they change: reload.enabled: false modules: path: ${path.config}/modules.d/*.yml # Reload module configs as they change: reload.enabled: false filebeat.autodiscover: providers: - type: docker templates: - condition: equals: docker.container.labels.io.kubernetes.container.name: \"nginx-ingress-controller\" config: - module: nginx access: input: type: docker containers.stream: stdout containers.ids: - \"${data.docker.container.id}\" error: input: type: docker containers.stream: stderr containers.ids: - \"${data.docker.container.id}\" processors: - drop_event: when.not: or: - contains: docker.container.labels.io.kubernetes.container.name: \"nginx-ingress-controller\" processors: - add_cloud_metadata: cloud.id: ${ELASTIC_CLOUD_ID} cloud.auth: ${ELASTIC_CLOUD_AUTH} output: logstash: hosts: [\"elk.lktest1.com:5044\"] prospectors file;--- kubernetes.yml: - type: docker containers: ids: \"*\" path: /var/lib/docker/containers fields: type: k8s_nginx_ingress_ctrls registry_file: \"/var/lib/filebeat/registry\" multiline: pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}' negate: true match: after processors: - add_kubernetes_metadata: in_cluster: true namespace: ingress-nginx using deamonset to load above 2 configmap files. errors getting from filebeat pods: Exiting: error loading config file: yaml: line 17: did not find expected '-' indicator",
    "website_area": "discuss"
  },
  {
    "id": "809ac0d3-dd00-4983-a374-40b0388727f1",
    "url": "https://discuss.elastic.co/t/exclude-part-of-kubernetes-log/213348",
    "title": "Exclude part of kubernetes log",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "markrity",
    "date": "December 30, 2019, 12:19pm December 30, 2019, 2:22pm December 30, 2019, 4:33pm January 6, 2020, 7:29pm",
    "body": "I'm trying to make Filebeat to exclude part of the whole log that being sent to Logstash through filebeat config xml file. I have log like this: { \"@timestamp\" : \"timestamp\" , \"name\" : \"general\" , \"kubernetes\" : { \"namespace\" : \"namespace\" , \"pod\" : { \"name\" : \"name\" , \"uid\" : \"uid\" }, \"container\" : { \"name\" : \"name\" , \"image\" : \"image\" }, \"node\" : { \"name\" : \"name\" }, \"labels\" : { \"env\" : \"env\" , \"app\" : \"app\" , \"pod-template-hash\" : \"pod-template-hash\" , \"labels\" : \"\" }, \"replicaset\" : { \"name\" : \"name\" } }, \"hostname\" : \"hostname\" , \"system\" : \"system\" , \"tag\" : \"tag\" , \"host\" : { \"name\" : \"name\" }, \"ecs\" : { \"version\" : \"version\" }, \"agent\" : { \"ephemeral_id\" : \"ephemeral_id\" , \"version\" : \"version\" , \"hostname\" : \"hostname\" , \"id\" : \"id\" , \"type\" : \"filebeat\" }, \"v\" : 0 , \"version\" : \"version\" , \"pid\" : 8 , \"stream\" : \"stdout\" , \"app\" : \"app\" , \"message\" : \"message\" , \"time\" : \"time\" , \"@version\" : \"1\" , \"input\" : { \"type\" : \"container\" }, \"env\" : \"staging\" , \"level\" : \"DEBUG\" } I want to remove from this log just the \"kubernetes\" map part. Is there is a way to do that? Since exclude_line excludes the whole log.",
    "website_area": "discuss"
  },
  {
    "id": "e0f8ca69-e6dc-497d-96e1-07cced8feb75",
    "url": "https://discuss.elastic.co/t/cant-get-filebeats-to-produce-any-info-from-redis-or-postgres/213582",
    "title": "Can't get filebeats to produce any info from redis or postgres",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "aleksas",
    "date": "January 3, 2020, 8:56am January 6, 2020, 12:43pm",
    "body": "Using helm-charts to configure sandbox cluster monitoring. Managed to configure metribeats for redis and postgres, but struggling to do same with filebeats. kibana show almost (2) no documents in filebeat-* index. Neither redis nor postgres container logs show any error... kubernetes is running on minikube virtualbox vm on windows 10. here is output of helm upgrade --dry-run --debug my-release . > out.yaml https://gist.githubusercontent.com/aleksas/92099179b7995c047143c724e8e3e3dd/raw/60fc0f4e4cf44d554319dda6fac991b9cfb3ccb0/out.yaml how to make sure that either filebeats autodiscovered pod and monitoring it? could it be that with default settings for redis and postgres there are no logs to be registered?..",
    "website_area": "discuss"
  },
  {
    "id": "9cd28b2f-2a95-43b1-8df3-fc8641f0103c",
    "url": "https://discuss.elastic.co/t/auditbeat-permission-error/213318",
    "title": "Auditbeat permission Error",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "Vamsi_Vutukuri",
    "date": "December 30, 2019, 8:02am December 31, 2019, 4:28pm January 2, 2020, 6:46am January 6, 2020, 12:14pm",
    "body": "Hi, i'm using elk stack 7.1.1 with x-pack installed and i'm trying to setup auditbeat but i am getting the following error: 2019-12-30T13:18:04.176+0530 ERROR instance/beat.go:802 Exiting: 1 error: 1 error: failed to create audit client: failed to get audit status: operation not permitted Exiting: 1 error: 1 error: failed to create audit client: failed to get audit status: operation not permitted auditbeat.yml conf auditbeat.modules: - module: auditd # Load audit rules from separate files. Same format as audit.rules(7). audit_rule_files: [ '${path.config}/audit.rules.d/*.conf' ] audit_rules: | - module: file_integrity paths: - /bin - /usr/bin - /sbin - /usr/sbin - /etc - module: system datasets: - host - login - package - process - socket - user state.period: 12h user.detect_password_changes: true login.wtmp_file_pattern: /var/log/wtmp* login.btmp_file_pattern: /var/log/btmp* setup.template.settings: index.number_of_shards: 1 index.codec: best_compression setup.kibana: host: \"localhost:5601\" output.elasticsearch: hosts: [\"localhost:9200\"] index: \"auditbeat-7.1.1-%{+yyyy.MM.dd}\" protocol: \"https\" username: \"elastic\" password: \"mypassword\" setup.template: name: 'auditbeat' pattern: 'auditbeat-*' enabled: false Please help me solve it.",
    "website_area": "discuss"
  },
  {
    "id": "310890d9-9c77-4502-85eb-eefe93c79cc4",
    "url": "https://discuss.elastic.co/t/metricbeat-add-a-custom-field-with-existing-value/213873",
    "title": "Metricbeat - Add a custom field with existing value",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Sudharshan_K_S",
    "date": "January 6, 2020, 10:12am",
    "body": "How can i add a custom field whose value will be equal to an exiting field Eg processors: -add_fields: fields: new_field: host.name",
    "website_area": "discuss"
  },
  {
    "id": "40a53806-27a2-44fc-8fd4-5461111918b1",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-include-separate-processor-config-files-from-sub-dir/213871",
    "title": "Is it possible to include separate processor config files from sub dir",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "stefws",
    "date": "January 6, 2020, 10:04am",
    "body": "Wondering if might be possible to define winlogbeat processors in separate YML files to be included by the main config file thus to ease maintenance through automation tools?",
    "website_area": "discuss"
  },
  {
    "id": "651556de-8eda-4999-83f0-ab001f296e76",
    "url": "https://discuss.elastic.co/t/pipeline-grok-truncating-at-n-in-multiline-message-from-filebeat/211800",
    "title": "Pipeline - grok - truncating at \"\\n\" in [multiline] message from filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "nigel.piggott",
    "date": "December 16, 2019, 9:07am December 16, 2019, 9:07am December 16, 2019, 10:23am December 16, 2019, 12:12pm December 17, 2019, 11:40am December 20, 2019, 10:41am January 6, 2020, 8:47am",
    "body": "Filebeat is configured to correctly process a mutline file Using the ingest pipeline the grok processor extracts fields from the \"message\" However it is truncating the message when the message contains the regex \"\\n\" note this worked perfeectly fine in [a very] early version of ELK e.g file contains 2019-12-12 14:30:49.0276 ERROR Core.Processors.PurchaseInvoiceProcessor Failed to create Purchase Invoice for Purchase Order with Order # 'NNNNN' not found. at Vantage.Services.SupplierLiveLink.Core.Processors.PurchaseInvoiceProcessor.<Process>d__7.MoveNext() pipeline grok pattern %{TIMESTAMP_ISO8601:timestamp} %{LEVEL:level} %{GREEDYDATA:msg} , { \"set\" : { \"field\" : \"message_original\" , \"value\" : \"{{message}}\" } } , { \"remove\" : { \"field\" : \"message\" } } , { \"rename\" : { \"field\" : \"msg\" , \"target_field\" : \"message\" } } but the message field appears in kibana as being truncated at the new line e.g. message Core.Processors.PurchaseInvoiceProcessor Failed to create Purchase Invoice for Purchase Order with Order # 'NNNNN' not found. and the message_original field 2019-12-12 14:30:49.0276 ERROR Core.Processors.PurchaseInvoiceProcessor Failed to create Purchase Invoice for Purchase Order with Order # 'NNNNN' not found.\\n at Vantage.Services.SupplierLiveLink.Core.Processors.PurchaseInvoiceProcessor.<Process>d__7.MoveNext()",
    "website_area": "discuss"
  },
  {
    "id": "6235ed06-9717-4155-b171-602c04504697",
    "url": "https://discuss.elastic.co/t/pipeline-grok-truncating-at-n-in-multiline-message-from-filebeat/211800",
    "title": "Pipeline - grok - truncating at \"\\n\" in [multiline] message from filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "nigel.piggott",
    "date": "December 16, 2019, 9:07am December 16, 2019, 9:07am December 16, 2019, 10:23am December 16, 2019, 12:12pm December 17, 2019, 11:40am December 20, 2019, 10:41am January 6, 2020, 8:47am",
    "body": "Filebeat is configured to correctly process a mutline file Using the ingest pipeline the grok processor extracts fields from the \"message\" However it is truncating the message when the message contains the regex \"\\n\" note this worked perfeectly fine in [a very] early version of ELK e.g file contains 2019-12-12 14:30:49.0276 ERROR Core.Processors.PurchaseInvoiceProcessor Failed to create Purchase Invoice for Purchase Order with Order # 'NNNNN' not found. at Vantage.Services.SupplierLiveLink.Core.Processors.PurchaseInvoiceProcessor.<Process>d__7.MoveNext() pipeline grok pattern %{TIMESTAMP_ISO8601:timestamp} %{LEVEL:level} %{GREEDYDATA:msg} , { \"set\" : { \"field\" : \"message_original\" , \"value\" : \"{{message}}\" } } , { \"remove\" : { \"field\" : \"message\" } } , { \"rename\" : { \"field\" : \"msg\" , \"target_field\" : \"message\" } } but the message field appears in kibana as being truncated at the new line e.g. message Core.Processors.PurchaseInvoiceProcessor Failed to create Purchase Invoice for Purchase Order with Order # 'NNNNN' not found. and the message_original field 2019-12-12 14:30:49.0276 ERROR Core.Processors.PurchaseInvoiceProcessor Failed to create Purchase Invoice for Purchase Order with Order # 'NNNNN' not found.\\n at Vantage.Services.SupplierLiveLink.Core.Processors.PurchaseInvoiceProcessor.<Process>d__7.MoveNext()",
    "website_area": "discuss"
  },
  {
    "id": "e71c16f7-c933-45d9-a239-6b6c49af50cd",
    "url": "https://discuss.elastic.co/t/filebeat-config-to-read-logs-using-docker/213672",
    "title": "Filebeat config to read logs using docker",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 3, 2020, 7:39am January 3, 2020, 4:49pm January 6, 2020, 6:50am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "37d7d0ee-90f5-4406-9891-f37f6d052348",
    "url": "https://discuss.elastic.co/t/kubernetes-collecting-allocate-resource-percentages-for-nodes/213817",
    "title": "Kubernetes: collecting allocate resource percentages for nodes",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "collinwright",
    "date": "January 5, 2020, 6:28pm",
    "body": "I'm interested in using Metricbeat to monitor the resources Kubernetes has allocated (but may not necessarily be using) on each node. These numbers can be seen, for example, by running \"kubectl describe node\": ... Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 800m (5%) 500m (3%) memory 812Mi (5%) 1112Mi (7%) ephemeral-storage 10Gi (39%) 1 ... I'm not seeing how I could collect this data easily with Metricbeat. I've looked at the available kubernetes.node fields, but the closest I can find is kubernetes.node.cpu.allocatable.cores and kubernetes.node.memory.allocatable.cores. These fields are the upper limit that requests and limits can reach, but do not reflect the actual amount of resources currently scheduled. Is there any way to get this information currently, or would I be able to help submit a PR to add this functionality?",
    "website_area": "discuss"
  },
  {
    "id": "af9177d1-a4c3-42c0-87fa-fd98e25079b0",
    "url": "https://discuss.elastic.co/t/filebeat-is-always-read-file-from-beginning/213563",
    "title": "Filebeat is always read file from beginning",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "rguptarg",
    "date": "January 2, 2020, 11:09am January 4, 2020, 12:11pm",
    "body": "HI Team, I am using filebeat-7.3.2-1.x86_64 for application log reading. but all the time filebeat read log from starting, hence duplicate data is present in elastic please suggest what configuration required?",
    "website_area": "discuss"
  },
  {
    "id": "b966686b-47ee-4abb-92fa-460f9a0904c4",
    "url": "https://discuss.elastic.co/t/setting-host-hostname-mapping-to-static/213224",
    "title": "Setting host.hostname mapping to static",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Ryan_Downey",
    "date": "December 27, 2019, 7:04pm December 30, 2019, 1:40pm December 31, 2019, 6:01pm January 2, 2020, 1:47pm January 3, 2020, 6:52pm January 4, 2020, 8:44am",
    "body": "ECE 2.3 Metricbeat 7.0.0 & 7.5.0 I need some guidance on how to stop the dynamic templating for host.hostname of our metricbeats. We've got a third party product that can only read data from the host.hostname field if the mapping is the same as metricbeat 7.0.0. From the dev tools I got part of the host.hostname mapping and as you can see its just has a \"type\" of \"keyword\". \"host\" : { \"properties\" : { \"architecture\" : { \"type\" : \"keyword\", \"ignore_above\" : 1024 }, \"containerized\" : { \"type\" : \"boolean\" }, \"geo\" : { \"properties\" : { \"city_name\" : { \"type\" : \"keyword\", \"ignore_above\" : 1024 }, \"continent_name\" : { \"type\" : \"keyword\", \"ignore_above\" : 1024 }, \"country_iso_code\" : { \"type\" : \"keyword\", \"ignore_above\" : 1024 }, \"country_name\" : { \"type\" : \"keyword\", \"ignore_above\" : 1024 }, \"location\" : { \"type\" : \"geo_point\" }, \"name\" : { \"type\" : \"keyword\", \"ignore_above\" : 1024 }, \"region_iso_code\" : { \"type\" : \"keyword\", \"ignore_above\" : 1024 }, \"region_name\" : { \"type\" : \"keyword\", \"ignore_above\" : 1024 } } }, \"group\" : { \"type\" : \"keyword\" }, \"hostname\" : { \"type\" : \"keyword\", \"ignore_above\" : 1024 }, I've tried to load the 7.0.0 template manually and apply it to metricbeat-7.5.0 so that the application can read the data. What I'm having problems with is that the host.hostname field is being mapped as a text field and keyword field it appears which is preventing the app from reading the data correctly. How do I set the mapping up so that hostname is just type keyword and not text. \"host\": { \"properties\": { \"architecture\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"containerized\": { \"type\": \"boolean\" }, \"group\": { \"type\": \"keyword\" }, \"hostname\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } },",
    "website_area": "discuss"
  },
  {
    "id": "eb2332c9-2cde-4e1d-a7ea-bc0d3e442895",
    "url": "https://discuss.elastic.co/t/beats-central-management-questions/204123",
    "title": "Beats Central Management Questions",
    "category": [
      "Beats",
      "Central Management"
    ],
    "author": "mgotechlock",
    "date": "October 17, 2019, 8:14pm January 3, 2020, 8:25pm",
    "body": "Easy questions that I have not been able to determine the answer to yet. Does Beats Central Management support other beats besides FileBeat and MetricBeat because those are my only two options when I click Enroll Beats? Winlogbeat? Do other people successfully enroll the manual way, via filebeat enroll url --username....? I just get \"Error creating a new enrollment token: error while parsing Kibana response: json: cannot unmarshal string into Go struct field BaseResponse.error of type api.ErrorResponse\" everytime. Maybe somebody smarter than me can post how you've automated the deployment of a CM managed filebeat?",
    "website_area": "discuss"
  },
  {
    "id": "2b97ab6d-30da-47f9-a5ad-cd2ed7b56749",
    "url": "https://discuss.elastic.co/t/elasticsearch-json-logs-with-filebeat-module-and-ingest-pipeline/213376",
    "title": "Elasticsearch json logs with filebeat module and ingest pipeline",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "jeffspahr",
    "date": "December 30, 2019, 6:35pm December 30, 2019, 8:22pm December 31, 2019, 6:13pm December 31, 2019, 8:07pm January 3, 2020, 5:11pm",
    "body": "Hello, Elasticsearch writes json logs by default in 7.x (thanks!). When configuring Filebeat's Elasticsearch module, I was thinking I could ignore the ingest pipeline part since it's already shipping structured logs, but it looks like there are ingest pipelines specific to the json logs as well. What's the logic in doing parsing at this stage when the logs are already structured and Elasticsearch and Filebeat have full control over field names at the source? I ask because adding in the ingest pipeline adds a decent amount of complexity. There's a lot more to managing the lifecycle of run once setups like ingest pipelines across beat versions than there is in just doing config management for filebeat configs. Thanks, Jeff",
    "website_area": "discuss"
  },
  {
    "id": "834fd171-e540-4d93-97fc-edeac3df10e3",
    "url": "https://discuss.elastic.co/t/problem-with-ansible-beats-and-processors-config/213690",
    "title": "Problem with ansible-beats and processors config",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "blabu23",
    "date": "January 3, 2020, 5:05pm January 3, 2020, 4:57pm January 3, 2020, 5:03pm January 3, 2020, 5:05pm",
    "body": "Hi there! I am trying to install/configure filebeat with the ansible role from elastic. My playbook looks like this: - hosts: filebeat_hosts roles: - role: role-beats vars: use_repository: true beats_version: 6.8.5 beat: filebeat beat_conf: inputs: - type: log enabled: false paths: - type: docker enabled: true containers.ids: '*' filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false setup.template.settings: index.number_of_shards: 3 processors: - add_locale: ~ - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ After running ansible-playbook with this the result on my host is fine with the exception of the processors section. It results in: processors: - add_locale: null - add_host_metadata: null - add_cloud_metadata: null - add_docker_metadata: null This is AFAIK normal since ansible/jinja2 is treating the ~ as an alias for null. Escaping the ~ with single or double quotes results in this: processors: - add_locale: '~' - add_host_metadata: '~' - add_cloud_metadata: '~' - add_docker_metadata: '~' but filebeat then complaints with this message when trying to start and finally stops starting: Exiting: error unpacking config data: required 'object', but found 'string' in field 'processors.0.add_locale' (source:'/etc/filebeat/filebeat.yml') Any idea, how to configure this section with ansible? Cheers, Martin",
    "website_area": "discuss"
  },
  {
    "id": "c835cabf-2624-4e2b-a415-b0878f5e8df4",
    "url": "https://discuss.elastic.co/t/cant-parse-event-as-syslog-rfc3164/212182",
    "title": "Can't parse event as syslog rfc3164",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "vipin.suthar",
    "date": "December 17, 2019, 3:08pm January 3, 2020, 11:11am",
    "body": "Hello, We are facing a known issue with syslog input of filebeat, And running our Elasticsearch cluster on CentOS Linux release 7.7.1908 (Core) VM environment. We are working with Cisco ASA and FTD firewall logs, But on filebeat journalctl getting following error messages: Dec 17 19:11:03 ELKFWLOGGING filebeat[8010]: 2019-12-17T19:11:03.214+0530 ERROR [syslog] syslog/input.go:134 can't parse event as syslog rfc3164 {\"message\": \"<166>Dec 17 2019 19:11:03 10.1.3.4 : %ASA-6-302016: Teardown UDP connection 3187545438 for XXXXX:1.2.3.4/12345 to XXXXX:5.6.7.8/12 duration 0:00:00 bytes 56\\n\"} Dec 17 19:11:03 ELKFWLOGGING filebeat[8010]: 2019-12-17T19:11:03.215+0530 ERROR [syslog] syslog/input.go:134 can't parse event as syslog rfc3164 {\"message\": \"<166>Dec 17 2019 19:11:03 10.1.3.4 : %ASA-6-302015: Built inbound UDP connection 3187545439 for XXXXX:1.2.3.4/12398 1.2.3.4/12398) to XXXXX:5.6.7.8/90 (5.6.7.8/90)\\n\"} My Filebeat file configuration is: filebeat.inputs: - type: syslog protocol.udp: host: \"0.0.0.0:9001\" filebeat.config.modules: path: ${path.config}/modules.d/*.yml setup.template.settings: index.number_of_shards: 1 index.codec: best_compression #_source.enabled: false setup.kibana: host: \"kibana.nw-elastic.dev.com:5601\" output.elasticsearch: hosts: [\"n1.nw-elastic.dev.com:9200\"] processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ - add_kubernetes_metadata: ~ The Cisco module config file at /etc/filebeat/modules.d/cisco.yml configuration is: - module: cisco asa: enabled: true var.input: syslog var.syslog_host: 0.0.0.0 var.syslog_port: 9001 var.log_level: 6 ftd: enabled: true #var.input: syslog #var.syslog_host: localhost #var.syslog_port: 9003 #var.log_level: 7 ios: enabled: true #var.input: syslog #var.syslog_host: localhost #var.syslog_port: 9002 #var.paths: We are receiving logs at elasticsearch end but without parsing! We've tested the cluster with both Version 7.4.2 and the latest 7.5.0, But still don't found any working solution suggested on discuss, github, etc. forums about the subjected issue. Any help will be much appreciated Thanks",
    "website_area": "discuss"
  },
  {
    "id": "ec2e488c-ed77-4162-ab9a-e9249da047fc",
    "url": "https://discuss.elastic.co/t/how-packetbeat-determine-source-and-destination-in-tcp-flow/212313",
    "title": "How packetbeat determine source and destination in TCP flow?",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "jaypark81",
    "date": "December 18, 2019, 12:05pm January 3, 2020, 3:39am January 3, 2020, 6:35am",
    "body": "Hi, I'm investigating network flows. And Packetbeat collect network traffic including forwarding. I found some flow's destination is port 80 for http. Otherwise, some flows source port is port 80. I'm very sure the port 80 must be destination. At this point, how does Packetbeat determine source and destination? How can we solve this issue? Thank you in advance, jaypark81",
    "website_area": "discuss"
  },
  {
    "id": "23ad8c22-74f1-4785-9c4d-3792d506f586",
    "url": "https://discuss.elastic.co/t/how-to-ignore-bookmark-winlogbeat-yml-when-start-the-winlogbeat/212787",
    "title": "How to ignore bookmark(.winlogbeat.yml) when start the winlogbeat?",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "e997cd7e8d9915436150",
    "date": "December 22, 2019, 5:08pm January 3, 2020, 4:39am January 3, 2020, 4:41am",
    "body": "Quick question. How can I play one event log on repeat or loop? How to ignore bookmark when start the winlogbeat? And can i auto restart the winlogbeat when changed the winlogbeat.yml? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "d10d1b46-e3e0-4269-8d12-2c9e51afe725",
    "url": "https://discuss.elastic.co/t/processor-in-cascade/210971",
    "title": "Processor in cascade",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "franco.federico",
    "date": "December 7, 2019, 3:11pm January 3, 2020, 3:36am",
    "body": "Hi all I think to use processor, I'd like to know if it possible to do this processors: - add_filed: fields: name: \"pippo\",\"pluto\",\"paperino\" processors: - if: contains: name: \"pluto\" then: - add_tags tags: true target: \"is_found\" else: - drop_event Do you think that is a correct configuration? I'd like to improve this processor in cascade with if contains change contains with another condition in order to check a field in winlogbeat like this - if: contains: name: winlog.eventdata.user Thank you Franco",
    "website_area": "discuss"
  },
  {
    "id": "e9953312-4329-43cf-9719-cd977942f373",
    "url": "https://discuss.elastic.co/t/winlog-beat-data-issue/212385",
    "title": "Winlog beat data issue",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "mparp",
    "date": "December 18, 2019, 6:36pm January 3, 2020, 3:07am",
    "body": "How many beats (winlog) I can connect after Trail License expired ? The problem is I dont see events from newly configured beats starting from moment when license has expired. Is my issue related to license or I am doing something wrong ? Regards, Maxim",
    "website_area": "discuss"
  },
  {
    "id": "779692e9-947e-418b-ac5a-c4355874344f",
    "url": "https://discuss.elastic.co/t/which-is-more-efficient-sequence-or-or/213640",
    "title": "Which is more efficient, sequence or \"or\"?",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "barely47",
    "date": "January 3, 2020, 1:03am",
    "body": "- drop_event: when: or: [{equals: {process.executable: /bin/date}}, {equals: {process.executable: /bin/df}}, {equals: {process.executable: /bin/dmesg}}] or - drop_event: equals: {process.executable: /bin/date} - drop_event: equals: {process.executable: /bin/df} - drop_event: equals: {process.executable: /bin/dmesg} I have a large number of these kinds of contructs so would like them to be as efficient as possible.",
    "website_area": "discuss"
  },
  {
    "id": "29ec4849-7c82-4ab9-a5e2-0e68b46ed032",
    "url": "https://discuss.elastic.co/t/kubernetes-autodiscover-not-working/213630",
    "title": "Kubernetes autodiscover not working",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "kluvi",
    "date": "January 2, 2020, 9:37pm",
    "body": "Hi. I have one Pod with running Heartbeat in our K8s cluster and tried to setup autodiscovery, but without any success... kluvi@kluvi:~$ k log -f deployment/service-monitoring-heartbeat-autodiscover log is DEPRECATED and will be removed in a future version. Use logs instead. 2020-01-02T21:14:39.139Z INFO instance/beat.go:610 Home path: [/usr/share/heartbeat] Config path: [/usr/share/heartbeat] Data path: [/usr/share/heartbeat/data] Logs path: [/usr/share/heartbeat/logs] 2020-01-02T21:14:39.139Z DEBUG [beat] instance/beat.go:662 Beat metadata path: /usr/share/heartbeat/data/meta.json 2020-01-02T21:14:39.140Z INFO instance/beat.go:618 Beat ID: f75d791a-4033-4e36-8580-33604a49b4b8 2020-01-02T21:14:39.149Z INFO add_kubernetes_metadata/kubernetes.go:68 add_kubernetes_metadata: kubernetes env detected, with version: v1.15.3 2020-01-02T21:14:39.149Z DEBUG [kubernetes] add_kubernetes_metadata/kubernetes.go:130 add_kubernetes_metadata: could not initialize kubernetes plugin with zero matcher plugins 2020-01-02T21:14:39.149Z DEBUG [processors] processors/processor.go:101 Generated new processors: add_kubernetes_metadata 2020-01-02T21:14:39.150Z INFO [api] api/server.go:62 Starting stats endpoint 2020-01-02T21:14:39.151Z INFO [api] api/server.go:64 Metrics endpoint listening on: 127.0.0.1:5066 (configured: localhost) 2020-01-02T21:14:39.150Z DEBUG [seccomp] seccomp/seccomp.go:117 Loading syscall filter {\"seccomp_filter\": {\"no_new_privs\":true,\"flag\":\"tsync\",\"policy\":{\"default_action\":\"errno\",\"syscalls\":[{\"names\":[\"accept\",\"accept4\",\"access\",\"arch_prctl\",\"bind\",\"brk\",\"chmod\",\"clock_gettime\",\"clone\",\"close\",\"connect\",\"dup\",\"dup2\",\"epoll_create\",\"epoll_create1\",\"epoll_ctl\",\"epoll_pwait\",\"epoll_wait\",\"exit\",\"exit_group\",\"fchdir\",\"fchmod\",\"fchown\",\"fcntl\",\"fdatasync\",\"flock\",\"fstat\",\"fstatfs\",\"fsync\",\"ftruncate\",\"futex\",\"getcwd\",\"getdents\",\"getdents64\",\"geteuid\",\"getgid\",\"getpeername\",\"getpid\",\"getppid\",\"getrandom\",\"getrlimit\",\"getrusage\",\"getsockname\",\"getsockopt\",\"gettid\",\"gettimeofday\",\"getuid\",\"inotify_add_watch\",\"inotify_init1\",\"inotify_rm_watch\",\"ioctl\",\"kill\",\"listen\",\"lseek\",\"lstat\",\"madvise\",\"mincore\",\"mkdirat\",\"mmap\",\"mprotect\",\"munmap\",\"nanosleep\",\"newfstatat\",\"open\",\"openat\",\"pipe\",\"pipe2\",\"poll\",\"ppoll\",\"pread64\",\"pselect6\",\"pwrite64\",\"read\",\"readlink\",\"readlinkat\",\"recvfrom\",\"recvmmsg\",\"recvmsg\",\"rename\",\"renameat\",\"rt_sigaction\",\"rt_sigprocmask\",\"rt_sigreturn\",\"sched_getaffinity\",\"sched_yield\",\"sendfile\",\"sendmmsg\",\"sendmsg\",\"sendto\",\"set_robust_list\",\"setitimer\",\"setsockopt\",\"shutdown\",\"sigaltstack\",\"socket\",\"splice\",\"stat\",\"statfs\",\"sysinfo\",\"tgkill\",\"time\",\"tkill\",\"uname\",\"unlink\",\"unlinkat\",\"wait4\",\"waitid\",\"write\",\"writev\"],\"action\":\"allow\"}]}}} 2020-01-02T21:14:39.151Z INFO [seccomp] seccomp/seccomp.go:124 Syscall filter successfully installed 2020-01-02T21:14:39.151Z INFO [beat] instance/beat.go:941 Beat info {\"system_info\": {\"beat\": {\"path\": {\"config\": \"/usr/share/heartbeat\", \"data\": \"/usr/share/heartbeat/data\", \"home\": \"/usr/share/heartbeat\", \"logs\": \"/usr/share/heartbeat/logs\"}, \"type\": \"heartbeat\", \"uuid\": \"f75d791a-4033-4e36-8580-33604a49b4b8\"}}} 2020-01-02T21:14:39.151Z INFO [beat] instance/beat.go:950 Build info {\"system_info\": {\"build\": {\"commit\": \"6d0d0ae079e5cb1d4f224801ac6df926dfb1594c\", \"libbeat\": \"7.5.0\", \"time\": \"2019-11-25T23:40:34.000Z\", \"version\": \"7.5.0\"}}} 2020-01-02T21:14:39.151Z INFO [beat] instance/beat.go:953 Go runtime info {\"system_info\": {\"go\": {\"os\":\"linux\",\"arch\":\"amd64\",\"max_procs\":64,\"version\":\"go1.12.12\"}}} 2020-01-02T21:14:39.170Z INFO [beat] instance/beat.go:957 Host info {\"system_info\": {\"host\": {\"architecture\":\"x86_64\",\"boot_time\":\"2019-12-23T15:15:23Z\",\"containerized\":false,\"name\":\"service-monitoring-heartbeat-autodiscover-dd8f5889-rzjlw\",\"ip\":[\"127.0.0.1/8\",\"10.233.91.39/32\"],\"kernel_version\":\"3.10.0-1062.9.1.el7.x86_64\",\"mac\":[\"82:49:80:6d:fb:c2\"],\"os\":{\"family\":\"redhat\",\"platform\":\"centos\",\"name\":\"CentOS Linux\",\"version\":\"7 (Core)\",\"major\":7,\"minor\":7,\"patch\":1908,\"codename\":\"Core\"},\"timezone\":\"UTC\",\"timezone_offset_sec\":0}}} 2020-01-02T21:14:39.170Z INFO [beat] instance/beat.go:986 Process info {\"system_info\": {\"process\": {\"capabilities\": {\"inheritable\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"permitted\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"effective\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"bounding\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"ambient\":null}, \"cwd\": \"/usr/share/heartbeat\", \"exe\": \"/usr/share/heartbeat/heartbeat\", \"name\": \"heartbeat\", \"pid\": 1, \"ppid\": 0, \"seccomp\": {\"mode\":\"filter\",\"no_new_privs\":true}, \"start_time\": \"2020-01-02T21:14:38.080Z\"}}} 2020-01-02T21:14:39.170Z INFO instance/beat.go:297 Setup Beat: heartbeat; Version: 7.5.0 2020-01-02T21:14:39.170Z DEBUG [beat] instance/beat.go:323 Initializing output plugins 2020-01-02T21:14:39.171Z INFO [index-management] idxmgmt/std.go:182 Set output.elasticsearch.index to 'heartbeat-7.5.0' as ILM is enabled. 2020-01-02T21:14:39.171Z INFO elasticsearch/client.go:171 Elasticsearch url: http://service-monitoring-elasticsearch.bf.svc:9200 2020-01-02T21:14:39.171Z DEBUG [publisher] pipeline/consumer.go:137 start pipeline event consumer 2020-01-02T21:14:39.171Z INFO [publisher] pipeline/module.go:97 Beat name: autodiscover 2020-01-02T21:14:39.171Z INFO instance/beat.go:429 heartbeat start running. 2020-01-02T21:14:39.171Z INFO beater/heartbeat.go:80 heartbeat is running! Hit CTRL-C to stop it. 2020-01-02T21:14:39.171Z INFO [monitoring] log/log.go:118 Starting metrics logging every 30s 2020-01-02T21:14:39.171Z WARN [cfgwarn] kubernetes/kubernetes.go:59 BETA: The kubernetes autodiscover is beta 2020-01-02T21:14:39.172Z INFO kubernetes/util.go:79 kubernetes: Using node kubernetes.default.svc provided in the config 2020-01-02T21:14:39.172Z DEBUG [autodiscover] kubernetes/kubernetes.go:84 Initializing a new Kubernetes watcher using host: kubernetes.default.svc 2020-01-02T21:14:39.172Z DEBUG [conditions] conditions/conditions.go:98 New condition contains: map[] 2020-01-02T21:14:39.173Z DEBUG [autodiscover] autodiscover/autodiscover.go:82 Configured autodiscover provider: kubernetes 2020-01-02T21:14:39.173Z INFO [autodiscover] autodiscover/autodiscover.go:104 Starting autodiscover manager 2020-01-02T21:14:39.273Z DEBUG [kubernetes] kubernetes/watcher.go:241 cache sync done 2020-01-02T21:14:39.273Z DEBUG [scheduler] scheduler/scheduler.go:186 Start scheduler. 2020-01-02T21:15:09.175Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":30,\"time\":{\"ms\":38}},\"total\":{\"ticks\":110,\"time\":{\"ms\":125},\"value\":110},\"user\":{\"ticks\":80,\"time\":{\"ms\":87}}},\"handles\":{\"limit\":{\"hard\":1048576,\"soft\":1048576},\"open\":9},\"info\":{\"ephemeral_id\":\"0c02389c-1770-40b0-a2f2-6f5a7734fe62\",\"uptime\":{\"ms\":30049}},\"memstats\":{\"gc_next\":9020496,\"memory_alloc\":4639840,\"memory_total\":9716216,\"rss\":38191104},\"runtime\":{\"goroutines\":26}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"output\":{\"type\":\"elasticsearch\"},\"pipeline\":{\"clients\":0,\"events\":{\"active\":0}}},\"system\":{\"cpu\":{\"cores\":64},\"load\":{\"1\":14.33,\"15\":16.71,\"5\":16.23,\"norm\":{\"1\":0.2239,\"15\":0.2611,\"5\":0.2536}}}}}} Config (part of it - it does not contain any heartbeat.monitor): heartbeat.autodiscover: providers: - type: kubernetes host: kubernetes.default.svc namespace: bf resource: service scope: cluster include_annotations: [\"cz.bdfx.heartbeat.type\"] templates: - condition: contains: kubernetes.annotations.cz.bdfx.heartbeat.type: \"http\" config: - type: http enabled: true name: \"${data.kubernetes.annotations.cz.bdfx.heartbeat.name}\" mode: any hosts: [\"${data.kubernetes.service.name}.bf.svc:${data.kubernetes.annotations.cz.bdfx.heartbeat.port}\"] schedule: \"@every ${data.kubernetes.annotations.cz.bdfx.heartbeat.seconds}s\" timeout: 16s max_redirects: 0 check: request: method: GET response: status: 200 Service: kluvi@kluvi:~$ k get svc service-monitoring-kibana -o yaml apiVersion: v1 kind: Service metadata: annotations: cz.bdfx.heartbeat.name: Kibana @monitoring cz.bdfx.heartbeat.port: \"80\" cz.bdfx.heartbeat.seconds: \"10\" cz.bdfx.heartbeat.type: http ... Another Pod with another Heartbeat (but manually configured monitors) works well.",
    "website_area": "discuss"
  },
  {
    "id": "18f05ed9-e0a7-4589-9b10-75175035a02f",
    "url": "https://discuss.elastic.co/t/7-5-1-filebeat-container-still-creating-standalone-cluster/213366",
    "title": "7.5.1 Filebeat container still creating \"Standalone Cluster\"",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 30, 2019, 4:42pm January 2, 2020, 6:42pm January 2, 2020, 9:35pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f51aa7c4-2cc2-4bbe-8bb6-86c50956fdf3",
    "url": "https://discuss.elastic.co/t/hostname-split-in-kibana/213628",
    "title": "Hostname split in Kibana",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "RunningSmurf",
    "date": "January 2, 2020, 9:18pm",
    "body": "Hello, I installed MetricBeat 7.5.1 on a remote Linux host (relative to the ES host), turned on and configured the system module. When I look at the dashboard for the system, I see that the name of the host has been split into two. The name is atlv-psgmon.xxxx.xxxx.xxxx, and it shows in Kibana as two entities: \"atlv\" and \"psgmon.xxx.xxxx.xxxx\".... It is as if it split it into 2 entities at the \"-\". I have another server, although Windows, and on a different version of MetricBeat (7.4.2) (and therefore different index), that has a dash, but it is not split. Please see the screenshot below: image1829789 74.7 KB Has anyone encountered this issue? Thank you! Sincerely, RS",
    "website_area": "discuss"
  },
  {
    "id": "823c1be0-a4f2-4ab6-ba81-62e0889b2726",
    "url": "https://discuss.elastic.co/t/filebeat-zeek-module-integration/213598",
    "title": "Filebeat \"zeek\" module integration",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "January 2, 2020, 4:03pm January 2, 2020, 6:27pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e28a72b0-1839-4a52-986a-d5037bffba30",
    "url": "https://discuss.elastic.co/t/metricbeat-to-use-ec2-role-permissions-to-access-aws-elk-stack/213613",
    "title": "Metricbeat to use EC2 role permissions to access AWS ELK stack",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "January 2, 2020, 6:25pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "80f384cb-2d7a-4a92-94bf-34e29b7b7a1f",
    "url": "https://discuss.elastic.co/t/resources-for-newcomers/213607",
    "title": "Resources for newcomers?",
    "category": [
      "Beats",
      "Beats Developers"
    ],
    "author": "alexolivan",
    "date": "January 2, 2020, 5:51pm",
    "body": "Hi forum. Since I succeeded in the past creating (publicly available in github) plugins for real-time monitoring / graphing tools munin and nagios-based Check_MK/OMD for some popular streaming servers (mostly Icecast2, but also SHOUTCast and probably WowzaStreamingEngine), I'm exploring the feassibility of doing the same with metricbeat. The problem is that I feel metricbeat being much more complex than those I know. Also, I'm not finding the typical blog article explaining how to create a basic 'plugin' (I guess I should say a 'beat') to do something easy (such as getting hdd temp from smart, apache sessions from /status or an API call) The point is that the more I try to gather information to have a clear picture on where to start reading and working on, the more I'm getting unsure of whether what I have in mind is even possible at all, having I misunderstood the very use-case of metricbeat itself. So... If I could read (and graph) current (or almost current) CPU usage, RAM status and so on ... My though is that I could use metricbeat to query an Icecast2 instance and monitor its current status likewise .... Is that the use-case of Metricbeat? If so (then the door to achieve my goal would be opened) Isn't there any step-to-step guide to create a kinda 'hello world' beat? or where should I start from? And, finally (provide this all makes any sense) do I need to master Go to create Beat? Thank you very much in advance. Regards.",
    "website_area": "discuss"
  },
  {
    "id": "bfb05c2f-9259-461a-8d22-b0b1379d64e7",
    "url": "https://discuss.elastic.co/t/icmp-monitoring-using-list-file/212862",
    "title": "Icmp monitoring using list file",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "fadjar340",
    "date": "December 23, 2019, 3:11pm December 23, 2019, 5:30pm December 24, 2019, 1:27am January 2, 2020, 1:34pm January 2, 2020, 1:57pm",
    "body": "Is it possible to list the icmp monitoring host using list file, because I need to monitor more than 100 IP addresses? If no, is there any workaround to list a lots of hosts in convinient way, let say from csv file? Thanks in advance.. Regards, Fadjar Tandabawana",
    "website_area": "discuss"
  },
  {
    "id": "4a5fe583-ba1e-40a6-8821-3b3466e36e40",
    "url": "https://discuss.elastic.co/t/heartbeat-get-response-and-exec-another-request/212571",
    "title": "Heartbeat get response and exec another request",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "rschirin",
    "date": "December 19, 2019, 10:08pm January 2, 2020, 1:36pm",
    "body": "Hi, is there a way to instrument a GET operation, check the response body to extract an information and then use it to perform immediately another GET call?",
    "website_area": "discuss"
  },
  {
    "id": "bebbb6e3-3c72-4a6f-90a7-363febc23cc3",
    "url": "https://discuss.elastic.co/t/filebeat-fail-to-execute-the-http-get-request/213442",
    "title": "Filebeat: fail to execute the HTTP GET request",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Robin020",
    "date": "December 31, 2019, 11:38am January 1, 2020, 5:24pm January 2, 2020, 1:38pm",
    "body": "Hi, Filebeat wants to do a http get request to http://localhost:5601/api/status but it fails. I am having trouble with configure this on the right way in the filebeat.yml. For Kibana I created a certificate and used that. See my config: # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. # This requires a Kibana endpoint configuration. setup.kibana: # Kibana Host # Scheme and port can be left out and will be set to the default (http and 5601) # In case you specify and additional path, the scheme is required: http://localhost:5601/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 host: \"localhost:5601\" protocol: \"https\" username: \"user\" password: \"password\" server.ssl.enabled: true server.ssl.certificate: \"<path to crt file>\" server.ssl.key: \"path to key file>\" Do I miss something? Is it possible to disable this check?",
    "website_area": "discuss"
  },
  {
    "id": "468e5b75-5cea-49ff-83cd-4d207be5a9bc",
    "url": "https://discuss.elastic.co/t/getting-data-from-three-sources-to-the-one-metricbeat/213577",
    "title": "Getting data from three sources to the one metricbeat",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "awer1967",
    "date": "January 2, 2020, 1:16pm",
    "body": "Hi ! We intend to apply a \"performance getting\" scheme with metricbeat and logstash for our Vmware environment , consists of three vCenter hosts work independently. We have only one host with Metricbeat installed, which may poll all of those VCenter hosts to get data . A default Metricbeat configuration with one vSphere module allows data obtaining only from one vCenter server, not from three at the same time . Is there some trick to enable getting data from three VCenter server through only one Metricbeat to logstash ?",
    "website_area": "discuss"
  },
  {
    "id": "c8cf8825-a581-47ed-8ee3-87ea39672865",
    "url": "https://discuss.elastic.co/t/cloud-auth-setup-error/213502",
    "title": "Cloud.auth setup error",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "mxixm",
    "date": "January 1, 2020, 9:41pm January 2, 2020, 6:36am January 2, 2020, 10:40am",
    "body": "Good afternoon all, I am trying to configure my logs to go to elastic cloud via winlogbeat. When I run the \".\\winlogbeat.exe setup\" command, I get the following error: \"Exiting: error loading config file: yaml: line 105: could not find expected ':'\" Line 105 on my file is the cloud.auth line. Tried editing the yaml file with these syntax entries: \"cloud.auth: \"user:password\"\" AND \"cloud.auth: \"user:password\"\" Command doesn't work with either entry. Any help on this would be greatly appreciated. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "f3d69a17-64c7-462d-a65e-404ca350cc43",
    "url": "https://discuss.elastic.co/t/libbeat-elasticsearch-default-client/213533",
    "title": "Libbeat Elasticsearch default client",
    "category": [
      "Beats"
    ],
    "author": "sashker",
    "date": "January 2, 2020, 8:22am",
    "body": "Hello, guys. I'm working with the libbeat and I cannot figure out how I can do something with an ES client which sends events to a server. I need to find a way how I can sign all requests with AWS signature in order to send events to AWS Elasticsearch. But for that, I probably have to substitute the default http.Client which libbeat uses for sending something.",
    "website_area": "discuss"
  },
  {
    "id": "e5a6cde5-38c9-4184-a121-f52b41c5b877",
    "url": "https://discuss.elastic.co/t/auditbeat-error-failed-to-open-audit-netlink-socket-bind-failed-operation-not-permitted/213427",
    "title": "Auditbeat Error : failed to open audit netlink socket: bind failed: operation not permitted",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "Vamsi_Vutukuri",
    "date": "January 2, 2020, 7:08am January 21, 2020, 9:51am",
    "body": "Hi, I'm using ELK stack of version 7.5.1 with x-pack installed and while running auditbeat showing the following error 2020-01-02T12:18:41.065+0530 ERROR instance/beat.go:916 Exiting: 1 error: 1 error: failed to create audit client: failed to open audit netlink socket: bind failed: operation not permitted Exiting: 1 error: 1 error: failed to create audit client: failed to open audit netlink socket: bind failed: operation not permitted Please help me solve it.",
    "website_area": "discuss"
  },
  {
    "id": "00f2ba31-b6bc-4fd8-a330-4ab47eb6b752",
    "url": "https://discuss.elastic.co/t/hostname-is-same-for-10-devices-but-now-in-kibana-i-am-able-to-see-one-device-data-only/213473",
    "title": "Hostname is same for 10 devices , but now in kibana i am able to see one device data only",
    "category": [
      "Beats"
    ],
    "author": "vijaya12",
    "date": "January 1, 2020, 5:17am",
    "body": "hostname is same for 10 devices , but now in kibana i am able to see one device data only.but 2 metricbeat modules running on 2 different devices. but i am getting only one device data i kibana dashboard",
    "website_area": "discuss"
  },
  {
    "id": "40e9145a-bb46-4e5f-b1b6-3cc4a070e9c0",
    "url": "https://discuss.elastic.co/t/filebeat-yml-logging-issue/213386",
    "title": "Filebeat.yml logging issue",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "RunningSmurf",
    "date": "December 30, 2019, 9:58pm December 30, 2019, 10:28pm December 31, 2019, 7:22pm",
    "body": "Hello, I noticed that my filebeat beat was not producing a log file. I realized that I never set a filename, path, etc... I set the following, but I get the following error with the file path: image949684 106 KB Any suggestion would be greatly appreciated. Sincerely, RS",
    "website_area": "discuss"
  },
  {
    "id": "8a75105d-5656-4f69-b9bc-47a3b7cafc06",
    "url": "https://discuss.elastic.co/t/metricbeat-storage-in-elasticsearch-index/213212",
    "title": "Metricbeat Storage in ElasticSearch index",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "benjamin.watson",
    "date": "December 27, 2019, 4:59pm December 31, 2019, 3:32pm",
    "body": "Greetings all, I'm designing a monitoring and alerting platform and intend to base it largely on ElasticSearch and leverage Metricbeat for per host metrics collection. I've looked up some general (and often conflicting) ElasticSearch \"best practices\" when it comes to cluster, index, and shard configuration. And I know the issue I'm facing is the general answer \"it depends\". It depends on how much data you want to keep, how hot/warm/cold you want it, if you'll do roll-up indices, which metricbeat modules and metricsets you will use, etc. Given that, I still need to start planning and get an initial cluster sized \"the best I can\". Assuming I know the number of hosts (machines/VMs) I want to monitor via metricbeat, and assuming I use the default out-of-the-box configuration for the system module, approximately how much data can I expect to ingest into ElasticSearch per monitored host per hour (or day, or week)? For example, assume my default metricbeat config looks like this: metricbeat.modules: - module: system metricsets: - cpu - load - memory - network - process - process_summary # - uptime # - core # - diskio # - filesystem # - fsstat # - raid # - socket enabled: true period: 10s processes: ['.*'] How much ES index storage does that translate to on a per-host basis? I know there is a lot of fine tuning that can be done (e.g. filtering top_N processes, etc., etc.). But what does the above \"default\" configuration typically yield (assuming ES/Beats 7.x). Are there any \"rules of thumb\" guides out there that generalize the storage volume generated by the various metricbeat modules/configurations? Thanks! Ben",
    "website_area": "discuss"
  },
  {
    "id": "1ff9618a-b69c-43b3-9b62-2931e092e998",
    "url": "https://discuss.elastic.co/t/metricbeat-autodiscover-hints-for-mysql/211575",
    "title": "Metricbeat autodiscover hints for MySQL",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "December 12, 2019, 7:18am December 12, 2019, 10:26am December 14, 2019, 4:46am December 16, 2019, 1:24pm December 16, 2019, 9:56am December 31, 2019, 10:33am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e3854d23-f8ff-41fc-85f7-fb5d002e6518",
    "url": "https://discuss.elastic.co/t/indentation-issue-with-apache-module-filebeat-7-5/213435",
    "title": "Indentation issue with apache module ( Filebeat 7.5 )",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 31, 2019, 10:32am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d3da2114-2608-417c-b8df-63f6559cecb2",
    "url": "https://discuss.elastic.co/t/shipping-gitlab-job-logs/213005",
    "title": "Shipping gitlab job logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "meir",
    "date": "December 26, 2019, 7:23am December 26, 2019, 2:03pm December 30, 2019, 11:49am December 30, 2019, 8:07pm December 31, 2019, 7:54am",
    "body": "HI, I have started using filebeat for log shipping for my gitlab logs, and I have a directory structure that is changed every few minutes, each directory contains a log file which contains data that I want to ship to my elasticsearch and create dashboard from it. I've configured my \"filebeat\" yaml file to scan such type of directory - type: log enabled: true paths: - /artifacts/*/*/*/*/*/*/*.log fields: log: jobs-log it starts scanning the system and when it's finished it does not continue scanning new files creation. any idea how can I ship all the gitlab logs to elk? Thankms",
    "website_area": "discuss"
  },
  {
    "id": "b06661ec-9037-4b12-98ba-e502e61a8ff6",
    "url": "https://discuss.elastic.co/t/getting-error-unpacking-config-data-more-than-one-namespace-configured-accessing-output-source-usr-local-etc-filebeat-filebeat-yml/213346",
    "title": "Getting error unpacking config data: more than one namespace configured accessing 'output' (source:'/usr/local/etc/filebeat/filebeat.yml')",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "sakshi1234",
    "date": "December 31, 2019, 6:18am December 30, 2019, 2:21pm December 31, 2019, 6:19am",
    "body": "Hi, When i am running below command: filebeat modules enable logstash i m getting unpacking config data: more than one namespace configured accessing 'output' (source:'/usr/local/etc/filebeat/filebeat.yml') Can somebody please help below is my filebeat.yml file: ###################### Filebeat Configuration Example ######################### # This file is an example configuration file highlighting only the most common # options. The filebeat.reference.yml file from the same directory contains all the # supported options with more comments. You can use it as a reference. # # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/filebeat/index.html # For more available modules and options, please see the filebeat.reference.yml sample # configuration file. #=========================== Filebeat prospectors ============================= filebeat.prospectors: # Each - is a prospector. Most options can be set at the prospector level, so # you can use different prospectors for various configurations. # Below are the prospector specific configurations. - type: log # Change to true to enable this prospector configuration. enabled: false # Paths that should be crawled and fetched. Glob based paths. paths: - /var/lib/jenkins/jobs/*/builds/*/log #- c:\\programdata\\elasticsearch\\logs\\* # Exclude lines. A list of regular expressions to match. It drops the lines that are # matching any regular expression from the list. #exclude_lines: ['^DBG'] # Include lines. A list of regular expressions to match. It exports the lines that are # matching any regular expression from the list. #include_lines: ['^ERR', '^WARN'] # Exclude files. A list of regular expressions to match. Filebeat drops the files that # are matching any regular expression from the list. By default, no files are dropped. #exclude_files: ['.gz$'] # Optional additional fields. These fields can be freely picked # to add additional information to the crawled log files for filtering #fields: # level: debug # review: 1 ### Multiline options # Mutiline can be used for log messages spanning multiple lines. This is common # for Java Stack Traces or C-Line Continuation # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ #multiline.pattern: ^\\[ # Defines if the pattern set under pattern should be negated or not. Default is false. #multiline.negate: false # Match can be set to \"after\" or \"before\". It is used to define if lines should be append to a pattern # that was (not) matched before or after or as long as a pattern is not matched based on negate. # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash #multiline.match: after #============================= Filebeat modules =============================== filebeat.config.modules: # Glob pattern for configuration loading path: ${path.config}/modules.d/*.yml # Set to true to enable config reloading reload.enabled: true # Period on which files under path should be checked for changes #reload.period: 10s #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 3 #index.codec: best_compression #_source.enabled: false #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. #name: # The tags of the shipper are included in their own field with each # transaction published. #tags: [\"service-X\", \"web-tier\"] # Optional fields that you can specify to add additional information to the # output. #fields: # env: staging #============================== Dashboards ===================================== # These settings control loading the sample dashboards to the Kibana index. Loading # the dashboards is disabled by default and can be enabled either by setting the # options here, or by using the `-setup` CLI flag or the `setup` command. #setup.dashboards.enabled: false # The URL from where to download the dashboards archive. By default this URL # has a value which is computed based on the Beat name and version. For released # versions, this URL points to the dashboard archive on the artifacts.elastic.co # website. #setup.dashboards.url: #============================== Kibana ===================================== # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. # This requires a Kibana endpoint configuration. setup.kibana: # Kibana Host # Scheme and port can be left out and will be set to the default (http and 5601) # In case you specify and additional path, the scheme is required: http://localhost:5601/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 host: \"localhost:5601\" #============================= Elastic Cloud ================================== # These settings simplify using filebeat with the Elastic Cloud (https://cloud.elastic.co/). # The cloud.id setting overwrites the `output.elasticsearch.hosts` and # `setup.kibana.host` options. # You can find the `cloud.id` in the Elastic Cloud web UI. #cloud.id: # The cloud.auth setting overwrites the `output.elasticsearch.username` and # `output.elasticsearch.password` settings. The format is `<user>:<pass>`. #cloud.auth: #================================ Outputs ===================================== # Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. #hosts: [\"localhost:9200\"] #Optional protocol and basic auth credentials. #protocol: \"https\" #username: \"elastic\" #password: \"changeme\" #----------------------------- Logstash output -------------------------------- output.logstash: # The Logstash hosts hosts: [\"localhost:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] #============================== Xpack Monitoring =============================== # filebeat can export internal metrics to a central Elasticsearch monitoring # cluster. This requires xpack monitoring to be enabled in Elasticsearch. The # reporting is disabled by default. # Set to true to enable the monitoring reporter. #xpack.monitoring.enabled: false # Uncomment to send the metrics to Elasticsearch. Most settings from the # Elasticsearch output are accepted here as well. Any setting that is not set is # automatically inherited from the Elasticsearch output configuration, so if you # have the Elasticsearch output configured, you can simply uncomment the # following line. #xpack.monitoring.elasticsearch:",
    "website_area": "discuss"
  },
  {
    "id": "8b3780ce-3caf-4b51-aacf-b2a4396bf4e7",
    "url": "https://discuss.elastic.co/t/how-to-send-an-application-systemd-service-logs-to-logstash-using-journalbeat/212435",
    "title": "How to send an application (systemd service) logs to Logstash using Journalbeat?",
    "category": [
      "Beats",
      "Journalbeat"
    ],
    "author": "arblr",
    "date": "December 19, 2019, 4:14am December 19, 2019, 6:09pm December 31, 2019, 6:04am",
    "body": "My application / service name 'appdb', running as systemd service. I can observe logs from this application using shall command 'journalctl -u appdb.service'. I want to send these logs to Logstash. I am using the following software (versions). Elasticsearch 6.3.0 (IP: 10.111.151.149) journalbeat-7.5.0-1.x86_64 to send logs from application host (IP: 10.111.151.118) to Logstash-6.3.0 (IP: 10.111.151.66) with logstash-input-journald-2.0.2 installed. Logstash-6.3.0 is able to reach Elasticsearch-6.3.0 (10.111.151.149:9200) journalbeat-7.5.0-1.x86_64 (from 10.111.151.118) is NOT able to reach Logstash-6.3.0 (10.111.151.66:5044) using following configuration. Please help. $ cat /etc/journalbeat/journalbeat.yml journalbeat.inputs: paths: include_matches: \"systemd.unit=appdb\" setup.template.settings: index.number_of_shards: 1 #index.codec: best_compression #_source.enabled: false fields: env: staging output.logstash: hosts: [\"10.111.151.66:5044\"] logging.level: debug logging.selectors: [\"*\"] cat logstash.conf input { journald { lowercase => true seekto => \"head\" thisboot => true type => \"systemd\" tags => [\"influxdb\"] } } output { elasticsearch { hosts => [\"http://10.111.151.149:9200\"] manage_template => false } } $ systemctl status logstash630  logstash630.service - Logstash v6.3.0 Service Loaded: loaded (/usr/lib/systemd/system/logstash630.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2019-12-10 16:56:20 UTC; 53s ago Main PID: 3070 (java) Tasks: 21 Memory: 651.8M CGroup: /system.slice/logstash630.service 3070 /bin/java -Xms1g -Xmx1g -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOc... Dec 10 16:56:20 PreProd_Logstash systemd[1]: Started (PreProd) Logstash v6.3.0 Service. $ tail -f logstash-plain.log [2019-12-10T16:57:19,448][ERROR][logstash.pipeline ] Pipeline aborted due to error {:pipeline_id=>\"main\", :exception=>#<Systemd::JournalError: No such file or directory>, :backtrace=>[\"/home/centos/logstash-6.3.0/vendor/bundle/jruby/2.3.0/gems/systemd-journal-1.2.3/lib/systemd/journal.rb:52:in initialize'\", \"/home/centos/logstash-6.3.0/vendor/bundle/jruby/2.3.0/gems/logstash-input-journald-2.0.2/lib/logstash/inputs/journald.rb:67:in register'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:340:in register_plugin'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:351:in block in register_plugins'\", \"org/jruby/RubyArray.java:1734:in each'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:351:in register_plugins'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:498:in start_inputs'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:392:in start_workers'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:288:in run'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:248:in block in start'\"], :thread=>\"#<Thread:0x6fa01436 run>\"} [2019-12-10T16:57:19,528][ERROR][logstash.agent ] Failed to execute action {:id=>:main, :action_type=>LogStash::ConvergeResult::FailedAction, :message=>\"Could not execute action: PipelineAction::Create, action_result: false\", :backtrace=>nil} [2019-12-10T16:57:20,239][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2019-12-10T16:58:13,943][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2019-12-10T16:58:16,310][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"6.3.0\"} [2019-12-10T16:58:25,224][INFO ][logstash.pipeline ] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>4, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50} [2019-12-10T16:58:25,617][ERROR][logstash.pipeline ] Error registering plugin {:pipeline_id=>\"main\", :plugin=>\"<LogStash::Inputs::Journald lowercase=>true, seekto=>\"head\", thisboot=>true, type=>\"systemd\", tags=>[\"influxdb\"], id=>\"b3fff89f140a734538e00984c983e881203be97ee59faa0013b15bb25530c48c\", enable_metric=>true, codec=><LogStash::Codecs::Plain id=>\"plain_00a87d0d-2a19-4571-a19b-cf87cd3d4ac0\", enable_metric=>true, charset=>\"UTF-8\">, threads=>1, flags=>0, path=>\"/var/log/journal\", sincedb_write_interval=>15, wait_timeout=>3000000>\", :error=>\"No such file or directory\", :thread=>\"#<Thread:0x63a50cd3 run>\"} [2019-12-10T16:58:25,818][ERROR][logstash.pipeline ] Pipeline aborted due to error {:pipeline_id=>\"main\", :exception=>#<Systemd::JournalError: No such file or directory>, :backtrace=>[\"/home/centos/logstash-6.3.0/vendor/bundle/jruby/2.3.0/gems/systemd-journal-1.2.3/lib/systemd/journal.rb:52:in initialize'\", \"/home/centos/logstash-6.3.0/vendor/bundle/jruby/2.3.0/gems/logstash-input-journald-2.0.2/lib/logstash/inputs/journald.rb:67:in register'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:340:in register_plugin'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:351:in block in register_plugins'\", \"org/jruby/RubyArray.java:1734:in each'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:351:in register_plugins'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:498:in start_inputs'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:392:in start_workers'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:288:in run'\", \"/home/centos/logstash-6.3.0/logstash-core/lib/logstash/pipeline.rb:248:in block in start'\"], :thread=>\"#<Thread:0x63a50cd3 run>\"} [2019-12-10T16:58:25,905][ERROR][logstash.agent ] Failed to execute action {:id=>:main, :action_type=>LogStash::ConvergeResult::FailedAction, :message=>\"Could not execute action: PipelineAction::Create, action_result: false\", :backtrace=>nil} ^C $",
    "website_area": "discuss"
  },
  {
    "id": "905241c0-421e-4c72-ab5c-46223bd99827",
    "url": "https://discuss.elastic.co/t/view-apache-extended-status/212895",
    "title": "View apache extended status",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "jamesbro",
    "date": "December 23, 2019, 8:11pm December 23, 2019, 10:22pm December 30, 2019, 9:14pm December 30, 2019, 9:16pm December 31, 2019, 4:08am",
    "body": "I am currently monitoring the Apache status page, however. the extended status is not viewable/searchable in Kibana. Is there a way to get the extended status info from the status page into elastic using the metricbeat apache module? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "3bbaaf96-1392-4f5f-8519-73ba84dd9ef6",
    "url": "https://discuss.elastic.co/t/send-data-in-seperate-lines-from-beats/210169",
    "title": "Send data in seperate lines from beats",
    "category": [
      "Beats"
    ],
    "author": "Saravana37",
    "date": "December 2, 2019, 11:16am December 4, 2019, 8:35am December 16, 2019, 7:13am December 16, 2019, 12:41pm December 17, 2019, 4:12am December 17, 2019, 12:30pm",
    "body": "Hi , We are configuring Siebel app server monitoring and we are running a batch file in execbeat . our output looks like below . Siebel Enterprise Applications Siebel Server Manager, Version 16.19.0.0 [23057] LANG_INDEPENDENT Copyright (c) 2008,2016, Oracle. All rights reserved. The Programs (which include both the software and documentation) contain proprietary information; they are provided under a license agreement containing restrictions on use and disclosure and are also protected by copyright, patent, and other intellectual and industrial property laws. Reverse engineering, disassembly, or decompilation of the Programs, except to the extent required to obtain interoperability with other independently created software or as specified by law, is prohibited. Oracle, JD Edwards, PeopleSoft, and Siebel are registered trademarks of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. If you have received this software in error, please notify Oracle Corporation immediately at 1.800.ORACLE1. Type \"help\" for list of commands, \"help \" for detailed help Connected to 13 server(s) out of a total of 13 server(s) in the enterprise srvrmgr> list comp show SV_NAME,CC_ALIAS,CP_DISP_RUN_STATE,CP_STARTMODE,CP_NUM_RUN_TASKS,CP_MAX_TASKS,CC_NAME order by CP_DISP_RUN_STATE SV_NAME CC_ALIAS CP_DISP_RUN_STATE CP_STARTMODE CP_NUM_RUN_TASKS CP_MAX_TASKS CC_NAME D1220001027A FSMSrvr Online Auto 0 20 File System Manager D1220001027A ServerMgr Running Auto 4 20 Server Manager D1220001027A SRBroker Running Auto 29 100 Server Request Broker D1220001027A SRProc Running Auto 2 20 Server Request Processor D1220001027A SvrTaskPersist Running Auto 1 1 Server Task Persistance D1220001027A SCBroker Running Auto 2 2 Siebel Connection Broker D1220001027A SvrTblCleanup Shutdown Manual 0 1 Server Tables Cleanup D1220001030A CACWfProcMgrPE Online Auto 0 20 CAC Workflow Process Manager Process Engine D1220001030A FSMSrvr Online Auto 0 20 File System Manager D1220001030A SFMWfProcMgrPE Online Auto 0 20 SFM Workflow Process Manager Process Engine D1220001030A ServerMgr Running Auto 4 20 Server Manager I am sending this whole data to Logstash to parse from Execbeat and trying to filter same using Grok , I want only the info data which sample looks like below , and no other info D1220001030A ServerMgr Running Auto 4 20 Server Manager It should be stored in separate each line , but the whole file to logstash coming as a single line . My Grok filter which is matching above line is : filter { grok { match => {\"message\" => \"%{WORD:ServerName}%{SPACE}%{WORD:Comp_Alias}%{SPACE}%{WORD:CompStatus}%{SPACE}%{WORD:CompStartMode}%{SPACE}%{WORD:RunningTasks}%{SPACE}%{WORD:MaxTasks}%{GREEDYDATA:CompName}\"} } } So , If i see in Kibana , i see whole thing , where as i want beats to send as seperate lines to Logstash so that Logstash will consider each line . Any way can we send the output of my command as seperate lines to logstash ? .Please help . Thanks in Advance Saravana S",
    "website_area": "discuss"
  },
  {
    "id": "9af49367-9a26-4152-b00e-5a473677863d",
    "url": "https://discuss.elastic.co/t/metricbeat-l/213181",
    "title": "Metricbeat L",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Lucas_Hyuck_Joo",
    "date": "December 27, 2019, 11:14am December 30, 2019, 6:08pm",
    "body": "Hi, My teams are trying to get statistics on network In/Out traffic from CentOS 7.3. I've tried get but it doesn't know Sometimes a several network metircs are lost and I can't see in Grafana. I am wondering if there is another beat that could gather statistics from the network ( I DO NOT want Lose Network metric) The metic pipeline following is metricbeat 7.4 - kafka2.3 - logstash 7.4- Grafana. Any Help or suggestions? Thanks, Lucas",
    "website_area": "discuss"
  },
  {
    "id": "0bba70b8-0286-48f1-9b53-d9cb203b659e",
    "url": "https://discuss.elastic.co/t/filebeat-include-lines-issue/213211",
    "title": "Filebeat include_lines issue",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "RunningSmurf",
    "date": "December 27, 2019, 4:45pm December 27, 2019, 4:47pm December 30, 2019, 2:30pm December 30, 2019, 9:12pm",
    "body": "Hello, I have Filebeat installed on a Windows server. I configured it to read logs from SQL Server. I can see that all lines are individually passed to ES, as I can see them in Kibana. My goal is to only send the lines that contain the sentence \"Login failed for user\" The documentation is pretty straight forward: The following example exports all log lines that contain sometext filebeat.inputs: type: log ... include_lines: ['sometext'] This is my yml file: filebeat.inputs: type: log paths: M:\\sqlroot\\MSSQL11.MSSQLSERVER\\MSSQL\\Log\\ERRORLOG include_lines: ['Login failed for user'] But this results in no messages being forwarded to ES. I have tried multiple wildcard inputs, but no luck. Would anyone see anything wrong with my configuration? Any help would be greatly appreciated. Thanks! Sincerely, RS",
    "website_area": "discuss"
  },
  {
    "id": "a9a9e04d-05a6-4cb2-936b-c8aa196f4fb5",
    "url": "https://discuss.elastic.co/t/filebeat-error-connecting-to-elasticsearch/213079",
    "title": "Filebeat error connecting to Elasticsearch",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "JY_DT",
    "date": "December 26, 2019, 1:13pm December 26, 2019, 3:29pm December 26, 2019, 5:43pm December 26, 2019, 5:55pm December 26, 2019, 8:20pm December 27, 2019, 9:41am December 30, 2019, 8:04pm",
    "body": "I have Filebeat on a Windows 10 machine sending log data to Elasticsearch on a Windows Server 2016. The services are running correctly. As I'd posted earlier, I'm using basic authentication. I've created a user named filebeat_internal for this purpose. In the filebeat.yml file: < output.elasticsearch: #Array of hosts to connect to. #hosts: [\"localhost:9200\"] hosts: [\"192.168.103.84:9200\"] #Optional protocol and basic auth credentials. #protocol: \"https\" username: \"filebeat_internal\" password: \"password\" /> Also, on the server machine, I've logged into Elasticsearch and Kibana using the built-in user - kibana. But my logs are not reaching Elasticsearch. Please see some Filebeat log file entries below: 2019-12-26T17:58:38.719+0530 INFO [index-management] idxmgmt/std.go:178 Set output.elasticsearch.index to 'filebeat-7.4.2' as ILM is enabled. 2019-12-26T17:58:38.719+0530 INFO elasticsearch/client.go:170 Elasticsearch url: http://192.168.103.84:9200 2019-12-26T17:58:38.719+0530 INFO [publisher] pipeline/module.go:97 Beat name: Sucharit 2019-12-26T17:58:38.721+0530 INFO instance/beat.go:422 filebeat start running. 2019-12-26T17:58:38.721+0530 INFO [monitoring] log/log.go:118 Starting metrics logging every 30s 2019-12-26T17:58:38.721+0530 INFO registrar/registrar.go:145 Loading registrar data from C:\\ProgramData\\filebeat\\registry\\filebeat\\data.json 2019-12-26T17:58:38.721+0530 INFO registrar/registrar.go:152 States Loaded from registrar: 7 2019-12-26T17:58:38.721+0530 INFO crawler/crawler.go:72 Loading Inputs: 1 2019-12-26T17:58:38.722+0530 INFO log/input.go:152 Configured paths: [C:\\ProgramData\\LogTest*] 2019-12-26T17:58:38.722+0530 INFO input/input.go:114 Starting input of type: log; ID: 15460377364020981247 2019-12-26T17:58:38.722+0530 INFO crawler/crawler.go:106 Loading and starting Inputs completed. Enabled inputs: 1 2019-12-26T17:58:38.722+0530 INFO cfgfile/reload.go:171 Config reloader started 2019-12-26T17:58:38.722+0530 INFO cfgfile/reload.go:226 Loading of config files completed. 2019-12-26T17:58:38.723+0530 INFO log/harvester.go:251 Harvester started for file: C:\\ProgramData\\LogTest\\CW2.log 2019-12-26T17:58:41.693+0530 INFO add_cloud_metadata/add_cloud_metadata.go:87 add_cloud_metadata: hosting provider type not detected. 2019-12-26T17:58:42.694+0530 INFO pipeline/output.go:95 Connecting to backoff(elasticsearch(http://192.168.103.84:9200)) 2019-12-26T17:58:46.415+0530 ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(http://192.168.103.84:9200)): Get http://192.168.103.84:9200: dial tcp 192.168.103.84:9200: connectex: No connection could be made because the target machine actively refused it. 2019-12-26T17:58:46.415+0530 INFO pipeline/output.go:93 Attempting to reconnect to backoff(elasticsearch(http://192.168.103.84:9200)) with 1 reconnect attempt(s) Appreciate any help in resolving this. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "114b7b79-b156-412a-8874-8004111b6469",
    "url": "https://discuss.elastic.co/t/filebeat-service-should-automatically-starts/213057",
    "title": "Filebeat - service should automatically starts",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "aolvikash",
    "date": "December 26, 2019, 11:25am December 26, 2019, 1:49pm December 30, 2019, 10:28am December 30, 2019, 7:59pm",
    "body": "Hi, I have the filebeat running on Freebsd and started the filebeat service via \"service filebeat onestart\" however I want to run filebeat service automatically in case service has stopped or if the server is rebooted, etc. Filebeat should run all the time. I know we can use \"nohup\" or cron script, just want to know if any other options. Any suggestions could be appreciated !!",
    "website_area": "discuss"
  },
  {
    "id": "dc165ce0-526b-4273-ae08-8abfd8b942b1",
    "url": "https://discuss.elastic.co/t/empty-lines-in-multiline-pattern-python-error-traceback-in-filebeat-input-are-not-getting-parsed-correctly/213355",
    "title": "Empty lines in multiline pattern(python error traceback) in filebeat input are not getting parsed correctly?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Jaaved_Ali_Khan",
    "date": "December 30, 2019, 1:31pm December 30, 2019, 1:50pm",
    "body": "The log line which should be harvested and published to logstash as a single line: [pid: 17318|app: 0|req: 1/2] 10.14.206.28 (jaavedkhan) {60 vars in 1296 bytes} [Mon Dec 30 15:51:38 2019] GET /en/ => generated 27 bytes in 711 msecs (HTTP/1.1 500) 6 headers in 316 bytes (1 switches on core 0) Mon Dec 30 15:51:39 2019 - announcing my loyalty to the Emperor... Internal Server Error: /en/ Traceback (most recent call last): File \"/opt/dsr-incentives/venv/lib/python3.5/site-packages/django/core/handlers/exception.py\", line 34, in inner response = get_response(request) File \"/opt/dsr-incentives/venv/lib/python3.5/site-packages/django/core/handlers/base.py\", line 126, in _get_response response = self.process_exception_by_middleware(e, request) File \"/opt/dsr-incentives/venv/lib/python3.5/site-packages/django/core/handlers/base.py\", line 124, in _get_response response = wrapped_callback(request, *callback_args, **callback_kwargs) File \"/opt/dsr-incentives/venv/lib/python3.5/site-packages/django/views/generic/base.py\", line 68, in view return self.dispatch(request, *args, **kwargs) File \"/opt/dsr-incentives/venv/lib/python3.5/site-packages/django/views/generic/base.py\", line 88, in dispatch return handler(request, *args, **kwargs) File \"./core/views.py\", line 31, in get 1/0 ZeroDivisionError: division by zero my filebeat configurations: filebeat: inputs: - type: log paths: - \"/var/log/uwsgi/vassals/dsr-incentives.log\" fields_under_root: true multiline: pattern: '\\[pid:\\s*\\d*\\|app:' negate: true match: after fields: log_type: app-access appserver: uwsgi app: dsr-incentives server_name: server-name.domain.com I checked the multiline pattern https://play.golang.org with the log line: The result is as expected but the harvester is splitting the log line at \"Internat server error\" Publish event: { \"@timestamp\": \"2019-12-30T13:02:56.564Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"_doc\", \"version\": \"7.5.1\" }, \"log\": { \"offset\": 128736, \"file\": { \"path\": \"/var/log/uwsgi/vassals/dsr-incentives.log\" }, \"flags\": [ \"multiline\" ] }, \"appserver\": \"uwsgi\", \"server_name\": \"xyz\", \"log_type\": \"app-access\", \"host\": { \"name\": \"xyz\" }, \"agent\": { \"hostname\": \"apps-1\", \"id\": \"d3417bc3-213c-4d5e-a9b5-2273178262d0\", \"version\": \"7.5.1\", \"name\": \"xyz\", \"type\": \"filebeat\", \"ephemeral_id\": \"125578d6-44d1-4103-94bc-a1d062091487\" }, \"message\": \"Internal Server Error: /en/\\nTraceback (most recent call last):\\n File \\\"/opt/dsr-incentives/venv/lib/python3.5/site-packages/django/core/handlers/exception.py\\\", line 34, in inner\\n response = get_response(request)\\n File \\\"/opt/dsr-incentives/venv/lib/python3.5/site-packages/django/core/handlers/base.py\\\", line 126, in _get_response\\n response = self.process_exception_by_middleware(e, request)\\n File \\\"/opt/dsr-incentives/venv/lib/python3.5/site-packages/django/core/handlers/base.py\\\", line 124, in _get_response\\n response = wrapped_callback(request, *callback_args, **callback_kwargs)\\n File \\\"/opt/dsr-incentives/venv/lib/python3.5/site-packages/django/views/generic/base.py\\\", line 68, in view\\n return self.dispatch(request, *args, **kwargs)\\n File \\\"/opt/dsr-incentives/venv/lib/python3.5/site-packages/django/views/generic/base.py\\\", line 88, in dispatch\\n return handler(request, *args, **kwargs)\\n File \\\"./core/views.py\\\", line 31, in get\\n 1/0\\nZeroDivisionError: division by zero\", \"tags\": [ \"filebeat\" ], \"input\": { \"type\": \"log\" }, \"app\": \"dsr-incentives\", \"ecs\": { \"version\": \"1.1.0\" } } I think the problem is that the multiline is getting split at empty lines that are appearing in logs before \"Internal Server Error\".",
    "website_area": "discuss"
  },
  {
    "id": "f88949d3-8828-4054-867d-1cc46b8d03c5",
    "url": "https://discuss.elastic.co/t/filebeat-indices-get-grouped-by-ilm/210144",
    "title": "Filebeat indices get grouped by ILM",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 2, 2019, 9:27am December 6, 2019, 5:54pm December 10, 2019, 12:25pm December 29, 2019, 2:16pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "beaeb8e4-2f34-4603-bc07-8ff2469094db",
    "url": "https://discuss.elastic.co/t/heartbeat-not-writing-to-log-file/213213",
    "title": "Heartbeat not writing to log file",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "yodog",
    "date": "December 27, 2019, 5:09pm",
    "body": "heartbeat is not writing to log file. i know it is loging to journal, because this works: journalctl -u heartbeat-elastic Dez 27 14:04:15 elastic-zmb-01 systemd[1]: Started Ping remote services for availability and log results to Elasticsearch or send to Logstash.. Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.958-0300 INFO instance/beat.go:610 Home path: [/usr/share/heartbeat] Config path: [/etc/heartbeat] Data path: [/var/lib/heartbeat] Logs path: [/var/log/heartbeat] Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.959-0300 INFO instance/beat.go:618 Beat ID: 6cd8e62f-d182-4296-a15c-87543c183f60 Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.965-0300 INFO [api] api/server.go:62 Starting stats endpoint Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.965-0300 INFO [seccomp] seccomp/seccomp.go:124 Syscall filter successfully installed Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.965-0300 INFO [beat] instance/beat.go:941 Beat info {\"system_info\": {\"beat\": {\"path\": {\"config\": \"/etc/heartbeat\", \"data\": \"/var/lib/heartbeat\", \"home\": \"/usr/share/heartbeat\", \"logs\": \"/var/log/heartbeat\"}, \"type\": \"heartbeat\", \"uuid\": \"6cd8e62f-d182-4296-a15c-87543c183f60\"}}} Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.965-0300 INFO [beat] instance/beat.go:950 Build info {\"system_info\": {\"build\": {\"commit\": \"60dd883ca29e1fdd5b8b075bd5f3698948b1d44d\", \"libbeat\": \"7.5.1\", \"time\": \"2019-12-16T21:29:47.000Z\", \"version\": \"7.5.1\"}}} Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.965-0300 INFO [api] api/server.go:64 Metrics endpoint listening on: 127.0.0.1:5066 (configured: localhost) Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.966-0300 INFO [beat] instance/beat.go:953 Go runtime info {\"system_info\": {\"go\": {\"os\":\"linux\",\"arch\":\"amd64\",\"max_procs\":20,\"version\":\"go1.12.12\"}}} Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.969-0300 INFO instance/beat.go:297 Setup Beat: heartbeat; Version: 7.5.1 Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.969-0300 INFO [index-management] idxmgmt/std.go:182 Set output.elasticsearch.index to 'heartbeat-7.5.1' as ILM is enabled. Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.970-0300 INFO elasticsearch/client.go:171 Elasticsearch url: http://elastic-zmb-01:9200 Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.970-0300 INFO elasticsearch/client.go:171 Elasticsearch url: http://elastic-zmb-02:9200 Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.970-0300 INFO elasticsearch/client.go:171 Elasticsearch url: http://elastic-zmb-03:9200 Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.970-0300 INFO elasticsearch/client.go:171 Elasticsearch url: http://elastic-zmb-04:9200 Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.970-0300 INFO elasticsearch/client.go:171 Elasticsearch url: http://elastic-zmb-05:9200 Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.971-0300 INFO elasticsearch/client.go:171 Elasticsearch url: http://elastic-zmb-06:9200 Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.971-0300 INFO elasticsearch/client.go:171 Elasticsearch url: http://elastic-zmb-07:9200 Dez 27 14:04:15 elastic-zmb-01 heartbeat[107392]: 2019-12-27T14:04:15.971-0300 INFO elasticsearch/client.go:171 Elasticsearch url: http://elastic-zmb-08:9200 even if i manually create /var/log/heartbeat/heartbeat it never writes to this file. this is my config: heartbeat.config.monitors: path: ${path.config}/monitors.d/*.yml reload.enabled: true reload.period: 1m processors: - add_host_metadata: netinfo.enabled: true - add_locale: format: offset - add_process_metadata: match_pids: [\"system.process.ppid\"] target: system.process.parent heartbeat.monitors: - type: tcp schedule: '@every 10s' hosts: localhost ipv4: true mode: any ports: [80, 5044, 5066, 9200] timeout: 5s monitoring.enabled: true http: { enabled: true, port: 5066 } setup.kibana: host: \"elastic-zmb-pool\" output.elasticsearch: compression_level: 5 hosts: [\"elastic-zmb-01\", \"elastic-zmb-02\", \"elastic-zmb-03\", \"elastic-zmb-04\", \"elastic-zmb-05\", \"elastic-zmb-06\", \"elastic-zmb-07\", \"elastic-zmb-08\"] loadbalance: true monitoring.elasticsearch: ${output.elasticsearch} logging: files: { path: /var/log/heartbeat, name: heartbeat, keepfiles: 2, permissions: 0664, redirect_stderr: true } level: info metrics: { enabled: true, period: 1m } to_files: true to_syslog: false",
    "website_area": "discuss"
  },
  {
    "id": "971fa627-e393-4722-9ca0-b0ac114618cf",
    "url": "https://discuss.elastic.co/t/basic-apache-filebeat-log-question-missing-fields/213103",
    "title": "Basic Apache Filebeat Log question - missing fields",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Shorthills",
    "date": "December 26, 2019, 4:31pm December 26, 2019, 8:49pm December 26, 2019, 9:40pm December 27, 2019, 12:50am December 27, 2019, 2:00am December 27, 2019, 2:03am December 27, 2019, 2:11am December 27, 2019, 2:13am December 27, 2019, 4:10pm",
    "body": "Hi, Sorry for the newbie question: I'm able to pull in my apache logs and when I try to filter by apache2.access.response_code , I get zilch. I looked and the response code data is in the message from apache. Is the problem that I'm having because I'm going directly from the filebeat to elastic? And that I need to use logstash in the middle to expose the apache2.access.response_code parameter? Just want to make sure I'm on the right path. Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "28c92213-e071-462c-91fb-07b0dd402298",
    "url": "https://discuss.elastic.co/t/filebeat-startup-problem/213168",
    "title": "Filebeat Startup problem",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Gregory_Anne",
    "date": "December 27, 2019, 9:44am December 27, 2019, 12:06pm December 27, 2019, 12:51pm",
    "body": "Hello Env description : I have a cluster of two nodes with a Jetty app. The application generate log on the file /var/log/jetty/jetty.log The log rotation is handle by logrotate avery day at 6:30. On both I run filebeat with the same configuration: filebeat.inputs: - type: log enabled: true multiline: pattern: '^[?[0-9]{4}-[0-9]{2}-[0-9]{2}|^[0-9]{4}/[0-9]{2}/[0-9]{2}|^ts=[0-9]{4}-[0-9]{2}-[0-9]{2}|^[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3} - - [[0-9]{1,2}/\\w{3}/[0-9]{4}:' negate: true match: after paths: - /var/logs/jetty/*.log fields_under_root: true fields: application: \"jetty\" function: \"str\" environment: \"prod\" filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false setup.template.settings: index.number_of_shards: 1 setup.kibana: host: \"supervision.XXXXXXXX.fr:XXXX\" output: logstash: hosts: [\"supervision.XXXXXXXX.fr:XXXX\"] username: \"XXXXXXXXXXXXXXXXXXX\" password: \"XXXX\" processors: - add_host_metadata: ~ # - dissect: # when: # message: \"^[?[0-9]{4}-[0-9]{2}-[0-9]{2}\" # tokenizer: \"%{timestamp->} : %{level}\" # field: \"message\" # target_prefix: \"dissect\" # %{TIMESTAMP_ISO8601:timestamp} : %{LOGLEVEL:level} - %{WORD:service} | %{GREEDYDATA:logmessage} and with the logstash module configuration /etc/filebeat/modules.d/logstash.yml : - module: logstash # logs log: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. #var.paths: # Slow logs slowlog: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. #var.paths: But only on one of them I have a consumption problem. First this morning I observe that the logs stop to be sent to logstash since 6:30, and the Filebeat process consume lot of memory and CPU, so I decide to restart it, but it does not restart to log file processing. Any ideas ? Do you want more detail ?",
    "website_area": "discuss"
  },
  {
    "id": "990a4c39-aa39-4201-acc2-056be5753a76",
    "url": "https://discuss.elastic.co/t/filebeat-7-2-for-rhel-5-1-init-file-changes/213175",
    "title": "Filebeat 7.2 for RHEL 5.1: init file changes",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "katara",
    "date": "December 27, 2019, 12:03pm",
    "body": "Hi, I am currently trying to install filebeat 7.2 on an RHEL 5.1. I am aware Filebeat doesnt support RHEL 5. But I read the possibility after hacking the init file to not use the god parts in the below answer: filebeat-5-1-1-running-on-red-hat-5-enterprise The below is the init script for filebeat 5.1: #!/bin/bash # filebeat custom daemon DAEMON_PATH=\"/usr/bin/filebeat\" DAEMON=filebeat DAEMONOPTS=\"-c /etc/filebeat/filebeat.yml\" NAME=filebeat DESC=\"Filebeat 1.3 32/64 bit rhel5\" PIDFILE=/var/run/$NAME.pid SCRIPTNAME=/etc/init.d/$NAME case \"$1\" in start) printf \"%-50s\" \"Starting $NAME...\" #cd $DAEMON_PATH PID=`$DAEMON $DAEMONOPTS > /dev/null 2>&1 & echo $!` #echo \"Saving PID\" $PID \" to \" $PIDFILE if [ -z $PID ]; then printf \"%s\\n\" \"Fail\" else echo $PID > $PIDFILE printf \"%s\\n\" \"Ok\" fi ;; status) printf \"%-50s\" \"Checking $NAME...\" if [ -f $PIDFILE ]; then PID=`cat $PIDFILE` if [ -z \"`ps axf | grep ${PID} | grep -v grep`\" ]; then printf \"%s\\n\" \"Process dead but pidfile exists\" else echo \"Running\" fi else printf \"%s\\n\" \"Service not running\" fi ;; stop) printf \"%-50s\" \"Stopping $NAME\" PID=`cat $PIDFILE` #cd $DAEMON_PATH if [ -f $PIDFILE ]; then kill -HUP $PID printf \"%s\\n\" \"Ok\" rm -f $PIDFILE else printf \"%s\\n\" \"pidfile not found\" fi ;; restart) $0 stop $0 start ;; *) echo \"Usage: $0 {status|start|stop|restart}\" exit 1 esac Can use the same init script for 7.2? If, not , what changes do I need to make? Please help me out. Thanks, Katara.",
    "website_area": "discuss"
  },
  {
    "id": "e8ae2da2-cef8-4e43-b433-bc92b701a74d",
    "url": "https://discuss.elastic.co/t/filebeat-registry-inode-clean-case-restart-slow/213116",
    "title": "Filebeat registry (inode) clean case restart slow",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "john_am",
    "date": "December 27, 2019, 1:53am",
    "body": "1 What's go on? Filebeat terrible slow to start harvest after I configure the inode clean. 2 My config are as follow: </> type: log enabled: true paths: /data/aaa/aaa/** tags: [\"aaa\"] fields: public_ip: 14.116.220.177 agent: name: guangzhou4_10.10.3.207 exclude_lines: ['^\\n'] exclude_files: ['.lz4$'] ignore_older: 24h clean_inactive: 100h clean_removed: true </> 3 Why so slow ? 1 I have 30000 file per day. In this configure, data/registry/filebeat/data.json file has nearly 100000 record; 2 Each time When i try to restart , Filebeat check each record in data/registry/filebeat/data.json file. What terrible is filebeat check each record in sync into a new file ,.What the ......",
    "website_area": "discuss"
  },
  {
    "id": "83caee25-42e1-4d23-b70d-e156f48f911d",
    "url": "https://discuss.elastic.co/t/filebeats-parsing-docker-logs-with-max-size-limit/213015",
    "title": "Filebeats parsing docker logs with max-size limit",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 26, 2019, 2:08pm December 26, 2019, 2:38pm December 26, 2019, 8:52pm December 27, 2019, 12:56am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4b9ad47b-27ed-4d32-aae1-204286d13bd6",
    "url": "https://discuss.elastic.co/t/if-i-change-one-of-the-yml-files-do-i-need-to-run-metricbeat-or-filebeat-setup-each-time-to-reindex-etc/213113",
    "title": "If I change one of the .yml files, do I need to run [metricbeat or filebeat] setup each time to reindex, etc?",
    "category": [
      "Beats"
    ],
    "author": "Shorthills",
    "date": "December 27, 2019, 12:10am",
    "body": "Question above says it all. I ask b/c I'm trying to troubleshoot my own issues and it would be helpful to know. Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "b201f23d-c969-4df6-add1-a1772d9fe189",
    "url": "https://discuss.elastic.co/t/cant-monitor-kafka-module-metrics-like-producer-consumer-broker-in-6-8-1-and-later/213044",
    "title": "Can't monitor kafka module metrics like producer, consumer, broker in 6.8.1 and later?",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "17earlgrey",
    "date": "December 26, 2019, 5:48am December 26, 2019, 2:59pm",
    "body": "https://www.elastic.co/guide/en/beats/metricbeat/master/metricbeat-module-kafka.html If you look at the link here, it seems that earlier versions provided the functionality (consumer, producer, broker) in beta. https://www.elastic.co/guide/en/beats/metricbeat/6.8/metricbeat-module-kafka.html However, if you look at version 6.8 and above, the setting is gone as above. (Only partitions and consumer groups available) When I put it in the configuration I get this error metricset 'kafka / producer' is not registered, metricset not found. Now I wonder if it doesn't provide consumer, producer, or broker metrics. Version Kafka : 5.3.1-ccs (Docker Image : confluentinc/cp-kafka:5.3.1) Metricbeat : 6.8.1",
    "website_area": "discuss"
  },
  {
    "id": "c9dd2a06-b133-4894-b934-b1f9a916c299",
    "url": "https://discuss.elastic.co/t/filebeat-in-kubernetes-does-not-inclue-kubernetes-metadata-on-node-restart/213019",
    "title": "Filebeat in Kubernetes does not inclue Kubernetes metadata on node restart",
    "category": [
      "Beats"
    ],
    "author": "",
    "date": "December 25, 2019, 3:23pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a89edb86-f919-44b9-b087-5701813d68ae",
    "url": "https://discuss.elastic.co/t/ilm-parameters-in-config/212885",
    "title": "ILM parameters in config",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "torten",
    "date": "December 23, 2019, 7:00pm December 23, 2019, 7:34pm December 23, 2019, 7:52pm December 24, 2019, 12:06pm December 24, 2019, 12:06pm January 21, 2020, 12:03pm",
    "body": "Good day, I want to set up ILM rollover_alias and policy_name parameters from the event fields, but no luck, all I got is errors like: `ERROR instance/beat.go:878 Exiting: failed to read the ilm rollover alias: key not found` Config example: filebeat.inputs: - type: log enabled: true paths: - /var/log/app/*.log json.keys_under_root: true json.add_error_key: true scan_frequency: 1s fields: rollover_alias: \"infra-application\" policy_name: \"2_shards_30_days\" output.elasticsearch: enabled: true hosts: [\"http://host1:9200\", \"http://host2:9200\"] setup.ilm.enabled: true setup.ilm.rollover_alias: \"%{[fields.rollover_alias]}\" setup.ilm.pattern: \"{now/d}-000001\" setup.ilm.policy_name: \"%{[fields.policy_name]}\"",
    "website_area": "discuss"
  },
  {
    "id": "e57d1ed8-3848-4e63-bb75-f9769ddf2ed6",
    "url": "https://discuss.elastic.co/t/event-size-from-beats/212860",
    "title": "Event size from beats",
    "category": [
      "Beats"
    ],
    "author": "Robin020",
    "date": "December 23, 2019, 3:10pm December 23, 2019, 5:34pm December 24, 2019, 9:37am January 21, 2020, 11:51am",
    "body": "Hi, I am trying to found out what the size is of an event of Metricbeat or Packetbeat etc. Does somebody know where to find documentation about the size of events? I only found something of APM but nothing about the Beats family. Robin",
    "website_area": "discuss"
  },
  {
    "id": "7f71ab61-0348-4c7e-97d9-814344df3dfa",
    "url": "https://discuss.elastic.co/t/found-duplicated-http-events-in-kubernetes-cluster/212909",
    "title": "Found Duplicated HTTP Events in Kubernetes Cluster",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "CoolDarran",
    "date": "December 24, 2019, 3:57am December 24, 2019, 4:04am January 21, 2020, 4:04am",
    "body": "Hi, We're using Packetbeat to capture http traffic in Kubernetes Cluster. We've observed that there are duplicated http events even set ignore_outgoing to true. Example as blow: image3078400 72.6 KB same network.community_id, source.ip, source.port, destination.ip, destination.port the difference between these two event is agent.hostname, hostname and node.name the actually event is the one with kubernetes.pod.uid attached My question is if there's any sort of method to avoid duplicated events?",
    "website_area": "discuss"
  },
  {
    "id": "670b9909-ac4f-4596-98e6-a8b86721e6c1",
    "url": "https://discuss.elastic.co/t/filebeat-stops-proccessing-s3-input-with-no-error/212590",
    "title": "Filebeat stops proccessing s3 input with no error",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Ronin",
    "date": "December 20, 2019, 12:06pm December 22, 2019, 3:07pm December 22, 2019, 7:52pm December 23, 2019, 9:42pm January 20, 2020, 9:42pm",
    "body": "Using filebeat 7.4.2 and stack monitoring we see where the throughput of filebeat drops to zero and the only abnormal log message is: 2019-12-20T03:47:31.711Z INFO [s3] s3/input.go:293 Message visibility timeout updated to 300 2019-12-20T03:47:39.581Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":5770,\"time\":{\"ms\":16}},\"total\":{\"ticks\":108770,\"time\":{\"ms\":28},\"value\":108770},\"user\":{\"ticks\":103000,\"time\":{\"ms\":12}}},\"handles\":{\"limit\":{\"hard\":1048576,\"soft\":1048576},\"open\":8},\"info\":{\"ephemeral_id\":\"e868953c-5e67-4695-ac95-0ffd16ed8822\",\"uptime\":{\"ms\":8280035}},\"memstats\":{\"gc_next\":42940960,\"memory_alloc\":22161072,\"memory_total\":16862697496},\"runtime\":{\"goroutines\":40}},\"filebeat\":{\"harvester\":{\"open_files\":0,\"running\":0}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":0}}},\"registrar\":{\"states\":{\"current\":0}},\"system\":{\"load\":{\"1\":0.28,\"15\":0.84,\"5\":0.63,\"norm\":{\"1\":0.14,\"15\":0.42,\"5\":0.315}}}}}} 2019-12-20T03:48:09.581Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":5780,\"time\":{\"ms\":8}},\"total\":{\"ticks\":108780,\"time\":{\"ms\":12},\"value\":108780},\"user\":{\"ticks\":103000,\"time\":{\"ms\":4}}},\"handles\":{\"limit\":{\"hard\":1048576,\"soft\":1048576},\"open\":7},\"info\":{\"ephemeral_id\":\"e868953c-5e67-4695-ac95-0ffd16ed8822\",\"uptime\":{\"ms\":8310035}},\"memstats\":{\"gc_next\":42940960,\"memory_alloc\":23817144,\"memory_total\":16864353568},\"runtime\":{\"goroutines\":38}},\"filebeat\":{\"harvester\":{\"open_files\":0,\"running\":0}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":0}}},\"registrar\":{\"states\":{\"current\":0}},\"system\":{\"load\":{\"1\":0.17,\"15\":0.81,\"5\":0.57,\"norm\":{\"1\":0.085,\"15\":0.405,\"5\":0.285}}}}}} The Message visibility timeout updated to 300 log line does not appear in our other filebeat instances that are running correctly processing the same s3 bucket. This seemingly happens random from what I can tell and a simple restart of the process will get it processing events again. image22061049 285 KB Configuration: filebeat.inputs: - type: s3 queue_url: https://sqs.us-east-1.amazonaws.com/xxxxxxx/queue processors: - decode_json_fields: fields: [\"message\"] process_array: true max_depth: 1 target: \"\" overwrite_keys: true - rename: fields: - from: \"log_level\" to: \"level\" ignore_missing: true processors: - drop_fields: fields: [\"agent.ephemeral_id\", \"agent.hostname\", \"agent.id\", \"agent.type\", \"agent.version\", \"host.name\", \"ecs.version\", \"input.type\", \"log.file.path\", \"log.offset\", \"stream\", \"suricata.eve.timestamp\", \"time\", \"timestamp\", \"kubernetes.container.image\", \"kubernetes.labels.pod-template-hash\", \"container.labels\", \"container.name\", \"container.image\", \"kubernetes.labels.ci_branch\", \"kubernetes.labels.ci_build_number\", \"kubernetes.labels.ci_dep_build_number\", \"kubernetes.labels.ci_dep_user\", \"kubernetes.labels.ci_user\", \"kubernetes.labels.generator\", \"kubernetes.labels.updated\", \"aws.s3.bucket.arn\", \"log.file.path\"] ignore_missing: true cloud.id: \"xxxxxxxxxx\" cloud.auth: \"xxxxxxxxx\" output.elasticsearch: compression_level: 5 worker: 10 bulk_max_size: 4096 monitoring: enabled: true cluster_uuid: xxxxxx elasticsearch: hosts: [\"https://xxxxxxxxx.us-east-1.aws.found.io:9243\"] username: xxxxx password: xxxxx Edit: Was able to find one instance that finally did output an error: 2019-12-20T12:03:09.533Z ERROR [s3] s3/input.go:291 change message visibility failed: InvalidParameterValue: Value AQEBK/v0lshsHIKTA2NI3HPr2EgFD8LQ73QAIalGrhp8q+l2LGJBxjbLivyVuytp8hbdXn+hXGp+b7WeBQEZiiKF3arYK/CToGiDN+ZgDBosW+YJhHetQu8pU4/rSkhpunsP1ywG1RvZ4O8IoKhqqjOHYsPZul2+ntjDNpOf5u16MA6ZbNUY0+ZDaxH6V61XcrYKsrUuIGBTMqp3EmRkROWgacE6XYYNTCuP0m1LfvRsAMHvKAUPu5O4fiyA1Wf+61EGVP6FDOc9+4Zk/rRt/IZh/y4YLMW3l7NyUFdVt6hjwovevHfEzlI9POWshBVZ3brrmHfaq08Lga1/M/auTM4ENImVH9H7cCQr36a/tlqJTemJwG8pCSJNzye8fZ2oO13UL6zGB+BaBDR4YcoNA+HRlrjBDegIBkDPkvqcM6Afk7g= for parameter ReceiptHandle is invalid. Reason: The receipt handle has expired. status code: 400, request id: 12500646-eb3b-5276-a0a3-503051863927 2019-12-20T12:03:09.533Z INFO [s3] s3/input.go:293 Message visibility timeout updated to 300 2019-12-20T12:03:23.548Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":354180},\"total\":{\"ticks\":1403390,\"time\":{\"ms\":12},\"value\":1403390},\"user\":{\"ticks\":1049210,\"time\":{\"ms\":12}}},\"handles\":{\"limit\":{\"hard\":1048576,\"soft\":1048576},\"open\":7},\"info\":{\"ephemeral_id\":\"3f765775-ef09-448b-8c66-c6315cee5030\",\"uptime\":{\"ms\":1974180053}},\"memstats\":{\"gc_next\":10904960,\"memory_alloc\":6704520,\"memory_total\":118881164656,\"rss\":-81920},\"runtime\":{\"goroutines\":22}},\"filebeat\":{\"harvester\":{\"open_files\":0,\"running\":0}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":0}}},\"registrar\":{\"states\":{\"current\":0}},\"system\":{\"load\":{\"1\":0.14,\"15\":0.36,\"5\":0.29,\"norm\":{\"1\":0.07,\"15\":0.18,\"5\":0.145}}}}}} 2019-12-20T12:03:53.548Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":354180,\"time\":{\"ms\":4}},\"total\":{\"ticks\":1403400,\"time\":{\"ms\":12},\"value\":1403400},\"user\":{\"ticks\":1049220,\"time\":{\"ms\":8}}},\"handles\":{\"limit\":{\"hard\":1048576,\"soft\":1048576},\"open\":6},\"info\":{\"ephemeral_id\":\"3f765775-ef09-448b-8c66-c6315cee5030\",\"uptime\":{\"ms\":1974210054}},\"memstats\":{\"gc_next\":10885360,\"memory_alloc\":5532072,\"memory_total\":118881547792},\"runtime\":{\"goroutines\":20}},\"filebeat\":{\"harvester\":{\"open_files\":0,\"running\":0}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":0}}},\"registrar\":{\"states\":{\"current\":0}},\"system\":{\"load\":{\"1\":0.3,\"15\":0.37,\"5\":0.31,\"norm\":{\"1\":0.15,\"15\":0.185,\"5\":0.155}}}}}} 2019-12-20T12:04:23.549Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":354180},\"total\":{\"ticks\":1403400,\"time\":{\"ms\":4},\"value\":1403400},\"user\":{\"ticks\":1049220,\"time\":{\"ms\":4}}},\"handles\":{\"limit\":{\"hard\":1048576,\"soft\":1048576},\"open\":6},\"info\":{\"ephemeral_id\":\"3f765775-ef09-448b-8c66-c6315cee5030\",\"uptime\":{\"ms\":1974240054}},\"memstats\":{\"gc_next\":10885360,\"memory_alloc\":5864024,\"memory_total\":118881879744},\"runtime\":{\"goroutines\":20}},\"filebeat\":{\"harvester\":{\"open_files\":0,\"running\":0}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":0}}},\"registrar\":{\"states\":{\"current\":0}},\"system\":{\"load\":{\"1\":0.18,\"15\":0.35,\"5\":0.28,\"norm\":{\"1\":0.09,\"15\":0.175,\"5\":0.14}}}}}} 2019-12-20T12:04:53.548Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":354180},\"total\":{\"ticks\":1403410,\"time\":{\"ms\":4},\"value\":1403410},\"user\":{\"ticks\":1049230,\"time\":{\"ms\":4}}},\"handles\":{\"limit\":{\"hard\":1048576,\"soft\":1048576},\"open\":6},\"info\":{\"ephemeral_id\":\"3f765775-ef09-448b-8c66-c6315cee5030\",\"uptime\":{\"ms\":1974270053}},\"memstats\":{\"gc_next\":10885360,\"memory_alloc\":6078752,\"memory_total\":118882094472},\"runtime\":{\"goroutines\":20}},\"filebeat\":{\"harvester\":{\"open_files\":0,\"running\":0}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":0}}},\"registrar\":{\"states\":{\"current\":0}},\"system\":{\"load\":{\"1\":0.44,\"15\":0.37,\"5\":0.33,\"norm\":{\"1\":0.22,\"15\":0.185,\"5\":0.165}}}}}}",
    "website_area": "discuss"
  },
  {
    "id": "152edca9-c11d-4bc7-8f1c-3a946b2166a6",
    "url": "https://discuss.elastic.co/t/dynamic-web-page-ingestion/212879",
    "title": "Dynamic Web Page Ingestion",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "mancharagopan",
    "date": "December 23, 2019, 6:10pm December 23, 2019, 6:17pm December 23, 2019, 7:54pm January 20, 2020, 7:54pm",
    "body": "is there any way to download dynamic web page with search results to ingest? I need to download a dynamic web page with search results?",
    "website_area": "discuss"
  },
  {
    "id": "c34c9722-405a-40a3-a11a-18a0896c216a",
    "url": "https://discuss.elastic.co/t/test-network-data-to-test-beats-inputs/212890",
    "title": "Test network data to test beats inputs",
    "category": [
      "Beats"
    ],
    "author": "bitsutah",
    "date": "December 23, 2019, 7:37pm January 20, 2020, 9:38pm",
    "body": "Is there a standard way to test a Beats input (like say, Graylog or Logstash) to make sure that an installation or configuration is working as expected? I would think that tcpreplay with a known-good pcap of Filebeat or Packetbeat would be a possible approach. Do reference pcaps exist? Or can you pipe some reference data thru netcat pointed at the logstash listener? Ideas?",
    "website_area": "discuss"
  },
  {
    "id": "def494fc-a689-4cca-8b65-9f087642a164",
    "url": "https://discuss.elastic.co/t/kubernetes-filebeat-how-to-handle-json-logging-for-some-containers/212888",
    "title": "Kubernetes/Filebeat - How to Handle JSON Logging for some containers",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Justin_Seiser",
    "date": "December 23, 2019, 7:11pm January 20, 2020, 7:11pm",
    "body": "Hello, I understand the basic premise, I need to configure auto discover, and then configure different filters within that, to specify how to handle the logs. I have not been able to find a working example. We have multiple deployments, deployments that log to JSON, have 'logging: json' set as a label, and an annotation. So that should be an easy way to identify, that I need to handle these logs as JSON. Our current filebeat.yaml setup.dashboards.enabled: true logging.metrics: enabled: false filebeat.autodiscover: providers: - type: kubernetes host: ${NODE_NAME} hints.enabled: true hints.default_config: type: container paths: - /var/log/containers/*${data.kubernetes.container.id}.log exclude_lines: [\"^\\\\s+[\\\\-`('.|_]\"] processors: - add_kubernetes_metadata: - add_cloud_metadata: Example Log App 1: {\"time\":1577127646.304000000,\"level\":\"INFO\",\"logger\":\"net.idauto.arms.gmm.import2.GroupImportEngine\",\"thread\":\"main\",\"message\":\"Starting the Group Import Engine...\"} Example Log App 2: {\"level\":\"info\",\"msg\":\"found role\",\"pod.iam.role\":\"arn:aws:iam::xxxxxx:role/rolename\",\"pod.ip\":\"172.21.66.172\",\"time\":\"2019-12-23T18:58:21Z\"} Complicated by the fact we run linkerd, so every pod, has a sidecar container, that does not log json Example Sidecar log: INFO [ 0.002965s] linkerd2_proxy::app::main using identity service at Name(NameAddr { name: \"linkerd-identity.linkerd.svc.cluster.local\", port: 8080 }) It would be such a help to our workflow, if we could figure out, how to properly parse/send JSON to our elasticcloud deployment For the pods that have containers, that do output JSON, we would/could also know the container name ahead of time, if that would help differentiate it from the linkerd-proxy container.",
    "website_area": "discuss"
  },
  {
    "id": "abbc6432-8bdd-471b-a3e1-f94b97db175d",
    "url": "https://discuss.elastic.co/t/filter-a-specific-text-from-a-file/212737",
    "title": "Filter a specific text from a file",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "mancharagopan",
    "date": "December 21, 2019, 1:56pm December 23, 2019, 4:40pm December 23, 2019, 6:11pm December 23, 2019, 6:03pm December 23, 2019, 6:07pm January 20, 2020, 6:07pm",
    "body": "I have a html file and i need to ingest only a specific line from that file to elasticsearch through logstash. Is there a way to do it? How do i filter only a line?",
    "website_area": "discuss"
  },
  {
    "id": "21aed5bb-19b2-449a-b0f8-4646b979d160",
    "url": "https://discuss.elastic.co/t/metricbeat-http-module-giving-invalid-character-looking-for-beginning-of-value/212840",
    "title": "MetricBeat HTTP Module giving \"invalid character '<' looking for beginning of value\"",
    "category": [
      "Beats"
    ],
    "author": "Rahul12",
    "date": "December 23, 2019, 12:00pm December 23, 2019, 5:25pm January 20, 2020, 7:25pm",
    "body": "Hi, I am using Metricbeat HTTP module to check the availability of Urls. HTTP module working fine when I tested it with localhost:9200 url. But when I am using any custom URL it is giving me error. \"invalid character '<' looking for beginning of value\" Below Configuration I am using : module: http metricsets: json period: 10s hosts: [\"http://www.google.com\"] namespace: \"json_namespace\" path: \"/\" #body: \"\" method: \"GET\" #username: \"elastic\" #password: \"*****\" #request.enabled: true response.enabled: true #json.is_array: true #dedot.enabled: true",
    "website_area": "discuss"
  },
  {
    "id": "3580f279-50fc-4f8c-ba16-2d06fc9c00c6",
    "url": "https://discuss.elastic.co/t/how-to-produce-beats-that-will-ingest-ecs-stats-into-elastic/212819",
    "title": "How to produce beats that will ingest ecs stats into elastic",
    "category": [
      "Beats"
    ],
    "author": "vanshika_agrawal",
    "date": "December 23, 2019, 8:04am December 23, 2019, 5:01pm January 20, 2020, 7:01pm",
    "body": "need help on how to produce beats that will ingest tor ecs stats into elastic so I can monitor them.",
    "website_area": "discuss"
  },
  {
    "id": "ea320bda-d132-4a3b-b1b8-8f4ba5b458cb",
    "url": "https://discuss.elastic.co/t/msssql-metricbeat-module/212292",
    "title": "MSSSQL metricbeat module",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Aleksandar",
    "date": "December 18, 2019, 9:25am December 19, 2019, 5:56pm December 23, 2019, 9:52am January 20, 2020, 9:52am",
    "body": "Hi, We want to use metricbeat mssql module, but do not know minimum priviliges for sql user. Our version MB 7.4.2, Best regards, Aleksandar",
    "website_area": "discuss"
  },
  {
    "id": "9ba8bf30-0eae-47fe-9499-eebcffb651ce",
    "url": "https://discuss.elastic.co/t/new-hosts-dont-show-in-the-filter/211570",
    "title": "New hosts don't show in the filter",
    "category": [
      "Beats"
    ],
    "author": "NogNeetMachinaal",
    "date": "December 12, 2019, 3:20pm December 21, 2019, 4:43pm January 18, 2020, 4:47pm",
    "body": "Team, I have installed auditbeat and filebeat to 2 Ubuntu hosts (on top of the 5 already done). I have used the same yml-config for all 7 hosts. These 2 new hosts don't show in the filter-by-agent.hostname when filebeat is selected. But they do show when auditbeat is selected. If I fill-in the hostname manually, collected data is shown for these new hosts. Any suggestions as to what can be done to make them show in the filter? Or where to start troubleshooting this? Thanks - Will",
    "website_area": "discuss"
  },
  {
    "id": "74b5c765-34ca-478d-bee5-e80d3ea1789b",
    "url": "https://discuss.elastic.co/t/beats-7-5-x/212388",
    "title": "Beats 7.5.x",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "shoopdas",
    "date": "December 18, 2019, 6:51pm December 21, 2019, 11:56am January 18, 2020, 11:55am",
    "body": "Heya all, new here. Looking at the changelog on fir beats 7.5.0, more specifically the breaking change for umask for files. Is there any way of giving the 'other' users read permissions? WIth umask beign set to 0027, others can never get read permissions to filebeat logs logging.to_files: true logging.files: path: /var/log/filebeat name: filebeat keepfiles: 7 permissions: 0644 will result in 0640 on filebeat log files any way of chaning the default umask? making changes to the service file with UMask=0000 or environment UMask=0000 doesnt work debian stretch is the OS",
    "website_area": "discuss"
  },
  {
    "id": "b0c8603e-a375-4161-bde0-c14685d0e80e",
    "url": "https://discuss.elastic.co/t/filebeat-6-8-changing-floating-point-zero-to-integer-in-json/211903",
    "title": "Filebeat 6.8 changing floating point zero to integer in json",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Zulu",
    "date": "December 16, 2019, 12:34am December 16, 2019, 1:07pm December 17, 2019, 8:47am December 17, 2019, 12:50pm December 21, 2019, 2:23am January 18, 2020, 4:23am",
    "body": "Hi, I am using filebeat to process telemetry data in json format from my application. The application generates files with json objects. When I need to log the floating point number zero, I log it as 0.0 with the intention of elasticsearch auto detecting the type as float type. Unfortunately, filebeat when parsing the json document converts 0.0 to 0 which makes elasticsearch interpret it as long. Heres' how the original document looks like in my json log file: {\"@t\":\"2019-12-15T20:32:15.3321532+11:00\",\"DemoJsonTelemetry\":{\"demoFloatZero\":0.0,\"demoDoubleZero\":0.0},\"type\":\"DemoJsonTelemetry\"} However, in filebeat DEBUG log I see the 0.0 changed to 0 2019-12-15T20:32:22.027+1100 DEBUG [publish] pipeline/processor.go:309 Publish event: { \"@timestamp\": \"2019-12-15T09:32:22.026Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"doc\", \"version\": \"6.8.1\", \"pipeline\": \"zl-events\" }, \"DemoJsonTelemetry\": { \"demoFloatZero\": 0, \"demoDoubleZero\": 0 }, \"type\": \"DemoJsonTelemetry\", \"log\": { \"file\": {} }, \"beat\": { \"timezone\": \"+11:00\", \"hostname\": \"DESKTOP-26QI0CF\", \"version\": \"6.8.1\" }, \"@t\": \"2019-12-15T20:32:15.3321532+11:00\" } My filebeat config looks like this: # structured json events - type: log enabled: true fields_under_root: true pipeline: \"zl-events\" paths: - c:\\Logging\\*.json json.keys_under_root: true json.add_error_key: true I'd really like to stick to type auto detection and would prefer not sending floats as strings. Is there a workaround or some trick to overcome this problem? Any help is very much appreciated. Thank You Zee Nastalski",
    "website_area": "discuss"
  },
  {
    "id": "494479df-fea3-4cbe-8ebb-b64f38c5e8cb",
    "url": "https://discuss.elastic.co/t/streaming-binary-filess-decoded-data-to-elastic/212712",
    "title": "Streaming Binary files's decoded data to elastic",
    "category": [
      "Beats"
    ],
    "author": "liron_gofberg",
    "date": "December 20, 2019, 10:19pm January 18, 2020, 12:19am",
    "body": "I am wondering what tools I can use for streaming decoded data from binary files to elastic the the fastest way. files are located at library and i want the library to be continuously checked for new files so the new data will be transfered automatically I need to know the fastest way to do that. Here is what I am trying to do. Those files are binary encoded files to save space. After decoding each file I get human readable structs (in c++). A lot of regular c++ structs. I want my folder to be checked for new files automatically and then decode the new files into structs . i want every struct object to be a record in elastic. and i want every struct object to be moved to elastic automatically. Please help me , its important",
    "website_area": "discuss"
  },
  {
    "id": "f9e14fd1-7b79-4714-b15a-1e241ffc46f0",
    "url": "https://discuss.elastic.co/t/filebeat-not-collecting-docker-logs/212568",
    "title": "Filebeat not collecting docker logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "algallagher",
    "date": "December 19, 2019, 10:02pm December 20, 2019, 4:45pm December 20, 2019, 5:00pm December 20, 2019, 9:44pm December 20, 2019, 9:43pm January 17, 2020, 7:29pm",
    "body": "I am running an ELK stack as 3 separate containers running locally (kibana, logstash, elasticsearch). I am running Docker Desktop for Windows (though I plan to migrate this entire setup to AWS). I am also running a java microservice separately in another container, and I've added a Filebeat container to the same docker-compose.yaml in order to collect logs from that microservice and forward the logs to ELK. The ELK stack runs fine but Filebeat is not collecting any logs. I've enabled DEBUG log level in Filebeat, which is showing 0 log files being harvested. What am I doing wrong? Here is my filebeat.yml filebeat.config: modules: path: ${path.config}/modules.d/*.yml reload.enabled: false filebeat.autodiscover: providers: - type: docker hints.enabled: true output.elasticsearch: hosts: [\"127.0.0.1:9200\"] setup.kibana: host: \"127.0.0.1:5601\" logging.level: info logging.to_files: true logging.files: path: /var/log/filebeat name: filebeat.log keepfiles: 7 ssl.verification_mode: none filebeat Dockerfile: FROM docker.elastic.co/beats/filebeat:7.5.1 COPY filebeat.yml /usr/share/filebeat/filebeat.yml USER root RUN chown root:filebeat /usr/share/filebeat/filebeat.yml USER filebeat docker-compose.yml: version: '3.5' services: my-app: build: context: . dockerfile: Dockerfile ports: # http port - \"9080:8080\" filebeat: container_name: filebeat user: root image: filebeat:latest volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/containers:/hostfs/var/lib/docker/containers command: filebeat -e -E output.elasticsearch.username=elastic -E output.elasticsearch.password=changeme -strict.perms=false default: external: name: my-network And here are the DEBUG logs from Filebeat. It shows that it's actually seeing the container and grabbing its ID. filebeat | 2019-12-19T21:40:33.089Z INFO log/input.go:152 Configured paths: [/var/lib/docker/containers/986f0c5d737f075322c71e8b59c8ad939e345f5891fb2c659ad7a55fa77a479e/*-json.log] filebeat | 2019-12-19T21:40:33.089Z DEBUG [autodiscover] cfgfile/list.go:101 Starting runner: input [type=container, ID=3747588001660246069] filebeat | 2019-12-19T21:40:33.089Z INFO input/input.go:114 Starting input of type: container; ID: 3747588001660246069 filebeat | 2019-12-19T21:40:33.090Z DEBUG [input] log/input.go:191 Start next scan filebeat | 2019-12-19T21:40:33.090Z DEBUG [input] log/input.go:212 input states cleaned up. Before: 0, After: 0, Pending: 0",
    "website_area": "discuss"
  },
  {
    "id": "45e3a14a-655c-40b7-a0e4-83934cfccdb5",
    "url": "https://discuss.elastic.co/t/searching-custom-fields-does-not-work-with-full-text-query-mode/212683",
    "title": "Searching custom fields does not work with Full Text query mode(?)",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "john_eapen",
    "date": "December 20, 2019, 4:18pm December 20, 2019, 5:47pm January 17, 2020, 5:47pm",
    "body": "Hi My log data contains several fields that Filebeat doesnt know about in advance. ( eg: myapp.traceId, myapp.host.name etc. ). To avoid conflicts these are not directly root but under a root property \"myapp\". Now, when I query explicitly as \"myapp.traceId: abc123\" , then this works BUT this does not work if I query simply as \"abc123\". I am trying to play around with this setting \"setup.template.append_fields [experimental]\" Or do I need to add these properties to fields.yml or something else to get them included into \"default_fields\" Appreciate the help Thx",
    "website_area": "discuss"
  },
  {
    "id": "71ff526a-0798-4469-83c5-4b57ede08ad6",
    "url": "https://discuss.elastic.co/t/would-dropping-events-by-regexp-match-be-possible/212642",
    "title": "Would dropping events by regexp match be possible?",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "stefws",
    "date": "December 20, 2019, 10:18am December 20, 2019, 10:59am December 20, 2019, 5:29pm January 17, 2020, 5:29pm",
    "body": "Any suggestions on how we best would target just below events from our backup client for the purpose of dropping such in winlogbeat? Could we do a regexp match on event_data.ProcessName as event.code or event.action might drop too much we fear? Hints appreciated, TIA! { \"_index\": \"siempoc_winlogbeat-7.5.0-2019.12.15\", \"_type\": \"_doc\", \"_id\": \"pQ-eCm8BLpqjdLaNYc-Z\", \"_version\": 1, \"_score\": null, \"_source\": { \"@timestamp\": \"2019-12-15T17:32:36.508Z\", \"agent\": { \"version\": \"7.5.0\", \"type\": \"winlogbeat\", \"ephemeral_id\": \"0bcd4dcf-f906-4bb9-b234-3d37021cbeaa\", \"hostname\": \"tdchprt03\", \"id\": \"ed2a3cc9-c384-46af-8aaa-cbddc05d628e\" }, \"log\": { \"level\": \"information\" }, \"message\": \"The handle to an object was closed.\\n\\nSubject :\\n\\tSecurity ID:\\t\\tS-1-5-18\\n\\tAccount Name:\\t\\tTDCHPRT03$\\n\\tAccount Domain:\\t\\t<redacted>\\n\\tLogon ID:\\t\\t0x3e7\\n\\nObject:\\n\\tObject Server:\\t\\tSecurity\\n\\tHandle ID:\\t\\t0xfa0\\n\\nProcess Information:\\n\\tProcess ID:\\t\\t0x83c\\n\\tProcess Name:\\t\\tC:\\\\Program Files\\\\Tivoli\\\\TSM\\\\baclient\\\\dsmcsvc.exe\", \"winlog\": { \"api\": \"wineventlog\", \"provider_name\": \"Microsoft-Windows-Security-Auditing\", \"event_id\": 4658, \"computer_name\": \"tdchprt03<redacted>\", \"opcode\": \"Info\", \"record_id\": 950353023, \"task\": \"File System\", \"keywords\": [ \"Audit Success\" ], \"provider_guid\": \"{54849625-5478-4994-a5ba-3e3b0328c30d}\", \"process\": { \"pid\": 4, \"thread\": { \"id\": 80 } }, \"event_data\": { \"ObjectServer\": \"Security\", \"HandleId\": \"0xfa0\", \"ProcessId\": \"0x83c\", \"ProcessName\": \"C:\\\\Program Files\\\\Tivoli\\\\TSM\\\\baclient\\\\dsmcsvc.exe\", \"SubjectUserSid\": \"S-1-5-18\", \"SubjectUserName\": \"TDCHPRT03$\", \"SubjectDomainName\": \"<redacted>\", \"SubjectLogonId\": \"0x3e7\" }, \"channel\": \"Security\" }, \"event\": { \"provider\": \"Microsoft-Windows-Security-Auditing\", \"action\": \"File System\", \"created\": \"2019-12-15T17:32:39.308Z\", \"kind\": \"event\", \"code\": 4658 }, \"ecs\": { \"version\": \"1.1.0\" }, \"host\": { \"name\": \"tdchprt03\", \"hostname\": \"tdchprt03\", \"id\": \"a543946a-cda2-43ba-8b27-a5d91690bec8\" } } }",
    "website_area": "discuss"
  },
  {
    "id": "483bb30c-d1db-42e1-bcd5-3dec42ef57cb",
    "url": "https://discuss.elastic.co/t/no-kibana-dashboards-in-saved-objects-dashboards-with-correct-metric-beat-settings/212567",
    "title": "No Kibana Dashboards in Saved Objects/Dashboards with correct metric beat settings",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "December 19, 2019, 9:41pm December 20, 2019, 2:09pm December 20, 2019, 2:17pm December 20, 2019, 2:31pm December 20, 2019, 4:08pm January 17, 2020, 4:08pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f404bdfc-e636-412a-8a27-f6a669891fbb",
    "url": "https://discuss.elastic.co/t/mapping-for-elasticsearch-easy/212036",
    "title": "Mapping for Elasticsearch [easy]",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Kevin_Csuka",
    "date": "December 16, 2019, 4:41pm December 20, 2019, 12:25pm January 17, 2020, 12:25pm",
    "body": "I've got an easy question. I've got this file: # epoch, metric1, metric2, metric3 1576425930,0.0718,0.0127,1 How can I tell Filebeat to send it to Elasticsearch and use the correct mapping. My config file currently looks like this: filebeat.inputs: - type: log enabled: true paths: - /tmp/file.csv filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false reload.period: 10s setup.template.name: \"test\" setup.template.fields: \"/etc/filebeat/map.yml\" setup.template.settings: index.number_of_shards: 1 cloud.id: <snip> cloud.auth: <snip> processors: - drop_fields: fields: [\"type\", \"beat.name\", \"beat.version\", \"_type\", \"_score\", \"_id\", \"@version\", \"offset\", \"host\", \"container\", \"input\", \"host\", \"agent\", \"log\", \"_score\"] Data is send to Elasticsearch, and a index is created with name; filebeat-, this is undesired. Anyone can help me out?",
    "website_area": "discuss"
  },
  {
    "id": "9fe37600-afca-41eb-b3f9-db8cb3a17f8d",
    "url": "https://discuss.elastic.co/t/services-down-of-services-down/212660",
    "title": "Services Down & % of Services Down",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "ginu",
    "date": "December 20, 2019, 12:18pm January 17, 2020, 12:18pm",
    "body": "Hi All I have a metricbeat which checks the service status which monitors the WWWServices. See below the Query event.dataset:windows.service AND windows.service.name: W3SVC AND windows.service.start_type.keyword:\"Automatic\" I would like to query for a service down, i know that I can run event.dataset:windows.service AND windows.service.name: W3SVC AND windows.service.start_type.keyword:\"Automatic\" AND NOT windows.service.state: Running My requirement is to identify when a 20 % of the services are down on multiple host to raise an alert. Any idea ? regards, Ginu",
    "website_area": "discuss"
  },
  {
    "id": "77dbc7d4-bd66-486e-ae22-09cd78dcc1cd",
    "url": "https://discuss.elastic.co/t/metricbeat-infrastructure-kubernetes-inventory-empty/209054",
    "title": "[METRICBEAT] Infrastructure kubernetes inventory empty",
    "category": [
      "Beats"
    ],
    "author": "Benjamin_Carriou",
    "date": "November 22, 2019, 11:32am November 26, 2019, 10:06am November 27, 2019, 8:27am December 7, 2019, 10:26pm December 10, 2019, 11:27am December 20, 2019, 11:32am January 17, 2020, 1:32pm",
    "body": "Hi, I have install metricbeats on my k8s cluster and I can see hosts metrics in infrastructure inventory module but kubernetes metrics (pods) are empty: Capture dcran de 2019-11-22 12-28-02.png1820531 48.4 KB We are some persons who this issue appears: Kibana/ES not showing kube pod metrics Metric beat kubernetes example not showing pod metrics Thx for your help ...",
    "website_area": "discuss"
  },
  {
    "id": "36dd5708-f581-4136-9169-9ec41efe303b",
    "url": "https://discuss.elastic.co/t/fielddata-is-disabled-on-text-fields-by-default-set-fielddata-true-on-host-name-in-order-to-load-fielddata-in-memory-by-uninverting-the-inverted-index/212111",
    "title": "Fielddata is disabled on text fields by default. Set fielddata=true on [host.name] in order to load fielddata in memory by uninverting the inverted index",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "skdasari",
    "date": "December 17, 2019, 7:56am December 17, 2019, 5:08pm December 18, 2019, 6:42am December 18, 2019, 12:01pm December 20, 2019, 8:57am January 17, 2020, 8:57am",
    "body": "Hello, Something weird happend lately in my elasticsearch setup letely. Metricbeat index got replaced with a new name like the old one is like metricbeat.x.date, But now the index is showing as metricbeat .version and when trying to visualize. I am getting the bellow error. Error: [esaggs] > Request to Elasticsearch failed: {\"error\":{\"root_cause\":[{\"type\":\"illegal_argument_exception\",\"reason\":\"Fielddata is disabled on text fields by default. Set fielddata=true on [host.name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"metricbeat-7.2.1\",\"node\":\"iD4ffU_sR2-uLU_SP01Ukw\",\"reason\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Fielddata is disabled on text fields by default. Set fielddata=true on [host.name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\"}}],\"caused_by\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Fielddata is disabled on text fields by default. Set fielddata=true on [host.name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\",\"caused_by\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Fielddata is disabled on text fields by default. Set fielddata=true on [host.name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\"}}},\"status\":400} Tried solutions: Provided the following in console. PUT my_index/_doc/host.name.keyword { \"properties\": { \"my_field\": { \"type\": \"text\", \"fielddata\": true } } } output: { \"_index\" : \"my_index\", \"_type\" : \"_doc\", \"_id\" : \"host.name\", \"_version\" : 1, \"_seq_no\" : 0, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"properties\" : { \"my_field\" : { \"type\" : \"text\", \"fielddata\" : true } } } } And also PUT my_index/_doc/host.name { \"properties\": { \"my_field\": { \"type\": \"text\", \"fielddata\": true } } } Output: { \"_index\" : \"my_index\", \"_type\" : \"_doc\", \"_id\" : \"host.name\", \"_version\" : 2, \"result\" : \"updated\", \"_shards\" : { \"total\" : 2, \"successful\" : 1, \"failed\" : 0 }, \"_seq_no\" : 2, \"_primary_term\" : 2 } Can any one help me out on my issue.",
    "website_area": "discuss"
  },
  {
    "id": "d5c5a70f-5bc0-45c2-bdca-1586e8fb500a",
    "url": "https://discuss.elastic.co/t/helm-chart-for-auditbeat/212559",
    "title": "Helm chart for Auditbeat",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "KenRider",
    "date": "December 19, 2019, 8:54pm December 20, 2019, 6:35am January 10, 2020, 6:35am",
    "body": "Is there going to be a Helm chart available for Auditbeat any time soon?",
    "website_area": "discuss"
  },
  {
    "id": "f09c6ac1-36da-45eb-a4dd-03fad253ced3",
    "url": "https://discuss.elastic.co/t/setup-template-name-not-working-with-ilm/212562",
    "title": "Setup.template.name not working with ILM (?)",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "john_eapen",
    "date": "December 19, 2019, 9:02pm December 20, 2019, 2:52am January 17, 2020, 2:51am",
    "body": "Hi, I have slightly confused about some of the posts I have seen on this. Does custom template not work when ILM is enabled in filebeat ? (Needless to say,I need both these features.) I hv specified my ES index as: index: \"logstash-fb-%{[agent.version]}-%{+yyyy.MM.dd}\" But the actual index are filebeat-* . See below filebeat logs snip 2019-12-19T20:55:03.747Z INFO [index-management] idxmgmt/std.go:265 ILM policy successfully loaded. 2019-12-19T20:55:03.747Z INFO [index-management] idxmgmt/std.go:394 Set setup.template.name to '{filebeat-7.4.1 {now/d}-000001}' as ILM is enabled. 2019-12-19T20:55:03.747Z INFO [index-management] idxmgmt/std.go:399 Set setup.template.pattern to 'filebeat-7.4.1-*' as ILM is enabled. 2019-12-19T20:55:03.747Z INFO [index-management] idxmgmt/std.go:433 Set settings.index.lifecycle.rollover_alias in template to {filebeat-7.4.1 {now/d}-000001} as ILM is enabled.",
    "website_area": "discuss"
  },
  {
    "id": "dda0a36c-89bd-45d4-9141-85651760deef",
    "url": "https://discuss.elastic.co/t/cisco-module-cannot-query-event-original-field/212433",
    "title": "Cisco Module - Cannot Query Event.Original field",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "jameswatson3",
    "date": "December 19, 2019, 4:00am December 19, 2019, 8:24pm January 16, 2020, 8:24pm",
    "body": "The filebeat 7.5 Cisco module is successfully sending asa, ftd and ios documents to elasticsearch. Using the pipelines created by filebeat modules cisco --pipelines, the documents are being parsed nicely and stats are visible in the respective kibana dashboards. However, I can now no longer query any word or phrase in the event.original field which is equivalent to the message field for documents that did not use the pipeline. If a document is indexed without using the pipeline, I can query words or phrases in the equivalent message field. I can also query the log.original field of the cisco-ios pipeline but not event.original of the cisco-ftp pipeline. I see from GET _ingest/pipeline that the filebeat-7.5.0-cisco-ftd-asa-ftd-pipeline seems to convert message to log.original then to event.orginal in some way. I have to assume that something about this process is what makes that field unqueryable. \"rename\" : { \"field\" : \"log.original\", \"target_field\" : \"event.original\", \"ignore_missing\" : true } Any advice on how I might be able to query words or phrases on event.original in these documents?",
    "website_area": "discuss"
  },
  {
    "id": "9dcae91a-28e4-473b-8622-64ebacd54b5a",
    "url": "https://discuss.elastic.co/t/filebeat-6-5-x-massive-memory-usage/212508",
    "title": "Filebeat 6.5.X - massive memory usage",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "urvi",
    "date": "December 19, 2019, 3:53pm December 19, 2019, 5:51pm January 16, 2020, 5:58pm",
    "body": "Hi team, I am using filebeat version 6.5.X. I have observed memory used by filebeat keep on increasing. memstat shows memory usage as below : \"memstats\":{\"gc_next\":11992835024,\"memory_alloc\":6185623680,\"memory_total\":4964881788840}} memory_alloc\":6185623680 - ~ 6GB while on task manager it shows memory usage 9196.8 MB (~ 9 GB ). It is using 28% of RAM is too much. Can you please suggest some tweaked in filebeat config ? Filebeat is light wight component then why it is using this much memory. We have 2-3 incident when it almost reach to 50% of total memory. Details : Registery file size : 10515 KB (10 MB ) Total size of D:\\data\\logfiles : 58 GB 1 day log produce nearly 10 GB data. Below is config file : =========================== Filebeat inputs ============================= filebeat.inputs: # Each - is an input. Most options can be set at the input level, so # you can use different inputs for various configurations. # Below are the input specific configurations. - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: #- /var/log/*.log - D:\\DATA\\LogFiles\\** # Exclude lines. A list of regular expressions to match. It drops the lines that are # matching any regular expression from the list. #exclude_lines: ['^DBG'] # Include lines. A list of regular expressions to match. It exports the lines that are # matching any regular expression from the list. #include_lines: ['^ERR', '^WARN'] # Exclude files. A list of regular expressions to match. Filebeat drops the files that # are matching any regular expression from the list. By default, no files are dropped. exclude_files: ['^D:\\\\(?i)(data)\\\\LogFiles\\\\GHS\\\\','^D:\\\\(?i)(data)\\\\LogFiles\\\\(?i)(AutoSys)\\\\','\\b(\\w*HouseKeeping\\w*)\\b','\\b(\\w*gmt_c_\\w*)\\b','\\b(\\w*TPMonitor.Application_\\w*)\\b','\\.zip$','\\.cal$','\\.gz$','\\.xml$','\\.txt.*\\d*$','\\.ser$','\\.mdmp$','\\.csv$','\\.hprof$'] ignore_older: 72h close_removed: true clean_inactive: 72h5m",
    "website_area": "discuss"
  },
  {
    "id": "b65b0128-25aa-402d-ad93-ab72a3e53078",
    "url": "https://discuss.elastic.co/t/filebeat-to-qradar/212535",
    "title": "Filebeat to QRadar",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "KenRider",
    "date": "December 19, 2019, 5:49pm January 16, 2020, 5:49pm",
    "body": "What is the easiest/best/recommended way to get Filebeat output to QRadar?",
    "website_area": "discuss"
  },
  {
    "id": "9d0f305f-0bb4-4f5d-ab99-23d05438a0d4",
    "url": "https://discuss.elastic.co/t/filebeat-logstash-syslog-fields/210074",
    "title": "Filebeat+logstash syslog fields",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 2, 2019, 4:11pm December 31, 2019, 9:22pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c88c87bd-4f54-482d-bad5-3e060b295856",
    "url": "https://discuss.elastic.co/t/importing-metricbeat-events-from-json-file-to-elastic-using-filebeat/212457",
    "title": "Importing metricbeat events from json-file to elastic using filebeat",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "dimuskin",
    "date": "December 19, 2019, 9:26am December 19, 2019, 11:35am December 19, 2019, 11:48am December 19, 2019, 3:47pm January 16, 2020, 3:47pm",
    "body": "Hi, I have a case when metricbeat can't deliver messages directly to elastic, instead, it writes JSON-style events to a file and later filebeat deliver it to elastic. But unfortunately I cant use \"json.keys_under_root\" in filebeat if json already contains \"@metadata\" fields. Filebeat will crash with error: 2019-12-18T23:42:33.354+0200 DEBUG [publish] pipeline/client.go:193 Pipeline client receives callback 'onFilteredOut' for event: %+v{0001-01-01 00:00:00 +0000 UTC null null { true 0xc42054c750 /tmp/metricbeat 613 2019-12-18 23:42:33.350883336 +0200 EET m=+0.026620145 -1ns log map 1483843-2050}} Steps to reproduce: Create json file with system metrics inside: metricbeat.modules: - module: system period: 30s metricsets: - cpu - load - memory - network - process - process_summary - diskio - module: system period: 1m metricsets: - filesystem - fsstat processors: - drop_event.when.regexp: system.filesystem.mount_point: '^/(sys|cgroup|proc|dev|etc|host|lib)($|/)' output.file: path: \"/tmp\" filename: metricbeat.json After that try to deliver this file to ELK by filebeat with next config: filebeat.inputs: - type: log paths: [\"/tmp/metricbeat.json\"] json.keys_under_root: true json.overwrite_keys: true output.logstash: hosts: [\"my.server.com:5555\"] logging.level: debug Filebeat refuses to process JSON if it already contains \"@metadata\" (with beat, type and version fields). Is any workaround for it? I already tried to use \"processors\" to remove this fields on both side (metricbeat and filebeat) but looks you can't remove system field.",
    "website_area": "discuss"
  },
  {
    "id": "40565eb2-a064-4140-b7a2-d16dbf862b67",
    "url": "https://discuss.elastic.co/t/apache-fields-are-disable/212349",
    "title": "Apache fields are disable",
    "category": [
      "Beats"
    ],
    "author": "Yacine_Mourchid",
    "date": "December 20, 2019, 2:29pm December 18, 2019, 5:04pm December 18, 2019, 6:55pm December 19, 2019, 9:33am December 19, 2019, 3:39pm January 16, 2020, 3:39pm",
    "body": "Hello, When I want to see all fields of my index in Discovery, my apache's fields are disable (I can see them only if I uncheck \"hide missing fields\"). In visualization, I can choose these fields but no data appears. I added apache.yml and enabled it but It still doesn't work. Someone have an idea ?",
    "website_area": "discuss"
  },
  {
    "id": "77307e50-d443-46b6-87b2-efa758d1215b",
    "url": "https://discuss.elastic.co/t/could-not-get-windows-performance-counters-since-metricbeats-7-3-0/198676",
    "title": "Could not get Windows Performance Counters since Metricbeats 7.3.0",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "jbeyer",
    "date": "September 9, 2019, 12:18pm September 11, 2019, 8:06am September 11, 2019, 10:02am September 18, 2019, 7:45am September 26, 2019, 8:11am September 26, 2019, 10:31am October 2, 2019, 2:14pm October 12, 2019, 6:50pm October 24, 2019, 8:01am November 13, 2019, 10:11am November 13, 2019, 10:12am November 13, 2019, 10:16am November 13, 2019, 12:52pm November 21, 2019, 1:31pm November 21, 2019, 4:18pm November 27, 2019, 2:23pm December 4, 2019, 3:27pm December 9, 2019, 11:48am December 10, 2019, 1:42am December 19, 2019, 2:07pm",
    "body": "I use Metricbeats to send Windows Perfomance Counters, like processor time and private bytes to elasticsearch. Until Metricbeats version 7.2.0 it runs well, but the same configuration doesn't run in version 7.3.0 and 7.3.1. If I run \"metricbeats.exe test modules\", I get an error that the performance counter does not exist. But it exists and the Metricbeats 7.2.0 can access this performance counter. The error message, that I get from \"metricbeats.exe test modules\" is: metricbeat.exe test modules Error getting metricbeat modules: module initialization error: 1 error: initialization of reader failed: failed to expand counter (query=\"\\Process(bbGateService)% Processor Time\"): Das angegebene Objekt wurde nicht auf dem Computer gefunden. The content of the windows.yml file: - module: windows metricsets: [\"perfmon\"] period: 1s perfmon.counters: # CPU - instance_label: \"ECM_Gate2\" instance_name: \"gate2\" measurement_label: \"ecm.gate2.gateservice.cpu\" query: '\\Process(bbGateService)\\% Processor Time' format: \"float\" # Memory - instance_label: \"ECM_Gate2\" instance_name: \"gate2\" measurement_label: \"ecm.gate2.gateservice.memory\" query: '\\Process(bbGateService)\\Private Bytes' format: \"float\"",
    "website_area": "discuss"
  },
  {
    "id": "cb25c34a-49e3-4a98-b0ce-1ce99c6a8db9",
    "url": "https://discuss.elastic.co/t/filebeat-creates-wrong-index-name/212456",
    "title": "Filebeat creates wrong index name",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 19, 2019, 9:22am December 19, 2019, 10:02am December 19, 2019, 11:03am December 19, 2019, 11:19am January 16, 2020, 11:30am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ce3ec109-c0d5-4c29-a746-8b459d228571",
    "url": "https://discuss.elastic.co/t/filebeat-cpu-utilization-metrics-are-not-normalized-by-default/210659",
    "title": "Filebeat CPU utilization metrics are not normalized by default",
    "category": [
      "Beats"
    ],
    "author": "",
    "date": "December 5, 2019, 9:24am December 9, 2019, 9:24pm December 10, 2019, 8:58am December 10, 2019, 8:08pm December 19, 2019, 8:38am December 12, 2019, 6:33pm December 19, 2019, 8:38am January 16, 2020, 10:38am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5e7b2454-f59e-4fee-a472-bb323bfa4b08",
    "url": "https://discuss.elastic.co/t/is-the-logs-collected-by-winlogbeat-legally-valid-as-an-evidence-in-court/212441",
    "title": "Is the logs collected by winlogbeat legally valid as an evidence in court?",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "jawadak",
    "date": "December 19, 2019, 5:12am December 19, 2019, 7:18am December 19, 2019, 7:21am January 16, 2020, 7:27am",
    "body": "Hi, I have a query related to legal purpose. I am using winlogbeat to ship windows event logs to the elasticsearch. And winlogbeat process the logs and add extra fields to it and store it as docs in an index. So, If I have a requirement to submit the RAW logs to an US court for investigation and forensics, can I submit these logs from the elasticsearch indices as an evidence? If NO, what's the solution?",
    "website_area": "discuss"
  },
  {
    "id": "48aa83fc-8cc2-4a88-9e49-3f6bd6f7c22f",
    "url": "https://discuss.elastic.co/t/logstash-output-from-filebeat-what-is-index-configuration-option/212421",
    "title": "Logstash output from filebeat. What is 'index' configuration option?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Anand9",
    "date": "December 19, 2019, 12:31am December 19, 2019, 2:20am January 16, 2020, 2:20am",
    "body": "This is an excerpt taken from filebeat config for logstash output here I'm wondering what does index have to do with logstash. In my logstash configuration itslef, if I redirect logs to ElasticSearch I believe my logs will be indexed under \"logstash-%{+YYYY.MM.dd}\" as the documentation says here. So why is there an option to set index for filebeat's logstash output?",
    "website_area": "discuss"
  },
  {
    "id": "7cecd855-6416-4ad4-9c55-ebeb7d43e78d",
    "url": "https://discuss.elastic.co/t/netflow-module-bi-directinal-flows-input/212420",
    "title": "Netflow module bi-directinal flows input",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "fpr",
    "date": "December 19, 2019, 12:29am January 16, 2020, 12:29am",
    "body": "Hi, I want to collect bi-directional netflow generated by nprobe and suricata. But it seems with the netflow module the output is always converted to uni-directional flows. Is that expected, a bug or have i overlooked an option? I am wondering because with the logstash netflow input codec i get the expected bi-directional flow output (when collecting bi-directional flows).",
    "website_area": "discuss"
  },
  {
    "id": "1704ed63-88c4-43d1-950e-ec56822946c0",
    "url": "https://discuss.elastic.co/t/system-socket-dataset-setup-failed/210574",
    "title": "System/socket dataset setup failed",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "Willl",
    "date": "December 4, 2019, 5:21pm December 12, 2019, 12:37pm December 18, 2019, 6:22pm January 8, 2020, 6:22pm",
    "body": "Testing auditbeat-7.5.0-1-x86_64 on SLES 12 SP4. When the socket dataset is enabled under module system, auditbeat restarts. I checked the release note which says ipv6.disable=1 is taken care of. Any suggestion? Blockquote auditbeat[20739]: 2019-12-04T12:13:18.733-0500#011WARN#011[cfgwarn]#011socket/socket_linux.go:87#011BETA: The system/socket dataset is beta. auditbeat[20739]: 2019-12-04T12:13:18.765-0500#011INFO#011[socket]#011socket/socket_linux.go:223#011Setting up system/socket for kernel 4.12.14-94.41-default auditbeat[20739]: 2019-12-04T12:13:18.877-0500#011INFO#011[socket]#011guess/guess.go:258#011Running 16 guesses ... auditbeat[20739]: 2019-12-04T12:13:21.720-0500#011INFO#011add_cloud_metadata/add_cloud_metadata.go:89#011add_cloud_metadata: hosting provider type not detected. auditbeat[20739]: 2019-12-04T12:13:34.068-0500#011INFO#011instance/beat.go:402#011auditbeat stopped. auditbeat[20739]: 2019-12-04T12:13:34.069-0500#011ERROR#011instance/beat.go:916#011Exiting: 1 error: 1 error: system/socket dataset setup failed: unable to guess one or more required parameters: guess_sk_buff_proto failed: timeout while waiting for event auditbeat[20739]: Exiting: 1 error: 1 error: system/socket dataset setup failed: unable to guess one or more required parameters: guess_sk_buff_proto failed: timeout while waiting for event systemd[1]: auditbeat.service: Main process exited, code=exited, status=1/FAILURE systemd[1]: auditbeat.service: Unit entered failed state. systemd[1]: auditbeat.service: Failed with result 'exit-code'. systemd[1]: auditbeat.service: Service RestartSec=100ms expired, scheduling restart. systemd[1]: Stopped Audit the activities of users and processes on your system.. systemd[1]: Started Audit the activities of users and processes on your system..",
    "website_area": "discuss"
  },
  {
    "id": "a3b15f6b-275f-4d77-a5cd-313381b2c327",
    "url": "https://discuss.elastic.co/t/using-filebeat-modules-and-logstash-conf/212373",
    "title": "Using Filebeat modules and logstash conf",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "admin_berlin",
    "date": "December 18, 2019, 4:46pm January 15, 2020, 4:46pm",
    "body": "Hi, i'am trying to use the apache ECS dashboards, but the panels only display \"No results found\". Heres my setup: Webserver (with Filebeat and apache module enabled) --> Logstash (with multiple inputs and outputs ) --> Elastisearch --> Kibana Please help :S My configurations: filebeat.yml #----------------------------- Logstash output -------------------------------- output.logstash: # The Logstash hosts hosts: [\"172.22.64.62:5044\"] logstash.conf ############## INPUT input { # alle system logs beats { port => 5544 type => syslog } # alle apache logs beats { port => 5044 type => apache } # alle netflow logs beats { port => 2255 type => netflow } } ############## FILTER filter { } ############## OUTPUT output { if [type] == \"netflow\" { elasticsearch { hosts => [\"172.22.64.63:9200\"] pipeline => \"%{[@metadata][pipeline]}\" user => xxxxx password => xxxxx index => \"logstash-netflow-%{+YYYY.MM.dd}\" } } if [type] == \"syslog\" { elasticsearch { hosts => [\"172.22.64.63:9200\"] user => xxxxx password => xxxxx index => \"logstash-syslog-%{+YYYY.MM.dd}\" } } if [type] == \"apache\" { elasticsearch { hosts => [\"172.22.64.63:9200\"] pipeline => \"%{[@metadata][pipeline]}\" user => xxxxx password => xxxxx index => \"logstash-apache-%{+YYYY.MM.dd}\" } } } It seems like that something went wrong in the logstash config. When i use the elasticsearch output in the filebeat from the webserver, everything works fine. thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "9c5616d2-b15d-490a-9c73-407a5078f09f",
    "url": "https://discuss.elastic.co/t/getting-tomcat-logs-from-kubernetes-pods/211880",
    "title": "Getting Tomcat logs from Kubernetes pods",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "KenRider",
    "date": "December 14, 2019, 8:43pm December 16, 2019, 1:23pm December 16, 2019, 2:39pm December 16, 2019, 3:10pm December 16, 2019, 4:15pm December 16, 2019, 4:42pm December 16, 2019, 5:23pm December 18, 2019, 1:39pm January 15, 2020, 1:39pm",
    "body": "What is the recommended way to get Tomcat logs from Kubernetes pods? I have Elasticsearch, Kibana, and Filebeat 7.5.0 helm charts installed and am getting sysout and syserr from the pods but I also need the logs from /usr/local/tomcat/logs/ of each pod.",
    "website_area": "discuss"
  },
  {
    "id": "e060c9e2-b4fe-480a-ae6d-bcc378fb5804",
    "url": "https://discuss.elastic.co/t/filebeat-to-logstash-basic-authentication/212320",
    "title": "Filebeat to Logstash Basic Authentication",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "JY_DT",
    "date": "December 18, 2019, 12:27pm December 18, 2019, 12:49pm January 15, 2020, 12:58pm",
    "body": "Hello, I'm using a typical setup with Filebeat agents running on different Windows clients and Logstash and Elasticsearch setup on a separate server (Windows). I'm using the latest version 7.5. I need to configure basic authentication from Filebeat to Logstash. This option is available for Elasticsearch: output.elasticsearch: #Optional protocol and basic auth credentials. #protocol: \"https\" #username: \"elastic\" #password: \"changeme\" but not for Logstash. So, how do I configure basic authentication between Filebeat and Logstash in that case. Thanks in advance, Jy",
    "website_area": "discuss"
  },
  {
    "id": "4b0e3cdf-1dbb-42e1-91df-13eea2e8c24f",
    "url": "https://discuss.elastic.co/t/field-not-comming-in-audit-beat-logs-user-audit-name/210465",
    "title": "Field not comming in audit beat logs user.audit.name",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "akki2208",
    "date": "December 4, 2019, 6:10am December 4, 2019, 4:46pm December 18, 2019, 5:49am December 18, 2019, 12:02pm January 8, 2020, 12:02pm",
    "body": "I am using audit beat version 7.4 in audit beat logs field user.audit.name is not coming. Can anyone explain what might be the cause of it?",
    "website_area": "discuss"
  },
  {
    "id": "c68fb3d1-0395-4648-a61f-3c1611c35b89",
    "url": "https://discuss.elastic.co/t/custom-field-name-not-showing-in-filebeat/212088",
    "title": "Custom field name not showing in Filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "katara",
    "date": "December 17, 2019, 4:14am December 17, 2019, 8:29am December 17, 2019, 10:11am December 18, 2019, 9:57am January 15, 2020, 9:59am",
    "body": "Below is how im trying to add a custom fiels name in my filebeat 7.2.0 filebeat.inputs: - type: log enabled: true paths: - D:\\Oasis\\Logs\\Admin_Log\\* - D:\\Oasis\\Logs\\ERA_Log\\* - D:\\OasisServices\\Logs\\* processors: - add_fields: fields: application: oasis and with this, im expecting a new field called application whose data entries will be 'oasis' . But i dont get any. I also tried fields: application: oasis/'oasis' Help me with this.",
    "website_area": "discuss"
  },
  {
    "id": "292adc7b-0d9f-4241-9460-280c213ccb67",
    "url": "https://discuss.elastic.co/t/metricbeat-issue/212293",
    "title": "Metricbeat issue",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "tanthiamhuat",
    "date": "December 18, 2019, 9:36am January 15, 2020, 9:36am",
    "body": "I follow the instructions for installations: curl -L -O https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.5.0-amd64.deb sudo dpkg -i metricbeat-7.5.0-amd64.deb Above 2 is OK Modify /etc/metricbeat/metricbeat.yml to set the connection information for Elastic Cloud: cloud.id: \"FirstElasticDeployment:abcdefblahblahblah==\" cloud.auth: \"elastic:defghijkblahblah\" line 85 is cloud.auth: \"elastic:defghijkblahblah\" which I find there is no issue, why does it complains below: sudo metricbeat modules enable elasticsearch Error initializing beat: error loading config file: yaml: line 85: could not find expected ':'",
    "website_area": "discuss"
  },
  {
    "id": "3844bd54-c0fe-4fc1-a3e4-02c59c5441cd",
    "url": "https://discuss.elastic.co/t/metricbeat-docker-module-docker-cpu-total-pct-service-or-container/212223",
    "title": "Metricbeat Docker Module docker.cpu.total.pct service or container?",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "December 17, 2019, 8:20pm January 14, 2020, 8:20pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0415828b-aa47-4b32-aa19-186576d6df72",
    "url": "https://discuss.elastic.co/t/how-to-test-filebeat-can-take-log-files-and-stdout-to-terminal/212047",
    "title": "How to test filebeat can take log files and stdout to terminal?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Mehak_Bhargava",
    "date": "December 16, 2019, 7:02pm December 17, 2019, 4:23pm December 17, 2019, 5:59pm January 14, 2020, 5:59pm",
    "body": "This is my filebeat.yml and it is not harvesting the logs from the defined file path- filebeat.inputs: - type: log enabled: true paths: -/home/mehak/Documents/filebeat-7.4.0-linux-x86_64/logs/log2.log fields: type: other fields_under_root: true output.logstash: hosts: [\"localhost:5044\"] index: \"logstash-%{[beat.version]}-%{+yyyy.MM.dd}\" When I run in debug mode, I see this to confirm no file is harvested- filebeat\":{\"harvester\":{\"open_files\":0,\"running\":0}} Please help!",
    "website_area": "discuss"
  },
  {
    "id": "0c6a92b9-31ca-44bf-8850-c90a18ccc15c",
    "url": "https://discuss.elastic.co/t/metricbeat-aws-module-which-metrics-are-pulled/212175",
    "title": "Metricbeat AWS Module - Which metrics are pulled?",
    "category": [
      "Beats"
    ],
    "author": "cesarlino",
    "date": "December 17, 2019, 2:27pm December 17, 2019, 5:43pm December 17, 2019, 5:48pm January 14, 2020, 7:46pm",
    "body": "Hi, We want to setup AWS module to collect only RDS and S3 metrics with the following frequency: RDS: every minute S3: once a day AWS charges by the number of metrics requested. It is not clear how many and which metrics this AWS module configuration will pull from AWS, in order to calculate the cost. Does anybody know that or could point where this is written? Regards, Cesar Lino",
    "website_area": "discuss"
  },
  {
    "id": "0004daba-bd21-4515-aa46-c0a5bcf296a1",
    "url": "https://discuss.elastic.co/t/use-packetbeat-inside-rancher-cluster/208657",
    "title": "Use packetbeat inside Rancher cluster",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "",
    "date": "November 20, 2019, 9:13am December 17, 2019, 5:41pm January 14, 2020, 5:41pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "2acb214b-0c41-4a4e-bff1-774c891b9458",
    "url": "https://discuss.elastic.co/t/filebeat-cannot-connect-to-logstash-using-ssl-curl-35-tcp-connection-reset-by-peer/212193",
    "title": "Filebeat cannot connect to Logstash using ssl. curl: (35) TCP connection reset by peer",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Kosodrom",
    "date": "December 17, 2019, 4:49pm December 17, 2019, 4:25pm December 17, 2019, 5:15pm January 14, 2020, 4:42pm",
    "body": "Hi guys, i've set up security for ELK. Everythings works, except the communication between Filebeat and Logstash. Everything is version 7.4 filebeat.yml output.logstash: # The Logstash hosts hosts: [\"ip-address:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications ssl.certificate_authorities: [\"/certs/ca.crt\"] # Certificate for SSL client authentication ssl.certificate: \"/certs/filebeat_client.crt\" # Client Certificate Key ssl.key: \"/filebeat_client.key\" ssl.key_passphrase: password logstash config file: input { beats { port => \"5044\" ssl => true ssl_certificate_authorities => [\"/certs/logstash-01/ca.crt\"] ssl_certificate => \"/certs/logstash-01/logstash-01.crt\" ssl_key => \"/certs/logstash-01/logstash01.pkcs8.key\" ssl_verify_mode => \"peer\" } } filter { csv { separator => \",\" columns => [\"id\",\"insert_time\",\"unix_time\"] convert => { \"id\" => \"integer\" } } } output { elasticsearch { hosts => [\"elastic01.com:9200\"] index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" ssl => true ssl_certificate_verification => true cacert => '/certs/logstash-01/elastic-ca.pem' user => logstash_internal password => password } } When I try to start filebeat, I see following error in the filebeat logs: |2019-12-17T16:01:20.284Z|INFO|pipeline/output.go:95|Connecting to backoff(async(tcp://logstash-ip-address:5044))| |---|---|---|---| |2019-12-17T16:01:21.434Z|ERROR|pipeline/output.go:100|Failed to connect to backoff(async(tcp://logstash-ip-address:5044)): read tcp filebeat-machine-ip-addres:54108->logstash-ip-address:5044: read: connection reset by peer| following in the logstash logs: [2019-12-17T16:01:20,304][INFO ][org.logstash.beats.BeatsHandler][main] [local: 0.0.0.0:5044, remote: undefined] Handling exception: javax.net.ssl.SSLException: java.lang.Exception: Error setting private key (error:0b000074:X.509 certificate routines:OPENSSL_internal:KEY_VALUES_MISMATCH) [2019-12-17T16:01:20,304][INFO ][org.logstash.beats.BeatsHandler][main] [local: 0.0.0.0:5044, remote: undefined] Handling exception: javax.net.ssl.SSLException: java.lang.Exception: Error setting private key (error:0b000074:X.509 certificate routines:OPENSSL_internal:KEY_VALUES_MISMATCH) [2019-12-17T16:01:20,305][WARN ][io.netty.channel.DefaultChannelPipeline][main] An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception. io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: java.lang.Exception: Error setting private key (error:0b000074:X.509 certificate routines:OPENSSL_internal:KEY_VALUES_MISMATCH) I tried to: curl -v --cacert ./certs/ca.crt https://logstash-ip-address:5044 and I received following: * About to connect() to logstash-ip-address port 5044 (#0) * Trying logstash-ip-address... * Connected to logstash-ip-address (logstash-ip-address) port 5044 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * CAfile: /certs/ca.crt CApath: none * NSS error -5961 (PR_CONNECT_RESET_ERROR) * TCP connection reset by peer * Closing connection 0 curl: (35) TCP connection reset by peer Info: I've extracted the ca.crt file from elastic-stack-ca.p12, which I've created using this documentation: https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-tls.html. I used elastic-stack-ca.p12 to sign all keys that I have created for all ELK nodes. I've created key and cert for filebeat and signed with elastic-stack-ca.p12 also. For Filebeat i've also created key and crt using this elastic-stack-ca.p12. What is my mistake?",
    "website_area": "discuss"
  },
  {
    "id": "279ddadb-cd5d-42f1-895a-6be0ec1681fd",
    "url": "https://discuss.elastic.co/t/filebeat-multiple-ports/211762",
    "title": "Filebeat multiple ports",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "katara",
    "date": "December 13, 2019, 8:22am December 13, 2019, 9:44am December 13, 2019, 10:01am December 13, 2019, 10:07am December 13, 2019, 10:17am December 13, 2019, 12:55pm December 13, 2019, 1:12pm December 13, 2019, 1:18pm December 16, 2019, 4:46am December 16, 2019, 7:55am December 16, 2019, 11:09am December 17, 2019, 4:13am December 17, 2019, 4:25am December 17, 2019, 8:06am December 17, 2019, 9:53am December 17, 2019, 10:45am December 17, 2019, 10:48am December 17, 2019, 10:50am January 14, 2020, 10:50am",
    "body": "Hi, I have 2 servers where OASIS logs are getting monitored with filebeat. I want them to be saved in the same index. Should I port themm from different ports( Server 1 : 5044, server 2 : 5045)? Or can i use the same port for both servers? If I use different ports , can i map them to the same index? Kindly help me out.",
    "website_area": "discuss"
  },
  {
    "id": "ccd87544-d2ab-472e-94ba-2bd0cdf556fe",
    "url": "https://discuss.elastic.co/t/error-loading-vsphere-dashboards/211925",
    "title": "Error loading Vsphere Dashboards",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "richard_N",
    "date": "December 15, 2019, 9:11pm December 16, 2019, 1:09pm December 16, 2019, 1:24pm December 16, 2019, 1:32pm December 16, 2019, 2:20pm December 16, 2019, 4:25pm December 16, 2019, 6:17pm December 16, 2019, 7:13pm December 16, 2019, 10:06pm December 17, 2019, 10:18am January 14, 2020, 10:18am",
    "body": "It won't let me load the vsphere dashboards in 7.5. The plugin works fine but get unsupported media type using API and UI just says sorry there was an error. Has anyone else been able to get this to load?",
    "website_area": "discuss"
  },
  {
    "id": "c530b562-dc57-4078-8c52-df3578ccfe53",
    "url": "https://discuss.elastic.co/t/metric-beat-service-turns-off-automatically-in-windows-sever/211060",
    "title": "Metric beat service turns off automatically in windows sever",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "shiva13",
    "date": "December 9, 2019, 7:16am December 9, 2019, 10:50am December 9, 2019, 10:59am December 9, 2019, 2:29pm December 10, 2019, 10:13am December 10, 2019, 1:03pm December 17, 2019, 8:48am January 14, 2020, 8:48am",
    "body": "Hi, I have integrated my windows server using metric beat to ELK. But the services automatically goes to off mode every few minutes. Please help. Thanks, Shiva",
    "website_area": "discuss"
  },
  {
    "id": "3eaeb10e-22c5-4492-85b8-5c6299b72901",
    "url": "https://discuss.elastic.co/t/how-to-add-multiple-metricbeat-parameter-to-query/211634",
    "title": "How to add multiple metricbeat parameter to query",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "arvindK_sharma",
    "date": "December 12, 2019, 11:29am December 12, 2019, 12:05pm December 13, 2019, 3:13am December 13, 2019, 8:59am December 17, 2019, 5:59am December 17, 2019, 6:04am December 17, 2019, 8:23am January 14, 2020, 8:23am",
    "body": "Hi, I want to expose endpoint of Metricbeat system parameters (Same as KIBANA) with the help of NEST to create a page similar to KIBANA dashboard in my application. I want to fetch all the parameters like CPU, Memory, diskio etc. I write following query which gives me latest one hit for cpu parameter get /metricbeat*/_search?pretty=true {\"size\":1, \"query\":{\"bool\":{\"must\": [{\"range\": {\"system.cpu.idle.pct\": {\"gte\": 0}}}]}}, \"sort\":[{\"@timestamp\": {\"order\": \"desc\"}}]} I want to add memoery, diskio and some more parameter to it so I get on hit per parameter. I tried to add by this way but no luck {\"range\": {\"system.cpu.idle.pct\": {\"gte\": 0}}}, {\"range\": {\"system.memory.used.pct\": {\"gte\": 0}}} Please suggest how to implement it.",
    "website_area": "discuss"
  },
  {
    "id": "6b9f0838-c53e-44e7-ae30-0b706d8e6fcc",
    "url": "https://discuss.elastic.co/t/heartbeat-cant-connect-to-endpoint-with-cert-and-key/208436",
    "title": "Heartbeat can't connect to endpoint with cert and key",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "rdaimler",
    "date": "November 19, 2019, 5:59am November 26, 2019, 12:21pm November 26, 2019, 6:09pm November 26, 2019, 6:13pm November 26, 2019, 8:41pm November 26, 2019, 10:37pm December 16, 2019, 10:39pm December 17, 2019, 1:45am December 17, 2019, 1:56am January 14, 2020, 1:56am",
    "body": "Hello, I'm trying to check an endpoint that requires a cert and key. I get a Client.Timeout exceeded while awaiting headers error. - type: http id: Bitbucket schedule: '@every 20s' urls: [\"https://bark.tgyu.com/bitbucket\"] # ssl.verification_mode: none ssl: certificate: /root/A/public-cert.pem key: /root/A/key.pem check.response.status: 302 check.request.method: HEAD timeout: 20 I can successfully curl the endpoint from the same server. I'm running elastic 7.1.1 Thanks, Rob",
    "website_area": "discuss"
  },
  {
    "id": "769e087d-381b-4185-84e5-15c29e2a6280",
    "url": "https://discuss.elastic.co/t/merging-old-metricbeat-data-to-save-disk-space/212068",
    "title": "Merging old Metricbeat data to save disk space",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "jerrac",
    "date": "December 16, 2019, 10:26pm January 13, 2020, 10:34pm",
    "body": "I'm in the middle of a long running project to choose the best monitoring toolset we can. Right now I'm testing ELKStack and Prometheus+Grafana. During my research I remember reading about the idea of merging old metric data into fewer data points. So, if you have data from every 1m, you would merge 15m worth of data into one data point. The basic idea is that you don't need a very high resolution on data more than N days old. So you can reduce your long term storage by 15x. Since storage is one of the resources my workplace is low on, the idea took root. A) Is it a good idea? B) Has anyone implemented it? B.2) If you've implemented it, how has it worked out? B.3) If you've implemented it, how did you do so?",
    "website_area": "discuss"
  },
  {
    "id": "64ddb47d-2bc1-4dba-adc9-c5e9faabeb62",
    "url": "https://discuss.elastic.co/t/mysql-module-not-retrieving-data/211859",
    "title": "Mysql Module not retrieving data",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "JValenzani",
    "date": "December 14, 2019, 1:32am December 16, 2019, 1:29pm December 16, 2019, 2:15pm December 16, 2019, 4:12pm December 16, 2019, 4:48pm December 16, 2019, 7:55pm December 16, 2019, 7:55pm January 13, 2020, 8:02pm",
    "body": "I have the exact same problem described in this topic (No data MySQL dashboard - version mismatch?). I'm using metricbeat 7.4.2 (amd64) with CentOS 7 and Percona Server 5.7.22 The topic ends with no resolution. So i'm wondering if there's something special about this config or i hit a bug. Logs doesn't show any errors regarding mysql.",
    "website_area": "discuss"
  },
  {
    "id": "2a28885d-722a-4a98-be39-dba78fba0acf",
    "url": "https://discuss.elastic.co/t/inconsistent-response-code-from-heartbeat-and-curl-command-from-same-machine/211793",
    "title": "Inconsistent response code from heartbeat and curl command from same machine",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "J_Weeda",
    "date": "December 13, 2019, 2:39pm December 16, 2019, 2:02pm December 16, 2019, 3:59pm December 16, 2019, 6:07pm December 16, 2019, 6:26pm December 16, 2019, 6:29pm December 16, 2019, 6:35pm December 16, 2019, 6:46pm December 16, 2019, 7:46pm January 13, 2020, 7:34pm",
    "body": "Hi, I'm facing an issue where heartbeat 7.5 reports a different response code (500) value than cUrl (302) reports on the same target URL I believe the heartbeat can me mimicked through curl -Iv. Below the heartbeat configuration in monitor.d, the corresponding response doc and curl output. What could be causing this? BTW, I have tried both with and without a trailing slash in urls. - type: http name: 'somesite.com' tags: ['customer'] id: 'somesite.com' schedule: '@every 60s' urls: [\"http://somesite.com:80/\"] timeout: 5 check.request.method: HEAD { \"_index\": \"heartbeat-7.5.0-customer-2019.12\", \"_type\": \"_doc\", \"_id\": \"4ThJ_24B-hakpvoIg4XP\", \"_version\": 1, \"_score\": null, \"_size\": 1479, \"_source\": { \"observer\": { \"ip\": \"###########\", \"geo\": { \"name\": \"AWS EU Frankfurt\", \"country_iso_code\": \"DE\", \"city_name\": \"Frankfurt\", \"continent_name\": \"Europe\" }, \"hostname\": \"###########\" }, \"summary\": { \"up\": 0, \"down\": 1 }, \"fields\": { \"customer\": \"customer\", \"logstash_used\": \"true\", \"index\": \"heartbeat-7.5.0-customer-2019.12\", \"logstash_host\": \"ip-172-31-24-135\", \"size\": 1604 }, \"tags\": [ \"customer\", \"beats_input_raw_event\" ], \"host\": { \"name\": \"###########\" }, \"resolve\": { \"ip\": \"###########\", \"rtt\": { \"us\": 321170 } }, \"@version\": \"1\", \"@timestamp\": \"2019-12-13T12:43:47.074Z\", \"url\": { \"scheme\": \"http\", \"full\": \"http://somesite.com:80/\", \"domain\": \"somesite.com\", \"path\": \"/\", \"port\": 80 }, \"error\": { \"type\": \"validate\", \"message\": \"500 Internal Server Error\" }, \"ecs\": { \"version\": \"1.1.0\" }, \"monitor\": { \"type\": \"http\", \"ip\": \"###########\", \"check_group\": \"10eedc6e-1da6-11ea-8856-029326932cd8\", \"name\": \"somesite.com\", \"duration\": { \"us\": 357687 }, \"id\": \"somesite.com\", \"status\": \"down\" }, \"tcp\": { \"rtt\": { \"connect\": { \"us\": 15703 } } }, \"event\": { \"dataset\": \"uptime\" }, \"agent\": { \"type\": \"heartbeat\", \"hostname\": \"###########\", \"id\": \"28dd306a-1424-42ce-a543-d68e85641532\", \"ephemeral_id\": \"cd79aee3-8de9-471d-8616-9cc991a3bbbd\", \"version\": \"7.5.0\" }, \"http\": { \"response\": { \"body\": { \"bytes\": 0, \"hash\": \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\", \"content\": \"\" }, \"status_code\": 500 }, \"rtt\": { \"total\": { \"us\": 36474 }, \"content\": { \"us\": 31 }, \"response_header\": { \"us\": 20692 }, \"write_request\": { \"us\": 25 }, \"validate\": { \"us\": 20723 } } } }, \"fields\": { \"availability\": [ 0 ], \"url.link\": [ \"http://somesite.com\" ], \"@timestamp\": [ \"2019-12-13T12:43:47.074Z\" ], \"url.suffix\": [ \"au\" ] }, \"highlight\": { \"monitor.name\": [ \"@kibana-highlighted-field@somesite.com@/kibana-highlighted-field@\" ] }, \"sort\": [ 1576241027074 ] } curl command: curl -Iv http://somesite.com:80 * Rebuilt URL to: http://somesite.com:80/ * Trying ###########... * TCP_NODELAY set * Connected to somesite.com (###########) port 80 (#0) > GET / HTTP/1.1 > Host: somesite.com > User-Agent: curl/7.58.0 > Accept: */* > < HTTP/1.1 302 Found < Keep-Alive: timeout=5, max=1024 < Content-Type: text/html; charset=UTF-8 < Location: http://somesite.com/studio/agegate?rurl=/ < Server: Apache < Date: Fri, 13 Dec 2019 12:39:21 GMT < Content-Length: 0 < * Connection #0 to host somesite.com left intact",
    "website_area": "discuss"
  },
  {
    "id": "cb997114-6b03-4f9a-add0-f3a085b21976",
    "url": "https://discuss.elastic.co/t/journalbeat-dashboards-fail/211542",
    "title": "Journalbeat Dashboards Fail",
    "category": [
      "Beats"
    ],
    "author": "alphaDev23",
    "date": "December 11, 2019, 11:50pm December 14, 2019, 1:54am December 14, 2019, 4:54am December 16, 2019, 7:17pm January 13, 2020, 9:19pm",
    "body": "Using journalbeat 7.4.2 and ELK 7.4.2. Enabled kibana dashboards in journalbeat.yml: setup.dashboards.enabled: true sudo journalbeat -c /etc/journalbeat/journalbeat.yml setup ILM policy and write alias loading not enabled. Index setup finished. Loading dashboards (Kibana must be running and reachable) Loaded dashboards However, there are no dashboards and even after creating a \"journalbeat-*\" kibana index, the following message is displayed when selecting dashboards: \"In order to visualize and explore data in Kibana, you'll need to create an index pattern to retrieve data from Elasticsearch.\" Why? Did I miss something in getting started? https://www.elastic.co/guide/en/beats/journalbeat/7.4/journalbeat-getting-started.html",
    "website_area": "discuss"
  },
  {
    "id": "eac7ff91-84a4-474b-a346-df3c12dfa4aa",
    "url": "https://discuss.elastic.co/t/getting-logs-from-elasticsearch-that-runs-in-a-container/211818",
    "title": "Getting logs from elasticsearch that runs in a container",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 16, 2019, 1:38pm December 16, 2019, 3:38pm December 16, 2019, 5:49pm January 13, 2020, 5:49pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ea3090e8-e2c4-4ea3-8bf2-9285dc7ec0b5",
    "url": "https://discuss.elastic.co/t/filebeat-nginx-module-does-not-work-with-aws-es/210449",
    "title": "Filebeat nginx module does not work with AWS ES",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "john_eapen",
    "date": "December 4, 2019, 3:12am December 5, 2019, 9:31pm December 6, 2019, 4:23pm December 6, 2019, 4:43pm December 12, 2019, 10:38pm December 16, 2019, 5:26pm January 13, 2020, 5:32pm",
    "body": "Filebeat nginx module does not work with AWS ES due to lack of support for ingest-geoip plugin in AWS ES. Is there any temporary workaround for this issue ? I don't need the geoip information for now. Also any update on https://github.com/elastic/beats/issues/10867 ? Thanks John",
    "website_area": "discuss"
  },
  {
    "id": "eb7a99a9-a494-4af6-b37b-9e3e89dcc9ae",
    "url": "https://discuss.elastic.co/t/configure-heartbeat-to-check-date-in-json-response-body/211958",
    "title": "Configure Heartbeat to check date in JSON response body",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "zuerisee",
    "date": "December 16, 2019, 8:40am December 16, 2019, 12:32pm December 16, 2019, 12:44pm December 16, 2019, 3:53pm January 13, 2020, 3:53pm",
    "body": "Hello everybody, I was wondering if it's possible to use Heartbeat to parse and check a date within a JSON response body against the current date. In this case I'd like to check if date is older than 2 weeks. { \"text\":\"Lorem ipsum dolor\", \"date\":\"2019-12-11T14:02:50\" } The docs don't mention anything like this. I'm using Heartbeat 6.8 with Elasticsearch 6.8. Thank you for your help.",
    "website_area": "discuss"
  },
  {
    "id": "88155d4e-8daf-4f33-9f6f-ec57522591ef",
    "url": "https://discuss.elastic.co/t/how-to-tail-files-once-in-filebeat/212007",
    "title": "How to tail_files once in filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "chenyahui",
    "date": "December 16, 2019, 12:51pm December 16, 2019, 1:35pm January 13, 2020, 1:36pm",
    "body": "How to invoke tail_files just once in filebeat? Whether or not the filebeat restart later. The usage scenario is that I don't want to deal the exist filesand I just want to process new data But if I only use tail_files, it may lost data when the filebeat restart. Is filebeat support this scenario?",
    "website_area": "discuss"
  },
  {
    "id": "e1b02f7f-1205-4c17-9e34-d147900ebdc9",
    "url": "https://discuss.elastic.co/t/collect-logstash-monitoring-data-with-metricbeat-6-2-4/211941",
    "title": "Collect Logstash monitoring data with Metricbeat - 6.2.4",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "SNJY",
    "date": "December 16, 2019, 5:24am December 16, 2019, 12:49pm January 13, 2020, 1:00pm",
    "body": "Hi, I am running Elasticsearch cluster on basic license of ES-6.2.4. To collect logstash monitoring data through metricbeat - 6.2.4 - I enabled metricbeat module of logstash in /etc/metricbeat/modules.d/logstash.yml and restarted metricbeat service. However there is no monitoring data getting indexed. I referred to below document but document is for higher versions, there is no procedure document for ES-6.2.4 -- https://www.elastic.co/guide/en/logstash/current/monitoring-with-metricbeat.html Could someone please help me by sharing steps to enable logstash monitoring. Thanks, SNJY",
    "website_area": "discuss"
  },
  {
    "id": "1e04bc8e-8954-476d-964d-6b619485c0df",
    "url": "https://discuss.elastic.co/t/drop-fields-not-working-on-doc/211545",
    "title": "Drop_fields not working on doc",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "brain",
    "date": "December 12, 2019, 1:05am December 12, 2019, 10:51am December 12, 2019, 11:23am December 12, 2019, 11:23am December 12, 2019, 11:28am December 12, 2019, 11:31am December 12, 2019, 11:46am December 16, 2019, 12:05pm January 13, 2020, 12:05pm",
    "body": "i'm trying to use drop_fields on part of a document: \"signatures\": [ { \"markcount\": 2, \"families\": [], \"description\": \"The binary likely contains encrypted or compressed data indicative of a packer\", \"severity\": 2, \"marks\": [ { \"entropy\": 7.90446336568116, \"section\": { \"size_of_data\": \"0x00083400\", \"virtual_address\": \"0x0036d000\", \"entropy\": 7.90446336568116, \"name\": \"UPX1\", \"virtual_size\": \"0x00084000\" }, \"type\": \"generic\", \"description\": \"A section with a high entropy has been found\" }, { \"entropy\": 0.9980988593155894, \"type\": \"generic\", \"description\": \"Overall entropy of this PE file is high\" } ],' specifically, on signatures.marks.section, however my drop_fields configuration does not work: - drop_fields: fields: [\"signatures.marks\"] i have tried signatures.marks.section, signatures.marks, signatures.marks etc to no avail. any ideas?",
    "website_area": "discuss"
  },
  {
    "id": "ddab1a9a-29bb-4aa8-9d84-dd09b5f5bd20",
    "url": "https://discuss.elastic.co/t/using-filebeat-with-multiple-indices-and-logstash/211490",
    "title": "Using Filebeat with multiple indices and logstash",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "JoeSmith",
    "date": "December 11, 2019, 3:49pm December 12, 2019, 2:45pm December 13, 2019, 5:26pm December 16, 2019, 10:09am January 13, 2020, 10:09am",
    "body": "Hi everyone, i've got a logstash instance that has an beats input on 5044. Im now trying to send filebeat data to that instance (which is working fine on a single index). I'd like to have multiple indices depending on the used module/import type. As an example let's say that I'm using the IIS and RabbitMQ modules. In order not to put too much into a single index I'd like to use a pattern like: module: rabbitmq log: enabled: true var.paths: [\"/.../log/rabbit@*.log*\"] input: ignore_older: 24h clean_removed: true fields: module_type: \"rabbitmq\" The idea was to use the variable module_type in the index field. I was trying to use the logstash output: output.logstash: # Boolean flag to enable or disable the output module. enabled: true # The Logstash hosts hosts: [\"mynode.com:5044\"] # Optional index name. The default index name is set to filebeat # in all lowercase. index: 'filebeat-test2-%{[agent.version]}' The example above shows the use of another variable \"agent.version\" which is taken from the examples of the elastic output and should work fine even if my field has not been set correctly. When sending the files to logstash it will only create the index \"filebeat-test2-%{[agent.version]}-2019.12.11\". It seems as if the logstash output does not resolve the variables. How can i achieve this without changing my logstash config or using two instances of filebeat? Regards, J",
    "website_area": "discuss"
  },
  {
    "id": "8bc23f79-315c-49fd-b13e-8d12f5f4fef3",
    "url": "https://discuss.elastic.co/t/filebeat-missing-rows-in-elastic-index/211942",
    "title": "Filebeat, missing rows in elastic index",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "skowron-line",
    "date": "December 16, 2019, 6:04am January 13, 2020, 6:00am",
    "body": "Hi I have filebeat (7.4) with config as follows filebeat: config: modules: path: /etc/filebeat/modules.d/*.yml reload: enabled: false inputs: - enabled: true paths: - /var/www/ewyszukiwarka/cli/var/log/dev-*.log type: log output: logstash: hosts: - audit.logstash:5044 path: config: /etc/filebeat data: /var/lib/filebeat home: /usr/share/filebeat logs: /var/log/filebeat processors: - add_host_metadata: null - add_cloud_metadata: null setup: kibana: null template: settings: index: number_of_shards: 1 And from time to time, filebeat is not reading whole file, some rows are missing in my elasticsearch, for example I have file in which I have 887 lines, and I elasticsearch index there is only 878 documents. All log lines are the same, so its not mapping problem",
    "website_area": "discuss"
  },
  {
    "id": "abde2dcc-19d0-4fb3-b6be-321258908f41",
    "url": "https://discuss.elastic.co/t/filebeat-monitoring-when-it-is-down/211870",
    "title": "Filebeat monitoring when it is down",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Sajal",
    "date": "December 14, 2019, 12:35pm January 11, 2020, 12:35pm",
    "body": "Hi All, How we can monitor if filebeat is down but server is up where it is installed ? Thanks Sajal",
    "website_area": "discuss"
  },
  {
    "id": "159f1bac-1f76-4ae9-9eab-2e98dafe2e1a",
    "url": "https://discuss.elastic.co/t/filebeat-error-watching-for-docker-events-context-deadline-exceeded/211113",
    "title": "Filebeat: Error watching for docker events: context deadline exceeded",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 9, 2019, 1:09pm December 9, 2019, 3:09pm December 9, 2019, 4:17pm December 10, 2019, 8:44am December 13, 2019, 3:08pm January 10, 2020, 3:08pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "df0d7b8f-b953-4a06-bcb3-114ec1e297e6",
    "url": "https://discuss.elastic.co/t/different-registry-for-different-inputs/210541",
    "title": "Different registry for different inputs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "mtudisco",
    "date": "December 4, 2019, 1:37pm December 13, 2019, 2:22pm January 10, 2020, 2:22pm",
    "body": "Is it possible in filebeat to have different registry fiiles for different inputs? It would be very usefull as when you add a new input and for instnace you decide to change something, deleting that registry will only cause to reload the files for that input and not all the files from all inputs. thanks",
    "website_area": "discuss"
  },
  {
    "id": "6cdce9ca-6f0e-437e-81a4-b58984e37b2d",
    "url": "https://discuss.elastic.co/t/how-does-filebeat-redis-module-work-for-slow-logs/211028",
    "title": "How does Filebeat Redis module work for Slow Logs?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "dandago",
    "date": "December 8, 2019, 9:21pm December 9, 2019, 9:34am December 12, 2019, 11:36pm December 13, 2019, 9:13am December 13, 2019, 9:38am December 13, 2019, 10:04am December 13, 2019, 10:16am December 13, 2019, 1:04pm December 13, 2019, 1:23pm December 13, 2019, 1:25pm December 13, 2019, 1:48pm January 10, 2020, 1:48pm",
    "body": "The configuration file of the Redis module for Filebeat seems to support normal logs (from the Redis server's log file) and slowlogs (from the API)... see: github.com elastic/beats/blob/master/filebeat/modules.d/redis.yml.disabled # Module: redis # Docs: https://www.elastic.co/guide/en/beats/filebeat/master/filebeat-module-redis.html - module: redis # Main logs log: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. #var.paths: [\"/var/log/redis/redis-server.log*\"] # Slow logs, retrieved via the Redis API (SLOWLOG) slowlog: enabled: true # The Redis hosts to connect to. #var.hosts: [\"localhost:6379\"] # Optional, the password to use when connecting to Redis. This file has been truncated. show original I can understand how the logs are picked up from the log files, but I had no idea that Filebeat had the capability to issue a command to a server and thus retrieve information. Is there any documentation on how this feature works, and in what format the slowlogs are shipped?",
    "website_area": "discuss"
  },
  {
    "id": "11ded960-ce7a-4433-8ea8-8e8a917db8ea",
    "url": "https://discuss.elastic.co/t/metricbeat-decode-json-fields-not-working-for-me/211632",
    "title": "Metricbeat decode_json_fields not working for me",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Ivan_Vazquez",
    "date": "December 12, 2019, 11:27am December 12, 2019, 2:03pm December 12, 2019, 4:37pm December 13, 2019, 1:41pm January 10, 2020, 8:47am",
    "body": "Hi community, I am new using all ELK stack and I'm recollecting data from an API with metricbeat with the http module, receiving a similar response: { \"@timestamp\": \"2019-12-12T10:56:42.880Z\", \"event\": { \"dataset\": \"http.TEST\", \"duration\": 237195965, \"module\": \"http\" }, \"http\": { \"TEST\": { \"data\": [ { \"52_week_high\": \"385.99\", \"52_week_low\": \"231.23\", \"change_pct\": \"1.98\", } ], \"symbols_requested\": 1, } }, \"metricset\": { \"name\": \"json\", \"period\": 120000 }, \"service\": { \"address\": \"apiurl.com\", \"type\": \"http\" } } My problem is that I need on the output the items in \"data\" field at the same level of \"symbols_requested\" and for this I'm using the process decode_json_fields: - decode_json_fields: fields: [\"data\"] process_array: false max_depth: 5 target: \"\" overwrite_keys: true add_error_key: false But the data is not processed and I can't work in Kibana creating Dashboards with the data on the json I'm using the processor correctly? Thanks for helping me Regards, Ivan",
    "website_area": "discuss"
  },
  {
    "id": "add8a002-4bf6-468b-86e0-8d6b3ea7cf00",
    "url": "https://discuss.elastic.co/t/filebeat-cutting-events-from-tiem-to-time/211795",
    "title": "Filebeat cutting events from tiem to time",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "mtudisco",
    "date": "December 13, 2019, 1:11pm January 10, 2020, 1:11pm",
    "body": "Hi, I have filebeat 7.3.0-1, that reads files from one directory and sends information to logstash, it process it and writes to elasticsearch. I have a monitoring daemon that from time to time, gets a report from a database and writes a file on the directory logstash reads, the report might have several lines, in my case for the moment just one line, and writes line by line to the file. /home | 25358| 30620|*ERROR* every field is separated by a \"|\" and there might be several spaces before or after ther pipe and the value. Filebeat reads line by line, filebeat.yml config is: - type: log enabled: true paths: - /stage/logs/*chk_df_autoextend.txt fields: type: oracle fields_under_root: true harvester_buffer_size: 2097153 close_inactive: 5m close_removed: true close_eof: true then with logstash i process the information with the following filter: filter { if [type] == \"oracle\" and [type2] == \"df_autoextend\" { #mount_point|free_mb|incremento|status dissect { mapping => { \"message\" => \"%{[df_autoextend][mount_point]->}|%{[df_autoextend][free_mb]}|%{[df_autoextend][incremento]}|%{[df_autoextend][status]->}\" } } mutate { convert => { \"[df_autoextend][mount_point]\" => \"string\" \"[df_autoextend][free_mb]\" => \"integer\" \"[df_autoextend][incremento]\" => \"integer\" \"[df_autoextend][status]\" => \"string\" } } } } Once in a while (once a day or even once with more than one day between) i get the message cut by logstash, for instance: { \"_index\": \"orachecks-2019.12\", \"_type\": \"_doc\", \"_id\": \"CSfE-W4BByjEVZh5ZWFm\", \"_version\": 1, \"_score\": null, \"_source\": { \"@timestamp\": \"2019-12-12T11:00:17.000Z\", \"type\": \"oracle\", \"ecs\": { \"version\": \"1.0.1\" }, \"agent\": { \"ephemeral_id\": \"36696bd3-a1c0-4146-8cd2-8c0e5810a5cb\", \"version\": \"7.3.0\", \"hostname\": \"elk-lab01.tilsor.com.uy\", \"id\": \"7502e50a-1256-42de-b007-d0c5336841ce\", \"type\": \"filebeat\" }, \"host\": { \"name\": \"elk-lab01.tilsor.com.uy\" }, \"log\": { \"offset\": 59, \"file\": { \"path\": \"/stage/logs/2019-12-12-080017-chk_df_autoextend.txt\" } }, \"type2\": \"df_autoextend\", \"tags\": [], \"df_autoextend\": { \"incremento\": 30620, \"mount_point\": \" \", \"status\": \"*ERROR* \", \"free_mb\": 25358 }, \"input\": { \"type\": \"log\" }, \"@version\": \"1\" }, \"fields\": { \"@timestamp\": [ \"2019-12-12T11:00:17.000Z\" ], \"tbs_space.tbs_used_space\": [ null ], \"tbs_space.tbs_status\": [ null ], \"detail_diskgroups.dg_used_mb\": [ null ] }, \"highlight\": { \"type2.keyword\": [ \"@kibana-highlighted-field@df_autoextend@/kibana-highlighted-field@\" ] }, \"sort\": [ 1576148417000 ] } The problem is why do i get a message that start at offset 59 when it should read the whole line and send it to logstash??. Its causing logstash to generate incomplete information in elasticsearch. I could add some filter and discard the malformed events, but its not malformed in the original file. Any clue?",
    "website_area": "discuss"
  },
  {
    "id": "a6193bf8-f24b-4845-a64b-b4f8608551f2",
    "url": "https://discuss.elastic.co/t/help-metricbeat-to-elasticsearch-using-ipv6/211373",
    "title": "Help: Metricbeat to elasticsearch using ipv6",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "briank5400",
    "date": "December 10, 2019, 8:27pm December 11, 2019, 9:08am December 11, 2019, 10:36am December 11, 2019, 3:56pm December 13, 2019, 10:56am December 13, 2019, 1:06pm January 10, 2020, 1:07pm",
    "body": "I have elasticsearch/Kibana setup on another host and configured basic authentication. now I am trying to configure metricbeat to send some data (over ipv6) but I cant seem to figure out why its failing to authenticate when I try to run metricbeat setup. (ive tried both with and without using keystore). It works fine if I do a test curl -6 -u. e.g. root@10-245-76-9:/etc/metricbeat# curl -6 http://[2a00:47c0:511:1297:7aae:5e12:607f:7372]:9200 -u bk-test:Testing1234 { \"name\" : \"10-245-76-12\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"NOvxDXzaSVaOJynxfr8a6A\", \"version\" : { \"number\" : \"7.5.0\", \"build_flavor\" : \"default\", \"build_type\" : \"deb\", \"build_hash\" : \"e9ccaed468e2fac2275a3761849cbee64b39519f\", \"build_date\" : \"2019-11-26T01:06:52.518245Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.3.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" root@10-245-76-9:/etc/metricbeat# metricbeat setup Exiting: Couldn't connect to any of the configured Elasticsearch hosts. Errors: [Error connection to Elasticsearch http://[2a00:47c0:511:1297:7aae:5e12:607f:7372]:9200: 401 Unauthorized: {\"error\":{\"root_cause\":[{\"type\":\"security_exception\",\"reason\":\"missing authentication credentials for REST request [/]\",\"header\":{\"WWW-Authenticate\":\"Basic realm=\"security\" charset=\"UTF-8\"\"}}],\"type\":\"security_exception\",\"reason\":\"missing authentication credentials for REST request [/]\",\"header\":{\"WWW-Authenticate\":\"Basic realm=\"security\" charset=\"UTF-8\"\"}},\"status\":401}] root@10-245-76-9:/etc/metricbeat# metricbeat test output elasticsearch: http://[2a00:47c0:511:1297:7aae:5e12:607f:7372]:9200... parse url... OK connection... parse host... ERROR address 2a00:47c0:511:1297:7aae:5e12:607f:7372:9200: too many colons in address my metricbeat.yml has the following elements: #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: Array of hosts to connect to. hosts: [\"http://[2a00:47c0:511:1297:7aae:5e12:607f:7372]:9200\"] Optional protocol and basic auth credentials. #protocol: \"https\" username: \"bk-test\" password: \"${bk-test}\" any advise?",
    "website_area": "discuss"
  },
  {
    "id": "adf8f478-f5de-436a-b5d9-5f262d55f8d0",
    "url": "https://discuss.elastic.co/t/filebeat-slow-to-send-logs-to-logstash/211765",
    "title": "Filebeat slow to send logs to logstash",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "yberrada",
    "date": "December 13, 2019, 8:40am December 13, 2019, 8:40am December 13, 2019, 11:24am January 10, 2020, 10:00am",
    "body": "Hello all, Logstash & Filebeat version: 5.5.1 I am new to the ELK stack. I have set up a elasticsearch cluster on k8s. In the begining filebeat was very slow to send logs to logstash ( took hours ) and the logs of filebeat were: 2019/12/13 10:15:07.798513 sync.go:85: ERR Failed to publish events caused by: read tcp 10.1.94.36:41834->10.0.59.193:5044: i/o timeout 2019/12/13 10:15:07.798527 single.go:91: INFO Error publishing events (retrying): read tcp 10.1.94.36:41834->10.0.59.193:5044: i/o timeout After some research, I applied the following modifications to filebeat and logstash configuration: logstash config: congestion_threshold => 180 client_inactivity_timeout => 1200 Filebeat config : scan_frequency: 30s bulk_max_size: 2048 After these changes, more events were able to be sent as the window size grew up to 400 sometimes ( it nevers goes beyond 400 even if the limit is 2048). Before these changes, the window size never grew more than 10. Also, I noted that filebeat is constantly sending new events ( even if not many logs are generated).is this a normal behavior? Below are the current configs and logs of filebeat and logstash. **Filebeat config:** apiVersion: v1 data: filebeat.yml: |- filebeat.prospectors: - input_type: log paths: - /var/log/containers/*.log scan_frequency: 30s symlinks: true json.message_key: log json.keys_under_root: true json.add_error_key: true fields_under_root: true fields: type: kube-logs node.hostname: ${NODE_HOSTNAME} pod.ip: ${POD_IP} tags: - k8s-app filebeat.config.modules: # Set to true to enable config reloading reload.enabled: true output.logstash: hosts: logstash:5044 timeout: 15 bulk_max_size: 2048 ssl.certificate_authorities: [\"/usr/share/elasticsearch/config/tls/ca.crt\"] ssl.certificate: \"/usr/share/elasticsearch/config/tls/filebeat.crt\" ssl.key: \"/usr/share/elasticsearch/config/tls/filebeat.key\" ssl.key_passphrase: ${APP_KEYSTORE_PASSWORD} logging.level: debug kind: ConfigMap metadata: creationTimestamp: 2019-12-12T13:06:29Z labels: app: logging-elk-filebeat-ds chart: ibm-icplogging-2.2.0 component: filebeat-ds heritage: Tiller release: logging name: logging-elk-filebeat-ds-config namespace: kube-system resourceVersion: \"91594477\" selfLink: /api/v1/namespaces/kube-system/configmaps/logging-elk-filebeat-ds-config uid: 35ed571f-1ce0-11ea-b481-506b8dfb518b Filebeat logs: 2019/12/13 10:26:17.025879 output.go:109: DBG output worker: publish 2048 events 2019/12/13 10:26:17.025891 sync.go:107: DBG Try to publish 2048 events to logstash with window size 120 2019/12/13 10:26:19.057002 metrics.go:39: INFO Non-zero metrics in the last 30s: libbeat.logstash.call_count.PublishEvents=2 libbeat.logstash.publish.read_bytes=1871 libbeat.logstash.publish.write_bytes=27259 libbeat.logstash.published_and_acked_events=247 libbeat.publisher.published_events=2048 publish.events=2048 registrar.states.current=1 registrar.states.update=2048 registrar.writes=1 2019/12/13 10:26:23.399544 prospector.go:183: DBG Run prospector 2019/12/13 10:26:23.399579 prospector_log.go:70: DBG Start next scan 2019/12/13 10:26:23.400226 prospector_log.go:226: DBG Check file for harvesting: /var/log/containers/nvidia-device-plugin-bc5x8_kube-system_nvidia-device-plugin-94ad7e80e7f62375df450bfb950f1bc2845dd63851af8e87dee32fc68229b3f6.log 2019/12/13 10:26:23.400256 prospector_log.go:259: DBG Update existing file for harvesting: /var/log/containers/nvidia-device-plugin-bc5x8_kube-system_nvidia-device-plugin-94ad7e80e7f62375df450bfb950f1bc2845dd63851af8e87dee32fc68229b3f6.log, offset: 140 2019/12/13 10:26:23.400263 prospector_log.go:313: DBG File didn't change: /var/log/containers/nvidia-device-plugin-bc5x8_kube-system_nvidia-device-plugin-94ad7e80e7f62375df450bfb950f1bc2845dd63851af8e87dee32fc68229b3f6.log 2019/12/13 10:26:23.400271 prospector_log.go:226: DBG Check file for harvesting: /var/log/containers/monitoring-prometheus-nodeexporter-t5bsq_kube-system_nodeexporter-f14734f3c57c2b266eeb058b4e32dbc78fcfaa63ad04e9deb20634aa60e66e84.log 2019/12/13 10:26:23.400279 prospector_log.go:259: DBG Update existing file for harvesting: /var/log/containers/monitoring-prometheus-nodeexporter-t5bsq_kube-system_nodeexporter-f14734f3c57c2b266eeb058b4e32dbc78fcfaa63ad04e9deb20634aa60e66e84.log, offset: 5605 2019/12/13 10:26:23.400284 prospector_log.go:313: DBG File didn't change: /var/log/containers/monitoring-prometheus-nodeexporter-t5bsq_kube-system_nodeexporter-f14734f3c57c2b266eeb058b4e32dbc78fcfaa63ad04e9deb20634aa60e66e84.log 2019/12/13 10:26:23.400291 prospector_log.go:226: DBG Check file for harvesting: /var/log/containers/calico-node-tgxkl_kube-system_install-cni-e65fb542a64c3fe9b3d1de82fbe5e5b79c1cc0cb23e1911156a28855a0f8952f.log 2019/12/13 10:26:23.400297 prospector_log.go:259: DBG Update existing file for harvesting: /var/log/containers/calico-node-tgxkl_kube-system_install-cni-e65fb542a64c3fe9b3d1de82fbe5e5b79c1cc0cb23e1911156a28855a0f8952f.log, offset: 3708 2019/12/13 10:26:23.400302 prospector_log.go:313: DBG File didn't change: /var/log/containers/calico-node-tgxkl_kube-system_install-cni-e65fb542a64c3fe9b3d1de82fbe5e5b79c1cc0cb23e1911156a28855a0f8952f.log 2019/12/13 10:26:23.400308 prospector_log.go:226: DBG Check file for harvesting: /var/log/containers/logging-elk-filebeat-ds-9c6bd_kube-system_filebeat-8e76c31819d6bc2d43e72cc81387294a1b566b3518c6694d542db9e0ed5f2798.log 2019/12/13 10:26:23.400320 prospector_log.go:245: DBG Start harvester for new file: /var/log/containers/logging-elk-filebeat-ds-9c6bd_kube-system_filebeat-8e76c31819d6bc2d43e72cc81387294a1b566b3518c6694d542db9e0ed5f2798.log 2019/12/13 10:26:26.650513 sync.go:78: DBG 120 events out of 2048 events sent to logstash. Continue sending 2019/12/13 10:26:26.650545 sync.go:107: DBG Try to publish 1928 events to logstash with window size 120 2019/12/13 10:26:31.125702 sync.go:78: DBG 120 events out of 1928 events sent to logstash. Continue sending 2019/12/13 10:26:31.125744 sync.go:107: DBG Try to publish 1808 events to logstash with window size 120 2019/12/13 10:26:36.691001 sync.go:78: DBG 120 events out of 1808 events sent to logstash. Continue sending 2019/12/13 10:26:36.691041 sync.go:107: DBG Try to publish 1688 events to logstash with window size 120 2019/12/13 10:26:48.675369 sync.go:78: DBG 120 events out of 1688 events sent to logstash. Continue sending 2019/12/13 10:26:48.675407 sync.go:107: DBG Try to publish 1568 events to logstash with window size 120 2019/12/13 10:26:49.056948 metrics.go:39: INFO Non-zero metrics in the last 30s: libbeat.logstash.publish.read_bytes=140 libbeat.logstash.publish.write_bytes=33934 2019/12/13 10:26:55.722936 sync.go:78: DBG 120 events out of 1568 events sent to logstash. Continue sending 2019/12/13 10:26:55.722971 sync.go:107: DBG Try to publish 1448 events to logstash with window size 120 2019/12/13 10:27:00.115661 sync.go:78: DBG 120 events out of 1448 events sent to logstash. Continue sending 2019/12/13 10:27:00.115704 sync.go:107: DBG Try to publish 1328 events to logstash with window size 120 2019/12/13 10:27:13.896299 sync.go:78: DBG 120 events out of 1328 events sent to logstash. Continue sending 2019/12/13 10:27:13.896345 sync.go:107: DBG Try to publish 1208 events to logstash with window size 120 2019/12/13 10:27:19.056918 metrics.go:39: INFO Non-zero metrics in the last 30s: libbeat.logstash.publish.read_bytes=105 libbeat.logstash.publish.write_bytes=24952 2019/12/13 10:27:24.090525 sync.go:78: DBG 120 events out of 1208 events sent to logstash. Continue sending 2019/12/13 10:27:24.090558 sync.go:107: DBG Try to publish 1088 events to logstash with window size 120 2019/12/13 10:27:30.011920 sync.go:78: DBG 120 events out of 1088 events sent to logstash. Continue sending 2019/12/13 10:27:30.011959 sync.go:107: DBG Try to publish 968 events to logstash with window size 120 2019/12/13 10:27:39.834750 sync.go:78: DBG 120 events out of 968 events sent to logstash. Continue sending 2019/12/13 10:27:39.834781 sync.go:107: DBG Try to publish 848 events to logstash with window size 120 2019/12/13 10:27:49.060311 metrics.go:39: INFO Non-zero metrics in the last 30s: libbeat.logstash.publish.read_bytes=105 libbeat.logstash.publish.write_bytes=24718 ^C2019/12/13 10:27:54.838827 client.go:194: DBG handle error: read tcp 10.1.94.36:42626->10.0.59.193:5044: i/o timeout 2019/12/13 10:27:54.838872 sync.go:78: DBG 0 events out of 848 events sent to logstash. Continue sending 2019/12/13 10:27:54.838882 sync.go:58: DBG close connection 2019/12/13 10:27:54.838889 client.go:110: DBG closing 2019/12/13 10:27:54.839175 sync.go:85: ERR Failed to publish events caused by: read tcp 10.1.94.36:42626->10.0.59.193:5044: i/o timeout 2019/12/13 10:27:54.839203 single.go:91: INFO Error publishing events (retrying): read tcp 10.1.94.36:42626->10.0.59.193:5044: i/o timeout 2019/12/13 10:27:54.839211 sync.go:58: DBG close connection 2019/12/13 10:27:54.839230 single.go:156: DBG send fail 2019/12/13 10:27:54.839237 single.go:160: DBG reset fails 2019/12/13 10:27:55.839410 sync.go:53: DBG connect 2019/12/13 10:27:55.859760 sync.go:107: DBG Try to publish 848 events to logstash with window size 60 2019/12/13 10:27:56.677498 window.go:51: DBG update current window size: 60 2019/12/13 10:27:56.677542 sync.go:78: DBG 60 events out of 848 events sent to logstash. Continue sending 2019/12/13 10:27:56.677556 sync.go:107: DBG Try to publish 788 events to logstash with window size 90",
    "website_area": "discuss"
  },
  {
    "id": "83c32ac4-f836-4103-b8d5-9ccef7dde2c1",
    "url": "https://discuss.elastic.co/t/cannot-reload-enabled-configuration/211763",
    "title": "Cannot reload \"enabled\" configuration",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "sewenew",
    "date": "December 13, 2019, 8:27am December 13, 2019, 10:49am December 13, 2019, 10:49am January 10, 2020, 10:49am",
    "body": "I want to reload input configuration with external configuration file. However, it seems that the enabled option cannot be reloaded. Is there any workaround? # filebeat.yml filebeat.config.inputs: enabled: true path: inputs/*.yml reload.enabled: true reload.period: 3s # other configurations # inputs/input.yml - type: log enabled: true # After change this option to false, filebeat still consumes new messges from this input. ignore_older: 10m paths: - /tmp/error/*.log # This option can be reloaded successfully - type: log enabled: true ignore_older: 10m paths: - /tmp/info/*.log The paths option can be reloaded successfully, however, the enabled option of input.yml file cannot be reloaded. After changing it to false, filebeat still consumes messages from the related input. Is there any way to dynamically disable an input? Regards",
    "website_area": "discuss"
  },
  {
    "id": "f7c7fbe0-6fee-48a3-9d02-6da4bbc7ebeb",
    "url": "https://discuss.elastic.co/t/reason-is-not-an-ip-string-literal/211750",
    "title": "\"reason\":\"'?' is not an IP string literal",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "December 13, 2019, 6:58am December 13, 2019, 1:11pm December 13, 2019, 10:14am January 10, 2020, 10:14am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "530f1b26-bbba-41d0-821c-366c9e278e18",
    "url": "https://discuss.elastic.co/t/help-for-space-separate-message/211672",
    "title": "Help for space separate message",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Patricio_Campos",
    "date": "December 12, 2019, 2:53pm December 13, 2019, 10:01am January 10, 2020, 10:01am",
    "body": "Hello, am new in Elastic, am working with SIEM module. I receive a log from Fortinet Firewall through a Rsyslog Linux and filebeat receive and send to Elasticsearcg, all importants field come in one large field named Message. I would like separate this information for example src_ip, src_port, etc etc date=2019-12-12 time=11:31:33 devname=\"Fwr1-Kaufmann\" devid=\"FGT6HD3916801908\" logid=\"0000000013\" type=\"traffic\" subtype=\"forward\" level=\"notice\" vd=\"root\" eventtime=1576161093 srcip=10.1.1.9 srcport=54091 srcintf=\"LACP_Interna\" srcintfrole=\"undefined\" dstip=208.67.222.222 dstport=53 dstintf=\"port10\" dstintfrole=\"undefined\" poluuid=\"0f497e7c-5f9a-51e8-2401-1473957296a8\" sessionid=796002608 proto=17 action=\"accept\" policyid=402 policytype=\"policy\" service=\"DNS\" dstcountry=\"United States\" srccountry=\"Reserved\" trandisp=\"snat\" transip=190.216.145.133 transport=54091 duration=60 sentbyte=72 rcvdbyte=88 sentpkt=1 rcvdpkt=1 appcat=\"unscanned\"",
    "website_area": "discuss"
  },
  {
    "id": "1be9795d-fec6-43d3-b24c-0cf3ac27fb89",
    "url": "https://discuss.elastic.co/t/filebeat-with-iis-logs/211756",
    "title": "Filebeat with IIS logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Aleix_Abrie_Prat",
    "date": "December 13, 2019, 8:02am December 13, 2019, 9:50am January 10, 2020, 9:50am",
    "body": "Hi everyone, I have a doubt with the module for IIS logs. I configured the output of filebeat to connect directly with the elasticsearch and then I've have done the command \".\\filebeat.exe setup to make the index in elasticsearch and the dashboards in kibana. But i have a problem... With the index created automatically, the index doesn't have a field for the IP that comes from \"X Forwarded for\". Now, my question is: Can i update the pipeline that parses the IIS logs to add the field for this IP? Thanks for advance",
    "website_area": "discuss"
  },
  {
    "id": "35e6b324-758f-48d0-8376-1f5ad915fd4d",
    "url": "https://discuss.elastic.co/t/server-health-status/211445",
    "title": "Server health status",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "aparnababu",
    "date": "December 11, 2019, 9:39am December 11, 2019, 11:07am December 11, 2019, 11:24am December 11, 2019, 12:43pm December 11, 2019, 1:38pm December 11, 2019, 1:39pm December 13, 2019, 7:09am December 13, 2019, 7:10am December 13, 2019, 8:27am January 10, 2020, 8:27am",
    "body": "how to find the unique count of good and bad health server status based on the system.cpu.user.pct and system.memory.actual.used.pct. I tried using the filter option but am getting the wrong count of server health status.",
    "website_area": "discuss"
  },
  {
    "id": "bfd64e63-2a16-46c4-890b-75335ae158f5",
    "url": "https://discuss.elastic.co/t/metricbeat-and-advanced-prometheus-queries/205412",
    "title": "Metricbeat and advanced Prometheus queries",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "hilt86",
    "date": "October 28, 2019, 6:51am October 29, 2019, 10:40am November 8, 2019, 8:00am November 20, 2019, 10:35am November 27, 2019, 9:02am December 2, 2019, 9:31pm December 13, 2019, 4:54am January 10, 2020, 4:54am",
    "body": "I'm trying to use the metricbeat prometheus module to run a query like : query: 'match[]' : 'sum(increase(unix_bytes[5m])) by (process)' however I get an error \"Unable to decode response from prometheus endpoint\". Am I kidding myself that I'm gonna be able to execute this type of query? Secondly in the prometheus module does the period: 300s directive tell metricbeat to query the last 5m or do I need to do that in the query? The module is pretty sparsely documented!",
    "website_area": "discuss"
  },
  {
    "id": "27ff1904-5974-4f07-9bbb-9d6d56937ef1",
    "url": "https://discuss.elastic.co/t/uptime-monitoring-of-ip-using-heart-beat/211225",
    "title": "Uptime monitoring of IP using heart beat",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "",
    "date": "December 10, 2019, 6:10am December 13, 2019, 4:29am December 13, 2019, 4:29am January 10, 2020, 4:29am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "718b1669-7aa6-4542-bfeb-8039b98b64a9",
    "url": "https://discuss.elastic.co/t/possible-regression-in-heartbeat-7-5-0/210423",
    "title": "Possible regression in Heartbeat 7.5.0",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "",
    "date": "December 3, 2019, 7:49pm December 5, 2019, 4:19pm December 5, 2019, 6:59pm December 12, 2019, 9:33pm January 9, 2020, 9:33pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "369db704-473a-45f5-a54d-eb795b46f604",
    "url": "https://discuss.elastic.co/t/exiting-1-error-error-making-http-request/211669",
    "title": "Exiting: 1 error: error making http request:",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "probson",
    "date": "December 12, 2019, 2:30pm December 16, 2019, 8:22am January 9, 2020, 8:11pm",
    "body": "I am setting up metricbeat on a node with Kibana and Elasticsearch (coordinator role) I have setup the kibana portion and it injected the dashboards but at the end of the run it still fails with Exiting: 1 error: error making http request: Get http://localhost:5601/api/status: dial tcp 127.0.0.1:5601: connect: connection refused ive setup the metricbeat.yml as (ive changed a few values) setup.kibana: Kibana Host host: \"es-kibana-node2.domain.domain.co.uk\" protocol: \"https\" ssl.enabled: true ssl.certificate_authorities: [\"/etc/kibana/certs/ca.crt\"] ssl.key: /etc/kibana/certs/es-kibana-node2.key ssl.certificate: /etc/kibana/certs/es-kibana-node2.crt Despite this, it still keeps trying http://127.0.0.1:5601 Any suggestions as to why it is ignoring the host: or if i'm missing a setting? from kibana.yml server.name: \"es-kibana-node2\" server.host: \"es-kibana-node2.domain.domain.co.uk\" Thanks Phil",
    "website_area": "discuss"
  },
  {
    "id": "4ea09b8b-f793-4c10-a041-49ee64c121e7",
    "url": "https://discuss.elastic.co/t/metricbeat-active-directory-ad-performance-metrics-perfmon-yaml-build/209687",
    "title": "Metricbeat Active Directory (AD) performance metrics (perfmon) yaml build",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Ryan_Downey",
    "date": "November 27, 2019, 1:21pm November 27, 2019, 4:41pm November 27, 2019, 4:44pm November 28, 2019, 9:54am December 2, 2019, 1:38pm December 12, 2019, 5:47pm December 3, 2019, 2:27pm December 6, 2019, 6:59pm December 12, 2019, 5:47pm December 12, 2019, 5:45pm January 9, 2020, 5:45pm",
    "body": "I'm looking for a little guidance on how to build out the windows.yml module in Metricbeat. If this is being built out correctly we may need 3-4 lines to ship the Active Directroy perfmon out. The issue that I'm having is that I'm not able to find a lot of specified info on AD perfmon so if anyone has any links that I could be missing that would be appreciated. From my research it seems like these perfmon counters are also under NTDS so I'm not sure how this fits into the syntax as well. I should be able to replicate what the memory section but that might not necessarily be the case. Overall were trying to replicate some Splunk dashboards in Kibana but there isnt a specific app like there is in Splunk. # ########## Memory - instance_label: memory.name instance_name: memory.page.total.reads_sec measurement_label: memory.page.total.reads_sec query: '\\Memory\\Page Reads/sec' # ########## Active Directory - instance_label: ntds.name instance_name: ntds.ds.directory.reads_sec measurement_label: ? query: '\\DS Directory Reads/sec'",
    "website_area": "discuss"
  },
  {
    "id": "fbb3b018-b8d0-4824-bf31-c90173fc023d",
    "url": "https://discuss.elastic.co/t/auditbeat-wont-start-on-deb-8-11/210712",
    "title": "Auditbeat won't start on deb 8_11",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "ethrbunny",
    "date": "December 5, 2019, 12:45pm December 12, 2019, 3:48pm January 2, 2020, 9:59am",
    "body": "Attempts to start auditbeat on this system give these errors: instance/beat.go:916 Exiting: 1 error: 1 error: system/socket dataset setup failed: unable to guess one or more required parameters: guess_sk_buff_proto failed: timeout while waiting for trigger to complete Suggestions on how to get around this? # apt list --installed | grep beat auditbeat/stable,now 7.5.0 amd64 [installed]",
    "website_area": "discuss"
  },
  {
    "id": "a1e82310-18ed-40f8-bf51-73bf24732265",
    "url": "https://discuss.elastic.co/t/metricbeat-failed-to-connect-eof/210939",
    "title": "Metricbeat Failed to connect EOF",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "elkuser2",
    "date": "December 6, 2019, 9:00pm December 9, 2019, 10:37am December 9, 2019, 2:44pm December 9, 2019, 3:03pm December 9, 2019, 3:52pm December 10, 2019, 8:55am December 10, 2019, 8:44pm December 11, 2019, 8:55am December 12, 2019, 2:52pm January 9, 2020, 2:51pm",
    "body": "I am trying to run metricbeat, but no data is showing up in the GUI and I keep getting the following error in the metricbeat logs: 2019-12-06T20:48:49.394Z ERROR pipeline/output.go:100 Failed to connect to backoff(async(tcp://elkurl.com:5040)): EOF 2019-12-06T20:48:49.394Z INFO pipeline/output.go:93 Attempting to reconnect to backoff(async(tcp://elkurl.com:5040)) with 247 reconnect attempt(s) 2019-12-06T20:48:49.394Z INFO [publish] pipeline/retry.go:189 retryer: send unwait-signal to consumer 2019-12-06T20:48:49.394Z INFO [publish] pipeline/retry.go:191 done 2019-12-06T20:48:49.394Z INFO [publish] pipeline/retry.go:166 retryer: send wait signal to consumer 2019-12-06T20:48:49.395Z INFO [publish] pipeline/retry.go:168 done 2019-12-06T20:49:02.718Z INFO [monitoring] log/log.go:144 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":1640,\"time\":{\"ms\":1}},\"total\":{\"ticks\":7500,\"time\":{\"ms\":5},\"value\":7500},\"user\":{\"ticks\":5860,\"time\":{\"ms\":4}}},\"handles\":{\"limit\":{\"hard\":4096,\"soft\":1024},\"open\":4},\"info\":{\"ephemeral_id\":\"b756f851-24ef-4295-8a36-bdb8ae710fc8\",\"uptime\":{\"ms\":10860061}},\"memstats\":{\"gc_next\":47922304,\"memory_alloc\":24504472,\"memory_total\":459542688}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"output\":{\"read\":{\"errors\":1},\"write\":{\"bytes\":170}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":4117,\"retry\":181}}},\"system\":{\"load\":{\"1\":0,\"15\":0,\"5\":0,\"norm\":{\"1\":0,\"15\":0,\"5\":0}}}}}} Is there somewhere I should check the timeout time? The error occurs every minute or so. Please let me know if there is a way to resolve this. Thank you in advance.",
    "website_area": "discuss"
  },
  {
    "id": "60850674-a9d1-4c31-91aa-3d784266760e",
    "url": "https://discuss.elastic.co/t/yaml-line-159-did-not-find-expected-key/211516",
    "title": "Yaml: line 159: did not find expected key",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "uek1967",
    "date": "December 11, 2019, 7:24pm December 12, 2019, 10:31am December 12, 2019, 10:57am December 12, 2019, 11:32am January 9, 2020, 11:32am",
    "body": "Hi folks, I need you help solving the problem with filebeat as described above: ./filebeat test config -e gives out the following error: Exiting: error loading config file: yaml: line 159: did not find expected key What key is meant and where is it to find? Here is the filebeat.yml file: https://pastebin.com/cVjhHn7T Many thanks in advance, Uli",
    "website_area": "discuss"
  },
  {
    "id": "8deac0ae-d6a7-4d1f-9ddf-7c32f88a6d4d",
    "url": "https://discuss.elastic.co/t/configuring-filebeat-to-use-x-pack-security-in-basic-version/211626",
    "title": "Configuring Filebeat to use X-Pack security in Basic version",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "JY_DT",
    "date": "December 12, 2019, 10:57am January 9, 2020, 10:57am",
    "body": "Hi, I'm trying to configure Filebeat to use X-Pack security in the Basic (free) version of the Elasticstack. I'm following what's stated here: https://www.elastic.co/guide/en/beats/filebeat/current/securing-beats.html and https://www.elastic.co/guide/en/beats/filebeat/current/feature-roles.html My question is how do I \"Grant privilieges and roles needed for setup\" ? What tool should I use and how do set the Types like \"Cluster\" and \"Index\" specified? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "9a152b1c-97ca-49ae-b220-18fa11ec8a9b",
    "url": "https://discuss.elastic.co/t/monitoring-filebeat-with-metricbeat/211565",
    "title": "Monitoring filebeat with metricbeat",
    "category": [
      "Beats"
    ],
    "author": "",
    "date": "December 12, 2019, 5:36am December 12, 2019, 10:45am January 9, 2020, 12:45pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1ec0b637-0b10-41d1-8fad-8ccacec05ba0",
    "url": "https://discuss.elastic.co/t/elasticsearch-module-slowlog-field-mapping/211522",
    "title": "Elasticsearch Module - Slowlog field mapping",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "AndrewMcQ",
    "date": "December 11, 2019, 9:16pm December 12, 2019, 10:35am January 9, 2020, 10:46am",
    "body": "Hi - While working on enabling the Elasticsearch module, specifically the slowlog fileset, I ran into a challenge when I went to use the data in the index to build visualizations. Some of the fields that get pulled out of the slowlog are being set as \"keyword\" type, when they would obviously benefit from being set to \"long\" or some other numerical data type. The field definitions are here: https://github.com/elastic/beats/blob/master/filebeat/module/elasticsearch/slowlog/_meta/fields.yml And, a few of the fields that would seem to be better as an actual number are: elasticsearch.slowlog.took_millis (renamed to event.duration in 7.x) elasticsearch.slowlog.total_hits elasticsearch.slowlog.total_shards And semi-related, the slowlog emits the \"took\" value in what appears to be a human-readable format (with \"s\" for seconds, \"m\" for minutes, etc). Is there any reason why these fields are set as \"keyword\"? What is the process to request that these be switched over? .. or, is there something I can do to override this on our end?",
    "website_area": "discuss"
  },
  {
    "id": "f07e3e33-d56d-45c1-9db4-92eac7b6a9f7",
    "url": "https://discuss.elastic.co/t/packetbeat-to-capture-all-traffic-except-few-port/211169",
    "title": "PacketBeat to capture all traffic except few port?",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "frenchy59",
    "date": "December 9, 2019, 5:43pm December 11, 2019, 2:55pm December 11, 2019, 6:16pm December 12, 2019, 10:30am January 9, 2020, 10:31am",
    "body": "Hi All, Is it possible to configure PacketBeat to capture all the traffic the box is having. except few exception traffic. and then report in Kibana which box is not having any traffic. The goal is to find out which box is doing nothing in our estate, hence if it can be decommissioned. Last time my team checked, we were able to collect specific traffic, by mentioning the port we were looking for. however in our case we want the other way around, which is collect everything except some port. Thanks for you help.",
    "website_area": "discuss"
  },
  {
    "id": "da325c3d-28ab-4740-8687-527b3c0e3917",
    "url": "https://discuss.elastic.co/t/sometimes-beginning-of-log-line-is-missing-resulting-in-jsonparsefailed-tag/209022",
    "title": "Sometimes beginning of log line is missing resulting in _jsonparsefailed tag",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "asp",
    "date": "November 22, 2019, 7:45am November 26, 2019, 10:03am November 26, 2019, 12:30pm November 27, 2019, 8:13am November 27, 2019, 8:57am November 29, 2019, 10:04am November 29, 2019, 10:16am November 29, 2019, 11:51am December 12, 2019, 9:12am January 9, 2020, 9:12am",
    "body": "Hi, I am on elastic stack 7.4.2 and using following pipeline: log -> filebeat -> redis (TLS via stunnel) -> logstash-> elasticsearch In logstash I use pipeline to pipeline communication, so that different inputs the same elasticsearch output. My Logfile is consisting of a json log. json decoding takes place in logstash, not in filebeat. I have following issue: Sometimes the beginning of a log line is truncated. The rest of the line is not a valid json, so it is tagged as _jsonparsefailed in logstash. image.png1847750 86.2 KB What I found out so far: I am not able to find the beginning of the line as _jsonparsefailed. So the beginning of the line seems to be lost somewhere in the processing pipeline. The affected log lines are not the first line in the file (see offset). My filebeat config looks like this: filebeat.yml filebeat.config: inputs: enabled: true path: inputs.d/*.yml reload.enabled: true reload.period: 30s filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false setup.template.settings: index.number_of_shards: 1 fields: {\"log\": {\"sourceGroup\": \"staging\"}} fields_under_root: true setup.kibana: output.redis: enabled: true hosts: [\"poc.redis.k8s-dev.local:16379\"] key: \"%{[logType]:fallback}\" ssl.enabled: true ssl.verification_mode: full ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2] ssl.certificate_authorities: [\"/etc/filebeat/ca.crt\"] processors: - add_host_metadata: ~ - add_cloud_metadata: ~ The included inputs.d looks like this: - type: log enabled: true paths: - /project/jboss/*/*/log/monitoring.json.log* encoding: windows-1252 fields: logType: generic-json log.format: json fields_under_root: true max_bytes: 90000000 Questions: In the past I needed to increase max_bytes because I've seen the log.flag: truncated in events. Do I need to increase any other parameter which configures internal filebeat queues, etc? How to prove that the error is made in filebeat and not in redis or logstash? Can I simultaneously dump the messages which filebeat sends to redis to a file too? Then I could search for incomplete messages in the file and see if they are correct there or truncated at the same position. Any Idea why I cannot find the beginning of the messgae? In my understanding I sould see the beginning as _jsonparsedfailed tagged message also in kibana, because this also must be an incomplete json. How does filebeat behave if a log line is written in two write operations, because OS or log4j buffer is full? But if that would be the problem that filebeat is splitting a message in this case into 2 or more parts, I should see the first part as incomplete message, correct? Any other ideas how to analyse this error? Thanks a lot, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "0cae0ac3-9f25-42e4-8dfd-611d07db8291",
    "url": "https://discuss.elastic.co/t/filebeat-and-logstash-logdata-not-passing/211459",
    "title": "Filebeat and Logstash logdata not passing",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "katara",
    "date": "December 11, 2019, 12:07pm December 11, 2019, 12:34pm December 12, 2019, 4:21am December 12, 2019, 5:19am January 9, 2020, 5:19am",
    "body": "Hi I have 2 servers in which one has Logstash 7.2.0 and another has filbeat 7.2.0. The below is my logstash conf file: > input { beats { port => 5044 ssl => false } } output { elasticsearch { hosts => [\"10.226.51.24:9200\"] index => \"oasis\" } stdout { codec => rubydebug } } below is my filebeat yml config: filebeat.inputs: # Each - is an input. Most options can be set at the input level, so # you can use different inputs for various configurations. # Below are the input specific configurations. - type: log # Change to true to enable this input configuration. enabled: false # Paths that should be crawled and fetched. Glob based paths. paths: - D:\\Oasis\\Logs\\InetPub_Pearson_k12internet\\Ordering_Log\\* - D:\\OasisServices\\Log\\REST\\DigitalOrders\\* #----------------------------- Logstash output -------------------------------- output.logstash: # The Logstash hosts hosts: [\"110.226.51.22:5044\"] Logstash logs: I restarted the service as well. [2019-12-11T06:41:53,380][WARN ][logstash.runner ] SIGTERM received. Shutting down. [2019-12-11T06:41:58,598][WARN ][org.logstash.execution.ShutdownWatcherExt] {\"inflight_count\"=>0, \"stalling_threads_info\"=>{\"other\"=>[{\"thread_id\"=>30, \"name\"=>\"[oasislogs]<beats\", \"current_call\"=>\"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-beats-6.0.0-java/lib/logstash/inputs/beats.rb:204:in run'\"}, {\"thread_id\"=>22, \"name\"=>\"[oasislogs]>worker0\", \"current_call\"=>\"[...]/logstash-core/lib/logstash/java_pipeline.rb:239:in block in start_workers'\"}, {\"thread_id\"=>23, \"name\"=>\"[oasislogs]>worker1\", \"current_call\"=>\"[...]/logstash-core/lib/logstash/java_pipeline.rb:239:in block in start_workers'\"}, {\"thread_id\"=>24, \"name\"=>\"[oasislogs]>worker2\", \"current_call\"=>\"[...]/logstash-core/lib/logstash/java_pipeline.rb:239:in block in start_workers'\"}, {\"thread_id\"=>25, \"name\"=>\"[oasislogs]>worker3\", \"current_call\"=>\"[...]/logstash-core/lib/logstash/java_pipeline.rb:239:in block in start_workers'\"}, {\"thread_id\"=>26, \"name\"=>\"[oasislogs]>worker4\", \"current_call\"=>\"[...]/logstash-core/lib/logstash/java_pipeline.rb:239:in block in start_workers'\"}, {\"thread_id\"=>27, \"name\"=>\"[oasislogs]>worker5\", \"current_call\"=>\"[...]/logstash-core/lib/logstash/java_pipeline.rb:239:in block in start_workers'\"}, {\"thread_id\"=>28, \"name\"=>\"[oasislogs]>worker6\", \"current_call\"=>\"[...]/logstash-core/lib/logstash/java_pipeline.rb:239:in block in start_workers'\"}, {\"thread_id\"=>29, \"name\"=>\"[oasislogs]>worker7\", \"current_call\"=>\"[...]/logstash-core/lib/logstash/java_pipeline.rb:239:in block in start_workers'\"}]}} [2019-12-11T06:41:58,602][ERROR][org.logstash.execution.ShutdownWatcherExt] The shutdown process appears to be stalled due to busy or blocked plugins. Check the logs for more information. [2019-12-11T06:41:59,876][INFO ][logstash.javapipeline ] Pipeline terminated {\"pipeline.id\"=>\"oasislogs\"} [2019-12-11T06:42:00,572][INFO ][logstash.runner ] Logstash shut down. [2019-12-11T06:42:10,757][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.2.0\"} [2019-12-11T06:42:14,859][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://10.16.5.24:9200/]}} [2019-12-11T06:42:14,997][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>\"http://10.16.5.24:9200/\"} [2019-12-11T06:42:15,033][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>7} [2019-12-11T06:42:15,035][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the type` event field won't be used to determine the document _type {:es_version=>7} [2019-12-11T06:42:15,058][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"//10.16.5.24:9200\"]} [2019-12-11T06:42:15,111][INFO ][logstash.outputs.elasticsearch] Using default mapping template [2019-12-11T06:42:15,135][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge] A gauge metric of an unknown type (org.jruby.specialized.RubyArrayOneObject) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2019-12-11T06:42:15,138][INFO ][logstash.javapipeline ] Starting pipeline {:pipeline_id=>\"oasislogs\", \"pipeline.workers\"=>8, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>1000, :thread=>\"#<Thread:0x67dc8344 run>\"} [2019-12-11T06:42:15,176][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} [2019-12-11T06:42:15,376][INFO ][logstash.inputs.beats ] Beats inputs: Starting input listener {:address=>\"0.0.0.0:5044\"} [2019-12-11T06:42:15,383][INFO ][logstash.javapipeline ] Pipeline started {\"pipeline.id\"=>\"oasislogs\"} [2019-12-11T06:42:15,454][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:oasislogs], :non_running_pipelines=>} [2019-12-11T06:42:15,467][INFO ][org.logstash.beats.Server] Starting server on port: 5044 [2019-12-11T06:42:15,702][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} This is however not producing any outputs. Where am I going wrong? Please help me out.",
    "website_area": "discuss"
  },
  {
    "id": "967a456f-e0c5-48b6-87b8-acfb822ad4b9",
    "url": "https://discuss.elastic.co/t/input-plugin-kafka-error-abandoned-subscription-to-k8s-pod-logs-kafka-topic-0-because-consuming-was-taking-too-long/211259",
    "title": "Input plugin: kafka, error: \"abandoned subscription to k8s-pod-logs-kafka-topic/0 because consuming was taking too long\"",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "111258",
    "date": "December 10, 2019, 9:45am December 10, 2019, 12:47pm December 10, 2019, 3:51pm December 11, 2019, 8:31am December 12, 2019, 1:54am January 9, 2020, 2:03am",
    "body": "filebeat: 7.4.2 ES:7.4.2 filebeat.yml: filebeat.inputs: - type: kafka hosts: - wn0-xxx.internal.chinacloudapp.cn:9092 - wn1-xxx.internal.chinacloudapp.cn:9092 - wn2-xxx.internal.chinacloudapp.cn:9092 topics: [\"k8s-pod-logs-qa-kafka-topic\"] group_id: \"fb-k8s-pod-qa-group\" output.elasticsearch: hosts: [\"127.0.0.1:9200\"] index: \"kafka-application-kube-%{+yyyy.MM.dd}\" pipeline: kafka-kubernetes-daemon xpack.monitoring: enabled: true setup.ilm.enabled: false setup.template.name: \"filebeat-%{[beat.version]}\" setup.template.pattern: \"filebeat-%{[beat.version]}-*\" 2019-12-10T08:33:54.799Z INFO kafka/log.go:53 consumer/broker/1001 abandoned subscription to k8s-pod-logs-qa-kafka-topic/1 because consuming was taking too long 2019-12-10T08:33:55.343Z INFO kafka/log.go:53 consumer/broker/1002 added subscription to k8s-pod-logs-qa-kafka-topic/2 2019-12-10T08:33:55.371Z INFO kafka/log.go:53 consumer/broker/1002 abandoned subscription to k8s-pod-logs-qa-kafka-topic/2 because consuming was taking too long 2019-12-10T08:33:56.230Z INFO kafka/log.go:53 consumer/broker/1003 added subscription to k8s-pod-logs-qa-kafka-topic/0 2019-12-10T08:33:56.307Z INFO kafka/log.go:53 consumer/broker/1003 abandoned subscription to k8s-pod-logs-qa-kafka-topic/0 because consuming was taking too long I think, when output to ES, it's take a long time, caused by \"Consumer.MaxProcessingTime\" in here is too short, but I can't setting this parameter in kafka plugin,how to resove it? When output to console, it's ok!",
    "website_area": "discuss"
  },
  {
    "id": "174cd1ca-1423-4646-bd40-32d97a5e2f76",
    "url": "https://discuss.elastic.co/t/custom-indexes-didnt-create-using-winlogbeat-7-2-1-version/209496",
    "title": "Custom indexes didn't create using winlogbeat-7.2.1 version",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "Orest_Gulman",
    "date": "November 26, 2019, 11:50am December 3, 2019, 4:55pm December 3, 2019, 4:59pm December 7, 2019, 7:49pm December 11, 2019, 4:32pm January 8, 2020, 4:32pm",
    "body": "hi, custom indexes didn't creating using winlogbeat-7.2.1 version. Below my winlogbeat configuration that properly works in winlogbeat 6.8.1. ###################### wlb Configuration ########################## winlogbeat.event_logs: - name: Application ignore_older: 72h level: critical, error, warning - name: System ignore_older: 72h level: critical, error, warning - name: DFS Replication ignore_older: 72h - name: Microsoft-Windows-TerminalServices-LocalSessionManager/Operational event_id: 21 ignore_older: 72h setup.template: name: \"custom-sys\" pattern: \"custom-sys-*\" settings: index.number_of_shards: 1 index.number_of_replicas: 1 index.codec: best_compression name: \"custom-app\" pattern: \"custom-app-*\" settings: index.number_of_shards: 1 index.number_of_replicas: 1 index.codec: best_compression name: \"custom-sec\" pattern: \"custom-sec-*\" settings: index.number_of_shards: 1 index.number_of_replicas: 1 index.codec: best_compression output.elasticsearch: # Array of hosts to connect to. indices: - index: \"custom-sys-%{+yyyy.MM}\" when: or: - equals.log_name: \"System\" - equals.log_name: \"DFS Replication\" - index: \"custom-app-%{+yyyy.MM}\" when.equals: log_name: \"Application\" - index: \"custom-sec-%{+yyyy.MM}\" when.equals: log_name: \"Microsoft-Windows-TerminalServices-LocalSessionManager/Operational\" hosts: [\"myelasticsearch:9200\"] Could you please help me to solve the issue, thanks.",
    "website_area": "discuss"
  },
  {
    "id": "2b3b2510-5efb-43f3-b31e-5d36091a9519",
    "url": "https://discuss.elastic.co/t/metricbeat-not-working-when-elasticsearch-is-upgraded-from-7-4-0-to-7-5-0/210443",
    "title": "Metricbeat not working when elasticsearch is upgraded from 7.4.0 to 7.5.0",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "sharmila.dev",
    "date": "December 5, 2019, 5:27am December 5, 2019, 5:28am December 5, 2019, 4:12pm December 5, 2019, 4:11pm December 10, 2019, 7:29pm December 11, 2019, 6:43am December 11, 2019, 4:11pm December 11, 2019, 4:16pm January 8, 2020, 4:17pm",
    "body": "Hi, Im unable to set up monitoring for elastic search using metricbeat. It was working with version 7.4.0 but broke when upgraded to 7.5.0. I was sending metrics using metricbeat to separate monitoring cluster. Metricbeat logs are not useful. Is there any place i can check to see why its not working? Below are metric beat configs # This file is an example configuration file highlighting only the most common # options. The metricbeat.reference.yml file from the same directory contains all the # supported options with more comments. You can use it as a reference. # # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/metricbeat/index.html #========================== Modules configuration ============================ metricbeat.config.modules: # Glob pattern for configuration loading path: ${path.config}/modules.d/*.yml # Set to true to enable config reloading reload.enabled: false # Period on which files under path should be checked for changes #reload.period: 10s #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 1 index.codec: best_compression #_source.enabled: false #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. #name: # The tags of the shipper are included in their own field with each # transaction published. #tags: [\"service-X\", \"web-tier\"] # Optional fields that you can specify to add additional information to the # output. #fields: # env: staging #============================== Dashboards ===================================== # These settings control loading the sample dashboards to the Kibana index. Loading # the dashboards is disabled by default and can be enabled either by setting the # options here or by using the `setup` command. #setup.dashboards.enabled: false # The URL from where to download the dashboards archive. By default this URL # has a value which is computed based on the Beat name and version. For released # versions, this URL points to the dashboard archive on the artifacts.elastic.co # website. #setup.dashboards.url: #============================== Kibana ===================================== # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. # This requires a Kibana endpoint configuration. setup.kibana: # Kibana Host # Scheme and port can be left out and will be set to the default (http and 5601) # In case you specify and additional path, the scheme is required: http://localhost:5601/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 #host: \"localhost:5601\" # Kibana Space ID # ID of the Kibana Space into which the dashboards should be loaded. By default, # the Default Space will be used. #space.id: #============================= Elastic Cloud ================================== # These settings simplify using Metricbeat with the Elastic Cloud (https://cloud.elastic.co/). # The cloud.id setting overwrites the `output.elasticsearch.hosts` and # `setup.kibana.host` options. # You can find the `cloud.id` in the Elastic Cloud web UI. #cloud.id: # The cloud.auth setting overwrites the `output.elasticsearch.username` and # `output.elasticsearch.password` settings. The format is `<user>:<pass>`. #cloud.auth: #================================ Outputs ===================================== # Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: ['http://lx1176.mycompany.com', 'http://lx1177.mycompany.com'] #----------------------------- Logstash output -------------------------------- #output.logstash: # The Logstash hosts #hosts: [\"localhost:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #================================ Processors ===================================== # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] #============================== X-Pack Monitoring =============================== # metricbeat can export internal metrics to a central Elasticsearch monitoring # cluster. This requires xpack monitoring to be enabled in Elasticsearch. The # reporting is disabled by default. # Set to true to enable the monitoring reporter. #monitoring.enabled: false # Sets the UUID of the Elasticsearch cluster under which monitoring data for this # Metricbeat instance will appear in the Stack Monitoring UI. If output.elasticsearch # is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch. #monitoring.cluster_uuid: # Uncomment to send the metrics to Elasticsearch. Most settings from the # Elasticsearch output are accepted here as well. # Note that the settings should point to your Elasticsearch *monitoring* cluster. # Any setting that is not set is automatically inherited from the Elasticsearch # output configuration, so if you have the Elasticsearch output configured such # that it is pointing to your Elasticsearch monitoring cluster, you can simply # uncomment the following line. #monitoring.elasticsearch: #================================= Migration ================================== # This allows to enable 6.7 migration aliases #migration.6_to_7.enabled: true - module: elasticsearch metricsets: - ccr - cluster_stats - index - index_recovery - index_summary - ml_job - node_stats - shard period: 10s hosts: [\"https://localhost:9200\"] username: \"metricbeat_admin\" password: ********* xpack.enabled: true ssl.verification_mode: 'none'",
    "website_area": "discuss"
  },
  {
    "id": "65dfccf8-0ace-47a1-8f91-96ae9125b472",
    "url": "https://discuss.elastic.co/t/not-receiving-state-metrics-in-azure-kubernetes/210490",
    "title": "Not receiving State_* metrics in Azure/Kubernetes",
    "category": [
      "Beats"
    ],
    "author": "Christiaan",
    "date": "December 4, 2019, 8:53am December 11, 2019, 10:37am January 8, 2020, 12:36pm",
    "body": "Hi all, I'm in the middle of setting up an ECK for a Kubernetes cluster on Azure, and I've hit a small snag on Metricbeat. Hoping someone can push me in the right direction. The setup I've got Elastic and related deployments running in a dedicated namespace on two dedicated nodes. Both Elastic and Kibana are up and running without error. The initial deployment was done using the Elastic Operator for Kubernetes, and is secured accordingly. The problem I've added Metricbeat to the mix. Metricbeat uses a Daemonset to collect metrics from the pods/nodes/etc and a Deployment to collect cluster state statistics. After a bit of trial-and-error on the security part, this is working fine for the output from the Daemons, but I am not receiving anything from the state-metrics Deployment. The Metricbeat pod for the Deployment seems to come up without any error and seems to be collecting metrics. 2019-12-04T08:07:13.195Z INFO instance/beat.go:292 Setup Beat: metricbeat; Version: 7.4.2 2019-12-04T08:07:13.196Z INFO elasticsearch/client.go:170 Elasticsearch url: https://elastic-es-http:9200 2019-12-04T08:07:13.196Z INFO [publisher] pipeline/module.go:97 Beat name: metricbeat-54f645684f-xn646 2019-12-04T08:07:13.198Z INFO [monitoring] log/log.go:118 Starting metrics logging every 30s 2019-12-04T08:07:13.198Z INFO instance/beat.go:422 metricbeat start running. 2019-12-04T08:07:13.198Z INFO cfgfile/reload.go:171 Config reloader started 2019-12-04T08:07:13.199Z INFO cfgfile/reload.go:226 Loading of config files completed. 2019-12-04T08:07:43.200Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":20,\"time\":{\"ms\":27}},\"total\":{\"ticks\":100,\"time\":{\"ms\":110},\"value\":100},\"user\":{\"ticks\":80,\"time\":{\"ms\":83}}},\"handles\":{\"limit\":{\"hard\":1048576,\"soft\":1048576},\"open\":8},\"info\":{\"ephemeral_id\":\"126e8a4b-6dbd-459e-bfc1-93d1ecdb7b3d\",\"uptime\":{\"ms\":30263}},\"memstats\":{\"gc_next\":9569520,\"memory_alloc\":5648288,\"memory_total\":15310032,\"rss\":52699136},\"runtime\":{\"goroutines\":30}},\"libbeat\":{\"config\":{\"module\":{\"running\":0},\"reloads\":1},\"output\":{\"type\":\"elasticsearch\"},\"pipeline\":{\"clients\":0,\"events\":{\"active\":0}}},\"system\":{\"cpu\":{\"cores\":2},\"load\":{\"1\":0.43,\"15\":0.75,\"5\":0.58,\"norm\":{\"1\":0.215,\"15\":0.375,\"5\":0.29}}}}}} I've restarted the kube-state-metrics pod on Kube-System just to be on the safe side. This too seems to start up without error: I1203 16:21:25.679571 1 main.go:184] Testing communication with server I1203 16:21:25.722699 1 main.go:189] Running with Kubernetes cluster version: v1.14. git version: v1.14.8. git tree state: clean. commit: 1da9875156ba0ad48e7d09a5d00e41489507f592. platform: linux/amd64 I1203 16:21:25.722726 1 main.go:191] Communication with server successful I1203 16:21:25.722915 1 main.go:225] Starting metrics server: 0.0.0.0:8080 I1203 16:21:25.723261 1 main.go:200] Starting kube-state-metrics self metrics server: 0.0.0.0:8081 I1203 16:21:25.723348 1 metrics_handler.go:96] Autosharding disabled I1203 16:21:25.724509 1 builder.go:144] Active collectors: certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,namespaces,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses All the shards on Elastic are status green, and I'm not seeing any connection or index errors (or any errors, for that matter) in the Elastic logs. In short, all seems fine, except there's no State_* data coming in. In fact, if I delete the metricbeat Daemonset, no data is coming into the Metricbeat index at all, so nothing seems to be coming from the Deployment pod. Configurations The configuration for the Metricbeat Deployment is mostly standard, with added SSL/auth. apiVersion: v1 kind: ConfigMap metadata: name: metricbeat-deployment-config namespace: elastic labels: k8s-app: metricbeat data: metricbeat.yml: |- metricbeat.config.modules: # Reload module configs as they change: reload.enabled: false processors: - add_cloud_metadata: - add_kubernetes_metadata: in_cluster: true setup.ilm.enabled: false output.elasticsearch: hosts: ['https://elastic-es-http:9200'] ssl.certificate_authorities: [\"/usr/share/elastic/certs/ca.crt\"] ssl.certificate: '/usr/share/elastic/certs/tls.crt' ssl.key: '/usr/share/elastic/certs/tls.key' username: '{username}' password: \"{password}\" --- apiVersion: v1 kind: ConfigMap metadata: name: metricbeat-deployment-modules namespace: elastic labels: k8s-app: metricbeat data: # This module requires `kube-state-metrics` up and running under `kube-system` namespace kubernetes.yml: |- - module: kubernetes labels.dedot: true annotations.dedot: true metricsets: - state_node - state_deployment - state_replicaset - state_pod - state_container - state_statefulset # Uncomment this to get k8s events: - event period: 10s hosts: [\"kube-state-metrics.kube-system.svc.cluster.local:8080\"] add_metadata: true in_cluster: true enabled: true --- # Deploy singleton instance in the whole cluster for some unique data sources, like kube-state-metrics apiVersion: apps/v1beta1 kind: Deployment metadata: name: metricbeat namespace: elastic labels: k8s-app: metricbeat spec: template: metadata: creationTimestamp: ~ labels: k8s-app: metricbeat spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: agentpool operator: In values: - elastic containers: - args: - \"-c\" - /etc/metricbeat.yml - \"-e\" image: \"docker.elastic.co/beats/metricbeat-oss:7.4.2\" imagePullPolicy: IfNotPresent name: metricbeat resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi securityContext: runAsUser: 0 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /usr/share/elastic/certs/ name: elastic-internal-http-certificates readOnly: true - mountPath: /etc/metricbeat.yml name: config readOnly: true subPath: metricbeat.yml - mountPath: /usr/share/metricbeat/modules.d name: modules readOnly: true dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: metricbeat serviceAccountName: metricbeat terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: restriction operator: Equal value: elastic volumes: - configMap: defaultMode: 384 name: metricbeat-deployment-config name: config - name: elastic-internal-http-certificates secret: defaultMode: 420 optional: false secretName: elastic-es-http-certs-internal - configMap: defaultMode: 384 name: metricbeat-deployment-modules name: modules I initially ran the Deployment in the Kube-system namespace and the Daemonset in the Elastic namespace. As that didn't work, I've moved everything to the Elastic namespace now, and am targetting the kube-state-metrics services in Kube-system using \"kube-state-metrics.kube-system.svc.cluster.local:8080\". In both cases the effect was the same. No visible errors in any of the logs, but no State_* metrics either. Does anyone have any ideas what could be going on and/or have any suggestions on what I can do to try and isolate the cause? Kind regards, Chris",
    "website_area": "discuss"
  },
  {
    "id": "a183bd26-3ce0-476f-b2ef-c484e8d9b2fc",
    "url": "https://discuss.elastic.co/t/filebeat-cant-read-container-logs/211419",
    "title": "Filebeat can't read container logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "langzichai",
    "date": "December 11, 2019, 10:02am December 11, 2019, 8:41am December 11, 2019, 10:15am January 8, 2020, 10:15am",
    "body": "hi , I use this ymal create in namesplace defuld; created success, but can't find any logs by type: container. Any help!!!! https://raw.githubusercontent.com/elastic/beats/7.4/deploy/kubernetes/filebeat-kubernetes.yaml And filebeat logs: log/input.go:297 stat(/var/log/containers/.log) failed: stat 5579.log: no such file or directory",
    "website_area": "discuss"
  },
  {
    "id": "76126cc5-28f0-42e8-871a-0ecb5be8cb7f",
    "url": "https://discuss.elastic.co/t/about-the-logstash-category/35",
    "title": "About the Logstash category",
    "category": [
      "Logstash"
    ],
    "author": "Leslie_Hawthorn",
    "date": "October 30, 2015, 11:52pm July 6, 2017, 4:25am",
    "body": "Everything related to your favorite centralized logging platform, including plugins and recipes.",
    "website_area": "discuss"
  },
  {
    "id": "9617b386-39f3-4c1d-a300-a2eb4d4040f3",
    "url": "https://discuss.elastic.co/t/single-data-source-split-to-different-indexes/215695",
    "title": "Single data source split to different indexes",
    "category": [
      "Logstash"
    ],
    "author": "BeMoore",
    "date": "January 20, 2020, 9:22am January 20, 2020, 3:30pm January 20, 2020, 5:41pm January 22, 2020, 4:36pm",
    "body": "Morning all ! i have a question, if i have a json data source like : { \"log_url\" => \"http://127.0.0.1/log.txt\", \"key\" => \"22op3dfe\", \"raw_msg\" => \"404.19  Denied by filtering rule\", \"MD5\" => \"2c5cddf13ab55a1d4eca955dfa32d245\", \"syntax\" => \"text\", \"@version\" => \"1\", \"SHA256\" => \"766be5c99ba674f985ce844add4bc5ec423e90811fbceer5ec84efa3cf1624f4\", \"user\" => \"user\", \"URL\" => \"http://127.0.0.1\", \"YaraRule\" => [ [0] \"no_match\" ], \"expire\" => \"0\", \"size\" => 107, \"source\" => \"localhost\", \"Msg\" => \"404 OK\", \"filename\" => \"log.txt\", \"@timestamp\" => 2020-01-07T13:59:04.000Z } and all of this is processed and send to index1 but i want to split off : \"MD5\" => \"2c5cddf13ab55a1d4eca955dfa32d245\" and \"@timestamp\" => 2020-01-07T13:59:04.000Z into index 2 does this mean i have to ; A) Run an ingest process twice on the same data source and in process one.. drop MD5 and Timestamp and push remaining to index 1 and then rerun the ingest and drop everything except MD5 and Timestamp into index 2 B) use IF statements on the output for fields; so IF field name is MD5 then > index 2 C) Config a complex conf that processes the json source, then pushes MD5 and Timestamp to @Metadata then at the end, read metadata and push it to Index 2 D) do some other uber process that i'm not aware of yet to accomplish this much easier leaving time to drink my warm coffee before it gets stone cold. If this is something that can or needs to be done with Filebeat for example, please let me know, i'm hoping it can all be done in logstash. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "8169d05c-b371-450b-9970-15be70af52a6",
    "url": "https://discuss.elastic.co/t/kafka-input-convert-to-json/215785",
    "title": "Kafka input convert to json",
    "category": [
      "Logstash"
    ],
    "author": "elasticforme",
    "date": "January 20, 2020, 6:22pm January 20, 2020, 9:22pm January 21, 2020, 4:06pm January 22, 2020, 2:05pm January 22, 2020, 3:57pm January 22, 2020, 4:23pm",
    "body": "I have use input as kafka and my message is like 2020-01-20 10:47:48: partition=0 offset=187965 machine=host1 queue=qhost1 comman=1298316293 action=REMOVE I want this to be separated out. how do I use filter input { kafka { bootstrap_servers => \"kaf001:9092\" topics => \"events.commands\" } } filter {} output { stdout { codec => rubydubug } } I want output like { mydate: 2020-01-20 10:47:48 partition: 0 offset:187965 machine: host1 queue: qhost1 command: 1298316293 action: REMOVE } but output has some kind of special character. and Looks like this at this time. \"message\" => \"\\u0001\\u000E\\u0010qhost1\\t\\fREMOVE\\u0002[\\u0000\"",
    "website_area": "discuss"
  },
  {
    "id": "98b31838-c378-4ede-81d9-798c20487433",
    "url": "https://discuss.elastic.co/t/retrieve-hour-and-day-from-timestamp/216093",
    "title": "Retrieve Hour and Day from Timestamp",
    "category": [
      "Logstash"
    ],
    "author": "Jim_Thunder",
    "date": "January 22, 2020, 4:16pm",
    "body": "Hello! My code here is getting the hour and day from a newly (and more accurate) timestamp. The timestamp is updated with a -0600 for the timezone. When I retrieve the hour from this timestamp, it doesn't include the fact that the timezone changed. So the hour is 6 hours ahead. How can I fix that? (I know I can't just subtract 6 b/c then I run into all kinds of issues with 3pm - 6 = \"a negative hour\" ..etc) if [sourceType] == \"zOS-SMF_030\" { csv{ columns => [ \"Correlator\", \"SMF30LEN\", \"SMF30SEG\", \"SMF30FLG\", \"SMF30RTY\", \"SMF30PSN\", \"SMF30CL8\", \"SMF30ISS\", \"SMF30IET\", \"SMF30SSN\", \"SMF30EXN\", \"SMF30ASI\", \"SMF30COR\" ] separator => \",\" } mutate { add_field => { \"@SMF_TIMESTAMP\" => \"%{SMF30DTE} %{SMF30TME}\" }} date { match => [\"@SMF_TIMESTAMP\", \"YYYY-MM-dd HH:mm:ss:SSS\"] target => \"@SMF_TIMESTAMP\" timezone => \"-0600\" } ruby { code => \"event.set('[day_of_week]',event.get('@SMF_TIMESTAMP').time.strftime('%a')) event.set('[hour]',event.get('@SMF_TIMESTAMP').time.strftime('%H')) event.set('[day]',event.get('@SMF_TIMESTAMP').time.strftime('%d'))\" } mutate { convert => { \"day\" => \"integer\" \"hour\" => \"integer\" } } } Here's an image to help explain: As you can see the timestamp shows 10am but the hour field shows 16 (4pm)",
    "website_area": "discuss"
  },
  {
    "id": "479f29d7-d8a9-4d36-9be8-a9dc47c4f67a",
    "url": "https://discuss.elastic.co/t/logstash-cant-seem-to-connect-after-years-or-working-fine/216091",
    "title": "Logstash can't seem to connect after years or working fine",
    "category": [
      "Logstash"
    ],
    "author": "javadevmtl",
    "date": "January 22, 2020, 3:59pm",
    "body": "Hi, I have logstash 6.4.2 running inside docker container. The logstash instance was working fine for over a year. Just last few hours the below errors started hapening... I loged into the container itself and I pigned and CURLed all the nodes and its connecting fine. Also the Elastic cluster seems 100% healthy. [2020-01-22T15:35:29,062][INFO ][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] [Consumer clientId=logstash-3, groupId=logstash] Setting newly assigned partitions [app-logs-12] [2020-01-22T15:35:29,071][INFO ][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] [Consumer clientId=logstash-2, groupId=logstash] Setting newly assigned partitions [app-logs-10, app-logs-11] [2020-01-22T15:35:29,072][INFO ][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] [Consumer clientId=logstash-1, groupId=logstash] Setting newly assigned partitions [app-logs-6, app-logs-7] [2020-01-22T15:35:31,173][ERROR][logstash.outputs.elasticsearch] Encountered a retryable error. Will Retry with exponential backoff {:code=>500, :url=>\"http://XXXXXX-0001.my.domain:9200/_xpack/monitoring/_bulk?system_id=logstash&system_api_version=2&interval=1s\"} [2020-01-22T15:35:44,657][WARN ][logstash.filters.json ] Error parsing json {:source=>\"log_message\", :raw=>\"java.io.IOException: Connection reset by peer\", :exception=>#<LogStash::Json::ParserError: Unrecognized token 'java': was expecting ('true', 'false' or 'null') at [Source: (byte[])\"java.io.IOException: Connection reset by peer\"; line: 1, column: 6]>} [2020-01-22T15:35:47,358][ERROR][logstash.outputs.elasticsearch] Encountered a retryable error. Will Retry with exponential backoff {:code=>500, :url=>\"http://XXXXXX-0001.my.domain:9200/_xpack/monitoring/_bulk?system_id=logstash&system_api_version=2&interval=1s\"} [2020-01-22T15:36:19,405][ERROR][logstash.outputs.elasticsearch] Encountered a retryable error. Will Retry with exponential backoff {:code=>500, :url=>\"http://XXXXXX-0001.my.domain:9200/_xpack/monitoring/_bulk?system_id=logstash&system_api_version=2&interval=1s\"} [2020-01-22T15:36:37,565][WARN ][logstash.outputs.elasticsearch] Marking url as dead. Last error: [LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError] Elasticsearch Unreachable: [http://XXXXXX-0002.my.domain:9200/][Manticore::SocketTimeout] Read timed out {:url=>http://XXXXXX-0002.my.domain:9200/, :error_message=>\"Elasticsearch Unreachable: [http://XXXXXX-0002.my.domain:9200/][Manticore::SocketTimeout] Read timed out\", :error_class=>\"LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError\"} [2020-01-22T15:36:37,592][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://XXXXXX-0002.my.domain:9200/, :path=>\"/\"} [2020-01-22T15:36:37,603][ERROR][logstash.outputs.elasticsearch] Attempted to send a bulk request to elasticsearch' but Elasticsearch appears to be unreachable or down! {:error_message=>\"Elasticsearch Unreachable: [http://XXXXXX-0002.my.domain:9200/][Manticore::SocketTimeout] Read timed out\", :class=>\"LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError\", :will_retry_in_seconds=>2} [2020-01-22T15:36:37,625][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>\"http://XXXXXX-0002.my.domain:9200/\"}",
    "website_area": "discuss"
  },
  {
    "id": "922cb0a5-e5a2-4ba6-968e-acead06ffe70",
    "url": "https://discuss.elastic.co/t/query-index-by-two-fields/216046",
    "title": "Query index by two fields",
    "category": [
      "Logstash"
    ],
    "author": "namali",
    "date": "January 22, 2020, 11:25am January 22, 2020, 11:36am January 22, 2020, 11:39am January 22, 2020, 12:20pm January 22, 2020, 4:08pm January 22, 2020, 3:45pm",
    "body": "how to find subempcategoryid from emp_category index emp_category index structure mainempcategoryid mainempcategoryname subempcategoryid subempcategoryname used elasticseaxh fillter pluging. but query not working when there two condition elasticsearch { hosts => [\"http://localhost:9200/\"] index => \"emp_category\" query => \"mainempcategoryid :%{maincategoryid} AND subempcategoryname:%{[data][categories][0]}\" fields => { \"subempcategoryid\" => \"subcategoryid\" } }",
    "website_area": "discuss"
  },
  {
    "id": "3ebb04b2-579d-41f1-8cd0-57a2704828e3",
    "url": "https://discuss.elastic.co/t/need-title-name-of-file-for-logstash-ingestion/211308",
    "title": "Need title/name of file for logstash ingestion",
    "category": [
      "Logstash"
    ],
    "author": "edster",
    "date": "December 10, 2019, 2:02pm December 11, 2019, 11:12pm January 6, 2020, 6:40pm January 9, 2020, 3:22pm January 9, 2020, 5:02pm January 10, 2020, 7:46pm January 10, 2020, 8:03pm January 14, 2020, 6:38pm January 22, 2020, 3:36pm",
    "body": "Hello, I am trying to get part of the file names for ingestion for each record, however, I am ingesting multiple files using \"cat /filespath/ | /logstashpath/ -f /logstashconfig/\" command. The reason i am running it like this is so that the logstash config file only runs \"once\" on each of the files in the filespath. I am not trying to have it run continuously without end. Doing it this way, however, does not give the path and filename for which to use the grok option and store part of the file name. How can I go about this?",
    "website_area": "discuss"
  },
  {
    "id": "875c0eb7-18b2-4264-8adf-90a310442f5d",
    "url": "https://discuss.elastic.co/t/unable-to-send-full-event-to-syslog/215339",
    "title": "Unable to send full event to syslog",
    "category": [
      "Logstash"
    ],
    "author": "Necus",
    "date": "January 16, 2020, 3:12pm January 22, 2020, 10:55am January 22, 2020, 2:59pm",
    "body": "Hi, I'm trying to send Windows logs through Windows Event Forwarder, through Winlogbeat and Logstash to syslog. (System1->System2 with Winlogbeat->Logstash->syslog). For some reason event which appear in syslog is cropped only to message field, no event fields, no winlog fields. The same event in Elasticsearch has all of the fields. Why output to syslog on Logstash cropped fields other than message?",
    "website_area": "discuss"
  },
  {
    "id": "9df80f3a-502d-4973-a5ac-8887ac53f947",
    "url": "https://discuss.elastic.co/t/how-to-map-nested-json-array-with-http-output-plugin/215950",
    "title": "How to map nested json array with http output plugin",
    "category": [
      "Logstash"
    ],
    "author": "rkhapre",
    "date": "January 21, 2020, 8:54pm January 21, 2020, 10:02pm January 22, 2020, 10:37am January 22, 2020, 10:53am January 22, 2020, 2:56pm",
    "body": "Hi All I have incoming events from input as 2 records Project_Num, Task Name,Task_num,Task Date PRJ-001,Task Name1,111,1-Jan-2020 PRJ-001,Task Name2,112,2-Jan-2020 How should i map it with my json payload like below in http output Basically, i need my data in below format to form a payload There can be a cases where i can have 3 records from same Project or 4 Records from same project as i need to form a array in json output for payload So the mapping should be dynamic Need to group it dynamically to form a one json record { \"Project\":{ \"Project_Num\":\"PRJ-001\", \"Task\":[ { \"Task Name\":\"Task Name1\", \"Task_num\":111, \"Task Date\":\"1-Jan-2020\", }, { \"Task Name\":\"Task Name2\", \"Task_num\":112, \"Task Date\":\"2-Jan-2020\", } ] } Thanks",
    "website_area": "discuss"
  },
  {
    "id": "bcfdcad0-c02f-4146-9fb2-11f8461ae425",
    "url": "https://discuss.elastic.co/t/logstash-split-type-failure/216018",
    "title": "Logstash _split_type_failure",
    "category": [
      "Logstash"
    ],
    "author": "ramypala",
    "date": "January 22, 2020, 9:29am January 22, 2020, 11:07am January 22, 2020, 11:10am January 22, 2020, 2:53pm",
    "body": "My Raw Data {\"records\":[{\"parent\":\"\",\"made_sla\":\"true\",\"caused_by\":\"\",\"watch_list\":\"\",\"upon_reject\":\"cancel\",\"sys_updated_on\":\"2020-01-22 05:47:33\",\"child_incidents\":\"0\",\"u_incident_age_in_days\":\"1970-07-21 00:13:56\",\"hold_reason\":\"\",\"approval_history\":\"\",\"skills\":\"\",\"number\":\"INC0011015\",\"resolved_by\":\"6816f79cc0a8016401c5a33be04be441\",\"sys_updated_by\":\"admin\",\"opened_by\":\"6816f79cc0a8016401c5a33be04be441\",\"user_input\":\"\"},{\"parent\":\"\",\"made_sla\":\"true\",\"caused_by\":\"\",\"watch_list\":\"\",\"upon_reject\":\"cancel\",\"sys_updated_on\":\"2020-01-22 05:47:33\",\"child_incidents\":\"0\",\"u_incident_age_in_days\":\"1970-07-21 00:16:40\",\"hold_reason\":\"\",\"approval_history\":\"\",\"skills\":\"\",\"number\":\"INC0010021\",\"resolved_by\":\"6816f79cc0a8016401c5a33be04be441\",\"sys_updated_by\":\"admin\",\"opened_by\":\"6816f79cc0a8016401c5a33be04be441\",\"user_input\":\"\"} ] My config file input { http_poller { urls => { url => \"https://dev63285.service-now.com/incident_list.do?JSONv2&display_value=True&sysparm_exclude_reference_link=True&sysparm_fields=number%2Cstate%2Copened_at%2Cclosed_at%2Cpriority%2Cassigned_to%2Cassignment_group%2Cactive%2Cimpact%2Curgency%2Cseverity&sysparm_limit=1sysparm_view=json_view\" } request_timeout => 60 proxy => { host => \"10.88.129.144\" port => \"80\" scheme => \"http\"} user => \"admin\" password => \"Snow2020*\" schedule => { cron => \"* * * * *\"} codec => \"json\" metadata_target => \"http_poller_metadata\" } } filter { split { field => \"records\" } } output { elasticsearch { hosts => [\"localhost:9200\"] index => \"servicenowinc\" } stdout { codec => rubydebug } } Getting below error when I run the config file.Please help. \"Only String and Array types are splittable. field:records is of type = NilClass\" C:/Elastic_Stack/logstash-7.5.1/vendor/bundle/jruby/2.5.0/gems/awesome_print-1.7.0/lib/awesome_print/formatters/base_formatter.rb:31: warning: constant ::Fixnum is deprecated { \"tags\" => [ [0] \"_http_request_failure\", [1] \"_split_type_failure\" ], \"http_request_failure\" => { \"error\" => \"connect timed out\", \"backtrace\" => nil, \"runtime_seconds\" => 10.24, \"request\" => { \"url\" => \"https://dev63285.service-now.com/incident_list.do?JSONv2&display_value=True&sysparm_exclude_reference_link=True&sysparm_fields=number%2Cstate%2Copened_at%2Cclosed_at%2Cpriority%2Cassigned_to%2Cassignment_group%2Cactive%2Cimpact%2Curgency%2Cseverity&sysparm_limit=1sysparm_view=json_view\", \"method\" => \"get\" }, \"name\" => \"url\" }, \"@timestamp\" => 2020-01-22T09:27:10.596Z, \"@version\" => \"1\", \"http_poller_metadata\" => { \"host\" => \"DESKTOP-JRAAHA2\", \"runtime_seconds\" => nil, \"request\" => { \"url\" => \"https://dev63285.service-now.com/incident_list.do?JSONv2&display_value=True&sysparm_exclude_reference_link=True&sysparm_fields=number%2Cstate%2Copened_at%2Cclosed_at%2Cpriority%2Cassigned_to%2Cassignment_group%2Cactive%2Cimpact%2Curgency%2Cseverity&sysparm_limit=1sysparm_view=json_view\", \"method\" => \"get\" }, \"name\" => \"url\" } } [2020-01-22T14:58:10,284][WARN ][logstash.filters.split ][main] Only String and Array types are splittable. field:records is of type = NilClass { \"tags\" => [ [0] \"_http_request_failure\", [1] \"_split_type_failure\"",
    "website_area": "discuss"
  },
  {
    "id": "24a1b958-cdcc-40d5-81bc-48d1e4b40460",
    "url": "https://discuss.elastic.co/t/logstash-error-413/216055",
    "title": "Logstash Error 413",
    "category": [
      "Logstash"
    ],
    "author": "bhargav_bharat",
    "date": "January 22, 2020, 12:22pm January 22, 2020, 2:51pm",
    "body": "Hello, I am facing below mentioned error and logstash stop sending log to elasticsearch and communication also broken. ########################################################### Jan 22 10:21:33 logstash_agent logstash[29058]: [2020-01-22T10:21:33,304][ERROR][logstash.outputs.elasticsearch][main] Encountered a retryable error. Will Retryable error. Will Retry with exponential backoff {:code=>413, :url=>\"htps://elasticsearc_IP:9202/_bulk\"} ########################################################### Any help will be appreciated:",
    "website_area": "discuss"
  },
  {
    "id": "9f5cf140-7611-4ebc-b135-0c5929f810f8",
    "url": "https://discuss.elastic.co/t/how-to-read-a-text-file-using-elastic-search/216045",
    "title": "How to read a text file using elastic search",
    "category": [
      "Logstash"
    ],
    "author": "prudhvi",
    "date": "January 22, 2020, 11:23am January 22, 2020, 11:30am January 22, 2020, 11:53am January 22, 2020, 11:55am January 22, 2020, 12:02pm January 22, 2020, 12:17pm January 22, 2020, 2:50pm",
    "body": "i am trying to read a text file using logstash and to push it to elasticsearch. How can i know if the input file is read, where can i see the data read by logstash.",
    "website_area": "discuss"
  },
  {
    "id": "97fca4b6-c63c-4b0b-b1c3-4f335b517be2",
    "url": "https://discuss.elastic.co/t/logstash-java-hotspot-tm-64-bit-server-vm-warning-option-useconcmarksweepgc-was-deprecated-in-version-9-0-and-will-likely-be-removed-in-a-future-release/216060",
    "title": "Logstash : Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release",
    "category": [
      "Logstash"
    ],
    "author": "Ekta",
    "date": "January 22, 2020, 12:54pm January 22, 2020, 2:48pm",
    "body": "while shashing first log it show below warning: /usr/share/logstash# bin/logstash -e 'input { stdin { } } output { stdout {} }' Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely b e removed in a future release. WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by com.headius.backport9.modules.Modules (file:/usr/share/logstash/logstash-core/ lib/jars/jruby-complete-9.2.7.0.jar) to field java.io.FileDescriptor.fd WARNING: Please consider reporting this to the maintainers of com.headius.backport9.modules.Modules WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release Thread.exclusive is deprecated, use Thread::Mutex WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console Can anyone help into this?",
    "website_area": "discuss"
  },
  {
    "id": "667f4a26-f182-4eab-bc1f-88ba0a410733",
    "url": "https://discuss.elastic.co/t/logstash-processing-twice-then-block-in-start-workers/216080",
    "title": "Logstash processing twice then 'block_in_start_workers'",
    "category": [
      "Logstash"
    ],
    "author": "guctum",
    "date": "January 22, 2020, 2:46pm",
    "body": "Using Logstash to get information from Oracle Enterprise Manager, keep getting an error relating to multi_recieve with 'block in start_workers' repeating over and over whenever Logstash tries again. I have the below conf file that queries every 2 minutes, I'm exporting this data to Grafana and can see that it runs twice and then I just see the block. Is it something with my query that's resulting in this? Do I need a different way of running Logstash? I'm running it on Windows via bash terminal, command \"./logstash -f oem.conf\". Thanks for any info that can be given. Here is my conf file: input { jdbc { jdbc_validate_connection => true jdbc_connection_string => \"connection\" jdbc_user => \"username\" jdbc_password => \"password\" jdbc_driver_class => \"\" statement => \"SELECT * FROM V$ACTIVE_SESSION_HISTORY\" schedule => \"*/2 * * * *\" } } filter { if [@timestamp] { mutate { add_field => { \"logstash_timestamp\" => \"%{@timestamp}\" } } } mutate { remove_field => [ \"force_matching_signature\" ] } mutate { convert => [ \"sample_time\" , \"string\" ] } date { match => [ \"sample_time\" , \"ISO8601\" ] } } output { elasticsearch { hosts => [\"localhost:9200\"] } # stdout { codec => rubydebug } }",
    "website_area": "discuss"
  },
  {
    "id": "a81fd8e5-e5d0-44ec-ac97-5e82ecab5710",
    "url": "https://discuss.elastic.co/t/high-availability-of-logstash-using-jdbc-plugin/216043",
    "title": "High availability of logstash using jdbc plugin",
    "category": [
      "Logstash"
    ],
    "author": "naveenbangalore",
    "date": "January 22, 2020, 11:03am January 22, 2020, 2:44pm",
    "body": "I have a 2 node logstash setup populating the elastic cluster. There are few configs that use the jdbc plugin in the input. Is there a way to setup high availability among the logstash nodes when one of them fails? My query is the same as the one in the below link, however, I am raising it again since there were no responses to that. Logstash JDBC plugin and high availability Logstash I have an Elasticsearch cluster populated via the Logstash JDBC plugin, Is there a way to make Logstash highly available? Ideally I want only one instance of Logstash polling the relational database at a time and have this instance failover if it crashes.",
    "website_area": "discuss"
  },
  {
    "id": "16b7a4e2-47e4-4a41-b720-a4e82cfea5cc",
    "url": "https://discuss.elastic.co/t/logstash-outputs-elasticsearch-attempted-to-resurrect-connection-to-dead-es-instance/216027",
    "title": "[logstash.outputs.elasticsearch] Attempted to resurrect connection to dead ES instance",
    "category": [
      "Logstash"
    ],
    "author": "pyerunka",
    "date": "January 22, 2020, 10:15am January 22, 2020, 1:36pm January 22, 2020, 1:53pm January 22, 2020, 2:05pm",
    "body": "Hello, I am trying to use jdbc input plugin using logstash so that i can fetch data from sql database and create the index. But when i am running config file for logstash. It is showing me below error: [2020-01-22T10:04:27,133][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>\"http://127.0.0.1:9200/\", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>\"Elasticsearch Unreachable: [http://127.0.0.1:9200/][Manticore::SocketException] Connection refused: connect\"} I have clustering enable with 3 nodes. Can anyone guide me on the same? Reagrds, Priyanka",
    "website_area": "discuss"
  },
  {
    "id": "778f1361-88d1-4d96-a4db-cde42c90f38f",
    "url": "https://discuss.elastic.co/t/logstash-status-failed-to-start-logstash/216065",
    "title": "Logstash status: Failed to start logstash",
    "category": [
      "Logstash"
    ],
    "author": "Ekta",
    "date": "January 22, 2020, 1:15pm January 22, 2020, 1:43pm",
    "body": "logstash failed to start. I am using logstash7 .2.0 Elasticsearch-7.2.0 Java-11.0.6 When I check the status after starting it shows Active:Failed. $: systemctl status logstash.service  logstash.service - logstash Loaded: loaded (/etc/systemd/system/logstash.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Wed 2020-01-22 08:04:59 EST; 37s ago Main PID: 12167 (code=exited, status=1/FAILURE) systemd[1]: logstash.service: Service hold-off time over, scheduling restart. Jan 22 08:04:59 xxxxxxx systemd[1]: logstash.service: Scheduled restart job, restart counter is at 5. Jan 22 08:04:59 xxxxxxx systemd[1]: Stopped logstash. Jan 22 08:04:59 xxxxxxx systemd[1]: logstash.service: Start request repeated too quickly. Jan 22 08:04:59 xxxxxxx systemd[1]: logstash.service: Failed with result 'exit-code'. Jan 22 08:04:59 xxxxxxx systemd[1]: Failed to start logstash. can any help why is it failed?",
    "website_area": "discuss"
  },
  {
    "id": "aef4537d-da94-4b50-84cb-5009f1cfc9bf",
    "url": "https://discuss.elastic.co/t/logstash-monitorin-in-kibana/216070",
    "title": "Logstash monitorin in kibana",
    "category": [
      "Logstash"
    ],
    "author": "mende",
    "date": "January 22, 2020, 1:33pm",
    "body": "Hi Team, I installed logstash version 7.4.2 in through Kubernetes my elastic cluster and kibana running well. If I see the monitoring in kibana dashboard, its showing only elastic cluster and kibana but its not showing logstash. I configure below setting logstash.yml file in logstash for monitoring xpack.monitoring.enabled: true xpack.monitoring.elasticsearch.hosts: https://test.com:243 xpack.monitoring.elasticsearch.username: xxxxx xpack.monitoring.elasticsearch.password: xxxxxxx logstash logs are showing below like this. [2020-01-22T10:53:04,552][ERROR][l.o.elasticsearch ] Encountered a retryable error. Will Retry with exponential backoff {:code=>400, :url=>\"https://test.com:243/_monitoring/bulk?system_id=logstash&system_api_version=7&interval=1s\"} Could you please suggest this. Thanks and regards, shivudu",
    "website_area": "discuss"
  },
  {
    "id": "84d46b08-f399-4429-9a55-062eb75b15e1",
    "url": "https://discuss.elastic.co/t/host-data-wont-populate-in-dashboards-when-upgrading-beats-from-7-0-to-7-3-and-using-logstash/215344",
    "title": "Host data wont populate in dashboards when upgrading beats from 7.0 to 7.3 and using Logstash",
    "category": [
      "Logstash"
    ],
    "author": "Ryan_Downey",
    "date": "January 21, 2020, 12:58pm January 16, 2020, 4:11pm January 17, 2020, 2:20pm January 21, 2020, 1:05pm January 21, 2020, 2:05pm January 21, 2020, 7:37pm January 22, 2020, 1:30pm",
    "body": "ECE - Elasticsearch 7.3 Kibana -7 .3 Our metricbeat system overview dashboard will not display a Beat thats running 7.3 after upgrading from 7.0 if we run the beat through Logstash. The beat will populate in the dashboard if we point the beat directly to Elasticsearch but not through Logstash. The data is populating in Elasticsearch as I can search for the tag that we want and the host.name that we are looking for but the dashboard will not populate the beat even though the dash is set to check metricbeat-*. We've tried a few other version of metricbeat that are higher than the first 7.0 version and none of them will populate in the dashboard if we run the beat through logstash. Any insight would be appreciated.",
    "website_area": "discuss"
  },
  {
    "id": "1744f110-1e1f-4b69-9592-d3f96d24ef4e",
    "url": "https://discuss.elastic.co/t/remove-alias-winlog-from-fields/216069",
    "title": "Remove alias winlog from fields",
    "category": [
      "Logstash"
    ],
    "author": "francescouk",
    "date": "January 22, 2020, 1:28pm",
    "body": "Hi there, I would like to know if is possivel to remove winlog alias from winlogbeat fields? eg: winlog.channel => channel winlog.user.domain => user.domain ??? Thanks for the attention.",
    "website_area": "discuss"
  },
  {
    "id": "33f01cc9-89fb-4061-8dcd-ef439417416c",
    "url": "https://discuss.elastic.co/t/remote-tomcat-logs/216063",
    "title": "Remote tomcat logs",
    "category": [
      "Logstash"
    ],
    "author": "sagi",
    "date": "January 22, 2020, 1:01pm",
    "body": "I have configured ELK 6.8.2 on ubuntu 16.04, remote Syslog are loaded using filebeat , please anyone help me how to load remote tomcat logs are catalina.out. I am new to ELK.",
    "website_area": "discuss"
  },
  {
    "id": "55c3c66a-95ae-4b07-913c-e07f3147168f",
    "url": "https://discuss.elastic.co/t/index-management-cisco-asa-device/216051",
    "title": "Index Management CISCO ASA device",
    "category": [
      "Logstash"
    ],
    "author": "Yashwant_Shettigar",
    "date": "January 22, 2020, 12:06pm",
    "body": "Hello, I have a centralized ELK server, where data is getting shipped via winlogbeat and filebeat installed on client machines. Now I am trying to route cisco-asa devices logs to this server. Upto some extent I am bit successful in getting the data, but facing issue to create a new index pattern for it via logstash configuration. Please check below configuration and help me in fixing it (below config is not working): Just to make it more clear, when I get data from winlogbeat and filebeat, I get indexes in winlogbeat-* filebeat-* format. I want the same kind of format for firewall logs, but instead I am getting them in %{[@metadata][beat]}-* format. I want it to achieve in firewall-* format. input.conf input { beats { port => 6099 } udp { port => 5000 type => \"cisco-asa\" } } output.conf output { elasticsearch { hosts => [\"localhost:9200\"] sniffing => true manage_template => false if [type] == \"cisco-asa\" { index => \"firewall-%{+YYYY.MM.dd}\" } if [type] != \"cisco-asa\" { index => \"%{[@metadata][beat]}-%{+YYYY.MM.dd}\" } } stdout { codec => rubydebug } }",
    "website_area": "discuss"
  },
  {
    "id": "a701a15c-f0e2-488d-b95c-822ffac33c98",
    "url": "https://discuss.elastic.co/t/help-with-grok-pattern-for-haproxy/215441",
    "title": "Help with grok pattern for haproxy",
    "category": [
      "Logstash"
    ],
    "author": "mmikulic",
    "date": "January 17, 2020, 10:58am January 17, 2020, 12:38pm January 17, 2020, 2:01pm January 22, 2020, 12:02pm",
    "body": "Hi,please assit with creating custom grok pattern for logstash. I've created a custom log format in haproxy conf for purpose of logging mutual ssl authentication details for client certificates. log-format %ci:%cp\\ [%t]\\ %ft\\ %b/%s\\ %TR/%Tw/%Tc/%Tr/%Ta\\ %ST\\ %B\\ %CC\\ %CS\\ %tsc\\ %ac/%fc/%bc/%sc/%rc\\ %sq/%bq\\ %hr\\ %hs\\ {%[ssl_c_verify],%{+Q}[ssl_c_s_dn],%{+Q}[ssl_c_i_dn]}\\ %{+Q}r\\ %sslv\\ %sslc Log line example looks like this: Jan 17 08:47:06 localhost haproxy[9870]: 10.251.121.117:10300 [17/Jan/2020:08:47:06.531] api.example.com~ test-backend/server1 0/0/1/8/9 201 589 - - ---- 421/419/1/1/0 0/0 {||10.105.74.60} {331260} {0,\"/C=GB/L=Tester/O=Some Company That Connects Co./CN=api.somecompany.com\",\"/C=US/O=CertAuth, Inc./OU=See www.certauth.net/legal-terms/OU=(c) 2012 CertAuth, Inc. - for authorized use only/CN=CertAuth Certification Authority - K1B\"} \"POST /some/1/path HTTP/1.1\" TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 When using standard haproxy grok pattern with additions for tls version and cipher: %{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\\{%{HAPROXYCAPTUREDREQUESTHEADERS}\\})?( )?(\\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\\})?( )?\"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?\"( %{NOTSPACE:tls_version})?( %{NOTSPACE:tls_ciphersuite})? Information from log line that I want to parse ends up in captured_response_headers: \"HAPROXYCAPTUREDRESPONSEHEADERS\": [ [ \"331260} {0,\"/C=GB/L=Tester/O=Some Company That Connects Co./CN=api.somecompany.com\",\"/C=US/O=CertAuth, Inc./OU=See www.certauth.net/legal-terms/OU=(c) 2012 CertAuth, Inc. - for authorized use only/CN=CertAuth Certification Authority - K1B\"\" ] ], \"captured_response_headers\": [ [ \"331260} {0,\"/C=GB/L=Tester/O=Some Company That Connects Co./CN=api.somecompany.com\",\"/C=US/O=CertAuth, Inc./OU=See www.certauth.net/legal-terms/OU=(c) 2012 CertAuth, Inc. - for authorized use only/CN=CertAuth Certification Authority - K1B\"\" ] ] I need to parse captured_response_headers so search is more easier like this: \"id\": [ [ \"331260\" ] ], \"ssl_verified\": [ [ \"0\" ] ], \"cert_details\": [ [ \"/C=GB/L=Tester/O=Some Company That Connects Co./CN=api.somecompany.com\",\"/C=US/O=CertAuth, Inc./OU=See www.certauth.net/legal-terms/OU=(c) 2012 CertAuth, Inc. - for authorized use only/CN=CertAuth Certification Authority - K1B\"\" ] ]",
    "website_area": "discuss"
  },
  {
    "id": "c75d9e2b-6042-4057-b0b9-43240682047a",
    "url": "https://discuss.elastic.co/t/multiple-match-in-grok-filter-logstash-is-not-working/215874",
    "title": "Multiple match in Grok filter logstash is not working",
    "category": [
      "Logstash"
    ],
    "author": "Saravana37",
    "date": "January 21, 2020, 11:46am January 21, 2020, 11:51am January 21, 2020, 12:01pm January 21, 2020, 1:11pm January 21, 2020, 1:31pm January 21, 2020, 2:17pm January 22, 2020, 6:02am January 22, 2020, 8:28am January 22, 2020, 8:56am January 22, 2020, 9:43am January 22, 2020, 9:45am January 22, 2020, 9:53am January 22, 2020, 10:04am January 22, 2020, 11:21am January 22, 2020, 11:24am",
    "body": "Hello , My Grok filter looks like below grok { match => [\"message\",\"%{WORD:ACT_SV_NAME}%{SPACE}%{WORD:ACT_CC_Alias}%{SPACE}%{WORD:ACT_CG_ALIAS}%{SPACE}%{WORD:ACT_Running_Status}%{SPACE}%{GREEDYDATA:ACT_OM_Session}\",\"%{WORD:SBLSRVR_NAME}%{SPACE}%{WORD:SBLSRVR_STATE}\",\"%{WORD:ServerName}%{SPACE}%{WORD:Comp_Alias}%{SPACE}%{WORD:CompStatus}%{SPACE}%{WORD:CompStartMode}%{SPACE}%{NUMBER:RunningTasks:int}%{SPACE}%{NUMBER:MaxTasks:int}\"] } Output looks as below Siebel Enterprise Applications Siebel Server Manager, Version 16.19.0.0 [23057] LANG_INDEPENDENT Copyright (c) 2008,2016, Oracle. All rights reserved. The Programs (which include both the software and documentation) contain proprietary information; they are provided under a license agreement containing restrictions on use and disclosure and are also protected by copyright, patent, and other intellectual and industrial property laws. Reverse engineering, disassembly, or decompilation of the Programs, except to the extent required to obtain interoperability with other independently created software or as specified by law, is prohibited. Oracle, JD Edwards, PeopleSoft, and Siebel are registered trademarks of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. If you have received this software in error, please notify Oracle Corporation immediately at 1.800.ORACLE1. Type \"help\" for list of commands, \"help <topic>\" for detailed help Connected to 13 server(s) out of a total of 13 server(s) in the enterprise srvrmgr> list active sessions show SV_NAME,CC_ALIAS,CG_ALIAS,TK_DISP_RUNSTATE,OM_LOGIN SV_NAME CC_ALIAS CG_ALIAS TK_DISP_RUNSTATE OM_LOGIN ------------ --------- --------- ---------------- ------------------------ D1220001028A ServerMgr System Running D1220001028A ServerMgr System Running D1220001028A ServerMgr System Running D1220001028A SRProc SystemAux Running D1220001028A SRProc SystemAux Running Forwarding Task D1220001028A SRBroker System Running SRVR:D1220001032A D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running SRVR:D1220001033A D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running SRVR:D1220001031A D1220001028A SRBroker System Running SRVR:D1220001022A D1220001028A SRBroker System Running SRVR:D1220001005A 8A SRBroker System Running SRVR:D1220001019A D1220001028A SRBroker System Running SRVR:D1220001026A D1220001028A SRBroker System Running SRVR:D1220001030A D1220001028A SRBroker System Running SRVR:D1220001017A D1220001028A SRBroker System Running SRVR:D1220001023A D1220001028A SRBroker System Running SRVR:D1220001027A D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running D1220001028A SRBroker System Running Store task D1220001028A SRBroker System Running D1220001028A SRBroker System Running Task creation task D1220001028A SRBroker System Running Response task D1220001028A SRBroker System Running Information caching task D1220001027A ServerMgr System Running D1220001027A ServerMgr System Running D1220001027A ServerMgr System Running D1220001027A SRProc SystemAux Running D1220001027A SRProc SystemAux Running Forwarding Task D1220001027A SRBroker System Running SRVR:D1220001032A D1220001027A SRBroker System Running D1220001027A SRBroker System Running D1220001027A SRBroker System Running D1220001027A SRBroker System Running SRVR:D1220001031A D1220001027A SRBroker System Running SRVR:D1220001033A D1220001027A SRBroker System Running SRVR:D1220001019A D1220001027A SRBroker System Running SRVR:D1220001017A D1220001027A SRBroker System Running SRVR:D1220001026A D1220001027A SRBroker System Running SRVR:D1220001022A D1220001027A SRBroker System Running SRVR:D1220001024A D1220001027A SRBroker System Running SRVR:D1220001005A D1220001027A SRBroker System Running SRVR:D1220001023A D1220001027A SRBroker System Running SRVR:D1220001030A D1220001027A SRBroker System Running D1220001027A SRBroker System Running D1220001027A SRBroker System Running D1220001027A SRBroker System Running D1220001027A SRBroker System Running D1220001027A SRBroker System Running D1220001027A SRBroker System Running D1220001027A SRBroker System Running D1220001027A SRBroker System Running SRVR:D1220001028A D1220001027A SRBroker System Running D1220001027A SRBroker System Running D1220001027A SRBroker System Running Store task D1220001027A SRBroker System Running Task creation task D1220001027A SRBroker System Running Response task D1220001031A SRBroker System Running Task creation task srvrmgr> list comp show SV_NAME,CC_ALIAS,CP_DISP_RUN_STATE,CP_STARTMODE,CP_NUM_RUN_TASKS,CP_MAX_TASKS order by CP_DISP_RUN_STATE SV_NAME CC_ALIAS CP_DISP_RUN_STATE CP_STARTMODE CP_NUM_RUN_TASKS CP_MAX_TASKS ------------ -------------- ----------------- ------------ ---------------- ------------ D1220001027A FSMSrvr Online Auto 0 20 D1220001027A ServerMgr Running Auto 3 20 D1220001027A SRBroker Running Auto 29 100 D1220001027A SRProc Running Auto 2 20 srvrmgr> list server show SBLSRVR_NAME,SBLSRVR_STATE SBLSRVR_NAME SBLSRVR_STATE ------------ ------------- D1220001005A Running D1220001022A Running srvrmgr> If you observe my output , it has 3 different kinds of patterns and hence I am trying to give 3 different grok filters in logstash. Earlier I was using the below filter which works with two filters and when i add 3rd filter , it does not work. grok { match => { \"message\" => [ #Most specific grok: \"%{WORD:ACT_SV_NAME}%{SPACE}%{WORD:ACT_CC_Alias}%{SPACE}%{WORD:ACT_CG_ALIAS}%{SPACE}%{WORD:ACT_Running_Status}%{SPACE}%{WORD:ACT_OM_Session}\", \"%{WORD:SBLSRVR_NAME}%{SPACE}%{WORD:SBLSRVR_STATE}\", \"%{WORD:ServerName}%{SPACE}%{WORD:Comp_Alias}%{SPACE}%{WORD:CompStatus}%{SPACE}%{WORD:CompStartMode}%{SPACE}%{NUMBER:RunningTasks:int}%{SPACE}%{NUMBER:MaxTasks:int}\" ] } Any suggestions please ?",
    "website_area": "discuss"
  },
  {
    "id": "35642d67-ff0e-4ae4-a5b9-17eb3d588777",
    "url": "https://discuss.elastic.co/t/cant-get-iis-log-row-to-match-grok-pattern/215915",
    "title": "Can't get IIS Log row to match Grok-pattern",
    "category": [
      "Logstash"
    ],
    "author": "johanwallenborg",
    "date": "January 21, 2020, 4:19pm January 22, 2020, 2:56pm January 22, 2020, 10:59am",
    "body": "Hello, I'm getting grokfailures on some of the lines on my IIS log parsing and currently i'm banging my head towards a wall. My pattern look like this: %{TIMESTAMP_ISO8601:iis.access.time} %{IPORHOST:destination.address} %{WORD:http.request.method} %{URIPATH:url.path} %{NOTSPACE:url.query} %{NUMBER:destination.port:long} %{NOTSPACE:user.name} %{IPORHOST:source.address} %{NOTSPACE:user_agent.original} %{NOTSPACE:http.request.referrer} %{NUMBER:http.response.status_code:long} %{NUMBER:iis.access.sub_status:long} %{NUMBER:iis.access.win32_status:long} %{NUMBER:http.response.body.bytes:long} %{NUMBER:http.request.body.bytes:long} %{NUMBER:event.duration:long} %{IPORHOST:temp.proxy.ip} And I want to match this row (probably more later): 2019-09-30 00:34:07 1.1.1.1 GET / - 443 - 2.2.2.2 Pingdom.com_bot_version_1.4_(http://www.pingdom.com/),Pingdom.com_bot_version_1.4_(http://www.pingdom.com/) - 200 0 0 40844 206 204 - Can you see see something obvious?",
    "website_area": "discuss"
  },
  {
    "id": "d308b2c7-9d1a-4fd2-8739-9540fe26c946",
    "url": "https://discuss.elastic.co/t/error-with-logstash-csv-filter-plugin/215879",
    "title": "Error with Logstash CSV Filter plugin",
    "category": [
      "Logstash"
    ],
    "author": "mhyager",
    "date": "January 21, 2020, 12:59pm January 21, 2020, 3:47pm January 22, 2020, 10:52am",
    "body": "I am having strange errors with a specific word used in Column headers. All automatic column header recognition works fine, except for when column headers are called FName. Is this a limitation of the CSV Filter plugin? The Error Log: [2020-01-21T13:13:55,817][WARN ][logstash.outputs.elasticsearch][pipelineLLM] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"llmlogs-2020.01-000001\", :routing=>nil, :_type=>\"_doc\"}, #<LogStash::Event:0x11a7198>], :response=>{\"index\"=>{\"_index\"=>\"llmlogs-2020.01-000001\", \"_type\"=>\"_doc\", \"_id\"=>\"k9cGyG8BK6Mt9ZOQBvTl\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse field [FName] of type [float] in document with id 'k9cGyG8BK6Mt9ZOQBvTl'. Preview of field's value: 'FName'\", \"caused_by\"=>{\"type\"=>\"number_format_exception\", \"reason\"=>\"For input string: \\\"FName\\\"\"}}}}}",
    "website_area": "discuss"
  },
  {
    "id": "23eec837-c14d-406e-b672-7ce656a1b19b",
    "url": "https://discuss.elastic.co/t/getting-nested-object-with-json-filter/215921",
    "title": "Getting nested object with json filter",
    "category": [
      "Logstash"
    ],
    "author": "toms130",
    "date": "January 21, 2020, 4:46pm January 21, 2020, 8:18pm January 22, 2020, 9:20am January 22, 2020, 9:26am January 22, 2020, 10:08am January 22, 2020, 10:09am January 22, 2020, 10:09am January 22, 2020, 10:47am January 22, 2020, 10:47am January 22, 2020, 10:49am",
    "body": "Hi all, I've got problem when trying to get some nested fields in a json object with json filter. My field look like this { \"total_count\" : 3, \"orders\" : [ { \"marketplace_order_id\" : \"malikilledme3\", \"order_lines\" : [ { \"order_line_status_pending\" : \"PENDING SHIPMENT\", \"order_line_status_current\" : \"TO SHIP\", \"marketplace_order_line_id\" : \"malikilledme3-1\" }, { \"order_line_status_pending\" : \"PENDING SHIPMENT\", \"order_line_status_current\" : \"TO SHIP\", \"marketplace_order_line_id\" : \"malikilledme3-0\" } ] }, Original field is [http][response][body][content], I want to get value from [http][response][body][content][orders][marketplace_order_id] I've extracted top level field , but \"orders\" value is a nested json, so I tried to parse it ith json filter too json { source => \"[http][response][body][content]\" target => \"[http][response][json]\" } json { source => \"[http][response][json][orders]\" target => \"[http][response][json][orders_json]\" } But when I do this, I got a java class error exception. 2020-01-21T17:23:37,932][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:api_ws], :non_running_pipelines=>} [2020-01-21T17:23:43,428][WARN ][logstash.filters.json ] Error parsing json {:source=>\"[http][response][json][orders]\", :raw=>[{\"marketplace_order_id\"=>\"malikilledme3\", \"order_lines\"=>[{\"marketplace_order_line_id\"=>\"malikilledme3-1\", \"order_line_status_pending\"=>\"PENDING SHIPMENT\", \"order_line_status_current\"=>\"TO SHIP\"}, {\"marketplace_order_line_id\"=>\"malikilledme3-0\", \"order_line_status_pending\"=>\"PENDING SHIPMENT\", \"order_line_status_current\"=>\"TO SHIP\"}]}, {\"marketplace_order_id\"=>\"malikilledme_again\", \"order_lines\"=>[{\"marketplace_order_line_id\"=>\"malikilledme_again-1\", \"order_line_status_pending\"=>\"PENDING SHIPMENT\", \"order_line_status_current\"=>\"TO SHIP\"}, {\"marketplace_order_line_id\"=>\"malikilledme_again-0\", \"order_line_status_pending\"=>\"PENDING SHIPMENT\", \"order_line_status_current\"=>\"TO SHIP\"}]}, {\"marketplace_order_id\"=>\"ws1-3_test_validation1\", \"order_lines\"=>[{\"marketplace_order_line_id\"=>\"ws1-3_test_validation1-0\", \"order_line_status_pending\"=>\"PENDING SHIPMENT\", \"order_line_status_current\"=>\"SHIPPED\"}, {\"marketplace_order_line_id\"=>\"ws1-3_test_validation1-1\", \"order_line_status_pending\"=>\"PENDING REFUND\", \"order_line_status_current\"=>\"SHIPPED\"}]}], :exception=>java.lang.ClassCastException} Any idea ? regards thomas",
    "website_area": "discuss"
  },
  {
    "id": "bc68edcb-3b21-4977-9a36-db13b96049a2",
    "url": "https://discuss.elastic.co/t/change-string-to-hours-date-filters/215834",
    "title": "Change String to Hours - date filters",
    "category": [
      "Logstash"
    ],
    "author": "katara",
    "date": "January 21, 2020, 8:19am January 21, 2020, 11:10am January 21, 2020, 11:32am January 22, 2020, 4:09am January 22, 2020, 9:11am January 22, 2020, 9:51am January 22, 2020, 10:01am January 22, 2020, 10:33am January 22, 2020, 10:48am",
    "body": "Hello everyone, I have a column like this: business_time_left 3 Hours 24 Minutes 59 Minutes 4 Days 23 Hours 58 Minutes 0 Seconds 1 Hour and so on.. What I want to do in Logstash is to convert this entirely into hours. So mu value should entirety convert to something like business_time_left 3.24 0.59 119.58 0 1 Is this possible? My config file: http_poller { urls => { snowinc => { url => \"https://service-now.com\" user => \"your_user\" password => \"yourpassword\" headers => {Accept => \"application/json\"} } } request_timeout => 60 metadata_target => \"http_poller_metadata\" schedule => { cron => \"* * * * * UTC\"} codec => \"json\" } } filter { json {source => \"result\" } split{ field => [\"result\"] } } output { elasticsearch { hosts => [\"yourelastuicIP\"] index => \"inc\" action=>update document_id => \"%{[result][number]}\" doc_as_upsert =>true } stdout { codec => rubydebug } } Sample Json input data, when the url is hit. {\"result\":[ { \"made_sla\":\"true\", \"Type\":\"incident resolution p3\", \"sys_updated_on\":\"2019-12-23 05:00:00\" \"business_time_left\":\" 59 Minutes\"} , { \"made_sla\":\"true\", \"Type\":\"incident resolution l1.5 p4\", \"sys_updated_on\":\"2019-12-24 07:00:00\" \"business_time_left\":\"3 Hours 24 Minutes\"}]} Thanks in advance Katara.",
    "website_area": "discuss"
  },
  {
    "id": "7cbfcd9e-fe72-4b8f-b48d-adb782ca5f12",
    "url": "https://discuss.elastic.co/t/tibco-ems-plugin-connection/216024",
    "title": "TIBCO EMS plugin connection",
    "category": [
      "Logstash"
    ],
    "author": "sumitsinghtomar09",
    "date": "January 22, 2020, 9:57am January 22, 2020, 10:16am",
    "body": "Hi All, I am trying to connect my local tibco ems server from logstanch, i am getting below error. Sending Logstash logs to C:/Users/vv19sn/Downloads/logstash-7.5.1/logstash-7.5.1/logs which is now configured via log4j2.properties [2020-01-22T15:20:37,441][FATAL][logstash.runner ] An unexpected error occurred! {:error=>#<ArgumentError: Setting \"ems.factory\" hasn't been registered>, :backtrace=>[\"C:/Users/vv19sn/Downloads/logstash-7.5.1/logstash-7.5.1/logstash-core/lib/logstash/settings.rb:69:in get_setting'\", \"C:/Users/vv19sn/Downloads/logstash-7.5.1/logstash-7.5.1/logstash-core/lib/logstash/settings.rb:102:in set_value'\", \"C:/Users/vv19sn/Downloads/logstash-7.5.1/logstash-7.5.1/logstash-core/lib/logstash/settings.rb:121:in block in merge'\", \"org/jruby/RubyHash.java:1417:in each'\", \"C:/Users/vv19sn/Downloads/logstash-7.5.1/logstash-7.5.1/logstash-core/lib/logstash/settings.rb:121:in merge'\", \"C:/Users/vv19sn/Downloads/logstash-7.5.1/logstash-7.5.1/logstash-core/lib/logstash/settings.rb:179:in validate_all'\", \"C:/Users/vv19sn/Downloads/logstash-7.5.1/logstash-7.5.1/logstash-core/lib/logstash/runner.rb:284:in execute'\", \"C:/Users/vv19sn/Downloads/logstash-7.5.1/logstash-7.5.1/vendor/bundle/jruby/2.5.0/gems/clamp-0.6.5/lib/clamp/command.rb:67:in run'\", \"C:/Users/vv19sn/Downloads/logstash-7.5.1/logstash-7.5.1/logstash-core/lib/logstash/runner.rb:242:in run'\", \"C:/Users/vv19sn/Downloads/logstash-7.5.1/logstash-7.5.1/vendor/bundle/jruby/2.5.0/gems/clamp-0.6.5/lib/clamp/command.rb:132:in run'\", \"C:\\Users\\vv19sn\\Downloads\\logstash-7.5.1\\logstash-7.5.1\\lib\\bootstrap\\environment.rb:73:in `'\"]} [2020-01-22T15:20:37,613][ERROR][org.logstash.Logstash ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit Content of logstach.yml file. ems: com.tibco.tibjms.TibjmsConnectionFactory :serverUrl: tcp://localhost:7222 :username: admin :password: :require_jars: - C:\\tibco\\Designer_for_BW_5.14.0\\ems\\8.3\\lib\\jms-2.0.jar - C:\\tibco\\Designer_for_BW_5.14.0\\ems\\8.3\\lib\\tibjms.jar - C:\\tibco\\Designer_for_BW_5.14.0\\ems\\8.3\\lib\\tibcrypt.jar content of logstach.conf file. input { jms { destination => \"queue.sample\" include_body => true include_header => false include_properties => false interval => 10 timeout => -1 threads => 20 use_jms_timestamp => false yaml_file => \"\\logstash.yml\" yaml_section => \"ems\" } } output{ elasticsearch { hosts => [\"localhost:9200\"] index => \"indextibco\" } }",
    "website_area": "discuss"
  },
  {
    "id": "55e8aac3-3af9-4488-9476-c7d3e12ba2be",
    "url": "https://discuss.elastic.co/t/how-to-get-the-nested-array-and-nested-json-from-input-message-in-logstash/215693",
    "title": "How to get the nested array and nested json from input message in logstash",
    "category": [
      "Logstash"
    ],
    "author": "Amit_Agarwal1",
    "date": "January 20, 2020, 9:21am January 20, 2020, 1:58pm January 21, 2020, 6:22am January 21, 2020, 6:39am January 21, 2020, 6:54am January 21, 2020, 10:34am January 21, 2020, 10:51am January 21, 2020, 10:52am January 21, 2020, 11:00am January 21, 2020, 12:53pm January 22, 2020, 9:27am January 22, 2020, 9:43am",
    "body": "message : {\"prdctId\":\"10066\",\"id\":\"10066\",\"owner\":{\"id\":\"E040547\",\"name\":\"Shashi Raghunandan\"},\"regAvail\":[\"ap\",\"lac\",\"mea\"] ,\"isActive\":true} Applied Filter: kv { source => \"message\" trim_key => \"\"\" trim_value => \"\"\" field_split => \",\" value_split => \":\" include_keys => [ \"prdctId\",\"id\", \"owner\", \"name\", \"isActive\", \"regAvail\" ] } mutate { remove_field => [\"splitmsg\", \"@version\", \"@timestamp\", \"message\"] } Output: The response coming with field names - prdctId, id, name, isActive but not showing for owner since it is a nested json, also for regAvail which is itself a nested array. { \"_id\" : \"10066\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"ComPass\", \"prdctId\" : \"10066\", \"isAvtive\" : true, \"id\" : \"10066\" } } Please let me know what filter should be applied to get owner nested json and regAvail nested array in output.",
    "website_area": "discuss"
  },
  {
    "id": "7052feac-26b8-4694-8bfb-b0b5aaedbde8",
    "url": "https://discuss.elastic.co/t/logstash-wait-for-elasticsearch/216012",
    "title": "Logstash wait for Elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "marcohald",
    "date": "January 22, 2020, 8:58am",
    "body": "Hi, is there a config variable which defines the retry count to elasticsearch on startup? In the moment it does a retry 10 times within 16 seconds. I already tried this as mentioned here https://stackoverflow.com/questions/44320705/understanding-the-logstash-retry-policy retry_max_interval => 600 retry_initial_interval => 20 But this changed nothing. Is there a way to configure more retries ?",
    "website_area": "discuss"
  },
  {
    "id": "c4f107f2-2a5e-4643-aa0f-d934975ba877",
    "url": "https://discuss.elastic.co/t/trying-to-get-the-data-from-oracle-database-through-logstash-but-data-is-not-coming-to-elasticsearch/215678",
    "title": "Trying to get the data from oracle database through logstash but data is not coming to elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "uday_kumar1",
    "date": "January 21, 2020, 6:48am January 21, 2020, 6:48am January 21, 2020, 8:33am January 22, 2020, 6:52am",
    "body": "I am trying to get the data of oracle database through logstash but data is not coming to elasticsearch. not sure where i was missed. I didn't see any error on logstash log file. below are the logstash conf file. input { jdbc { jdbc_validate_connection => \"true\" jdbc_connection_string => \"jdbc:oracle:thin:@//server:1521/db\" jdbc_user => \"user\" jdbc_password => \"pass\" jdbc_driver_library => \"/etc/logstash/files/ojdbc7.jar\" jdbc_driver_class => \"Java::oracle.jdbc.driver.OracleDriver\" jdbc_paging_enabled => \"true\" schedule => \"* * * * *\" statement_filepath => \"/etc/logstash/files/keycount.sql\" use_column_value => \"true\" tracking_column => \"timestamp\" last_run_metadata_path => \"/etc/logstash/files/.logstash_jdbc_last_run\" } } output { elasticsearch { hosts => \"localhost:9200\" index => \"keyinventory-%{+YYYY}\" } } Please some one help me, where I was missing. Thanks and regards, Uday.",
    "website_area": "discuss"
  },
  {
    "id": "224c649a-dc21-44c3-bd9c-c6d02a2ffee8",
    "url": "https://discuss.elastic.co/t/mib-oid-translation/29710",
    "title": "MIB OID Translation",
    "category": [
      "Logstash"
    ],
    "author": "tom_jonge",
    "date": "September 21, 2015, 3:34pm April 15, 2016, 5:44pm September 22, 2015, 12:41pm September 22, 2015, 1:17pm September 22, 2015, 1:31pm September 22, 2015, 1:36pm September 22, 2015, 1:46pm September 22, 2015, 2:04pm September 22, 2015, 2:11pm September 22, 2015, 2:13pm September 22, 2015, 2:15pm September 22, 2015, 2:50pm September 22, 2015, 3:23pm September 22, 2015, 5:50pm September 23, 2015, 7:40am September 23, 2015, 1:24pm February 2, 2016, 9:16am April 14, 2016, 3:23pm April 15, 2016, 5:43pm April 15, 2016, 11:21pm",
    "body": "Dear logstash, /opt/logstash/vendor/bundle/jruby/1.9/gems/snmp-1.2.0/data/ruby/snmp/mibs containts a venerable treasure trove of pre-converted YAML MIBs. However I have run into an issue: From Kibana: RFC1155-SMI::enterprises.12356.101.9.3.2.0 172.16.20.72 The start of the translated YAML MIB from Fortinet below has (it is my understanding) 0. as a shorthand to RFC1155-SMI::enterprises \\--- fortinet: '0.12356' fnCoreMib: 0.12356.100 As you can see part of the OID is translated. Namely 1.3.6.1.4.1 into RFC1155-SMI::enterprises. It stops at .12356 however. This number is registered at IANA http://www.iana.nl/assignments/enterprise-numbers/enterprise-numbers These enterprises arent centralized, meaning there's no MIB for those values. Apparently this number is lookup up by your snmp-manager. Therein lies my problem. Any way to get the .12356 enterprise number translated by Logstash?",
    "website_area": "discuss"
  },
  {
    "id": "0bbb22d5-dd8b-4d0e-a3e6-34abf22f3346",
    "url": "https://discuss.elastic.co/t/logstash-restful-api-input/215963",
    "title": "Logstash RESTFUL API Input",
    "category": [
      "Logstash"
    ],
    "author": "BDokas",
    "date": "January 21, 2020, 11:26pm",
    "body": "Has anyone had success exposing an endpoint on an AWS EC2 instance running the ELK stack using the Logstash http input plugin? I cannot for the life of me figure out how to hit the specified input.",
    "website_area": "discuss"
  },
  {
    "id": "678fa48a-167e-446e-a793-567491d494da",
    "url": "https://discuss.elastic.co/t/logstash-jdbc-static-filter-plugin-performing-very-poorly-nearly-not-at-all/214292",
    "title": "Logstash jdbc_static filter plugin performing very poorly (nearly not at all)",
    "category": [
      "Logstash"
    ],
    "author": "tomj",
    "date": "January 8, 2020, 8:07pm January 8, 2020, 7:52pm January 8, 2020, 8:06pm January 8, 2020, 8:08pm January 8, 2020, 8:58pm January 8, 2020, 9:36pm January 8, 2020, 9:40pm January 9, 2020, 6:13pm January 9, 2020, 6:54pm January 9, 2020, 7:01pm January 9, 2020, 7:10pm January 9, 2020, 7:57pm January 9, 2020, 8:36pm January 9, 2020, 8:53pm January 14, 2020, 7:23pm January 14, 2020, 8:06pm January 14, 2020, 8:27pm January 14, 2020, 8:31pm January 14, 2020, 8:36pm January 14, 2020, 8:56pm",
    "body": "Running Logstash 7.1.1 on Amazon Linux 2; jdbc_static plugin version 1.0.6 (stock installation for this version of Logstash) Sending data to Elasticsearch 7.1.1 (hosted on AWS Elasticsearch service) Logstash is running on OpenJDK 11 (openjdk version \"11.0.5\" 2019-10-15 LTS) I have a stream of logging events that arrive at a nearly constant rate of 15000 per second. I am attempting to enrich (add) two fields based on two separate databases, one column per database. I have two Logstash instances (8 vCPUs, 64 GB RAM) which are sharing the load evenly and the CPU time consumption is about 15 percent across each vCPU in steady state. These instances are running on AWS EC2 with EBS storage; I am using memory queues (no persistent queues at all). Each Logstash is running 32 workers, with a pipeline batch size of 5120, using the pipeline batch delay of 50 ms. 'top -H' shows that the worker threads are getting an equal share of the available CPU. The pipeline looks like: Input: Kinesis Filters: date -> (jdbc_static) -> fingerprint (MD5) Output: amazon_es These two Logstash instances have no problem keeping up until I add the jdbc_static filter into the filter part of the pipeline. When I add the JDBC filter to the pipeline on one of the two instances, throughput on that instance immediately drops to near zero; oddly, after the first minute, only 9999 events go through the pipeline in aggregate, and then no more progress is made. The second Logstash instance remains healthy and processes the events at the expected rate (about 460000 per minute). When I swap the old .conf file back in, the pipeline stalls, giving this message: [2020-01-08T18:19:17,606][WARN ][org.logstash.execution.ShutdownWatcherExt] {\"inflight_count\"=>0, \"stalling_threads_info\"=>{[\"LogStash::Filters::JdbcStatic\", ... ... so, I stop and start logstash and that allows the instance to return to processing events at the rate of about 460000 per minute. I have another log stream using a single Logstash instance, configured with the same pipeline, supporting a much smaller volume of events (150 per second) which runs fine for days on end. Here is the segment of the pipeline conf file (anonymized, of course) which defines the jdbc_filter: jdbc_static { loaders => [ { id => \"sql-table1\" query => \"SELECT rsid, rsname FROM table1 WHERE IsDeleted = 0\" local_table => \"local-table1\" }, { id => \"sql-table2\" query => \"SELECT ssid, ssname FROM table2 WHERE IsDeleted = 0\" local_table => \"local-table2\" } ] local_db_objects => [ { name => \"local-table1\" index_columns => [\"rsid\"] columns => [ [\"rsid\", \"varchar(36)\"], [\"rsname\", \"varchar(50)\"] ] }, { name => \"local-table2\" index_columns => [\"ssid\"] columns => [ [\"ssid\", \"varchar(36)\"], [\"ssname\", \"varchar(50)\"] ] } ] local_lookups => [ { id => \"local1\" query => \"select rsname from local-table1 WHERE rsid = :rsid\" parameters => {rsid => \"[fields][rsid]\"} target => \"[@metadata][f1]\" }, { id => \"local2\" query => \"select ssname from local-table2 WHERE ssid = :ssid\" parameters => {ssid => \"[fields][ssid]\"} target => \"[@metadata][f2]\" } ] add_field => { \"[fields][field1]\" => \"%{[@metadata][f1][0][rsname]}\" } add_field => { \"[fields][field2]\" => \"%{[@metadata][f2][0][ssname]}\" } staging_directory => \"/tmp/logstash/jdbc_static/import_data\" loader_schedule => \"* */12 * * *\" # run loaders every 12 hours jdbc_user => \"foo\" jdbc_password => \"bar\" jdbc_driver_class => \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" jdbc_driver_library => \"/etc/logstash/mssql-jdbc-7.4.1.jre11.jar\" jdbc_connection_string => \"jdbc:sqlserver://baz.com:1433;databaseName=quux\" } Here are relevant lines from the jdbc_static initialization, as they appear in logstash logs: (table names are anonymized): [2019-12-30T17:23:24,674][INFO ][logstash.filters.jdbcstatic] derby.system.home is: /usr/share/logstash [2019-12-30T17:23:28,760][INFO ][logstash.filters.jdbc.readwritedatabase] loader sql-table1, fetched 1169 records in: 0.521 seconds [2019-12-30T17:23:28,878][INFO ][logstash.filters.jdbc.readwritedatabase] loader sql-table1, saved fetched records to import file in: 0.114 seconds [2019-12-30T17:23:29,281][INFO ][logstash.filters.jdbc.readwritedatabase] loader sql-table1, imported all fetched records in: 0.399 seconds [2019-12-30T17:23:29,413][INFO ][logstash.filters.jdbc.readwritedatabase] loader sql-table2, fetched 2523 records in: 0.13 seconds [2019-12-30T17:23:29,575][INFO ][logstash.filters.jdbc.readwritedatabase] loader sql-table2, saved fetched records to import file in: 0.162 seconds [2019-12-30T17:23:29,643][INFO ][logstash.filters.jdbc.readwritedatabase] loader sql-table2, imported all fetched records in: 0.067 seconds [2019-12-30T17:23:29,658][INFO ][logstash.filters.jdbcstatic] Scheduler operations: Scheduled for: these hours in the day [0, 12];these seconds in the minute [0] [2019-12-30T17:23:29,660][INFO ][logstash.filters.jdbcstatic] Scheduler scan for work frequency is: 2.5 As you can see, the two tables are not large; also the refresh schedule is 12 hours (twice per day). Based on the documentation, these table sizes seem quite reasonable. These tables are exactly the same ones used on my low-volume instance, and they match the results I get from a SQL query issued from a SQL browser. Earlier, I had discovered that the jdbc_static plugin was generating at least one warning message for each document that was missing either the \"rsid\" or \"ssid\" fields; this was contributing to a lot of useless logging. Since our log events are not uniform, not every event will contain a field that would be looked up in the database, the overhead of writing the warning log messages was thought to be the culprit. There were many messages sent to the log file of this form (again, anonymized): [2019-12-30T21:19:38,304][WARN ][logstash.filters.jdbc.lookup] Parameter field not found in event {:lookup_id=>\"local-table1\", :invalid_parameters=>[\"[fields][rsid]\"]} [2019-12-30T21:19:38,304][WARN ][logstash.filters.jdbc.lookup] Parameter field not found in event {:lookup_id=>\"local-table2\", :invalid_parameters=>[\"[fields][ssid]\"]} I suppressed that noise in the log4j configuration using: logger.jdbclookup.name = logstash.filters.jdbc.lookup logger.jdbclookup.level = error No other error or warning messages are reported in the logstash-plain.log file, there are no messages in the \"slowlog\". There doesn't seem to be any additional Java GC activity associated with running the JDBC plugin over and above the simpler pipeline. Questions: What units are the \"Scheduler scan for work frequency\" expressed in? What does the \"scheduler scan\" pertain to? Am I missing some obvious mis-configuration of this pipeline?",
    "website_area": "discuss"
  },
  {
    "id": "f6a99fe1-ed0e-4951-84c6-2b656e140ecd",
    "url": "https://discuss.elastic.co/t/logstash-inputs-mongodb-fail-bson-1-is-an-invalid-objectid/215570",
    "title": "Logstash.inputs.mongodb, fail <BSON::ObjectId::Invalid: '1' is an invalid ObjectId.>",
    "category": [
      "Logstash"
    ],
    "author": "angelma83",
    "date": "January 18, 2020, 5:44pm January 21, 2020, 10:34pm",
    "body": "HI everybody, i am using logstash-input-mongodb, and i have the following mistake, [logstash.inputs.mongodb ] MongoDB Input threw an exception, restarting {:exception=>#<BSON::ObjectId::Invalid: '1' is an invalid ObjectId.>} Please can anybody help me. thanks in advance best regards",
    "website_area": "discuss"
  },
  {
    "id": "918039bd-c1ac-4b90-9fb9-12162ad7c5f9",
    "url": "https://discuss.elastic.co/t/logstash-jdbc-input-plugin-configuration-with-ssl-cert-information/215639",
    "title": "Logstash Jdbc input Plugin configuration with SSL cert information",
    "category": [
      "Logstash"
    ],
    "author": "mullan",
    "date": "January 19, 2020, 5:58pm January 21, 2020, 10:27pm",
    "body": "Hi, Iam new to Logstash. My project requires to configure logstash with jdbc input plugin.Below is the conf file I had used. input { jdbc { jdbc_driver_library => \"lib\\jars\\mongodb_unityjdbc_full.jar\" jdbc_driver_class => \"Java::mongodb.jdbc.MongoDriver\" jdbc_connection_string => \"jdbc:mongo://testdb:27017/dbtest? ssl=true&authMechanism=SCRAM-SHA-1\" jdbc_user => \"test\" jdbc_password => \"Asdf~123\" jdbc_validate_connection => true statement => \"Select userId as id,firstName as firstname, lastName as lastname From IntUser\" } } I have my SSL CA and key file . How do I configure my conf file to set the those cert information ? Any help or guidance on this would be of great help. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "7a608ec9-61bf-4dee-b27f-b2f7b1476d6e",
    "url": "https://discuss.elastic.co/t/logstash-stopped-accepting-events-from-winlogbeat-since-the-new-year/215941",
    "title": "Logstash stopped accepting events from winlogbeat since the new year",
    "category": [
      "Logstash"
    ],
    "author": "albbapm",
    "date": "January 21, 2020, 8:46pm January 21, 2020, 7:51pm January 21, 2020, 8:38pm",
    "body": "Posting this here as I think this is a logstash problem although I'm not really sure. ELK version 6.7.2 Winlogbeat version is a mix of 6.7.2 and 6.3.2 At midnight on January 1st, all winlogbeat events stopped going in to logstash and nothing has changed in our environment. I went from 26 million logs on December 31 to 0 on January 1. I have had a few events here and there sneak in but nothing consistent. Maybe a thousand in 1 day. Logstash logs show a 403 forbidden/8.index write error, however it looks like those started around the 5th of December so I don't think that error is related to my issue. Elasticsearch shows nothing. I have tried sending the events directly to elasticsearch and that works so I don't think it is a client issue. Logs from the client: C:\\Program Files\\Winlogbeat>winlogbeat.exe -e -c winlogbeat.yml 2020-01-21T14:32:23.982-0500 INFO instance/beat.go:611 Home path: [C:\\Program Files\\Winlogbeat] Config path: [C:\\Program Files\\Winlogbeat] Data path: [C:\\Program Files\\Winlogbeat\\data] Logs path: [C:\\Program Files\\Winlogbeat\\logs] 2020-01-21T14:32:23.984-0500 INFO instance/beat.go:618 Beat UUID: b36cbe30-cf3e-4de7-aed9-16c3f4cdc7f3 2020-01-21T14:32:23.987-0500 INFO [beat] instance/beat.go:931 Beat info {\"system_info\": {\"beat\": {\"path\": {\"config\": \"C:\\Program Files\\Winlogbeat\", \"data\": \"C:\\Program Files\\Winlogbeat\\data\", \"home\": \"C:\\Program Files\\Winlogbeat\", \"logs\": \"C:\\Program Files\\Winlogbeat\\logs\"}, \"type\": \"winlogbeat\", \"uuid\": \"b36cbe30-cf3e-4de7-aed9-16c3f4cdc7f3\"}}} 2020-01-21T14:32:23.987-0500 INFO [beat] instance/beat.go:940 Build info {\"system_info\": {\"build\": {\"commit\": \"a8ab26dd1f818d27c17c3049f643652c6a789d88\", \"libbeat\": \"6.7.2\", \"time\": \"2019-04-29T08:23:56.000Z\", \"version\": \"6.7.2\"}}} 2020-01-21T14:32:23.988-0500 INFO [beat] instance/beat.go:943 Go runtime info {\"system_info\": {\"go\": {\"os\":\"windows\",\"arch\":\"amd64\",\"max_procs\":2,\"version\":\"go1.10.8\"}}} 2020-01-21T14:32:23.996-0500 INFO [beat] instance/beat.go:947 Host info {\"system_info\": {\"host\": {\"architecture\":\"x86_64\",\"boot_time\":\"2019-10-30T12:02:09.92-04:00\",\"name\":\"BAPM31001\",\"ip\":[\"172.32.7.60/24\",\"::1/128\",\"127.0.0.1/8\"],\"kernel_version\":\"10.0.17134.1 (WinBuild.160101.0800)\",\"mac\":[\"00:50:56:a6:89:dd\"],\"os\":{\"family\":\"windows\",\"platform\":\"windows\",\"name\":\"Windows 10 Pro\",\"version\":\"10.0\",\"major\":10,\"minor\":0,\"patch\":0,\"build\":\"17134.1\"},\"timezone\":\"EST\",\"timezone_offset_sec\":-18000,\"id\":\"e1c2f043-6a3f-4945-ab0b-53307a97606b\"}}} 2020-01-21T14:32:24.004-0500 INFO [beat] instance/beat.go:976 Process info {\"system_info\": {\"process\": {\"cwd\": \"C:\\Program Files\\Winlogbeat\", \"exe\": \"C:\\Program Files\\Winlogbeat\\winlogbeat.exe\", \"name\": \"winlogbeat.exe\", \"pid\": 42312, \"ppid\": 42160, \"start_time\": \"2020-01-21T14:32:23.883-0500\"}}} 2020-01-21T14:32:24.006-0500 INFO instance/beat.go:280 Setup Beat: winlogbeat; Version: 6.7.2 2020-01-21T14:32:24.007-0500 INFO [publisher] pipeline/module.go:110 Beat name: BAPM31001 2020-01-21T14:32:24.011-0500 INFO beater/winlogbeat.go:68 State will be read from and persisted to C:\\Program Files\\Winlogbeat\\data.winlogbeat.yml 2020-01-21T14:32:24.013-0500 INFO elasticsearch/client.go:164 Elasticsearch url: http://elasticsearch_address_here:9200 2020-01-21T14:32:24.013-0500 INFO instance/beat.go:402 winlogbeat start running. 2020-01-21T14:32:24.034-0500 INFO [monitoring] log/log.go:117 Starting metrics logging every 30s 2020-01-21T14:32:24.100-0500 INFO [monitoring] elasticsearch/elasticsearch.go:247 Successfully connected to X-Pack Monitoring endpoint. 2020-01-21T14:32:24.115-0500 INFO [monitoring] elasticsearch/elasticsearch.go:261 Start monitoring stats metrics snapshot loop with period 10s. 2020-01-21T14:32:24.138-0500 INFO [monitoring] elasticsearch/elasticsearch.go:261 Start monitoring state metrics snapshot loop with period 1m0s. 2020-01-21T14:32:25.378-0500 INFO pipeline/output.go:95 Connecting to backoff(async(tcp://logstash_address_here:5045)) 2020-01-21T14:32:25.460-0500 INFO pipeline/output.go:105 Connection to backoff(async(tcp://logstash_address_here:5045)) established 2020-01-21T14:32:34.285-0500 INFO pipeline/output.go:95 Connecting to backoff(publish(elasticsearch(http://logstash_address_here:9200))) 2020-01-21T14:32:34.319-0500 INFO pipeline/output.go:105 Connection to backoff(publish(elasticsearch(http://logstash_address_here:9200))) established 2020-01-21T14:32:54.042-0500 INFO [monitoring] log/log.go:144 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":375,\"time\":{\"ms\":375}},\"total\":{\"ticks\":2375,\"time\":{\"ms\":2375},\"value\":2375},\"user\":{\"ticks\":2000,\"time\":{\"ms\":2000}}},\"handles\":{\"open\":370},\"info\":{\"ephemeral_id\":\"d7e61c20-a170-4fcd-bd0e-d60f89d068f6\",\"uptime\":{\"ms\":30135}},\"memstats\":{\"gc_next\":24508304,\"memory_alloc\":12812480,\"memory_total\":89719496,\"rss\":42831872}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"output\":{\"events\":{\"active\":2542,\"batches\":3,\"total\":2542},\"read\":{\"bytes\":3074},\"type\":\"logstash\",\"write\":{\"bytes\":317548}},\"pipeline\":{\"clients\":3,\"events\":{\"active\":2543,\"published\":2543,\"retry\":1404,\"total\":2543}}},\"msg_file_cache\":{\"ApplicationHits\":191,\"ApplicationMisses\":10,\"ApplicationSize\":10,\"SecurityHits\":1838,\"SecurityMisses\":1,\"SecuritySize\":1,\"SystemHits\":497,\"SystemMisses\":7,\"SystemSize\":7},\"system\":{\"cpu\":{\"cores\":2}}}}} Debug log from the client adds the following: 2020-01-21T14:34:37.996-0500 DEBUG [publish] pipeline/client.go:201 Pipeline client receives callback 'onDroppedOnPublish' for event: %+v{2020-01-19 12:41:55.5771433 +0000 UTC null {\"activity_id\":\"{2AAD78B0-8F3B-0000-3179-AD2A3B8FD501}\",\"computer_name\":\"computer.bapm.com\",\"event_data\":{\"AuditPolicyChanges\":\"%%8451\",\"CategoryId\":\"%%8279\",\"SubcategoryGuid\":\"{0CCE923E-69AE-11D9-BED3-505054503030}\",\"SubcategoryId\":\"%%14083\",\"SubjectDomainName\":\"BAPM.COM\",\"SubjectLogonId\":\"0x3e7\",\"SubjectUserName\":\"Computer$\",\"SubjectUserSid\":\"S-1-5-18\"},\"event_id\":4719,\"keywords\":[\"Audit Success\"],\"level\":\"Information\",\"log_name\":\"Security\",\"message\":\"System audit policy was changed.\\n\\nSubject:\\n\\tSecurity ID:\\t\\tS-1-5-18\\n\\tAccount Name:\\t\\tcomputer$\\n\\tAccount Domain:\\t\\tBAPM.COM\\n\\tLogon ID:\\t\\t0x3E7\\n\\nAudit Policy Change:\\n\\tCategory:\\t\\tDS Access\\n\\tSubcategory:\\t\\tDetailed Directory Service Replication\\n\\tSubcategory GUID:\\t{0CCE923E-69AE-11D9-BED3-505054503030}\\n\\tChanges:\\t\\tFailure added\",\"opcode\":\"Info\",\"process_id\":684,\"provider_guid\":\"{54849625-5478-4994-A5BA-3E3B0328C30D}\",\"record_number\":\"309277\",\"source_name\":\"Microsoft-Windows-Security-Auditing\",\"task\":\"Audit Policy Change\",\"thread_id\":38148,\"type\":\"wineventlog\"} {Security 309277 2020-01-19 12:41:55.5771433 +0000 UTC }} Config file for winlogbeat: > > #======================= Winlogbeat specific options ========================== > > # event_logs specifies a list of event logs to monitor as well as any > # accompanying options. The YAML data type of event_logs is a list of > # dictionaries. > # > # The supported keys are name (required), tags, fields, fields_under_root, > # forwarded, ignore_older, level, event_id, provider, and include_xml. Please > # visit the documentation for the complete details of each option. > # https://go.es.io/WinlogbeatConfig > winlogbeat.event_logs: > - name: Application > ignore_older: 72h > - name: Security > ignore_older: 72h > - name: System > ignore_older: 72h > > #==================== Elasticsearch template setting ========================== > > #setup.template.settings: > # index.number_of_shards: 3 > #index.codec: best_compression > #_source.enabled: false > > #================================ General ===================================== > > # The name of the shipper that publishes the network data. It can be used to group > # all the transactions sent by a single shipper in the web interface. > #name: > > # The tags of the shipper are included in their own field with each > # transaction published. > #tags: [\"service-X\", \"web-tier\"] > > # Optional fields that you can specify to add additional information to the > # output. > #fields: > # env: staging > > > #============================== Dashboards ===================================== > # These settings control loading the sample dashboards to the Kibana index. Loading > # the dashboards is disabled by default and can be enabled either by setting the > # options here, or by using the -setup CLI flag or the setup command. > #setup.dashboards.enabled: false > > # The URL from where to download the dashboards archive. By default this URL > # has a value which is computed based on the Beat name and version. For released > # versions, this URL points to the dashboard archive on the artifacts.elastic.co > # website. > #setup.dashboards.url: > > #============================== Kibana ===================================== > > # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. > # This requires a Kibana endpoint configuration. > setup.kibana: > > # Kibana Host > # Scheme and port can be left out and will be set to the default (http and 5601) > # In case you specify and additional path, the scheme is required: http://localhost:5601/path > # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 > #host: \"localhost:5601\" > > #============================= Elastic Cloud ================================== > > # These settings simplify using winlogbeat with the Elastic Cloud (https://cloud.elastic.co/). > > # The cloud.id setting overwrites the output.elasticsearch.hosts and > # setup.kibana.host options. > # You can find the cloud.id in the Elastic Cloud web UI. > #cloud.id: > > # The cloud.auth setting overwrites the output.elasticsearch.username and > # output.elasticsearch.password settings. The format is <user>:<pass>. > #cloud.auth: > > #================================ Outputs ===================================== > > # Configure what output to use when sending the data collected by the beat. > > #-------------------------- Elasticsearch output ------------------------------ > #output.elasticsearch: > # Array of hosts to connect to. > # hosts: [\"localhost:9200\"] > > # Optional protocol and basic auth credentials. > #protocol: \"https\" > #username: \"elastic\" > #password: \"changeme\" > > #----------------------------- Logstash output -------------------------------- > output.logstash: > # The Logstash hosts > # 5044 is unsecured and Servers > # 5045 is secured and for workstations. > hosts: [\"logstash_address:5045\"] > > # Enable SSL support. SSL is automatically enabled, if any SSL setting is set. > ssl.enabled: true > > # Configure SSL verification mode. If none is configured, all server hosts > # and certificates will be accepted. In this mode, SSL based connections are > # susceptible to man-in-the-middle attacks. Use only for testing. Default is > # full. > ssl.verification_mode: full > > # Optional SSL. By default is off. > # List of root certificates for HTTPS server verifications > #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] > > # Certificate for SSL client authentication > #ssl.certificate: \"/etc/pki/client/cert.pem\" > > # Client Certificate Key > #ssl.key: \"/etc/pki/client/cert.key\" > > #================================ Logging ===================================== > > # Sets log level. The default log level is info. > # Available log levels are: critical, error, warning, info, debug > #logging.level: debug > > # At debug level, you can selectively enable logging only for some components. > # To enable all selectors use [\"\"]. Examples of other selectors are \"beat\", > # \"publish\", \"service\". > #logging.selectors: [\"\"] > #============================== Xpack Monitoring ===================================== > xpack.monitoring: > enabled: true > elasticsearch: > hosts: [\"http://elasticsearch_address:9200\"] > username: username > password: password",
    "website_area": "discuss"
  },
  {
    "id": "62692152-8fd5-4fae-8d0d-110e2deffe57",
    "url": "https://discuss.elastic.co/t/date-filter-not-working-logstash/215880",
    "title": "Date filter not working - Logstash",
    "category": [
      "Logstash"
    ],
    "author": "Shahid_Mustafa",
    "date": "January 21, 2020, 6:36pm January 21, 2020, 1:10pm January 21, 2020, 1:20pm January 21, 2020, 1:22pm January 21, 2020, 6:37pm January 21, 2020, 8:15pm January 21, 2020, 8:34pm",
    "body": "I am trying to use following date filter to convert string to date but it doesn't seem to be working. Sample input data(string) - Jan 20 09:20:35 GMT 2020 I am first using a mutate gsub to remove GMT which renders following string output- Jan 20 09:20:35 2020 My gsub mutate filter looks like this - mutate { gsub => [ \"TimeStamp\", \"GMT\", \"\" ] } Now, I am using a date filter to convert gsub output to date format but it doesn't seem to be working- date { match => [ \"TimeStamp\", \"EEE MMM dd HH:mm:ss yyyy\" ] target => \"TimeStamp\" locale => \"en\" } I have also tried following with no success- date { match => [ \"TimeStamp\", \"EEE\\sMMM\\sdd\\sHH:mm:ss\\s+yyyy\" ] target => \"TimeStamp\" timezone => \"Etc/GMT\" locale => \"en\" }",
    "website_area": "discuss"
  },
  {
    "id": "07bc5aff-e376-4ea8-9c96-86fa929391ed",
    "url": "https://discuss.elastic.co/t/logstash-output-mongodb-issues-with-documentdb/215929",
    "title": "Logstash-output-mongodb issues with DocumentDB",
    "category": [
      "Logstash"
    ],
    "author": "Wayne_Taylor",
    "date": "January 21, 2020, 5:23pm January 21, 2020, 6:16pm",
    "body": "Hi, Has anyone tried accessing DocumentDB (essentially Mongo under hood) via logstash-ouput-mongodb. In logstash 7.5.1 I am getting the following error: logstash.outputs.mongodb ][main] MONGODB | Unsupported URI option 'sslPEMKeyFile' on URI 'mongodb://url:27017/?ssl=true&sslPEMKeyFile=~/Downloads/rds-combined-ca-bundle.pem'. It will be ignored. I've tried all different options including those listed: https://stackoverflow.com/questions/37431697/mongodb-ssl-pem-file-in-connection-string and no success. Any ideas?",
    "website_area": "discuss"
  },
  {
    "id": "6fc2bb44-00d4-4c08-8f3b-3c9ec13ad606",
    "url": "https://discuss.elastic.co/t/logstash-yum-updates-wont-reinstall-my-modules/215935",
    "title": "Logstash yum updates won't reinstall my modules",
    "category": [
      "Logstash"
    ],
    "author": "daniel_a",
    "date": "January 21, 2020, 5:53pm",
    "body": "I installed Logstash via yum, and now every time yum pushes the new Logstash package, it won't reinstall any of the modules I installed myself. For example, I have to reinstall logstash-input-google_pubsub module after yum updates Logstash. Is there a way to change it?",
    "website_area": "discuss"
  },
  {
    "id": "705bf3e5-1338-4854-a9d6-96406e21fcd6",
    "url": "https://discuss.elastic.co/t/parse-logs-in-logstash/215805",
    "title": "Parse Logs in Logstash",
    "category": [
      "Logstash"
    ],
    "author": "SerSSH",
    "date": "January 20, 2020, 11:20pm January 21, 2020, 2:36am January 21, 2020, 11:23am January 21, 2020, 2:45pm January 21, 2020, 3:44pm",
    "body": "Hi guys, I need help with the parsing of some logs. I have tried to do them with the grok plugin, however I need to optimize this for a large number of logs that have different formats and different fields or variables. The variables are currently separated by spaces between them, so the kv plugin has not worked for me, this only brings me the first word that is close to the colon (:). A log is as follows: Jan 20 17:40:04 btasdbora04 Oracle Audit[7687]: [ID 748625 local1.warning] LENGTH : '422' ACTION :[97] ' SELECT :\"SYS_B_0\", status, :\"SYS_B_1\",archiver, database_status, active_state FROM v$instance ' DATABASE USER:[3] 'SYS' PRIVILEGE :[6] 'SYSDBA' CLIENT USER:[6] 'oracle' CLIENT TERMINAL:[7] 'unknown' STATUS:[1] '0' DBID:[9] '365542621' SESSIONID:[10] '4294967295' USERHOST:[22] 'Domain' CLIENT ADDRESS:[58] I try to parse this log with kv filter { kv { field_split => \",\" value_split => \":\" } } but the space between variable is the problem: CLIENT USER:[6] CLIENT TERMINAL:[7] Thanks for you time.",
    "website_area": "discuss"
  },
  {
    "id": "8faaff96-4381-4af2-9297-5ca37dd30e34",
    "url": "https://discuss.elastic.co/t/parsing-lines-that-only-match-a-start-string/215125",
    "title": "Parsing lines that only match a start string",
    "category": [
      "Logstash"
    ],
    "author": "whoatemyjam",
    "date": "January 15, 2020, 11:38am January 15, 2020, 2:08pm January 21, 2020, 3:11pm",
    "body": "hi Please if you can help. How can we get logstash to log only the lines starting with \"central-logging\": \"true\" and ignore rest which we dont need. { \"central-logging\": \"true\", \"log-date\": \"2020-01-15 11:19:08 UTC\", \"severity\": \"INFO\", \"BuildSetID\": \"sim-12345678\", \"Process\": \"CheckJobInputs\", \"log-message\": \"Directory: /images exists\"}",
    "website_area": "discuss"
  },
  {
    "id": "550e7da7-53b6-49eb-95d4-bea8502d9b10",
    "url": "https://discuss.elastic.co/t/how-to-iterate-through-several-elements-and-create-a-new-field-with-the-content-of-each-element/215656",
    "title": "How to iterate through several elements and create a new field with the content of each element",
    "category": [
      "Logstash"
    ],
    "author": "alejovaz",
    "date": "January 20, 2020, 12:10am January 20, 2020, 3:25pm January 21, 2020, 12:09pm",
    "body": "My Logstash receive of a http input this field PANELS.memory { \"availableBytes\": 6212550656, \"totalBytes\": 8493137920, \"type\": \"physical\" }, { \"availableBytes\": 14702407680, \"totalBytes\": 17256087552, \"type\": \"virtual\" } If I split this field, it will save 2 (or more) different elasticsearch documents, but I need to save 1 only elasticsearch document of the form: PANELS.memory0 { \"availableBytes\": 6212550656, \"totalBytes\": 8493137920, \"type\": \"physical\" }, PANELS.memory1 { \"availableBytes\": 14702407680, \"totalBytes\": 17256087552, \"type\": \"virtual\" } PANELS.memoryN { \"availableBytes\": 14702407680, \"totalBytes\": 17256087552, \"type\": \"cache\" } I tried to create a ruby filter to iterate on \"n\" elements (if applicable) but I don't know enough ruby to solve the problem. I really appreciate your support In the label JSON, the field looks like this: \"PANELS\": { \"memory\": [ { \"availableBytes\": 6211796992, \"type\": \"physical\", \"totalBytes\": 8493137920 }, { \"availableBytes\": 14701268992, \"type\": \"virtual\", \"totalBytes\": 17256087552 }, { \"availableBytes\": 6211796992, \"type\": \"cache\", \"totalBytes\": 8493137920 } ], }",
    "website_area": "discuss"
  },
  {
    "id": "e2b4c233-b627-4c7a-be29-9091f01c76d4",
    "url": "https://discuss.elastic.co/t/how-effective-translate-field-in-logstash/214411",
    "title": "How effective translate field in logstash",
    "category": [
      "Logstash"
    ],
    "author": "vinu89",
    "date": "January 9, 2020, 10:17am January 9, 2020, 4:54pm January 20, 2020, 6:07am January 21, 2020, 11:44am",
    "body": "Hi, I have a set of threat intel feeds i want to check whether those blacklisted ip occurs in my logs... i tried using translate field it works fine...i want to know whether we can compare billions of blacklisted ip using translate field or is there any limitation Thanks",
    "website_area": "discuss"
  },
  {
    "id": "06f29cec-051e-45fe-a897-11445d1e4985",
    "url": "https://discuss.elastic.co/t/logstash-password-encryption-streangth/215868",
    "title": "Logstash password encryption streangth",
    "category": [
      "Logstash"
    ],
    "author": "Manjula_Piyumal",
    "date": "January 21, 2020, 11:31am",
    "body": "Hi, I'm going to use logstash-keystore to encrypt my jdbc passwords . May I know the default encryption strength(keysize) and algorithm if I use below steps, https://www.elastic.co/guide/en/logstash/current/keystore.html Thanks",
    "website_area": "discuss"
  },
  {
    "id": "6d1757d1-8266-449c-b0e3-1fa321fe683e",
    "url": "https://discuss.elastic.co/t/random-loss-of-data-import-between-filebeat-and-logstash/215288",
    "title": "Random loss of data import between filebeat and logstash",
    "category": [
      "Logstash"
    ],
    "author": "tkauffmann",
    "date": "January 16, 2020, 9:53am January 21, 2020, 11:25am",
    "body": "Hi, Every night, short after midnight we export data from a Mysql database into several csv files. Filebeat runs on the same server and is configured to send these data to the elastic server also running logstash. Logstash is configured to handle this data import and feeds elasticsearch indexes. It usually works fine and the data is available for use in Kibana in the morning. BUT every now and then (once or twice a week) the imported data gets truncated : only part of the data gets imported. For example only 4 lines over 50. And only some csv data files are concerned by this issue on the same day. In the morning, if we make a local copy of the file that was only partly imported it is detected by filebeat and successfully imported by logstash. We have to do this manually for every file that was badly imported. It looks like the error appears totally randomly. What could be the source of this error ? How can we solve this issue ? Kind regards, Thierry",
    "website_area": "discuss"
  },
  {
    "id": "1b47a415-5309-4c83-a630-8523eb55c921",
    "url": "https://discuss.elastic.co/t/logstash-index-naming-pattern-use-of-an-alternate-tag-if-one-is-missing/215767",
    "title": "Logstash index naming pattern - use of an alternate tag if one is missing",
    "category": [
      "Logstash"
    ],
    "author": "jreid",
    "date": "January 20, 2020, 4:13pm January 21, 2020, 11:16am",
    "body": "Data is sourced from filebeat and packetbeat with add_kubernetes_metadata and passed to logstash, using the logstash helm chart. In logstash, I want the logs to be output to an elasticsearch index, named based on a kubernetes metadata tag ({[kubernetes][labels][app]}) but there have been a few deployments in kubernetes that were not tagged with app, causing a large amount of data to pile up in indices named with a literal \"[kubernetes][labels][app]\" (e.g. \"logstash-production_us-east-1-00_filebeat-%{[kubernetes][labels][app]}-2020.01.20\" instead of \"logstash-production_us-east-1-00_filebeat-someapp-2020.01.20\") instead of the correct tag - in these cases, where this metadata field is missing, I would like to use another field instead (e.g. {[kubernetes][labels][name]}) Logstash config: outputs: main: |- output { elasticsearch { hosts => [\"${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}\"] manage_template => false #index => \"logstash-production_us-east-1-00_%{[@metadata][beat]}-%{+YYYY.MM.dd}\" index => \"logstash-production_us-east-1-00_%{[@metadata][beat]}-%{[kubernetes][labels][app]}-%{+YYYY.MM.dd}\" #document_type => \"%{[@metadata][type]}\" document_type => \"_doc\" } }",
    "website_area": "discuss"
  },
  {
    "id": "bf74135d-0cef-4917-bb10-02917b7f092b",
    "url": "https://discuss.elastic.co/t/lgot-an-error-mongodb-input-threw-an-exception-restarting-exception-bson-bson-0x25ae0d1c-is-an-invalid-objectid/214830",
    "title": "lGot an error - MongoDB Input threw an exception, restarting {:exception=>#<BSON::ObjectId::Invalid: '#<BSON::Binary:0x25ae0d1c>' is an invalid ObjectId.>}",
    "category": [
      "Logstash"
    ],
    "author": "shubhamblackstratus",
    "date": "January 13, 2020, 12:25pm January 21, 2020, 9:54am",
    "body": "Hello Guys, While working with \" [logstash-input-mongodb]\" plugin, I got the below error in logstash logs,. [WARN ][logstash.inputs.mongodb ] MongoDB Input threw an exception, restarting {:exception=>#<BSON::ObjectId::Invalid: '#BSON::Binary:0x25ae0d1c' is an invalid ObjectId.>} I've gone through various helps listed on web for the similar error and also tried to silve the issue by fixing the structure of sqlite but it could not get resolved. github.com/kurkop/logstash-input-mongodb [FIX] structure of sqlite committed 09:03PM - 02 Jun 17 UTC +1 -1 - Use String type for 'place' column in sqlite fix #38 Kindly help.. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "d6f21424-a309-4e24-a247-59b871b9a89b",
    "url": "https://discuss.elastic.co/t/xml-filter-exception-in-pipelineworker-the-pipeline-stopped-processing-new-events-please-check-your-filter-configuration-and-restart-logstash-how-to-resume-logstash-or-skip-failed-event/215845",
    "title": "XML filter - Exception in pipelineworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash. How to resume Logstash or skip failed event?",
    "category": [
      "Logstash"
    ],
    "author": "saif3r",
    "date": "January 21, 2020, 10:23am January 21, 2020, 9:47am January 21, 2020, 9:51am",
    "body": "Hi, I have a Logstash pipeline that parses events from multiple filebeat sources. Those filebeats are sending multiline XML files. After a month or so, it received an event that stops logstash due to xml parsing error (it seems like filebeat sent an incomplete xml, a separate issue i will troubleshoot later). I managed to find the page file that contains this particular xml event. The question is, how to resume Logstash or how to skip this event to make sure that Logstash will properly resume? Everytime i restart Logstash, it returns the same error from this exact xml. I enabled dead letter queue but seems like this event keeps blocking Logstash. Thanks in advance. Here's the error message: > [2020-01-21T10:19:44,824][WARN ][logstash.filters.xml ][pipeline_name] Error parsing xml with XmlSimple {:source=>\"grok_raw\", :value=>\"<ErrorInfo>\\n<Sender>Gantry_1_L.PCBCamera</Sender>\\n<File>f:\\\\b\\\\267\\\\src\\\\tpa\\\\station-sw\\\\rel\\\\711-rel\\\\src\\\\structcomps\\\\vision\\\\project\\\\activity.h</File>\\n<Line>311</Line>\\n<Function>GetDetailedError</Function>\\n<UniqueId>852</UniqueId>\\n<Severity>eERROR</Severity>\\n<ErrorContext>eDEFAULT</ErrorContext>\\n<Msg>\\n<Context></Context>\\n<Index>33208</Index>\\n<Name>ActivityDetailedErrorInfo</Name>\\n<Text>33208 For error tracking: activityID=%1, orderID=%2</Text>\\n<IsDeveloperOnly>1</IsDeveloperOnly>\\n<DisplaySender>1</DisplaySender>\\n<Params>\\n<Param>\\n<Type>0</Type>\\n<Value>403400001</Value>\\n</Param>\\n<Param>\\n<Type>0</Type>\\n<Value>0</Value>\\n</Param>\\n</Params>\\n</Msg>\\n<Time>\\n<Year>2020</Year>\\n<Month>1</Month>\\n<Day>21</Day>\", :exception=>#<REXML::ParseException: No close tag for /ErrorInfo/Time > Line: 30 > Position: 699 > Last 80 unconsumed characters: > >, :backtrace=>[\"uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rexml/parsers/treeparser.rb:28:in `parse'\", \"uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rexml/document.rb:288:in `build'\", \"uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rexml/document.rb:45:in `initialize'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/xml-simple-1.1.5/lib/xmlsimple.rb:971:in `parse'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/xml-simple-1.1.5/lib/xmlsimple.rb:164:in `xml_in'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/xml-simple-1.1.5/lib/xmlsimple.rb:203:in `xml_in'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-filter-xml-4.0.7/lib/logstash/filters/xml.rb:185:in `filter'\", \"/usr/share/logstash/logstash-core/lib/logstash/filters/base.rb:143:in `do_filter'\", \"/usr/share/logstash/logstash-core/lib/logstash/filters/base.rb:162:in `block in multi_filter'\", \"org/jruby/RubyArray.java:1800:in `each'\", \"/usr/share/logstash/logstash-core/lib/logstash/filters/base.rb:159:in `multi_filter'\", \"org/logstash/config/ir/compiler/AbstractFilterDelegatorExt.java:115:in `multi_filter'\", \"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:250:in `block in start_workers'\"]} > [2020-01-21T10:19:45,555][ERROR][org.logstash.execution.WorkerLoop][pipeline_name] Exception in pipelineworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash. > java.lang.NullPointerException: null > at org.logstash.config.ir.compiler.EventCondition$Compiler$UnexpectedTypeException.<init>(EventCondition.java:651) ~[logstash-core.jar:?] > at org.logstash.config.ir.compiler.EventCondition$Compiler.compare(EventCondition.java:431) ~[logstash-core.jar:?] > at org.logstash.config.ir.compiler.EventCondition$Compiler.lambda$compareFieldToConstant$11(EventCondition.java:422) ~[logstash-core.jar:?] > at org.logstash.config.ir.compiler.EventCondition$Compiler.lambda$booleanCondition$4(EventCondition.java:140) ~[logstash-core.jar:?] > at org.logstash.config.ir.compiler.Utils.filterEvents(Utils.java:27) ~[logstash-core.jar:?] > at org.logstash.generated.CompiledDataset42.compute(Unknown Source) ~[?:?] > at org.logstash.generated.CompiledDataset43.compute(Unknown Source) ~[?:?] > at org.logstash.generated.CompiledDataset47.compute(Unknown Source) ~[?:?] > at org.logstash.generated.CompiledDataset48.compute(Unknown Source) ~[?:?] > at org.logstash.execution.WorkerLoop.run(WorkerLoop.java:64) [logstash-core.jar:?] > at sun.reflect.GeneratedMethodAccessor132.invoke(Unknown Source) ~[?:?] > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181] > at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181] > at org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(JavaMethod.java:425) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.javasupport.JavaMethod.invokeDirect(JavaMethod.java:292) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:28) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:90) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.ir.targets.InvokeSite.invoke(InvokeSite.java:183) [jruby-complete-9.2.8.0.jar:?] > at usr.share.logstash.logstash_minus_core.lib.logstash.java_pipeline.RUBY$block$start_workers$2(/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:250) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.runtime.CompiledIRBlockBody.callDirect(CompiledIRBlockBody.java:136) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.runtime.IRBlockBody.call(IRBlockBody.java:77) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.runtime.Block.call(Block.java:129) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.RubyProc.call(RubyProc.java:295) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.RubyProc.call(RubyProc.java:274) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.RubyProc.call(RubyProc.java:270) [jruby-complete-9.2.8.0.jar:?] > at org.jruby.internal.runtime.RubyRunnable.run(RubyRunnable.java:105) [jruby-complete-9.2.8.0.jar:?] > at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181] > [2020-01-21T10:19:45,882][FATAL][logstash.runner ] An unexpected error occurred! {:error=>java.lang.IllegalStateException: java.lang.NullPointerException, :backtrace=>[\"org.logstash.execution.WorkerLoop.run(org/logstash/execution/WorkerLoop.java:85)\", \"java.lang.reflect.Method.invoke(java/lang/reflect/Method.java:498)\", \"org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(org/jruby/javasupport/JavaMethod.java:425)\", \"org.jruby.javasupport.JavaMethod.invokeDirect(org/jruby/javasupport/JavaMethod.java:292)\", \"usr.share.logstash.logstash_minus_core.lib.logstash.java_pipeline.start_workers(/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:250)\", \"org.jruby.RubyProc.call(org/jruby/RubyProc.java:295)\", \"org.jruby.RubyProc.call(org/jruby/RubyProc.java:274)\", \"org.jruby.RubyProc.call(org/jruby/RubyProc.java:270)\", \"java.lang.Thread.run(java/lang/Thread.java:748)\"]} > [2020-01-21T10:19:46,290][ERROR][org.logstash.Logstash ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit I have checked the contents of dead_letter_queue file cat /usr/share/logstash/data/dead_lettter_queue/pipeline_name/1.log but it's just this: 1",
    "website_area": "discuss"
  },
  {
    "id": "cf8abd9a-89fe-4881-b405-a01a2f50aca7",
    "url": "https://discuss.elastic.co/t/logstash-is-unable-to-connect-to-es-for-monitoring-due-to-bad-certificate/215841",
    "title": "Logstash is unable to connect to ES for Monitoring due to bad certificate",
    "category": [
      "Logstash"
    ],
    "author": "Kosodrom",
    "date": "January 21, 2020, 8:53am January 21, 2020, 9:38am",
    "body": "Hi guys, I cannot see my logstash node in Kibana because it cannot connect to elasticsearch due to bad certificate: [2020-01-21T08:26:01,246][ERROR][logstash.licensechecker.licensereader] Unable to retrieve license information from license server {:message=>\"Elasticsearch Unreachable: [https://logstash_system:xxxxxx@elastic01.node.com:9200/][Manticore::ClientProtocolException] Received fatal alert: bad_certificate\"} [2020-01-21T08:26:01,314][ERROR][logstash.monitoring.internalpipelinesource] Failed to fetch X-Pack information from Elasticsearch. This is likely due to failure to reach a live Elasticsearch cluster. [2020-01-21T08:26:05,378][WARN ][logstash.outputs.elasticsearch][logstash-filebeat] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>\"https://logstash_internal:xxxxxx@elastic01.node.com:9200/\", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>\"Elasticsearch Unreachable: [https://logstash_internal:xxxxxx@elastic01.node.com:9200/][Manticore::ClientProtocolException] PKIX path validation failed: java.security.cert.CertPathValidatorException: Path does not chain with any of the trust anchors\"} my elasticsearch.yml: xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.key: /certs/elastic01.key xpack.security.transport.ssl.certificate: /certs/elastic01.crt xpack.security.transport.ssl.certificate_authorities: [ \"/certs/ca.crt\" ] xpack.security.http.ssl.enabled: true xpack.security.http.ssl.client_authentication: required xpack.security.http.ssl.verification_mode: certificate xpack.security.http.ssl.key: /certs/elastic01.key xpack.security.http.ssl.certificate: /certs/elastic01.crt xpack.security.http.ssl.certificate_authorities: [ \"/certs/ca.crt\" ] logstash.yml: xpack.monitoring.enabled: true xpack.monitoring.elasticsearch.username: logstash_system xpack.monitoring.elasticsearch.password: password xpack.monitoring.elasticsearch.hosts: [\"elastic01.node.com:9200\"] xpack.monitoring.elasticsearch.ssl.certificate_authority: /certs/ca/ca.crt xpack.monitoring.elasticsearch.ssl.verification_mode: certificate I assume, because I have set xpack.security.http.ssl.client_authentication: required I must provide logstash key and logstash cert in the logstash.yml. Am I assuming right? But I am not able to find a documentation how to provide this information (key/crt) in logstash.yml. Do you have any suggestions how I can solve the problem? PS: I have created all certificates and ca in accordance to this guide: https://www.elastic.co/blog/configuring-ssl-tls-and-https-to-secure-elasticsearch-kibana-beats-and-logstash",
    "website_area": "discuss"
  },
  {
    "id": "fab69e64-6ee4-49ef-9411-fce42f8a42a8",
    "url": "https://discuss.elastic.co/t/will-eol-logstashs-logstash-plugin-update-keep-getting-updates-even-when-they-are-not-supported/215842",
    "title": "Will EOL Logstash's \"logstash-plugin update\" keep getting updates even when they are not supported?",
    "category": [
      "Logstash"
    ],
    "author": "denisnovac",
    "date": "January 21, 2020, 8:49am",
    "body": "There is no plugin versions support matrix on the documentation, but it says that \"The plugins listed below are maintained and supported by Elastic. Plugin versions that are installable on an Elastic supported Logstash release are considered supported\". Is it true about EOL releases? Will i get an update of plugin even if it does not work with my EOL Logstash version? Or will i stop getting plugin updates at some point? I am talking about \"logstash-plugin update\" operation.",
    "website_area": "discuss"
  },
  {
    "id": "b031e87b-b601-4e93-84b7-4aaf74e1d496",
    "url": "https://discuss.elastic.co/t/conditional-statement-not-working-in-filter/215817",
    "title": "Conditional statement not working in filter",
    "category": [
      "Logstash"
    ],
    "author": "Charles_Yuliansen",
    "date": "January 21, 2020, 7:56am January 21, 2020, 7:08am January 21, 2020, 8:01am January 21, 2020, 8:33am",
    "body": "Im trying to learn conditional filtering, the grok outside expression successfully parsed, but not with grok inside conditional expression. target [2020-01-09 08:32:46] VERBOSE[18962][C-0000ceae] pbx.c: Executing [s@macro-dialout-trunk:26] NoOp(\"PJSIP/3513-0001108e\", \"Dial failed for some reason with DIALSTATUS = BUSY and HANGUPCAUSE = 19\") in new stack [2020-01-09 08:32:46] VERBOSE[18962][C-0000ceae] pbx.c: Executing [s@macro-dialout-trunk:26] NoOp(\"PJSIP/3513-0001108e\", \"Dial failed for some reason with DIALSTATUS = BUSY and HANGUPCAUSE = 19\") in new stack filter { grok { match => { \"message\" => \"[%{TIMESTAMP_ISO8601:log_timestamp}] +(?<log_level>(?i)(?:debug|notice|warning|error|verbose|dtmf|fax|security)(?-i))[%{INT:thread_id}](?:[%{DATA:call_thread_id}])? %{DATA:module_name}: %{WORD:action}\\s[%{DATA:TARGET}@%{DATA:dialplan_context}:%{DATA:dialplan_priority}]\\s%{GREEDYDATA:log_message}\" } add_field => [\"receiver_timestamp\", \"%{@timestamp}\"] add_field => [\"process_name\",\"asterisk_failed\"] } if [action] == \"Executing\" and [dialplan_priority]==\"1\"{ grok { match => { \"log_message\"=>\"%{DATA:asterisk_app}(\"%{DATA:protocol}/%{DATA:EXT}-%{DATA:channel}\",\\s\"%{DATA:problem1}-\\s%{DATA:problem2}\")\\s%{GREEDYDATA:all}\" } } } if [action] == \"Executing\" and [dialplan_priority]==\"26\"{ grok { match => { \"log_message\"=>\"%{DATA:asterisk_app}(\"%{DATA:protocol}/%{DATA:EXT}-%{DATA:channel}\",\\s\"%{DATA:problem1}\\sand\\s%{DATA:problem2}\")\\s%{GREEDYDATA:all}\" } } } } are there some stuff that needed to be imported to use conditional if?",
    "website_area": "discuss"
  },
  {
    "id": "19604d62-2a5e-44f2-81f6-42e9b9ad7b9b",
    "url": "https://discuss.elastic.co/t/discarding-unnecessary-data/215677",
    "title": "Discarding unnecessary data",
    "category": [
      "Logstash"
    ],
    "author": "RichardH",
    "date": "January 20, 2020, 7:24am January 20, 2020, 2:08pm January 21, 2020, 7:34am",
    "body": "Hello, I am just starting with logstash and writing filters, and trying to better understand how it works. We want to push all syslog messages to logstash but I want to ensure that events are only sent to elastic if they match a grok filter. So if (using the Cisco example) I have a grok { match => [ # IOS \"message\", \"%{SYSLOG5424PRI}(%{NUMBER:log_sequence#})? .... etc What happens to messages that do not match the Grok pattern specified - are they discarded? Also, what if I want to discard a portion of a message. I know that %{NUMBER:log_sequence#} would assign the number to \"log_sequence), What if I want to drop the number altogether?",
    "website_area": "discuss"
  },
  {
    "id": "2c253cda-2d05-44ff-b190-e3677353b461",
    "url": "https://discuss.elastic.co/t/using-logstash-on-elastic-gcp-hosted-instance/215698",
    "title": "Using logstash on Elastic (GCP) hosted instance",
    "category": [
      "Logstash"
    ],
    "author": "prettysimpl",
    "date": "January 20, 2020, 9:25am January 21, 2020, 7:14am",
    "body": "Hello All, We have a new elastic hosted instance (on gcp) running and would like to use logstash on there. I could not find any documentation on this and the only recent topic on the forum on something similar has not been responded to. Can anyone please help or at least point me to some useful resource somewhere? Thanks in advance (cc @Bill_French)",
    "website_area": "discuss"
  },
  {
    "id": "0531d18b-6ac1-4c74-9f0c-d250895ba2ab",
    "url": "https://discuss.elastic.co/t/how-to-logstash-filter-xml/215680",
    "title": "How to logstash filter xml",
    "category": [
      "Logstash"
    ],
    "author": "peacher",
    "date": "January 20, 2020, 2:30pm January 20, 2020, 8:22am January 20, 2020, 2:46pm January 20, 2020, 6:01pm January 20, 2020, 6:16pm January 20, 2020, 9:41pm January 21, 2020, 12:42am January 21, 2020, 2:33am January 21, 2020, 2:44am",
    "body": "Hi, i have logstash config input { file { path => [\"/mnt/data/*/*.xml\"] start_position => beginning #sincedb_path => \"/opt/logstash/sincedb-xml49\" sincedb_path => \"/dev/null\" codec => multiline { #pattern => \"<?xml \" #pattern => \"^<NewDataSet.*\\>\" #pattern => \"<TransferLogDetailForExport>\" pattern => \"<Vouchers>\" negate => \"true\" what => \"previous\" } } } filter { xml { force_array => \"false\" store_xml => \"false\" source => \"message\" target => \"Vouchers\" #xpath => [\"/NewDataSet/TransferLogDetailForExport/LogID/text()\", \"LogID\"] #xpath => [\"/NewDataSet/TransferLogDetailForExport/LogDateTime/text()\", \"LogDateTime\"] xpath => [\"/Vouchers/VoucherID/text()\", \"VoucherID\"] xpath => [\"/Vouchers/VoucherTypeID/text()\", \"VoucherTypeID\"] xpath => [\"/Vouchers/ComputerID/text()\", \"ComputerID\"] xpath => [\"/Vouchers/Used/text()\", \"Used\"] } } output { #elasticsearch { # hosts => \"159.138.237.176:9200\" # index => \"cup49\" #} stdout { codec => rubydebug } } Output data { \"Used\" : \"1\", \"VoucherTypeID\" : \"24\", \"VoucherID\" : \"2\", \"ComputerID\" : \"0\" }, { \"Used\" : \"1\", \"VoucherTypeID\" : \"24\", \"VoucherID\" : \"73\", \"ComputerID\" : \"0\" }, { \"Used\" : \"1\", \"VoucherTypeID\" : \"24\", \"VoucherID\" : \"6\", \"ComputerID\" : \"0\" }, I would like output { \"LogID\": \"15237\", \"LogDateTime\": \"2020-01-07T17:00:47\", \"Used\" : \"1\", \"VoucherTypeID\" : \"24\", \"VoucherID\" : \"2\", \"ComputerID\" : \"0\" }, { \"LogID\": \"15237\", \"LogDateTime\": \"2020-01-07T17:00:47\", \"Used\" : \"1\", \"VoucherTypeID\" : \"24\", \"VoucherID\" : \"73\", \"ComputerID\" : \"0\" }, { \"LogID\": \"15237\", \"LogDateTime\": \"2020-01-07T17:00:47\", \"Used\" : \"1\", \"VoucherTypeID\" : \"24\", \"VoucherID\" : \"6\", \"ComputerID\" : \"0\" }, This is my xml data <?xml version=\"1.0\" standalone=\"yes\"?> <NewDataSet> <TransferLogDetailForExport> <LogID>15237</LogID> <LogDateTime>2020-01-07T17:00:47</LogDateTime> <GroupType>1</GroupType> <DataType>4</DataType> <FromShopID>53</FromShopID> <DestinationShopID>1</DestinationShopID> <FileName>001_TEST053_001_20200107_170047</FileName> <CriteriaStartTime>2020-01-06T17:00:16</CriteriaStartTime> <ISFromLastUpdate>1</ISFromLastUpdate> <StaffID>-1</StaffID> <UpdateDate>2020-01-07T17:00:47</UpdateDate> <RetryTime>0</RetryTime> <ResultCode>1</ResultCode> <DatabaseName>test_db</DatabaseName> <IPAddress>192.168.1.1</IPAddress> <ExportType>XML</ExportType> </TransferLogDetailForExport> <Vouchers> <VoucherID>2</VoucherID> <VoucherTypeID>24</VoucherTypeID> <ComputerID>0</ComputerID> <Used>1</Used> </Vouchers> <Vouchers> <VoucherID>3</VoucherID> <VoucherTypeID>24</VoucherTypeID> <ComputerID>0</ComputerID> <Used>1</Used> </Vouchers> <Vouchers> <VoucherID>6</VoucherID> <VoucherTypeID>24</VoucherTypeID> <ComputerID>0</ComputerID> <Used>1</Used> </Vouchers> <Vouchers> ........ </Vouchers> </NewDataSet> Please help. Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "6890fa8f-72d9-40c0-9ef0-e5fc09603025",
    "url": "https://discuss.elastic.co/t/how-to-move-all-the-json-fields-at-root-level-to-a-sub-json-node/215522",
    "title": "How to move all the json fields at root level to a sub json node?",
    "category": [
      "Logstash"
    ],
    "author": "STAR_DEVX",
    "date": "January 17, 2020, 8:36pm January 17, 2020, 10:49pm January 20, 2020, 9:55pm",
    "body": "I want to move all the json node at the root level to a sub level jason node. What's the most efficient way to use filters to achieve that? root level { \"a\": 1, \"b\": \"str\", \"c\": {\"key\": \"value\"} } I want the result to be: { \"data\": { \"a\": 1, \"b\": \"str\", \"c\": {\"key\": \"value\"} } } The most intuitive thought is to use mutate-rename filter. mutate { rename => { reference to the root level json => \"data\" } } However, I couldn't find a way to reference the root level json. Thank you in advance.",
    "website_area": "discuss"
  },
  {
    "id": "c4428c3a-76e7-486a-90fc-5b3ad01901c4",
    "url": "https://discuss.elastic.co/t/if-i-add-a-new-field-in-to-my-logstash-conf-my-old-index-logs-recognize-it/215788",
    "title": "If I add a new field in to my Logstash.conf, my old index logs recognize it?",
    "category": [
      "Logstash"
    ],
    "author": "Daniel_Gonzalez1",
    "date": "January 20, 2020, 6:54pm January 20, 2020, 7:15pm January 20, 2020, 7:15pm",
    "body": "Hello Everyone! Doing the endless comparasion and questions between Splunk and Elastic. With Splunk you can extract all the time new fields from the logs that are already indexed, this is very helpful because if you don't considere a field at the first time when you parsed the logs, you can always extract them with regular expressions and they will appear in the search bar. Reading the elastic docs, Kibana has a way to do this with the scripting fields, but this seems to be a pain in the (\"you know\") and it seems no to be such easy. We already know how logstash works and how to extract fields, my questions are: If we add a new field to logstash.conf that is running and we restart it, does the old logs that are already index will recognize the new field that we add? or only the new data that will come to elasticsearch will have this new field? Best Regards!",
    "website_area": "discuss"
  },
  {
    "id": "79ff0623-cd90-4a22-8098-8ca5394be9c8",
    "url": "https://discuss.elastic.co/t/search-engine-indexing-multimedia-files-with-elk/215779",
    "title": "[search engine] Indexing multimedia files with elk",
    "category": [
      "Logstash"
    ],
    "author": "Jassem_Torkhani",
    "date": "January 20, 2020, 4:19pm",
    "body": "Hello everyone, Am working on a search engine project and am new experiencing with the ELK stack , so after installing and configuring it , I am now called to pass csv files which came originally from parsed output via Hachoir python library. for the initial conducted tests I chose to feed logstash the syslogs from my operating system (Ubuntu 18.0.4) , I used the filter from the documentation. running ./logstash --path.settings /etc/logstash/ returned this: Thread.exclusive is deprecated, use Thread::Mutex Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties [2020-01-20T05:10:44,138][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.5.1\"} [2020-01-20T05:10:47,736][INFO ][org.reflections.Reflections] Reflections took 46 ms to scan 1 urls, producing 20 keys and 40 values [2020-01-20T05:10:48,990][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>, :added=>[http://localhost:9200/]}} [2020-01-20T05:10:49,249][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>\"http://localhost:9200/\"} [2020-01-20T05:10:49,323][INFO ][logstash.outputs.elasticsearch][main] ES Output version determined {:es_version=>7} [2020-01-20T05:10:49,330][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the type event field won't be used to determine the document _type {:es_version=>7} [2020-01-20T05:10:49,415][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"//localhost\"]} [2020-01-20T05:10:49,981][INFO ][logstash.filters.geoip ][main] Using geoip database {:path=>\"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-filter-geoip-6.0.3-java/vendor/GeoLite2-City.mmdb\"} [2020-01-20T05:10:50,146][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.specialized.RubyArrayOneObject) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-01-20T05:10:50,154][INFO ][logstash.javapipeline ][main] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>4, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>500, \"pipeline.sources\"=>[\"/etc/logstash/conf.d/10-syslog-filter.conf\"], :thread=>\"#<Thread:0x217552f2 run>\"} [2020-01-20T05:10:50,833][INFO ][logstash.inputs.beats ][main] Beats inputs: Starting input listener {:address=>\"0.0.0.0:5044\"} [2020-01-20T05:10:50,856][INFO ][logstash.javapipeline ][main] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-01-20T05:10:51,025][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>} [2020-01-20T05:10:51,094][INFO ][org.logstash.beats.Server][main] Starting server on port: 5044 [2020-01-20T05:10:51,679][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} All am asking is a sanity check from anyone who has been the same road I am walking today: 1/ how can I view the indexed syslogs on kibana and what are the possible operations one can perform ? 2/ what would be the appropriate filter (i wouldn't mind the input/output too ) needed for the kind of csv files i want to index on kibana ? any links related are very much appreciated too. Thank you in advance.",
    "website_area": "discuss"
  },
  {
    "id": "098304d1-fd73-4134-8071-33847b4e03a4",
    "url": "https://discuss.elastic.co/t/dynamic-json-parsing/215644",
    "title": "Dynamic JSON parsing",
    "category": [
      "Logstash"
    ],
    "author": "richard_N",
    "date": "January 19, 2020, 7:00pm January 20, 2020, 3:38pm",
    "body": "I'm trying to change a document from this. Is this possible? { \"zenoss01.hostname\": { \"issued_to\": null, \"cert_sans\": \"\", \"valid_till\": \"2021-01-17\", \"valid_from\": \"2016-01-18\", \"issuer_ou\": null, \"days_left\": 364, \"cert_ver\": 2, \"cert_alg\": \"sha256WithRSAEncryption\", \"issued_o\": \"Zenoss\", \"cert_exp\": false, \"cert_sha1\": \"83:E4:4B:8B:DF:E4:72:78:A3:7B:24:99:96:1D:6A:C6:ED:67:A5:8E\", \"issuer_c\": \"US\", \"issuer_cn\": null, \"issuer_o\": \"Zenoss\", \"validity_days\": 1826, \"cert_sn\": \"15935028744769600013\" } } To this. { \"hostname\": \"zenoss01.hostname\" \"issued_to\": null, \"cert_sans\": \"\", \"valid_till\": \"2021-01-17\", \"valid_from\": \"2016-01-18\", \"issuer_ou\": null, \"days_left\": 364, \"cert_ver\": 2, \"cert_alg\": \"sha256WithRSAEncryption\", \"issued_o\": \"Zenoss\", \"cert_exp\": false, \"cert_sha1\": \"83:E4:4B:8B:DF:E4:72:78:A3:7B:24:99:96:1D:6A:C6:ED:67:A5:8E\", \"issuer_c\": \"US\", \"issuer_cn\": null, \"issuer_o\": \"Zenoss\", \"validity_days\": 1826, \"cert_sn\": \"15935028744769600013\" }",
    "website_area": "discuss"
  },
  {
    "id": "536c96cb-f67b-4ef8-8ef2-cbc1dbcbc4f7",
    "url": "https://discuss.elastic.co/t/can-output-file-plugin-point-to-dev-null/215645",
    "title": "Can Output file-plugin point to /dev/null?",
    "category": [
      "Logstash"
    ],
    "author": "alexolivan",
    "date": "January 19, 2020, 7:26pm January 20, 2020, 3:23pm",
    "body": "Hi Forum. I'm just discovering how the new ELK+x-pack pieces work together, and, since I've realiced that I've got to create a parallel monitoring cluster to fully learn the current ELK potential, I'm thinking in advance of an scenario where some of my final outputs will go to the monitoring cluster. I've read about the guaranteed output policy (or something like that... probably I've not fully understood it) and, since right now, I only have the 'production cluster' mockup up, I'm concerned that having an output pointing to a still non-reachable endpoint, is a bad thing, and eventually if an event leaks there the whole pipeline structure would choke. Since I want to play with Logstash pipelines right now (I've read documentation today and I want to put hands on ... I'm lazy to create a monitoring cluster at this moment) I've though as an elegant, temporary solution, to configure a 'redirect to null' for everything that ends up in the output to the monitoring cluster (later, I could just change the output). Can this be done? Since I do not see a null plugin, I've though of using the file plugin pointing to /dev/null ... is this a dumb idea? has someone tried it? I guess I'm not the first one with this in mind for sure ... what is the elegant way to achieve this? or how can I have a pipeline with a dummy output? Thank you very much in advance. Best regards.",
    "website_area": "discuss"
  },
  {
    "id": "a79f3b90-ff6d-4fcb-a2e5-6cf229b00d00",
    "url": "https://discuss.elastic.co/t/aggregate-filter-usage/215744",
    "title": "Aggregate filter usage",
    "category": [
      "Logstash"
    ],
    "author": "akhilsharma.in",
    "date": "January 20, 2020, 1:45pm January 20, 2020, 2:05pm January 20, 2020, 3:10pm",
    "body": "Hi Team, I want to write filter to compare current event with previous one. My requirement is as below: Let's say first log has event id as 100, I want to check when the second log comes it should be 1 greater than previous one. That is, it should be 101. If it's not 101, then I wan't to add tag to the event. Please help out. I have never used aggregate filter. Thanks Akhil Sharma",
    "website_area": "discuss"
  },
  {
    "id": "71a60bcf-a47b-4ee7-b81b-351618553817",
    "url": "https://discuss.elastic.co/t/cannot-get-input-from-elasticsearch-logstash-terminates-itself/215713",
    "title": "Cannot get input from elasticsearch - logstash terminates itself",
    "category": [
      "Logstash"
    ],
    "author": "peter_pan",
    "date": "January 20, 2020, 10:35am January 20, 2020, 3:08pm",
    "body": "Hi ELKers, I would like to ask about your help on the following issue, which it is that I cannot get input from elasticsearch into logstash. My configuration is simple as that in the logstash-simple.conf: input { elasticsearch { hosts => \"localhost:9200\" query => '{ \"query\": { \"match_all\": {} } }' } } output { elasticsearch { hosts => \"localhost:9200\" index => \"test_logstash\" } } And the command that i use is the following: bin/logstash --log.level debug -f config/logstash-simple.yml When I run the logstash it seems that cannot get data from elastic search for some reason and terminates itself. A sample output is: Blockquote [2020-01-20T10:29:07,007][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>, :added=>[http://localhost:9200/]}} [2020-01-20T10:29:07,008][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>\"/\"} [2020-01-20T10:29:07,098][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>\"http://localhost:9200/\"} [2020-01-20T10:29:07,150][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil} [2020-01-20T10:29:07,154][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{\"template\"=>\"logstash-\", \"version\"=>50001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\"}, \"mappings\"=>{\"default\"=>{\"_all\"=>{\"enabled\"=>true, \"norms\"=>false}, \"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\", \"include_in_all\"=>false}, \"@version\"=>{\"type\"=>\"keyword\", \"include_in_all\"=>false}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}}} [2020-01-20T10:29:07,161][DEBUG][logstash.outputs.elasticsearch] Found existing Elasticsearch template. Skipping template management {:name=>\"logstash\"} [2020-01-20T10:29:07,161][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"//localhost:9200\"]} [2020-01-20T10:29:07,167][INFO ][logstash.pipeline ] Starting pipeline {\"id\"=>\"main\", \"pipeline.workers\"=>4, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>5, \"pipeline.max_inflight\"=>500} [2020-01-20T10:29:07,286][INFO ][logstash.pipeline ] Pipeline main started [2020-01-20T10:29:07,316][DEBUG][logstash.agent ] Starting puma [2020-01-20T10:29:07,320][DEBUG][logstash.agent ] Trying to start WebServer {:port=>9600} [2020-01-20T10:29:07,331][DEBUG][logstash.api.service ] [api-service] start [2020-01-20T10:29:07,371][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2020-01-20T10:29:07,373][DEBUG][logstash.inputs.elasticsearch] closing {:plugin=>\"LogStash::Inputs::Elasticsearch\"} [2020-01-20T10:29:07,374][DEBUG][logstash.pipeline ] Input plugins stopped! Will shutdown filter/output workers. [2020-01-20T10:29:07,418][DEBUG][logstash.pipeline ] Pushing flush onto pipeline [2020-01-20T10:29:07,419][DEBUG][logstash.pipeline ] Pushing shutdown {:thread=>\"#<Thread:0x1f26c632 sleep>\"} [2020-01-20T10:29:07,419][DEBUG][logstash.pipeline ] Pushing shutdown {:thread=>\"#<Thread:0x14dcc3d2 sleep>\"} [2020-01-20T10:29:07,420][DEBUG][logstash.pipeline ] Pushing shutdown {:thread=>\"#<Thread:0x34c17ed1 run>\"} [2020-01-20T10:29:07,421][DEBUG][logstash.pipeline ] Pushing shutdown {:thread=>\"#<Thread:0x55162334 sleep>\"} [2020-01-20T10:29:07,422][DEBUG][logstash.pipeline ] Shutdown waiting for worker thread #Thread:0x1f26c632 [2020-01-20T10:29:07,461][DEBUG][logstash.pipeline ] Shutdown waiting for worker thread #Thread:0x14dcc3d2 [2020-01-20T10:29:07,461][DEBUG][logstash.pipeline ] Shutdown waiting for worker thread #Thread:0x34c17ed1 [2020-01-20T10:29:07,463][DEBUG][logstash.pipeline ] Shutdown waiting for worker thread #Thread:0x55162334 [2020-01-20T10:29:07,463][DEBUG][logstash.outputs.elasticsearch] closing {:plugin=>\"LogStash::Outputs::ElasticSearch\"} [2020-01-20T10:29:07,464][DEBUG][logstash.outputs.elasticsearch] Stopping sniffer [2020-01-20T10:29:07,464][DEBUG][logstash.outputs.elasticsearch] Stopping resurrectionist [2020-01-20T10:29:08,152][DEBUG][logstash.outputs.elasticsearch] Waiting for in use manticore connections [2020-01-20T10:29:08,152][DEBUG][logstash.outputs.elasticsearch] Closing adapter #LogStash::Outputs::ElasticSearch::HttpClient::ManticoreAdapter:0x1c6336d9 [2020-01-20T10:29:08,153][DEBUG][logstash.pipeline ] Pipeline main has been shutdown [2020-01-20T10:29:10,311][DEBUG][logstash.instrument.periodicpoller.os] PeriodicPoller: Stopping [2020-01-20T10:29:10,311][DEBUG][logstash.instrument.periodicpoller.jvm] PeriodicPoller: Stopping [2020-01-20T10:29:10,311][DEBUG][logstash.instrument.periodicpoller.persistentqueue] PeriodicPoller: Stopping [2020-01-20T10:29:10,312][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] PeriodicPoller: Stopping [2020-01-20T10:29:10,314][WARN ][logstash.agent ] stopping pipeline {:id=>\"main\"} [2020-01-20T10:29:10,315][DEBUG][logstash.pipeline ] Closing inputs [2020-01-20T10:29:10,316][DEBUG][logstash.inputs.elasticsearch] stopping {:plugin=>\"LogStash::Inputs::Elasticsearch\"} [2020-01-20T10:29:10,316][DEBUG][logstash.pipeline ] Closed inputs Notes: I run the elasticsearch and the logstash on the same machine. When i try to give an input from stdin{} instead of the elasticseach it works fine and the new index \"text_logstash\" is created. A simple curl query to localhost:9200 works fine, I am getting the response back. Your help is appreciated!!",
    "website_area": "discuss"
  },
  {
    "id": "cd46bf5a-46b6-4b08-a6bc-e6884274cbef",
    "url": "https://discuss.elastic.co/t/logstash-jdbc-strange-time-convert/215760",
    "title": "Logstash JDBC strange time convert",
    "category": [
      "Logstash"
    ],
    "author": "GaffaOverflow",
    "date": "January 20, 2020, 2:56pm",
    "body": "Hi Everyone, Im trying to add Logs vom a MSSQL Database to my ELk-Stack. I had some trouble regarding the Timezone. I want a field with the UTC Time (will be the @timestamp field) i had problems with converting the field \"requestdate\" directly. So i added to the query the part RequestDate AT TIME ZONE 'UTC' AS UTCDate In the MS-SQL Server Managemnt Studio result i see the following When i copy the Query to the logstash conf file and execute them i get the resullts. All fields are correct, execpt the utcdate field. \"utcdate\":\"2020-01-18T12:58:58.740Z\" As you can see the date is exectly 2 days behind the real date in UTC. in the conf file i have no filter and as output stdout { codec => json_lines } I hope you can help me. Best",
    "website_area": "discuss"
  },
  {
    "id": "2855251f-c8bf-43b6-8f89-067c7ef6b858",
    "url": "https://discuss.elastic.co/t/how-can-i-remove-unwanted-parameters-in-logstash-geoip/215673",
    "title": "How can I remove unwanted parameters in logstash geoip",
    "category": [
      "Logstash"
    ],
    "author": "ruranga",
    "date": "January 20, 2020, 6:53am January 20, 2020, 2:37pm",
    "body": "Hi, Actually I don't need following parameters in logstash geoip out put geoip.city_name geoip.continent_code geoip.dma_code geoip.ip geoip.latitude geoip.location.lat geoip.location.lon geoip.longitude geoip.postal_code geoip.region_code geoip.region_name geoip.timezone I only need geoip.country_code2 geoip.country_code3 geoip.country_name how can i perform this in logstash filter{} Thanks in adavcne!",
    "website_area": "discuss"
  },
  {
    "id": "d6455209-53b2-4178-abea-9781f1b66451",
    "url": "https://discuss.elastic.co/t/hard-regex/215714",
    "title": "Hard regex",
    "category": [
      "Logstash"
    ],
    "author": "Arnodl",
    "date": "January 20, 2020, 10:38am January 20, 2020, 11:03am January 20, 2020, 12:08pm January 20, 2020, 1:42pm January 20, 2020, 2:01pm",
    "body": "hi, i need your help to write regex. It's my e.g message: scc_check_file ignored non-ASCII file /etc/sysconfig/auditd: '/etc/sysconfig/auditd: regular file, no read permission' I want to write if, which checks in part for : (scc_check_file ignored non-ASCII file /etc/sysconfig/auditd) there are no such characters as: \\ , / , * , ? , \" , < , > , | , space (the character, not the word), , , #",
    "website_area": "discuss"
  },
  {
    "id": "6e81d3d8-3f74-4536-a9a7-0aa963e20727",
    "url": "https://discuss.elastic.co/t/how-to-have-a-checkpoint-using-a-timestamp-field-sql-last-value/215745",
    "title": "How to have a checkpoint using a TimeStamp field (sql_last_value)",
    "category": [
      "Logstash"
    ],
    "author": "NaveenSilvester",
    "date": "January 20, 2020, 1:51pm",
    "body": "Hi ELK community, Here is a challenge that I am facing while tracking a TimeStamp filed in logstash. I am connecting to Postgresql table which has a field named \"Recorded_time\" with a datatype TimeStamp. I want to have this field as the tracking field in my Logstash configuration file. Here is the challenge that I am currently facing and need help from the community. The sql_last_value of the tracked column \"Recorded_time\" is not getting saved in the file \"logstash_jdbc_last_run_t_data.txt\" (refer to the config file detail: last_run_metadata_path => \"C:/ELK/logstash_jdbc_last_run_t_data.txt\") For your reference here is an Example: Value stored in the column \"Recorded_time\" = \"2018-01-28 04:22:59.171428-05\" Here is how my Config file looks like. use_column_value => true tracking_column => recorded_time record_last_run => true tracking_column_type => \"timestamp\" jdbc_default_timezone => \"US/Eastern\" last_run_metadata_path => \"C:/ELK/logstash_jdbc_last_run_t_data.txt\" Any help on this is highly appreciated. Thanks, Naveen Silvester",
    "website_area": "discuss"
  },
  {
    "id": "87f65fbe-172c-485a-b06a-53bd679867ad",
    "url": "https://discuss.elastic.co/t/single-field-object-to-elasticsearch-using-logstash/215735",
    "title": "Single field object to Elasticsearch using Logstash",
    "category": [
      "Logstash"
    ],
    "author": "KrishK",
    "date": "January 20, 2020, 12:21pm January 20, 2020, 1:50pm",
    "body": "Hi Support, I have group concat field values like Document_Type 1,2,3,4,5,6 In Elasticsearch field mapping with object type \"Document_Type\": { \"properties\": { \"d\": { \"type\": \"short\" } } } When I run Logstash getting this \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"object mapping for [Document_Type] tried to parse field [Document_Type] as object, but found a concrete value\"}}}} Can you please help me? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "684fa3c3-ef30-449d-b78f-c68a050688e6",
    "url": "https://discuss.elastic.co/t/avoid-reloading-duplicate-csv-records-into-same-index/215734",
    "title": "Avoid reloading (duplicate) csv records into same index",
    "category": [
      "Logstash"
    ],
    "author": "bharat1",
    "date": "January 20, 2020, 12:26pm January 20, 2020, 1:45pm",
    "body": "Hi All, I have a working config for loading csv records from logstash to elasticsearch. However when I try to restart the logstash service, the same csv file records are reloaded again into same index and creating duplicate record entries. I want to avoid this happening. Can someone point me whats wrong with this conf or anything missing. input { file { path => \"/etc/logstash/1.csv\" start_position => \"beginning\" sincedb_path => \"/etc/logstash/sincedb_sample1csv\" } file { path => \"/etc/logstash/2.csv\" start_position => \"beginning\" sincedb_path => \"/etc/logstash/sincedb_sample2csv\" } } filter { if [path] == \"/etc/logstash/1.csv\" { csv { separator => \",\" columns => [\"column1\",\"column2\"] } } if [path] == \"/etc/logstash/2.csv\" { csv { separator => \",\" columns => [\"column1\",\"column2\"] } } } output { if [path] == \"/etc/logstash/1.csv\" { elasticsearch { action => \"index\" hosts => [\"http://192.168.1.1:9200\"] index => \"sample1\" } } if [path] == \"/etc/logstash/2.csv\" { elasticsearch { action => \"index\" hosts => [\"http://192.168.1.1:9200\"] index => \"sample2\" } }",
    "website_area": "discuss"
  },
  {
    "id": "e7e6f8df-93ba-4ba5-be7a-7372fcc3f889",
    "url": "https://discuss.elastic.co/t/logstash-dns-unexpected-error/215741",
    "title": "Logstash DNS unexpected error",
    "category": [
      "Logstash"
    ],
    "author": "vmanyushin",
    "date": "January 20, 2020, 1:17pm",
    "body": "After upgrade to 7.5.1 a got error messages in log: [2020-01-20T16:00:39,036][ERROR][logstash.filters.dns ][main] DNS: Unexpected Error. {:field=>\"upstream_hostname\", :value=>\"172.16.21.57\", :message=>\"Mutex relocking by same thread\"} [2020-01-20T16:00:39,040][ERROR][logstash.filters.dns ][main] DNS: Unexpected Error. {:field=>\"upstream_hostname\", :value=>\"172.16.22.54\", :message=>\"Mutex relocking by same thread\"} [2020-01-20T16:00:39,046][ERROR][logstash.filters.dns ][main] DNS: Unexpected Error. {:field=>\"upstream_hostname\", :value=>\"172.16.32.17\", :message=>\"Mutex relocking by same thread\"} how to fix this problem?",
    "website_area": "discuss"
  },
  {
    "id": "03bf16e0-ab82-4814-a46f-5f646896eb13",
    "url": "https://discuss.elastic.co/t/output-how-to-save-a-field-to-a-pdf-file/211618",
    "title": "Output: how to save a field to a pdf file?",
    "category": [
      "Logstash"
    ],
    "author": "ITIC",
    "date": "December 12, 2019, 11:17am December 17, 2019, 12:56pm January 8, 2020, 8:11am January 20, 2020, 12:39pm",
    "body": "Hi there I'm using logstash (docker.elastic.co/logstash/logstash:7.4.0) to ingest data to our open data portal and saving said data to files (.csv, .geojson, etc.). All was well, until we got to one of our data sources which contains a pdf file, encoded in base64, in one of the fields. We get this data through an API and the pdf data is automagicaly ingested into a field (json codec in our input{}) as expected (thank you logstash!). I need to save the content of this field to an actual pdf file. In order to do so I've tried to use the file{} output plugin, after decoding the data, like this: filter { ruby { code => \" event.set('pdf_data', Base64.decode64(event.get('json_field_with_b64_data'))) \" } } output { file { path => \"data/%{my_file_name}.pdf\" codec => plain {format => \"%{pdfdata}\"} } } What I get is a .pdf file with, apparently, the right contents (if I open it with a text editor), but which turns to a blank page when I open it with Acrobat Reader. I have also tried with line codec instead of plain, with the same result. I've been searching the documentation looking for other output plugins, and other codecs, but I haven't been able to find an alternative. I have not found any similar case here either. Maybe a ruby filter could do the writing instead? Has anyone done something similar? Is it even possible to write a pdf file from logstash? Any ideas or pointers will be appreciated. Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "64f42435-ad61-4404-b6ac-39111a1312f4",
    "url": "https://discuss.elastic.co/t/more-input-based-on-first-input-multiple-pipelines/215442",
    "title": "More input based on first input(Multiple pipelines?)",
    "category": [
      "Logstash"
    ],
    "author": "_Louw",
    "date": "January 17, 2020, 10:53am January 17, 2020, 11:40am January 17, 2020, 12:26pm January 17, 2020, 1:01pm January 17, 2020, 1:54pm January 20, 2020, 6:53am January 20, 2020, 7:07am January 20, 2020, 11:05am",
    "body": "I am currently looking to build a solution, where I can load input based on some other input. So my setup is that I have two inputs: The data I need The timestamp of the latest update of this data I only want to get the data (which is multiple API-calls) if the timestamp is newer than last time I pulled the data. I already have a Logstash-script that can compare the dates. What I am unsure of is how I get the data when the timestamp has been updated. As the whole point is to not send too many requests to the data server, I can't load it in the same Logstash script during the input phase. I have been looking into multiple pipelines, where I only send data to a virtual address, if the timestamp has been updated. The other pipeline needs to make the API-calls, but it still runs even if there is no data in the virtual address. And as conditionals can't be used in the input phase, I can't see how to make it not run, when no data is sent to the virtual address. Any ideas? Is this even possible?",
    "website_area": "discuss"
  },
  {
    "id": "c61c27d5-3a3c-4f95-a655-aebb7f939d62",
    "url": "https://discuss.elastic.co/t/csv-filter-plugin-and-changing-column-headers/215493",
    "title": "CSV filter plugin and changing column headers",
    "category": [
      "Logstash"
    ],
    "author": "mhyager",
    "date": "January 17, 2020, 4:24pm January 17, 2020, 5:32pm January 20, 2020, 9:40am",
    "body": "I have a set of CSV files which have similar but not identical headers. I import these using the automatic header detection and autdetect numerics to set correct data types. Depending on the process that generates them, some columns may not match between files and feature different headers. Furthermore, some columns only start getting values added a few rows down into the file. Here's an example: capture721489 14.7 KB I am able to capture partial sets of these files into Elasticsearch, but end up getting many errors such as this one in the process as well: 2020-01-17T16:01:19,784][DEBUG][o.e.a.b.TransportShardBulkAction] [AMIENS] [llmlogs-2020.01-000001][0] failed to execute bulk item (index) index {[llmlogs-2020.01-000001][_doc][6wEFtG8B9YgiWpSw2Rkn], source[n/a, actual length: [10.9kb], max length: 2kb]} org.elasticsearch.index.mapper.MapperParsingException: failed to parse field [WorkingSetSize] of type [float] in document with id '6wEFtG8B9YgiWpSw2Rkn'. Preview of field's value: 'WorkingSetSize' It would appear the automatic column recognition is attempting to recognise the header as a value as well. Do you have any advice on why this may be happening?",
    "website_area": "discuss"
  },
  {
    "id": "ed7d036e-ff37-4d1b-b67a-e853f0dcc121",
    "url": "https://discuss.elastic.co/t/logstash-failed-to-execute/215568",
    "title": "Logstash Failed to Execute",
    "category": [
      "Logstash"
    ],
    "author": "Srinivas_Kondapally",
    "date": "January 18, 2020, 4:48pm January 20, 2020, 9:32am",
    "body": "Hello, I am new to Logstash, Just now download logstash and tried to run the command. logstash -e 'input{stdin{}}output{stdout{}}'. I got an error Sending Logstash logs to C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/logs which is now configured via log4j2.properties [2020-01-18T21:18:08,811][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>\"path.queue\", :path=>\"C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/data/queue\"} [2020-01-18T21:18:09,018][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>\"path.dead_letter_queue\", :path=>\"C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/data/dead_letter_queue\"} [2020-01-18T21:18:09,224][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-01-18T21:18:09,263][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.5.1\"} [2020-01-18T21:18:09,331][INFO ][logstash.agent ] No persistent UUID file found. Generating new UUID {:uuid=>\"3368f3e5-33fa-47bb-89a0-61e4f18d66a3\", :path=>\"C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/data/uuid\"} [2020-01-18T21:18:11,063][ERROR][logstash.agent ] Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>\"LogStash::ConfigurationError\", :message=>\"Expected one of [ \\t\\r\\n], \"#\", \"input\", \"filter\", \"output\" at line 1, column 1 (byte 1)\", :backtrace=>[\"C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/logstash-core/lib/logstash/compiler.rb:41:in compile_imperative'\", \"C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/logstash-core/lib/logstash/compiler.rb:49:in compile_graph'\", \"C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/logstash-core/lib/logstash/compiler.rb:11:in block in compile_sources'\", \"org/jruby/RubyArray.java:2584:in map'\", \"C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/logstash-core/lib/logstash/compiler.rb:10:in compile_sources'\", \"org/logstash/execution/AbstractPipelineExt.java:156:in initialize'\", \"org/logstash/execution/JavaBasePipelineExt.java:47:in initialize'\", \"C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/logstash-core/lib/logstash/java_pipeline.rb:27:in initialize'\", \"C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/logstash-core/lib/logstash/pipeline_action/create.rb:36:in execute'\", \"C:/Users/adity/Desktop/Elastic_Search/logstash-7.5.1/logstash-core/lib/logstash/agent.rb:326:in block in converge_state'\"]} [2020-01-18T21:18:12,260][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2020-01-18T21:18:16,372][INFO ][logstash.runner ] Logstash shut down. Please help me out from this situation. Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "d43e2e8d-efdf-41ca-a489-ad1793e15ba2",
    "url": "https://discuss.elastic.co/t/xml-response-from-unstructured-log/213916",
    "title": "XML response from unstructured log",
    "category": [
      "Logstash"
    ],
    "author": "Kishorekolla",
    "date": "January 6, 2020, 4:28pm January 6, 2020, 5:47pm January 9, 2020, 4:13pm January 14, 2020, 7:43am January 14, 2020, 7:45am January 14, 2020, 2:17pm January 20, 2020, 7:25am",
    "body": "Hi Team, I'm a beginner to logstash. Till now worked on configuring basic Input, Filebeats and Output plugins by following product documents which are available in the official website. Now I have a real time scenario below, I have an unstructured log file (abc.log) which is stored in the local folder, and it is the combination of Junk info and XML response in between. I need to extract only XML response from the log file through logstash pipeline. Please guide me to achieve this. Appreciate the quick response! Thanks Kishore K",
    "website_area": "discuss"
  },
  {
    "id": "223ff60e-38e0-4c8a-875d-cdb39712712a",
    "url": "https://discuss.elastic.co/t/how-to-logstash-convert-all-fields-to-integer-or-some-fields/215660",
    "title": "HOW TO logstash convert all fields to integer(or some fields)",
    "category": [
      "Logstash"
    ],
    "author": "esongi",
    "date": "January 20, 2020, 3:07am January 20, 2020, 5:36am January 20, 2020, 6:09am",
    "body": "Hello I am a foreign developer. Even if the translation is awkward, please understand My process is Elastic >> logstash >> fileBeat >> Kibana Logstash reads and uses csv file. The flow creates a csv file containing the headers, and then the data is stacked line by line. The header of the csv file is \"no, item, cherry, apple\". All subsequent values are numeric. ex) no, item, cherry, apple 1, 2, 3, 4 The first header is all text. I want to change all header types to integers when reading. \"no\" and \"item\" are fixed, but the \"cherry\" and \"apple\" are fluid, so they can't fix csv columns What method should I use? Should I use \"ruby filter\"? I don't know ruby yet This is my script now. Thank you for letting us know what you need to add or edit. input { beats { host => \"localhost\" port => \"5044\" } } filter { csv { #separator => \",\" #autogenerate_column_names => true skip_header => true autogenerate_column_names => true autodetect_column_names => true } mutate { remove_field => [ \"@version\", \"path\", \"host\", \"tags\", \"offset\", \"agent\" ] #convert => [\"[%{field}][long]\", \"integer\"] } grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } } date { #match => [ \"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\" ] match => [ \"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\" ] locale => \"ko\" } } output { elasticsearch { hosts => \"http://localhost:9200\" index => \"blank\" manage_template => false } }",
    "website_area": "discuss"
  },
  {
    "id": "8c48f728-61ee-4f89-bc47-7a76923c6751",
    "url": "https://discuss.elastic.co/t/optimizing-datefield-processing-by-moving-from-date-to-ruby-code/215665",
    "title": "Optimizing Datefield processing by moving from date{} to ruby code",
    "category": [
      "Logstash"
    ],
    "author": "scott_stash",
    "date": "January 20, 2020, 5:31am",
    "body": "Hello, Currently in my pipeline I'm parsing out loglines, each log line begins with a timestamp such as: \"2020-01-19T00:00:08.127+0800\" to a field called logtimestamp, and then later in my pipeline update this to the @timestamp field. For example.. date { match => [\"logTimestamp\", \"ISO8601\"] remove_field => [ \"logTimestamp\" ] } This works fine, but feels very slow. Looking at the stats under the pipeline, it is averaging between 0.05 - 0.06 ms/event on the date statement. To put in comparison, the dissect of my log line event takes on average 0.03 ms/event. I've started looking at the code in timestamp.rb and it looks like its taking this timestamp, converting it to milliseconds and creating a logstash timestamp object. If I run something like this it appears to drop me down to 0.03-0.04 ms/event: ruby { code => ' event.set(\"@timestamp\", LogStash::Timestamp.parse_iso8601(event.get(\"logtimestamp\"))) ' } A second note is that in my case the log files come from different timezones, but I want to have them all saved as UTC, so timezone data is always dropped... so for instance: event.set(\"@timestamp\", LogStash::Timestamp.parse_iso8601(event.get(\"logtimestamp\")[0..22])) event.remove(\"logtimestamp\")",
    "website_area": "discuss"
  },
  {
    "id": "9bd1948e-4a7c-4fc1-96f2-99ac62f39205",
    "url": "https://discuss.elastic.co/t/manipulation-with-logstash/215308",
    "title": "Manipulation with Logstash",
    "category": [
      "Logstash"
    ],
    "author": "katara",
    "date": "January 16, 2020, 12:24pm January 16, 2020, 1:32pm January 16, 2020, 2:01pm January 16, 2020, 3:00pm January 16, 2020, 3:08pm January 16, 2020, 3:12pm January 16, 2020, 3:14pm January 16, 2020, 3:20pm January 17, 2020, 6:09am January 17, 2020, 9:35am January 17, 2020, 10:48am January 17, 2020, 3:13pm January 20, 2020, 5:11am",
    "body": "Hi All, I am new to Logstash manipulations and I have no idea how to do the below. I have a sample data as below: Column:Type Incident Response P3 Incident Resolution L1.5 P2 ... The column data is not always the same so a grok may not help me. I want to extract the word 'Response' and 'Resolution' into a new column 'SLA type' Im looking for something very alike to the below SQL statement: case when Type like '%Resolution%' then Resolution when Type like '%Response%' then Response end as SLA_Type How do i manipulate this in Logstash? Any help is appreciated. Thanks in advance! Katara.",
    "website_area": "discuss"
  },
  {
    "id": "507e3068-78e7-454e-a3fb-30a33933f85e",
    "url": "https://discuss.elastic.co/t/logstash-timezone-while-creating-index/215623",
    "title": "Logstash timezone while creating index",
    "category": [
      "Logstash"
    ],
    "author": "bdn",
    "date": "January 19, 2020, 1:34pm January 19, 2020, 2:46pm",
    "body": "As I found logstash create an Index on UTC timezone. How can I create an index on the local timezone? I have set up an ES client that needs to query the index day-wise in local time. output { elasticsearch { hosts => [\"localhost:9200\"] index => \"logstash-%{+YYYY-MM-dd}\" manage_template => false } stdout { codec => rubydebug } }",
    "website_area": "discuss"
  },
  {
    "id": "f4cf4b1d-afb7-47e7-9c20-9049f8fcf108",
    "url": "https://discuss.elastic.co/t/logstash-losing-packets/215084",
    "title": "Logstash - Losing packets",
    "category": [
      "Logstash"
    ],
    "author": "ntran",
    "date": "January 15, 2020, 7:21am January 19, 2020, 12:29pm",
    "body": "HI all, I am ingesting syslogs through logstash and usually I have roughly 9k e/s. However, recently, I have seen it drop down to 5k e/s and fluctuate between 5k e/s - 9k e/s. Can someone help me diagnose why this is happening? Things to note: Logstash 7.4.0 CPU is usually around 80% Free memory is 5.9GB (I have 32GB allocated) Node pipeline stats: \"host\" : \"mgnt-logstash01\", \"version\" : \"7.4.0\", \"http_address\" : \"127.0.0.1:9600\", \"id\" : \"X\", \"name\" : \"mgnt-logstash-01\", \"ephemeral_id\" : \"X\", \"status\" : \"green\", \"snapshot\" : false, \"pipeline\" : { \"workers\" : 8, \"batch_size\" : 10000, \"batch_delay\" : 50 }, \"pipelines\" : { \"Infra_Syslog\" : { \"events\" : { \"out\" : 4470962057, \"duration_in_millis\" : 3003174588, \"in\" : 4470972521, \"queue_push_duration_in_millis\" : 550976911, \"filtered\" : 4470962057 }, \"plugins\" : { \"inputs\" : [ { \"id\" : \"670fc938f41298303b0b754578dc7e15d9497b2fa9ee1017810c3b097c0267d0\", \"queue_size\" : 2000, \"workers\" : 2, \"events\" : { \"out\" : 4458943645, \"queue_push_duration_in_millis\" : 534397844 }, \"name\" : \"udp\" }, { \"id\" : \"099f3eea267c36717652cb4341616d9bf93cba1e16a7af1d6e822de155583088\", \"events\" : { \"out\" : 12028878, \"queue_push_duration_in_millis\" : 16579067 }, \"name\" : \"tcp\" } ], \"codecs\" : [ { \"id\" : \"plain_6966be23-4ca7-4a68-b98e-73a2b9758e74\", \"decode\" : { \"out\" : 0, \"duration_in_millis\" : 0, \"writes_in\" : 0 }, \"encode\" : { \"duration_in_millis\" : 0, \"writes_in\" : 0 }, \"name\" : \"plain\" }, { \"id\" : \"json_b185f81d-56a8-41f8-9915-786c3da49090\", \"decode\" : { \"out\" : 0, \"duration_in_millis\" : 0, \"writes_in\" : 0 }, \"encode\" : { \"duration_in_millis\" : 53343934, \"writes_in\" : 4470962911 }, \"name\" : \"json\" }, { \"id\" : \"line_315db2b2-595d-4945-a11e-a738eb24c5d2\", \"decode\" : { \"out\" : 12028878, \"duration_in_millis\" : 17364669, \"writes_in\" : 1793739 }, \"encode\" : { \"duration_in_millis\" : 0, \"writes_in\" : 0 }, \"name\" : \"line\" }, { \"id\" : \"plain_77d2b923-a026-433b-8c5c-ec1fbc8abbf3\", \"decode\" : { \"out\" : 4458943649, \"duration_in_millis\" : 644170181, \"writes_in\" : 4458943650 }, \"encode\" : { \"duration_in_millis\" : 0, \"writes_in\" : 0 }, \"name\" : \"plain\" }, { \"id\" : \"plain_611b47de-ed9e-42b7-917f-a92ce818831c\", \"decode\" : { \"out\" : 0, \"duration_in_millis\" : 0, \"writes_in\" : 0 }, \"encode\" : { \"duration_in_millis\" : 0, \"writes_in\" : 0 }, \"name\" : \"plain\" } ], \"filters\" : [ { \"id\" : \"934a771838c9bcd1f436e379c0696a6f32058b1ebcdf7b5daf750bfca5bca9c2\", \"matches\" : 1, \"events\" : { \"out\" : 1, \"in\" : 1, \"duration_in_millis\" : 12045 }, \"patterns_per_field\" : { \"message\" : 1 }, \"failures\" : 0, \"name\" : \"grok\" }, { \"id\" : \"bdd0fd6f8cc37fa6072af652986607049fd404854a3dd569a7c97c740136cd2e\", \"matches\" : 440989731, \"events\" : { \"out\" : 4470969057, \"in\" : 4470969057, \"duration_in_millis\" : 145737941 }, \"patterns_per_field\" : { \"message\" : 1 }, \"failures\" : 4029979326, \"name\" : \"grok\" }, { \"id\" : \"6f598b9bc4ad7e3b20796a6850b249d9d35df1f9c679eabbc532a92a6a5e58dc\", \"matches\" : 15277710, \"events\" : { \"out\" : 37213132, \"in\" : 37213132, \"duration_in_millis\" : 2429139 }, \"failures\" : 21935422, \"name\" : \"dissect\" }, { \"id\" : \"217ad07965724e24768bf54001a67f85d30f0da9674c971ac56a2293ba453a83\", \"events\" : { \"out\" : 255838401, \"in\" : 255838668, \"duration_in_millis\" : 104272893 }, \"name\" : \"kv\" }, { \"id\" : \"04c5fb0683e49f7995aef5e2d24b1c10428510c2c4e2fe845e1551786f72992f\", \"events\" : { \"out\" : 13205943, \"in\" : 13205943, \"duration_in_millis\" : 1148899 }, \"failures\" : 13205943, \"name\" : \"dissect\" }, { \"id\" : \"0e8db4ac0424af114bfb2c67e3faaeefad27736a2470cdb58433cdb9a63567c3\", \"matches\" : 0, \"events\" : { \"out\" : 4029977656, \"in\" : 4029979326, \"duration_in_millis\" : 97451945 }, \"patterns_per_field\" : { \"message\" : 1 }, \"failures\" : 4029978667, \"name\" : \"grok\" }, { \"id\" : \"87b758b9304a426b4ded4d65f53c6dca5d7aa83bd60271cb99ec04aa7bf93706\", \"matches\" : 357601, \"events\" : { \"out\" : 37570738, \"in\" : 37570738, \"duration_in_millis\" : 5072079 }, \"failures\" : 37213137, \"name\" : \"dissect\" }, { \"id\" : \"061f1ab7f6a108e7a8da89af0a046d60c0892fd2ebb37b2e0a20646856313ec8\", \"events\" : { \"out\" : 1, \"in\" : 1, \"duration_in_millis\" : 3942 }, \"name\" : \"kv\" }, { \"id\" : \"fa9e17418c9cf8154457b5fd2ea5ab49db134494cbb38d622fe4411cafd221fc\", \"events\" : { \"out\" : 4470964056, \"in\" : 4470964056, \"duration_in_millis\" : 58656898 }, \"name\" : \"mutate\" }, { \"id\" : \"1279866a7ad11feddd713b1a607a98f0347c11310219d0aa129be66738f43d25\", \"matches\" : 8729470, \"events\" : { \"out\" : 21935412, \"in\" : 21935422, \"duration_in_millis\" : 1287181 }, \"failures\" : 13205950, \"name\" : \"dissect\" }, { \"id\" : \"7e2652d3a5abfd42e54eff6fdfc6c6d2eefebf891707098ea72aadf51f1d2001\", \"events\" : { \"out\" : 37213132, \"in\" : 37213132, \"duration_in_millis\" : 4043519 }, \"failures\" : 37213132, \"name\" : \"dissect\" }, { \"id\" : \"46a5277c0331012bd8182f90533d18a8e5fbbf2f4a0574b404240fc795e657d1\", \"events\" : { \"out\" : 255838401, \"in\" : 255838401, \"duration_in_millis\" : 2839473 }, \"name\" : \"mutate\" }, { \"id\" : \"f34120474514182b3339279327f0f27c9ef227d9098327dfe286765d8904273f\", \"matches\" : 8, \"events\" : { \"out\" : 13205943, \"in\" : 13205943, \"duration_in_millis\" : 1121878 }, \"failures\" : 13205935, \"name\" : \"dissect\" }, { \"id\" : \"0680d729de3501ab850aa52d20a5fbf9cd9766fbfa3cdbe4fb9302e67b1b412c\", \"matches\" : 3736568229, \"events\" : { \"out\" : 4029977656, \"in\" : 4029977656, \"duration_in_millis\" : 203891648 }, \"patterns_per_field\" : { \"message\" : 1 }, \"failures\" : 293409427, \"name\" : \"grok\" }, { \"id\" : \"6de2ec8bca57acc80224a79b9e63f8c0de83b1f2632453c40da90fef94591c04\", \"matches\" : 255838668, \"events\" : { \"out\" : 293409427, \"in\" : 293409427, \"duration_in_millis\" : 9612380 }, \"failures\" : 37570759, \"name\" : \"dissect\" }, { \"id\" : \"9a1159c054348705b1c00b8d6e42f443b45ac969884d8ea3108c52787e7085a7\", \"events\" : { \"out\" : 4470964056, \"in\" : 4470964056, \"duration_in_millis\" : 222271525 }, \"name\" : \"prune\" }, { \"id\" : \"218524cdbe33380671f5a459f90796e4d8e034e54b82a7a764046f10d304b07d\", \"events\" : { \"out\" : 1, \"in\" : 1, \"duration_in_millis\" : 3336 }, \"name\" : \"mutate\" } ], \"outputs\" : [ { \"id\" : \"c6ca9ff474e0f2c0432cc17124dde6dc03fcd6753be2bd0b6e75ba16f95d16dc\", \"events\" : { \"out\" : 4470962057, \"in\" : 4470963057, \"duration_in_millis\" : 296835858 }, \"name\" : \"udp\" }, { \"id\" : \"96f6a07550c3c42f5a5a411db143c2dc1a881d648590563009a56a7d4862d9b0\", \"bulk_requests\" : { \"successes\" : 1, \"responses\" : { \"200\" : 1 } }, \"events\" : { \"out\" : 1, \"in\" : 1, \"duration_in_millis\" : 27067 }, \"documents\" : { \"successes\" : 1 }, \"name\" : \"elasticsearch\" }, { \"id\" : \"9e7d9879fd913c7d24371735728a31c4edadbd0e641fedde20bf8ddcbc08ea3a\", \"bulk_requests\" : { \"successes\" : 4547103, \"responses\" : { \"200\" : 4547103 } }, \"events\" : { \"out\" : 4470963056, \"in\" : 4470964056, \"duration_in_millis\" : 1657533658 }, \"documents\" : { \"successes\" : 4470963056 }, \"name\" : \"elasticsearch\" } ] }, \"reloads\" : { \"successes\" : 0, \"last_error\" : null, \"last_failure_timestamp\" : null, \"failures\" : 18, \"last_success_timestamp\" : null }, \"queue\" : { \"type\" : \"persisted\", \"events_count\" : 19, \"queue_size_in_bytes\" : 19699803, \"max_queue_size_in_bytes\" : 966367641600 }, \"dead_letter_queue\" : { \"queue_size_in_bytes\" : 5 }, \"hash\" : \"45c4be097fd6462a583ae16b44bb741698f4774ef2e6abe6c6d2cf87bf033d1e\", \"ephemeral_id\" : \"07dd2c6e-98a2-43a1-bdba-c665e913e6f7\" }, \".monitoring-logstash\" : { \"events\" : null, \"plugins\" : { \"inputs\" : [ ], \"codecs\" : [ ], \"filters\" : [ ], \"outputs\" : [ ] }, \"reloads\" : { \"successes\" : 0, \"last_error\" : null, \"last_failure_timestamp\" : null, \"failures\" : 0, \"last_success_timestamp\" : null }, \"queue\" : null } }",
    "website_area": "discuss"
  },
  {
    "id": "4ec006bf-4a8a-435b-b15b-69d70f324211",
    "url": "https://discuss.elastic.co/t/logstash-breaks-my-log-line-into-multiple-documets/215156",
    "title": "Logstash breaks my log line into multiple documets",
    "category": [
      "Logstash"
    ],
    "author": "kasra",
    "date": "January 15, 2020, 3:32pm January 15, 2020, 3:33pm January 15, 2020, 4:09pm January 15, 2020, 5:34pm January 15, 2020, 6:05pm January 18, 2020, 6:55am January 18, 2020, 6:57am January 18, 2020, 7:09am January 18, 2020, 2:08pm January 19, 2020, 6:59am",
    "body": "I have nested JSON log files which I'm trying to parse and ship to my ES v.5 My logstash version is also 7.5. The problem is among for instance 3000 logs I get 250 _jsonparsefailure! My log format is nested JSON. here is my logstash config file: file { path => \"/home/kasra/test/*.log\" codec => json { charset => \"UTF-8\"} } } filter { json { source => \"message\" # skip_on_invalid_json => \"true\" } geoip { source => \"sourceIpAddress\" target => \"GeoLocation\" fields => [\"city_name\", \"country_name\", \"region_name\", \"location\"] } date { match => [\"eventTime\", \"ISO8601\"] locale => en timezone => \"Asia/Tehran\" target => \"@timestamp\" } } output { elasticsearch { hosts => ['X.X.X.X:9200'] index => [\"ali-%{+YYYY.MM.dd}\"] user => \"XXXX\" password => \"XXXX\" } } and here is my sample log which usually have it corrupted and broken by logstash into 2 or 3 logs.. [{\"acsRegion\":\"test\",\"additionalEventData\":{\"Scheme\":\"https\"},\"apiVersion\":\"2016-11-11\",\"eventId\":\"CA6D8ACD-884D-461A-AFBE-35B81D43036C\",\"eventName\":\"AttachPolicy\",\"eventSource\":\"resourcemanager-share\",\"eventTime\":\"2020-01-12T08:43:36Z\",\"eventType\":\"ApiCall\",\"eventVersion\":\"1\",\"referencedResources\":{\"Policy\":[\"test\"]},\"requestId\":\"CA6D8ACD-884D-461A-AFBE-35B81D43036C\",\"requestParameters\":{\" charset\":\"UTF-8\",\"PrincipalName\":\"test.com\",\"AcceptLanguage\":\"en-US\",\"SDSDSD\":\"asdasd\",\"PolicyName\":\"testAccess\",\"ResourceGroupId\":556422749542138042,\"RequestId\":\"CA6D8222ACD-884D-4261A-AFBE-35B81D43036C\",\"HostId\":\"test2.com\",\"PrincipalType\":\"IMSUser\",\"PolicyType\":\"System\"},\"serviceName\":\"ResourceManager\",\"sourceIpAddress\":\"y.x.z.z\",\"userAgent\":\"Apache-HttpClient/4.5.7 (Java/1.8.0_152)\",\"userIdentity\":{\"accountId\":\"556474922222542\",\"principalId\":\"556433954213\",\"sessionContext\":{\"attributes\":{\"creationDate\":\"2020-01-12T08:43:35Z\",\"mfaAuthenticated\":\"false\"}},\"type\":\"test-account\",\"userName\":\"test\"}},{\"acsRegion\":\"test\",\"additionalEventData\":{\"Scheme\":\"https\"},\"apiVersion\":\"2016-11-11\",\"eventId\":\"2BBED315-ABA2E-44E3EE-99335A-96sss265A69904C\",\"eventName\":\"ListPolicyAttachments\",\"eventSource\":\"resourcemanager-shsdsdsd.com\",\"eventTime\":\"2020-01-12T08:43:40Z\",\"eventType\":\"ApiCall\",\"eventVersion\":\"1\",\"requestId\":\"2BBED315-AB2E-44EE-995A-965904C\",\"requestParameters\":{\" charset\":\"UTF-8\",\"AcceptLanguage\":\"en-US\",\"SDSDSDSVVV\":\"asd\",\"RequestId\":\"2BBED3-ASSB2E-43334EE-995A-96265A69904C\",\"HostId\":\"resourcemanager-sdsdsdds.com\"},\"serviceName\":\"ResourceManager\",\"sourceIpAddress\":\"xx.xx.x.xx\",\"userAgent\":\"Apache-HttpClient/4.5.7 (Java/1.8.0_152)\",\"userIdentity\":{\"accountId\":\"5232323232323\",\"principalId\":\"5353535353535353535\",\"sessionContext\":{\"attributes\":{\"creationDate\":\"2020-01-12T08:43:39Z\",\"mfaAuthenticated\":\"false\"}},\"type\":\"test-account\",\"userName\":\"test\"}},{\"acsRegion\":\"test\",\"additionalEventData\":{\"Scheme\":\"https\"},\"apiVersion\":\"2016-11-11\",\"eventId\":\"BEsdsdsdsd-4B1C-BDED-50ssdsdSDSDSDECE5\",\"eventName\":\"AttachPolicy\",\"eventSource\":\"resourcemanager-shsdsdsdsd.com\",\"eventTime\":\"2020-01-12T08:44:00Z\",\"eventType\":\"ApiCall\",\"eventVersion\":\"1\",\"referencedResources\":{\"Policy\":[\"AdministratorAccess\"]},\"requestId\":\"BE1DSDSD6079-49AE-4B1C-BDED-50SDSDSD1DE5\",\"requestParameters\":{\" charset\":\"UTF-8\",\"PrincipalName\":\"test@test.com\",\"AcceptLanguage\":\"en-US\",\"AFFSF\":\"asdasd\",\"PolicyName\":\"AdministratorAccess\",\"ResourceGroupId\":556474954213823232323042,\"RequestId\":\"BE1D6SDSDSD079-41213239AE-4B1sdsdSDSDC-BDED-501DA82EECE5\",\"HostId\":\"resourcemanager-shsdsdsdsd.com\",\"PrincipalType\":\"AASCUser\",\"PolicyType\":\"System\"},\"serviceName\":\"ResourceManager\",\"sourceIpAddress\":\"XX.XX.X.X\",\"userAgent\":\"Apache-HttpClient/4.5.7 (Java/1.8.0_152)\",\"userIdentity\":{\"accountId\":\"5562323232542138042\",\"principalId\":\"556423232338042\",\"sessionContext\":{\"attributes\":{\"creationDate\":\"2020-01-12T08:43:59Z\",\"mfaAuthenticated\":\"false\"}},\"type\":\"test-account\",\"userName\":\"test\"}},{\"acsRegion\":\"test\",\"additionalEventData\":{\"Scheme\":\"http\"},\"apiVersion\":\"2017-04-30\",\"eventId\":\"CEBFE232323E06-F7C4-4B64-8FBF-7A6D61F47780\",\"eventName\":\"ListGroups\",\"eventSource\":\"Aas-shasdasdasdasd.com\",\"eventTime\":\"2020-01-12T08:44:10Z\",\"eventType\":\"ApiCall\",\"eventVersion\":\"1\",\"requestId\":\"CEBFESDSDSD23E06-F7SD2C4-4SD23B64-8FSD23BF-7A6SD23D61F47780\",\"requestParameters\":{\"MaxItems\":1000,\" charset\":\"UTF-8\",\"AcceptLanguage\":\"en-US\",\"AAAP\":\"rsd\",\"RequestId\":\"CEBFSDSDSDEE06-FSDSD7C4-4SDSDB64-8FSDSDBF-7A6D232323SDSD610\",\"HostId\":\"ssds-shasdsdsd.com\"},\"serviceName\":\"Asd\",\"sourceIpAddress\":\"X.XX.X.X\",\"userAgent\":\"Apache-HttpClient/4.5.7 (Java/1.8.0_152)\",\"userIdentity\":{\"accountId\":\"556232323232342\",\"principalId\":\"556343434343434042\",\"sessionContext\":{\"attributes\":{\"creationDate\":\"2020-01-12T08:44:10Z\",\"mfaAuthenticated\":\"false\"}},\"type\":\"test-account\",\"userName\":\"test\"}},{\"acsRegion\":\"test\",\"additionalEventData\":{\"Scheme\":\"http\"},\"apiVersion\":\"2017-04-30\",\"eventId\":\"1E01123235FB8-32323530-4E232321-B12323EB-523239C0DB7AEAA1\",\"eventName\":\"GetUser\",\"eventSource\":\"asd-shasdsdsdd.com\",\"eventTime\":\"2020-01-12T08:44:48Z\",\"eventType\":\"ApiCall\",\"eventVersion\":\"1\",\"referencedResources\":{\"User\":[\"testsssdsdsdaax.com\"]},\"requestId\":\"1E02323232315FB8-353232323230-4E2321-B1EB-59C0ASDASD23DDVB7AEAA1\",\"requestParameters\":{\" charset\":\"UTF-8\",\"AcceptLanguage\":\"en-US\",\"SDS\":\"asdsd\",\"RequestId\":\"1E023232315FB8-35232330-42323E21-B12323EB-59C02323DB7AEAA1\",\"HostId\":\"sdsd-shasdsdsdsd.com\",\"UserPrincipalName\":\"testttt.com\"},\"serviceName\":\"asd\",\"sourceIpAddress\":\"XX.XX.XX.XX\",\"userAgent\":\"Apache-HttpClient/4.5.7 (Java/1.8.0_152)\",\"userIdentity\":{\"accountId\":\"55647424443434\",\"principalId\":\"556455555343434\",\"sessionContext\":{\"attributes\":{\"creationDate\":\"2020-01-12T08:44:47Z\",\"mfaAuthenticated\":\"false\"}},\"type\":\"test-account\",\"userName\":\"test\"}},{\"acsRegion\":\"testtttt\",\"additionalEventData\":{\"Scheme\":\"https\"},\"apiVersion\":\"2016-11-11\",\"eventId\":\"4sdsdsdds57-633434-VF0E-AADA-80C4D5155F\",\"eventName\":\"ListPolicyAttachments\",\"eventSource\":\"resourcemanager-shasdasdasd.com\",\"eventTime\":\"2020-01-12T08:44:50Z\",\"eventType\":\"ApiCall\",\"eventVersion\":\"1\",\"requestId\":\"45023232323DE9B7-6402323234-4A0E-A2DA-82323232323155F\",\"requestParameters\":{\" charset\":\"UTF-8\",\"PrincipalName\":\"test@test.com\",\"AcceptLanguage\":\"en-US\",\"ASDDSX\":\"asd\",\"RequestId\":\"45232323230DE9B7-6232323404-4A5550E-A2DA-80C4D995155F\",\"HostId\":\"resourcemanager-sharsdasdasd.com\",\"PrincipalType\":\"ISDSD\"},\"serviceName\":\"ResourceManager\",\"sourceIpAddress\":\"XX.XX.XX.XX\",\"userAgent\":\"Apache-HttpClient/4.5.7 (Java/1.8.0_152)\",\"userIdentity\":{\"accountId\":\"4444444444444444555\",\"principalId\":\"5555555554433\",\"sessionContext\":{\"attributes\":{\"creationDate\":\"2020-01-12T08:44:49Z\",\"mfaAuthenticated\":\"false\"}},\"type\":\"test-account\",\"userName\":\"test\"}}] therefore I've got lots of ERROR in logstash which shows there are Json::ParserErrors it tries to parse broken logs which are not in JSON format obviously!",
    "website_area": "discuss"
  },
  {
    "id": "1a60fa64-7bf1-4ce3-9f58-80986c7683a7",
    "url": "https://discuss.elastic.co/t/exception-in-executing-the-jdbc-query/215564",
    "title": "Exception in executing the jdbc query",
    "category": [
      "Logstash"
    ],
    "author": "jagan",
    "date": "January 18, 2020, 2:15pm January 18, 2020, 2:56pm January 18, 2020, 5:17pm January 19, 2020, 2:20am",
    "body": "Hi guru's, Please help me to solve the issue the as i am facing the exception in executing the jdbc query: The error is as follows: [WARN ][logstash.inputs.jdbc ] Exception when executing JDBC query {:exception=>#<Sequel::DatabaseError: Java::JavaSql::SQLSyntaxErrorException: ORA-01722: invalid number }. I have taken the select statement as the source for the index creation.but when i run the query in oracle i have got the proper output.But when i try to execute the ingestion command from logstash directory...bin\\logstash -f config__.conf got the above error. please help me to solve the issue. image1315164 9.6 KB Thanks in advance, jagdish",
    "website_area": "discuss"
  },
  {
    "id": "ee03a723-5b64-4ee6-8887-b8628811018c",
    "url": "https://discuss.elastic.co/t/logstash-installation-on-aws-ubuntu-instance/213761",
    "title": "Logstash Installation on AWS Ubuntu Instance",
    "category": [
      "Logstash"
    ],
    "author": "droidus",
    "date": "January 4, 2020, 12:57am January 5, 2020, 4:02am January 5, 2020, 2:20pm January 18, 2020, 11:00pm January 19, 2020, 12:29am",
    "body": "I am trying to get ElasticSearch setup on my Ubuntu server 18 on AWS. logstash seems to be installed: \"logstash/stable,now 1:7.5.1-1 all [installed]\" But when I check the config of logstash, it seems like logstash is not installed: \"service logstash configtest logstash: unrecognized service\" I have been using this guide: https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-ubuntu-14-04.",
    "website_area": "discuss"
  },
  {
    "id": "a9aba21c-107e-4582-ba22-1aa00c542bb2",
    "url": "https://discuss.elastic.co/t/logstash-output-wrong-error-in-visualization-esaggs-field-is-a-required-parameter/215412",
    "title": "Logstash Output wrong - Error in visualization [esaggs] > \"field\" is a required parameter",
    "category": [
      "Logstash"
    ],
    "author": "hispeed",
    "date": "January 17, 2020, 5:29am January 17, 2020, 9:00am January 17, 2020, 8:26pm January 18, 2020, 2:58pm",
    "body": "Hi, I recieve in Kibana the following error: \"[esaggs] > Saved \"field\" parameter is now invalid. Please select a new field.\" In logstash log i see also: [2020-01-17T06:10:41,665][WARN ][logstash.outputs.elasticsearch][main] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"winlogbeat-2020.01.14\", :routing=>nil, :_type=>\"_doc\"}, #LogStash::Event:0x71629e42], :response=>{\"index\"=>{\"_index\"=>\"winlogbeat-2020.01.14\", \"_type\"=>\"_doc\", \"_id\"=>\"ar3psW8BXwWkOg89GOdx\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse field [winlog.event_data.param1] of type [date] in document with id 'ar3psW8BXwWkOg89GOdx'. Preview of field's value: 'Netzwerkeinrichtungsdienst'\", \"caused_by\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"failed to parse date field [Netzwerkeinrichtungsdienst] with format [strict_date_optional_time||epoch_millis]\", \"caused_by\"=>{\"type\"=>\"date_time_parse_exception\", \"reason\"=>\"Failed to parse with all enclosed parsers\"}}}}}} [2020-01-17T06:10:41,666][WARN ][logstash.outputs.elasticsearch][main] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"winlogbeat-2020.01.14\", :routing=>nil, :_type=>\"_doc\"}, #LogStash::Event:0x2c296414], :response=>{\"index\"=>{\"_index\"=>\"winlogbeat-2020.01.14\", \"_type\"=>\"_doc\", \"_id\"=>\"a73psW8BXwWkOg89GOdx\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse field [winlog.event_data.param1] of type [date] in document with id 'a73psW8BXwWkOg89GOdx'. Preview of field's value: 'Netzwerkeinrichtungsdienst'\", \"caused_by\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"failed to parse date field [Netzwerkeinrichtungsdienst] with format [strict_date_optional_time||epoch_millis]\", \"caused_by\"=>{\"type\"=>\"date_time_parse_exception\", \"reason\"=>\"Failed to parse with all enclosed parsers\"}}}}}} I have a Windows Server 2019 machine from where I want to ship all logs with Winlogbeat correctly to logstash and then into elasticsearch. V.7.5.1 (whole ELK stack and Winlogbeat). The problem is probably in the logstash output: output { if [@metadata][beat] { elasticsearch { hosts => [\"http://localhost:9200\"] manage_template => true index => \"%{[@packetbeat][beat]}-%{+YYYY.MM.dd}\" document_type => \"%{[@metadata][type]}\" } } else { elasticsearch { hosts => [\"http://localhost:9200\"] index => \"pf-%{+YYYY.MM.dd}\" } } } Can anyone help? I have replaced \"index => \"%{[@packetbeat][beat]}-%{+YYYY.MM.dd}\" \" with different setups but it doesn't work.",
    "website_area": "discuss"
  },
  {
    "id": "34f159a8-12a6-4776-a9fd-9f11af094707",
    "url": "https://discuss.elastic.co/t/json-data-parsed-from-logstash-http-poller-plugin-cannot-be-queried-in-kibana-maps/215559",
    "title": "Json data parsed from logstash http poller plugin cannot be queried in kibana maps",
    "category": [
      "Logstash"
    ],
    "author": "durgesh",
    "date": "January 18, 2020, 12:39pm",
    "body": "input { http_poller { urls => { test => { url => \"http://myurl:80/test\" headers => { Accept => \"application/json\" } } } schedule => { cron => \"* * * * * UTC\"} codec => \"json\" } } filter { split { field => \"features\" } } output { elasticsearch { hosts => [\"myhost:9200\"] hosts => \"myhost:9200\" codec => json } } The data coming from api is { \"type\": \"FeatureCollection\", \"name\": \"logan_international_airport\", \"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\" } }, \"features\": [ { \"type\": \"Feature\", \"properties\": { }, \"geometry\": { \"type\": \"Point\", \"coordinates\": [ -71.004550070725642, 42.364101185471185 ] } } ] }",
    "website_area": "discuss"
  },
  {
    "id": "6879b1b2-45c9-4445-861a-a997f9e055b1",
    "url": "https://discuss.elastic.co/t/cant-index-from-logstash-to-elasticsearch-using-ilm/215536",
    "title": "Can't index from logstash to Elasticsearch using ILM",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "January 18, 2020, 1:03am January 18, 2020, 2:33am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "87927a8b-2bb5-4319-9e05-77a3b4a3fa09",
    "url": "https://discuss.elastic.co/t/adding-characters-to-a-field/215513",
    "title": "Adding characters to a field",
    "category": [
      "Logstash"
    ],
    "author": "megajune",
    "date": "January 17, 2020, 8:05pm January 17, 2020, 10:52pm",
    "body": "I have this value in a field: 881908396bfd I want to format it like this: 88:19:08:39:6b:fd What's the \"best\" way to do that?",
    "website_area": "discuss"
  },
  {
    "id": "22206236-4db7-4ae0-baed-3a429757e340",
    "url": "https://discuss.elastic.co/t/splitting-a-string-to-json-like-results/215503",
    "title": "Splitting a string to json like results",
    "category": [
      "Logstash"
    ],
    "author": "Newtoelastic",
    "date": "January 17, 2020, 6:39pm January 17, 2020, 6:58pm January 17, 2020, 7:19pm January 17, 2020, 7:23pm January 17, 2020, 8:24pm January 17, 2020, 7:47pm January 17, 2020, 7:50pm January 17, 2020, 7:50pm January 17, 2020, 7:58pm January 17, 2020, 8:03pm January 17, 2020, 8:07pm",
    "body": "Hey there, I have my own logs coming into the logstash that are being created from my python script. The format for the logs is: %{DATA:Sender}|%{DATA:Recipient}|%{DATA:Subject}|(%{DATA:Links})?$ So far so good. Problem is I am getting multiple (unknown amount of links) in the last field. I would like to split these links so the end result will look something along the lines of: links:{ http://link1.com http://link2.com http://link3,com } I am really new to Elastic so help is greatly appreciated. I need to know how do I exactly split this field. This is top priority for me as tomorrow evening I need to show this to my client",
    "website_area": "discuss"
  },
  {
    "id": "a5b9a811-6ec5-4c18-93df-8ee41b86832c",
    "url": "https://discuss.elastic.co/t/couchdb-changes-gives-invalid-fieldreference-error-when-parsing-couchdb-which-hosts-couchapp/214685",
    "title": "Couchdb_changes gives Invalid FieldReference error when parsing couchdb which hosts couchapp",
    "category": [
      "Logstash"
    ],
    "author": "Raymond_Tsang",
    "date": "January 10, 2020, 11:10pm January 11, 2020, 12:51am January 13, 2020, 2:56pm January 13, 2020, 4:38pm January 13, 2020, 6:44pm January 13, 2020, 8:25pm January 13, 2020, 9:48pm January 13, 2020, 11:10pm January 13, 2020, 11:13pm January 14, 2020, 2:58pm January 14, 2020, 6:11pm January 17, 2020, 4:59pm",
    "body": "Here is the log: Jan 10 16:45:25 Andromeda logstash[12489]: [2020-01-10T16:45:25,081][INFO ][logstash.inputs.couchdbchanges][main] Connecting to CouchDB _changes stream at: {:host=>\"localhost\", :port=>\"5984\", :db=>\"material_database\"} Jan 10 16:45:25 Andromeda logstash[12489]: [2020-01-10T16:45:25,083][INFO ][logstash.inputs.couchdbchanges][main] Using service uri : {:uri=>#<URI::HTTP http://localhost:5984/material_database/_changes?feed=continuous&include_docs=true&since=950-g1AAAAJjeJyd0EsKwkAMANDBCrr1BHoCmcm0nXZlb6LzaSmlKoIudKO48xR6E72J3qTOpwsXRZgSSCAhD5IaITQuA4UmcnuQpRIZATbHOkitRwOOxLRpmqoMuFjrxigBYAxw18IfRsx0FotW2luJREVBmfCVMiMtW-lqpbSgKajcV1oZ6dxKRyuFjKZhHnpKm6HO6KKLxu5GO1kNxyxWkPTSHk57Gm1nNUoB59j3X057Oe39oxEuZEx6aR-n2b_d3KWcgeS8a6_6AnUdodA&heartbeat=1000>} Jan 10 16:45:25 Andromeda logstash[12489]: [2020-01-10T16:45:25,193][ERROR][logstash.javapipeline ][main] A plugin had an unrecoverable error. Will restart this plugin. Jan 10 16:45:25 Andromeda logstash[12489]: Pipeline_id:main Jan 10 16:45:25 Andromeda logstash[12489]: Plugin: <LogStash::Inputs::CouchDBChanges port=>5984, db=>\"material_database\", id=>\"403531702e10d9afa71b0a66780fe20bcc554cac3c90be7e534d12222897105f\", enable_metric=>true, codec=><LogStash::Codecs::Plain id=>\"plain_a0ec6ba6-e456-4605-b443-61d4b7b5d20b\", enable_metric=>true, charset=>\"UTF-8\">, host=>\"localhost\", secure=>false, password=><password>, heartbeat=>1000, keep_id=>false, keep_revision=>false, ignore_attachments=>true, always_reconnect=>true, reconnect_delay=>10> Jan 10 16:45:25 Andromeda logstash[12489]: Error: Invalid FieldReference: `a[href=#logout]` Jan 10 16:45:25 Andromeda logstash[12489]: Exception: Java::OrgLogstash::FieldReference::IllegalSyntaxException Jan 10 16:45:25 Andromeda logstash[12489]: Stack: org.logstash.FieldReference$StrictTokenizer.tokenize(FieldReference.java:283) My config file: input { couchdb_changes { db => material_database port => 5984 type => material_database } } output { elasticsearch { hosts => [\"http://localhost:9200\"] index => \"material_database\" } } How can I fix it?",
    "website_area": "discuss"
  },
  {
    "id": "f406a776-6d4d-4160-826b-ceafb261a9d7",
    "url": "https://discuss.elastic.co/t/grok-positional-structure-problem/215475",
    "title": "Grok positional structure problem",
    "category": [
      "Logstash"
    ],
    "author": "Ahmad_Moh_d_El_Hasan",
    "date": "January 17, 2020, 2:03pm January 17, 2020, 3:06pm January 17, 2020, 3:18pm January 17, 2020, 4:04pm January 17, 2020, 4:26pm",
    "body": "Dear sirs I have a question about the grok plugin: I have some logs which are built in the following way and my problem is that the order of my fields can mutate from log to log (in the example here I should extract from all the logs both the \"user id\" and the \"document id\"). So I am wondering how I should build the grok pattern to handle both the logs hopeing there is a better solution of writing two different patterns (or how to write a regex to find a specific attribute anywhere in the log without knowing before its exact position) 2019-01-15 INFO myclass - mymethod: user id: 12345 custom message 1 document id: 843572309845 2019-01-15 WARN myclass - mymethod1: document id: 43543534 custom message 2 user id: 98589348543 custom message 3 agent id: 98435734 the expected result is: date: 2019-01-15 level: INFO class: myclass method: mymethod message: user id: 12345 custom message 1 document id: 843572309845 userId: 12345 documentId: 843572309845 date: 2019-01-15 level: WARN class: myclass method: mymethod1 message: document id: 43543534 custom message 2 user id: 98589348543 custom message 3 agent id: 98435734 userId: 98589348543 documentId: 43543534 agentId: 98589348543 Looking forward for your answer, thank you.",
    "website_area": "discuss"
  },
  {
    "id": "61742ae6-6106-492e-9cda-0812de69d769",
    "url": "https://discuss.elastic.co/t/logstash-input-from-elastic-https/215313",
    "title": "Logstash Input From Elastic HTTPS",
    "category": [
      "Logstash"
    ],
    "author": "reyhanadp",
    "date": "January 16, 2020, 12:48pm January 17, 2020, 4:05pm",
    "body": "hi I want to retrieve data from an elasticsearch server that uses https. logstash config that I use to retrieve data from elasticsearch as follows : input { elasticsearch { hosts => [\"https://RVRPAOR1ABC19WD:9200\"] user => \"elastic\" password => \"password1234\" ca_file => \"D:\\logstash-7.5.1\\certificate.crt\" ssl => true index => \"default-2020.01\" query => ' { \"query\": { \"match_all\": {} } } ' } } output { csv { fields => [\"level\", \"Message\", \"source\", \"component\",\"logType\",\"fingerprint\",\"windowsIdentity\",\"machineName\",\"processName\",\"processVersion\",\"jobId\",\"robotName\",\"fileName\",\"tenantKey\"] path => \"csv-export.csv\" } } filter { mutate { convert => { \"machineId\" => \"integer\" \"levelOrdinal\" => \"integer\" } } } The error I get is as follows : image1920741 74.3 KB",
    "website_area": "discuss"
  },
  {
    "id": "e5aa51f5-542c-46b6-a682-f1bedaaa693b",
    "url": "https://discuss.elastic.co/t/differentiate-linux-syslog-log-from-windows-event-log/215449",
    "title": "Differentiate Linux syslog log from Windows event log",
    "category": [
      "Logstash"
    ],
    "author": "claud81",
    "date": "January 17, 2020, 11:35am January 17, 2020, 1:19pm January 17, 2020, 1:40pm January 17, 2020, 2:24pm January 17, 2020, 2:44pm January 17, 2020, 3:45pm",
    "body": "Hi everyone, we have 2 conf file in Logstash (version 6.8.6) conf.d directory to differentiate Microsoft log from Linux log, the first: input { tcp { port => 3515 codec => json } } filter { mutate { add_tag => [\"forwardedevtx\"] } } output { elasticsearch { hosts => [ \"nodeX:9200\", \"nodeX:9200\", \"nodeX:9200\" ] index => \"forwardedevtx-%{+YYYY.MM.dd}\" } } the second: input { tcp { port => 5000 type => syslog } } filter { if [type] == \"syslog\" { grok { match => { \"message\" => \"%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\\[%{POSINT:syslog_pid}\\])?: %{GREEDYDATA:syslog_message}\" } add_field => [ \"received_at\", \"%{@timestamp}\" ] add_field => [ \"received_from\", \"%{host}\" ] } syslog_pri { } date { match => [ \"syslog_timestamp\", \"MMM d HH:mm:ss\", \"MMM dd HH:mm:ss\" ] } mutate { add_tag => [\"forwardedsyslog\"] } } } output { elasticsearch { hosts => [ \"nodeX:9200\", \"nodeX:9200\", \"nodeX:9200\" ] index => \"forwardedsyslog-%{+YYYY.MM.dd}\" } } I can't explain why in Kibana I can't see this difference, if I select forwardedsyslog or forwardedevtx index pattern I see all the logs, both Microsoft and Linux. I would like to have into forwardedsyslog index only the Linux one, and into forwardedevtx only the Microsoft one, what am I missing? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "f3724fdb-7a82-4b54-a1cc-8a5acd4262a9",
    "url": "https://discuss.elastic.co/t/logstash-date-parsing-error-from-kinesis/215310",
    "title": "Logstash Date parsing error from Kinesis",
    "category": [
      "Logstash"
    ],
    "author": "Chris_Broll",
    "date": "January 16, 2020, 12:20pm January 17, 2020, 3:22pm",
    "body": "I have picked up a Logstash server to look after and I can see some incoming events from a Kinesis input that is failing to be indexed: Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"fargate-container-development-2020.01.16\", :_type=>\"doc\", :_routing=>nil}, #<LogStash::Event:0x7677b8d7>], :response=>{\"index\"=>{\"_index\"=>\"fargate-container-development-2020.01.16\", \"_type\"=>\"doc\", \"_id\"=>\"1234567890\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse [time]\", \"caused_by\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"For input string: \\\"2020-01-16T11:29:38.157Z\\\"\"}}}}} I thought that should be a simple fix and tried this: filter { if \"kinesis\" in [tags] and \"fargatecontainerlogs\" in [tags] { date { match => [ \"time\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\", \"ISO8601\" ] target => \"@timestamp\" remove_field => [ \"time\" ] } json { source => \"message\" remove_field => [ \"message\" ] } } } Any idea where I went wrong?",
    "website_area": "discuss"
  },
  {
    "id": "32ccda9c-1ae7-4870-adc3-968a6a0afdd2",
    "url": "https://discuss.elastic.co/t/using-logstash-to-correct-timestamps/215379",
    "title": "Using Logstash to correct timestamps",
    "category": [
      "Logstash"
    ],
    "author": "Hyperion0",
    "date": "January 16, 2020, 7:30pm January 16, 2020, 8:01pm January 16, 2020, 8:35pm January 16, 2020, 9:45pm January 17, 2020, 2:09pm January 17, 2020, 2:29pm January 17, 2020, 3:01pm January 17, 2020, 3:06pm January 17, 2020, 3:11pm",
    "body": "I'm very new to the ELK stack. I have several logs that starts collecting data before the systems time is set. The result is I have several hundred log entries with in accurate time stamps. Is there a way I can edit the incorrect timestamps in logstash based on the 1st correct entry in the log? Or is there a way to edit them after they have been entered into Elasticsearch?",
    "website_area": "discuss"
  },
  {
    "id": "2a548386-c7e3-4d1d-ac53-4bb73e3e7fb1",
    "url": "https://discuss.elastic.co/t/how-to-skip-some-lines-from-a-log-file-without-skipping-the-complete-message/215399",
    "title": "How to skip some lines from a log file without skipping the complete message",
    "category": [
      "Logstash"
    ],
    "author": "Daniel_Gonzalez1",
    "date": "January 17, 2020, 1:38am January 20, 2020, 6:37pm",
    "body": "Hello Everyone! I'm working with a file that has some lines that I want to skip when they come to logstash. These are a sample of the log lines: > 15Jan20 11:34:10.39 PRHTGRCE MXP1 RACF CONNECT success for PRHTGRCE: CONNECT C067684 > Jobname + id: PRHTGRCE > RACF command: CONNECT C067684 AUTHORITY(USE) GROUP(ATRBO01) NOADSP NOAUDITOR NOGRPACC NOOPERATIONS NOSPECIAL OWNER(ATRBO01) RESUME UACC(NONE) > Name : USER PARA GRUPO CER Instdata : USUARIO PARA LA CONEXION DE GRUPOS RACF REP. OSI > > 15Jan20 11:34:14.61 PRHTGRCE MXP1 RACF ALTUSER success for PRHTGRCE: ALTUSER C110250 > Jobname + id: PRHTGRCE > RACF command: ALTUSER C110250 NOUAUDIT > Name : USER PARA GRUPO CER Instdata : USUARIO PARA LA CONEXION DE GRUPOS RACF REP. OSI > 1zSecure Audit for RACF user events 15Jan20 00:31 to 15Jan20 18:36 list 51 > SMF records for all users with successful commandswq > > Date/time User Sys Description > > 15Jan20 11:34:15.14 PRHTGRCE MXP1 RACF CONNECT success for PRHTGRCE: CONNECT C110250 > Jobname + id: PRHTGRCE > RACF command: CONNECT C110250 AUTHORITY(USE) GROUP(ATRBO01) NOADSP NOAUDITOR NOGRPACC NOOPERATIONS NOSPECIAL OWNER(ATRBO01) RESUME UACC(NONE) > Name : USER PARA GRUPO CER Instdata : USUARIO PARA LA CONEXION DE GRUPOS RACF REP. OSI 15Jan20 11:51:28.40 PRHTGRCE MXP1 RACF ALTUSER success for PRHTGRCE: ALTUSER C067734 1zSecure Audit for RACF user events 15Jan20 00:31 to 15Jan20 18:36 list 52 SMF records for all users with successful commands Date/time User Sys Description Jobname + id: PRHTGRCE RACF command: ALTUSER C067734 NOUAUDIT Name : USER PARA GRUPO CER Instdata : USUARIO PARA LA CONEXION DE GRUPOS RACF REP. OSI The thing that I'm looking for is to omit the next lines: 1zSecure Audit for RACF user events 15Jan20 00:31 to 15Jan20 18:36 list 51 > SMF records for all users with successful commandswq > > Date/time User Sys Description without skipping the above or the continue of the log data. It suppose that the info that I want to omit are header of a report, this headers comes paste with the logs Each log line starts with the date 15Jan20 11:34:15.14 I used the next config to try to skip all the lines that came with those variables, but it skips the whole message. input { file { path => [ \"/home/linux/Downloads/reporte_mf\" ] start_position => \"beginning\" sincedb_path => \"/dev/null\" codec => multiline { pattern => \"(?<date>\\s\\d+\\d+\\w+\\d\\d\\s\\d+:\\d+:\\d+.\\d+)\" negate => true what => previous } } } # The filter part of this file is commented out to indicate that it is # optional. filter { grok{ match => { \"message\" => \"\\s(?<date>\\d\\d\\w+\\d\\d\\s\\d+:\\d+:\\d+.\\d+)\\s\" } } if [message] =~ /(1zSecure.+| SMF records for all users with successful commands| Date\\/time.+)/ { drop {} } } output { stdout { codec => rubydebug } } For example, if in the message the confing finds some of the lines I want to skip, it omit that message, not only the lines I want to skip Is it possible to erase or omit those lines of the log, without skipping the complete message?",
    "website_area": "discuss"
  },
  {
    "id": "73e59745-7886-49b0-a1c7-37a062067376",
    "url": "https://discuss.elastic.co/t/logstash-is-not-pulling-data-from-s3-bucket/215470",
    "title": "Logstash is not pulling data from s3 bucket",
    "category": [
      "Logstash"
    ],
    "author": "Sreekanth3",
    "date": "January 17, 2020, 2:56pm",
    "body": "Hi , This is my logstash conf input { s3 { access_key_id => \"xxxx\" secret_access_key => \"xxx\" #bucket => \"mybucketname/2020/01/16\" bucket => \"sftlcloudtrail\" sincedb_path => \"/tmp/last-s3-file-s3-access-logs\" additional_settings => { force_path_style => true follow_redirects => false } } } output { elasticsearch { hosts => [\"http://192.168.1.72:9200\",\"http://192.168.1.62:9200\"] index => \"s3flowlogs-16012020\" } stdout { codec => rubydebug } } The logstash is starting fine and the logs from the bucket are not pulled to the elasticsearch. I can't figure is there any error in the conf or what ? Sending Logstash logs to /opt/logstash-7.4.0/logs which is now configured via log4j2.properties [2020-01-17T19:07:41,613][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-01-17T19:07:41,632][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.4.0\"} [2020-01-17T19:07:51,180][INFO ][org.reflections.Reflections] Reflections took 278 ms to scan 1 urls, producing 20 keys and 40 values [2020-01-17T19:08:52,140][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>, :added=>[http://192.168.1.72:9200/, http://192.168.1.62:9200/]}} [2020-01-17T19:08:52,811][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>\"http://192.168.1.72:9200/\"} [2020-01-17T19:08:52,899][INFO ][logstash.outputs.elasticsearch][main] ES Output version determined {:es_version=>7} [2020-01-17T19:08:52,906][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the type event field won't be used to determine the document _type {:es_version=>7} [2020-01-17T19:08:52,916][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>\"http://192.168.1.62:9200/\"} [2020-01-17T19:08:52,971][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"http://192.168.1.72:9200\", \"http://192.168.1.62:9200\"]} [2020-01-17T19:08:53,330][INFO ][logstash.outputs.elasticsearch][main] Using default mapping template [2020-01-17T19:08:53,629][INFO ][logstash.outputs.elasticsearch][main] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} [2020-01-17T19:08:53,723][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.specialized.RubyArrayOneObject) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-01-17T19:08:53,786][INFO ][logstash.javapipeline ][main] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>4, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>500, :thread=>\"#<Thread:0x3ade9eb9 run>\"} [2020-01-17T19:08:54,036][INFO ][logstash.inputs.s3 ][main] Registering s3 input {:bucket=>\"xxxx\", :region=>\"us-east-1\"} [2020-01-17T19:08:56,204][INFO ][logstash.javapipeline ][main] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-01-17T19:08:56,815][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>} [2020-01-17T19:08:59,626][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9601} [2020-01-17T19:09:13,201][INFO ][logstash.inputs.s3 ][main] Using the provided sincedb_path {:sincedb_path=>\"/tmp/last-s3-file-s3-access-logs\"}",
    "website_area": "discuss"
  },
  {
    "id": "a7dac4ca-ec79-445c-a3d4-83df1f9af046",
    "url": "https://discuss.elastic.co/t/not-all-logs-are-forwarded-to-elasticsearch/215282",
    "title": "Not all logs are forwarded to elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "tennaen",
    "date": "January 16, 2020, 9:40am January 16, 2020, 9:59am January 17, 2020, 6:40am January 17, 2020, 2:47pm",
    "body": "Good morning, i have an issue with logstash. I use jdbc input to collect data from postgres and then forward them to elastic. I've noticed that there is a difference between what is forwarded to elastic (i use kibana to display indicies) and what is in database. This is how my configuration looks like: pipeline.yml pipeline.id: dev path.config: \"/u01/app/logstash/logstash-7.4.2/pipelines-config/focus_dev.conf\" pipeline.workers: 1 pipeline.batch.size: 125 pipeline.batch.delay: 50 queue.type: persisted queue.page_capacity: 1mb queue.max_events: 100 queue.max_bytes: 16mb pipeline config file: input { jdbc { jdbc_driver_class => \"Java::org.postgresql.Driver\" jdbc_connection_string => \"jdbc:postgresql://db_host:db_port/db_name\" jdbc_user => \"focus_dev\" jdbc_password => \"${devpass}\" #jdbc_default_timezone => \"CET\" #plugin_timezone => \"utc\" statement => \"select * from T_AUDIT_LOG_EVENT where id > :sql_last_value\" use_column_value => true tracking_column => id tracking_column_type => \"numeric\" #clean_run => true schedule => \"* * * * *\" last_run_metadata_path => \"/u01/app/logstash/logstash_logs/dev/.logstash_jdbc_last_run\" } } filter { json { source => \"value\" target => \"event\" } mutate { add_field => { \"[@metadata][event_type]\" => \"%{[event][event_type]}\" } } mutate { lowercase => [\"[@metadata][event_type]\"] } date { timezone => \"UTC\" match => [\"[event][event_date]\", \"YYYY-MM-dd HH:mm:ss.SSS\", \"ISO8601\"] target => \"@timestamp\" } } output { elasticsearch { hosts => \"elastic_host:port\" user => focusaudit password => \"${focauditpass}\" index => \"dev-focus-audit-log--%{[@metadata][event_type]}\" #document_type => \"audit-log-events\" document_id => \"%{id}\" } stdout { codec => \"rubydebug\" } } I will appreciate all the help. Best Regards, Norbert",
    "website_area": "discuss"
  },
  {
    "id": "7aaa5bd0-efcf-4787-82ba-672d6a283dd7",
    "url": "https://discuss.elastic.co/t/insert-all-records-of-a-table-in-mysql-with-no-unique-column-in-elastic-without-repitition/215377",
    "title": "Insert all records of a table in mysql with no unique column in elastic without repitition",
    "category": [
      "Logstash"
    ],
    "author": "RITZ_VERMA",
    "date": "January 16, 2020, 7:28pm January 17, 2020, 2:46pm",
    "body": "I want to send data from mysql table to elastic search .I am currently using jdbc plugin but it keep inserting same records again .I have no unique column in my mysql table.is there a way to send the records exactly once",
    "website_area": "discuss"
  },
  {
    "id": "1002ee75-8e17-44ba-b486-c27b8a4dbd3e",
    "url": "https://discuss.elastic.co/t/logstash-monitoring-problem/215468",
    "title": "Logstash monitoring problem",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "January 17, 2020, 1:46pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7b4dddc9-cf93-4e9c-9a70-6486670cba36",
    "url": "https://discuss.elastic.co/t/mutate-is-not-working-as-expected/215423",
    "title": "Mutate is not working as expected",
    "category": [
      "Logstash"
    ],
    "author": "Saravana37",
    "date": "January 17, 2020, 7:55am January 17, 2020, 8:55am January 17, 2020, 10:54am",
    "body": "Hello All , I am trying to convert MaxTasks field to Interger , I tried two ways here . Specifying the Number format in Grok filter also converting it to Integer using Mutate filter. But of no use. Anything wrong here ? please help. filter { mutate { remove_field => [ \"host\" ] } if \"srvr_logs\" in [tags] { grok { match => {\"message\" => \"%{WORD:EventType}%{SPACE}%{WORD:EventSubType}%{SPACE}%{INT:Severity}%{SPACE}%{WORD:SARMID}%{NOTSPACE}%{SPACE}%{PROG:EventDate}%{SPACE}%{TIME:EventTime}%{SPACE}%{GREEDYDATA:LogMessage}\"} } } else { ruby { code => ' event.set(\"message\", event.get(\"message\").split(\"\\n\")) ' } split { field => \"message\" } grok { match => { \"message\" => [ #Most specific grok: \"%{WORD:ServerName}%{SPACE}%{WORD:Comp_Alias}%{SPACE}%{WORD:CompStatus}%{SPACE}%{WORD:CompStartMode}%{SPACE}%{NUMBER:RunningTasks }%{SPACE}%{NUMBER:MaxTasks}%{SPACE}%{GREEDYDATA:CompName}\", #Less specific: \"%{WORD:SBLSRVR_NAME}%{SPACE}%{WORD:SBLSRVR_STATE}\" ] } } mutate { convert => { \"MaxTasks\" => \"integer\" } } } }",
    "website_area": "discuss"
  },
  {
    "id": "fbde04da-4ae5-441a-9b85-0a9b48c70723",
    "url": "https://discuss.elastic.co/t/drop-event-if-condition/215370",
    "title": "Drop event if condition",
    "category": [
      "Logstash"
    ],
    "author": "dennys_84",
    "date": "January 16, 2020, 6:07pm January 16, 2020, 6:17pm January 17, 2020, 7:58am January 17, 2020, 8:01am January 17, 2020, 10:31am January 17, 2020, 10:31am January 17, 2020, 10:41am",
    "body": "Hi all. We are trying to accomplish such task: Log auth events from windows security logs. We set winlogbeat to export data to logstash. But problem is that there are too much incoming events with the same id. So we need to filter it by certain field. We made logstash filter like: if [event][code] == 4624 and [winlog][event_data][LogonType] != 10 { drop { } But its no go. All 4624 events dropped Then we made winlogbeat settings like:  name: Security event_id: 4624 processors:  drop_event.when.not.equals.winlog.event_data.LogonType: 10 But it does not work too. Also blocks all 4624 events. Can someone explain, how correctly filter event by field value?",
    "website_area": "discuss"
  },
  {
    "id": "05f5291c-e558-4e38-b944-a98955949736",
    "url": "https://discuss.elastic.co/t/use-logstash-for-filtering-out-not-importen-data-and-as-output-use-filebeat/215429",
    "title": "Use Logstash for filtering out \"not importen data \"and as output use Filebeat",
    "category": [
      "Logstash"
    ],
    "author": "juuuhuuu",
    "date": "January 17, 2020, 9:08am January 17, 2020, 10:01am January 17, 2020, 10:27am",
    "body": "Hello everyone, I would like to know if its possible to have logstash for filtering data out and in output of logstash thanks filebeats send this logs to an other node where are elasticsearch and Kibana .. node a (Logstash -> filebeat )------> node b (elasticsearch -> kibana) Thank you very much for your answere",
    "website_area": "discuss"
  },
  {
    "id": "9983958c-0aa4-4c9a-aa06-0b7f53118431",
    "url": "https://discuss.elastic.co/t/logstash-or-re-index-api-to-chose-for-re-indexing/215435",
    "title": "Logstash or re-index API to chose for re-indexing?",
    "category": [
      "Logstash"
    ],
    "author": "arunpmohan",
    "date": "January 17, 2020, 10:18am",
    "body": "We have a cluster with the following configurations 12 nodes Elasticsearch 5.x 24 TB data I need to upgrade this cluster to Elasticsearch 7.x So we have come up with a plan to provision a new cluster with exact same hardware configurations as that of the original. We need to re-index the data from the 5.x to 7.x cluster. We are considering two options for that using a single powerfull logstash machine using the re-index API from the 7.x cluster Which one is a better choice? Does the re-index API makes use of all nodes (all nodes are master eligible, ingest and data nodes by default) and fare better than a single logstash machine?",
    "website_area": "discuss"
  },
  {
    "id": "aa9ab7bf-56de-46bf-b971-aa976d692bee",
    "url": "https://discuss.elastic.co/t/extract-all-jsons-from-log-to-one-field/215286",
    "title": "Extract all jsons from log to one field",
    "category": [
      "Logstash"
    ],
    "author": "dardev",
    "date": "January 16, 2020, 9:49am January 16, 2020, 11:15am January 16, 2020, 11:41am January 16, 2020, 11:50am January 16, 2020, 12:04pm January 16, 2020, 12:19pm January 16, 2020, 12:41pm January 17, 2020, 6:24am January 17, 2020, 6:23am January 17, 2020, 9:29am",
    "body": "Hello, I'm trying to extract few possible jsons from log and put them to one field into an array. This is the example of log I want to parse: This is one of example logs: [{\"lat\":12.33,\"lng\":55.44}] that I should correctly parse [{\"lat\":12.33,\"lng\":55.44}]. It can contain multiple jsons [{\"lat\":12.33,\"lng\":55.44}]. The acceptance criteria is an field visible in kibana which will contain all these jsons put in one field for example \"extracted_objects\": [ {..}, {..}, {..} ]. I'm able to parse the first json using following config: grok { match => { \"message\" => \"%{CISCO_REASON}: \\[%{GREEDYDATA:java_object_json}\\].?\" } } json { source => \"java_object_json\" target => \"java_object\" } but I hope this not the way I should follow... Because these jsons can be nested, it's hard to solve it using regexp.",
    "website_area": "discuss"
  },
  {
    "id": "e00baeda-0a63-4cae-a2c7-02ae45d19165",
    "url": "https://discuss.elastic.co/t/multiple-grok-pattern-filters-arent-filtering-multiple-logs-in-one-logstash-file/214654",
    "title": "Multiple Grok pattern filters arent filtering multiple logs in one logstash file",
    "category": [
      "Logstash"
    ],
    "author": "Mehak_Bhargava",
    "date": "January 10, 2020, 7:09pm January 11, 2020, 4:22am January 12, 2020, 1:47am January 12, 2020, 4:42pm January 13, 2020, 6:47pm January 16, 2020, 3:57am January 16, 2020, 3:17pm January 16, 2020, 11:33pm",
    "body": "Below is my pipeline.conf where I want the filter block to apply three separate grok patterns on three different log files. But currently, it isnt working. Should I add multiple pipelines to it? or create three different config files with one filter pattern each? Or is there another way to it? input { beats { port => 5044 } } filter { if[fields][log_type] ==\"access\" { grok { match => {\"message\" => \"%{DATESTAMP:timestamp} %{NONNEGINT:code} %{GREEDYDATA} %{LOGLEVEL} %{NONNEGINT:anum} %{GREEDYDATA} %{NONNEGINT:threadId}\"} } }else if [fields][log_type] == \"errors\" { grok { match => { \"message\" => \"%{DATESTAMP:timestamp} %{NONNEGINT:code} %{GREEDYDATA} %{LOGLEVEL} %{NONNEGINT:anum} %{GREEDYDATA:message}\" } } } else if [fields][log_type] == \"dispatch\" { grok { match => { \"message\" => \"\\A%{TIMESTAMP_ISO8601:timestamp}%{SPACE}\\[%{DATA:threadId}]%{SPACE}%{LOGLEVEL:logLevel}%{SPACE}%{JAVACLASS:javaClass}%{SPACE}-%{SPACE}?(\\[%{NONNEGINT:incidentId}])%{GREEDYDATA:message}\" } } } } output { elasticsearch { hosts => [\"localhost:9200\"] sniffing => true manage_template => false ilm_enabled => false index => \"%{[fields][log_type]}-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug } }",
    "website_area": "discuss"
  },
  {
    "id": "13e23414-8ef4-40c1-8f9b-1af31a1e2b9b",
    "url": "https://discuss.elastic.co/t/error-400-during-parsing-logs-from-logstash/214352",
    "title": "Error 400 during parsing logs from logstash",
    "category": [
      "Logstash"
    ],
    "author": "akshay_singh2",
    "date": "January 9, 2020, 6:26am January 9, 2020, 4:55pm January 14, 2020, 8:03am January 14, 2020, 2:18pm January 16, 2020, 6:22am January 16, 2020, 10:45am January 16, 2020, 10:53am January 16, 2020, 11:06pm",
    "body": "Hello Team, I have recently moved ES from 6.x to 7.x and faced a issue in one of pipeline which was working fine in ES 6.x version, but in ES 7.x I'm getting 400 error in logstash. Logstash is flooded with this error: [2020-01-09T06:19:16,088][ERROR][logstash.outputs.elasticsearch] Encountered a retryable error. Will Retry with exponential backoff {:code=>400, :url=>\"http://es-client.es.svc.cluster.local:9200/_bulk\"} [2020-01-09T06:19:16,089][ERROR][logstash.outputs.elasticsearch] Encountered a retryable error. Will Retry with exponential backoff {:code=>400, :url=>\"http://es-client.es.svc.cluster.local:9200/_bulk\"} [2020-01-09T06:19:16,089][ERROR][logstash.outputs.elasticsearch] Encountered a retryable error. Will Retry with exponential backoff {:code=>400, :url=>\"http://es-client.es.svc.cluster.local:9200/_bulk\"} [2020-01-09T06:19:16,090][ERROR][logstash.outputs.elasticsearch] Encountered a retryable error. Will Retry with exponential backoff {:code=>400, :url=>\"http://es-client.es.svc.cluster.local:9200/_bulk\"} [2020-01-09T06:19:16,092][ERROR][logstash.outputs.elasticsearch] Encountered a retryable error. Will Retry with exponential backoff {:code=>400, :url=>\"http://es-client.es.svc.cluster.local:9200/_bulk\"} Here are my output and filters: Filter: filter { mutate { add_field => { \"processedInFrankfurt\" => \"%{[@timestamp]}\" } } date { match => [ \"date\", \"YYYY-MM-dd\" ] } fingerprint { source => [ \"source\", \"account-id\", \"geo\", \"node-id\", \"product-id\", \"product-version\" ] target => \"[@metadata][fingerprint]\" method => \"SHA256\" key => \"afterallweareonlyordinarymen\" concatenate_sources => true } } Output: output { elasticsearch { retry_on_conflict => 5 action => \"update\" doc_as_upsert => true hosts => [\"elasticsearch_url:9200\"] index => \"logstash_index\" user => \"user\" password => \"password\" document_type => \"_doc\" document_id => \"%{[@metadata][fingerprint]}\" } } Thanks in advance!!!",
    "website_area": "discuss"
  },
  {
    "id": "60163d6b-f44b-457e-bf1b-46888911b9db",
    "url": "https://discuss.elastic.co/t/geo-point-for-conection-map/215363",
    "title": "Geo_point for conection map",
    "category": [
      "Logstash"
    ],
    "author": "Jose_Campos",
    "date": "January 16, 2020, 5:25pm January 16, 2020, 6:19pm",
    "body": "I am currently mapping the data of a snort, but I was trying to convert two fields, which are the IP source and destination to geo_point. I am doing this in order to be able to create the map connection visualization. Only location places me as geo_point But the field I need for connection maps is string This is what I have tried to do. geoip { source => \"source\" target => \"geoip_source\" } geoip { source => \"destination\" target => \"geoip_destination\" } Thanks for support. Greetings",
    "website_area": "discuss"
  },
  {
    "id": "4ba671d5-eb9d-4ce2-8032-135866e9e163",
    "url": "https://discuss.elastic.co/t/general-opensslengine-problem/215358",
    "title": "General OpenSslEngine problem",
    "category": [
      "Logstash"
    ],
    "author": "lsambolino",
    "date": "January 16, 2020, 4:36pm",
    "body": "Hi, In our infrastructure we have deployed logstash on Debian nodes. At the moment, it appears logstash is not producing logs anymore. Log timestamp is stucked at 10 hours before the moment of writing. After checking /var/log/logstash-plain.log, many WARN messages have appeared showing issues regarding SSL communication. Expected behaviour is regular logging with no warning regarding SSL. Any support on this regard ? Environment Info: Logstash version: 6.7.2 Operating System Debian 4.9.168-1 Config File: see bottom of page Sample Data: see following messages Steps to Reproduce: Install logstash (included version) in debian (included version). WARN message as appearing on log: [2020-01-16T07:57:05,996][WARN ][io.netty.channel.DefaultChannelPipeline] An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception. io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: General OpenSslEngine problem at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:472) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-all-4.1.30.Final.jar:4.1.30.Final] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_232] Caused by: javax.net.ssl.SSLHandshakeException: General OpenSslEngine problem at io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext Logstash configuration file: The configuration file of Logstash is here included. This configuration is as default and no changes are made: # Settings file in YAML # # Settings can be specified either in hierarchical form, e.g.: # # pipeline: # batch: # size: 125 # delay: 5 # # Or as flat keys: # # pipeline.batch.size: 125 # pipeline.batch.delay: 5 # # ------------ Node identity ------------ # # Use a descriptive name for the node: # # node.name: test # # If omitted the node name will default to the machine's host name # # ------------ Data path ------------------ # # Which directory should be used by logstash and its plugins # for any persistent needs. Defaults to LOGSTASH_HOME/data # path.data: /var/lib/logstash # # ------------ Pipeline Settings -------------- # # Set the number of workers that will, in parallel, execute the filters+outputs # stage of the pipeline. # # This defaults to the number of the host's CPU cores. # # pipeline.workers: 2 # # How many workers should be used per output plugin instance # # pipeline.output.workers: 1 # # How many events to retrieve from inputs before sending to filters+workers # # pipeline.batch.size: 125 # # How long to wait before dispatching an undersized batch to filters+workers # Value is in milliseconds. # # pipeline.batch.delay: 5 # # Force Logstash to exit during shutdown even if there are still inflight # events in memory. By default, logstash will refuse to quit until all # received events have been pushed to the outputs. # # WARNING: enabling this can lead to data loss during shutdown # # pipeline.unsafe_shutdown: false # # ------------ Pipeline Configuration Settings -------------- # # Where to fetch the pipeline configuration for the main pipeline # # path.config: /etc/logstash/conf.d # # Pipeline configuration string for the main pipeline # # config.string: # # At startup, test if the configuration is valid and exit (dry run) # # config.test_and_exit: false # # Periodically check if the configuration has changed and reload the pipeline # This can also be triggered manually through the SIGHUP signal # # config.reload.automatic: false # # How often to check if the pipeline configuration has changed (in seconds) # # config.reload.interval: 3 # # Show fully compiled configuration as debug log message # NOTE: --log.level must be 'debug' # # config.debug: false # # When enabled, process escaped characters such as \\n and \\\" in strings in the # pipeline configuration files. # # config.support_escapes: false # # ------------ Module Settings --------------- # Define modules here. Modules definitions must be defined as an array. # The simple way to see this is to prepend each `name` with a `-`, and keep # all associated variables under the `name` they are associated with, and # above the next, like this: # # modules: # - name: MODULE_NAME # var.PLUGINTYPE1.PLUGINNAME1.KEY1: VALUE # var.PLUGINTYPE1.PLUGINNAME1.KEY2: VALUE # var.PLUGINTYPE2.PLUGINNAME1.KEY1: VALUE # var.PLUGINTYPE3.PLUGINNAME3.KEY1: VALUE # # Module variable names must be in the format of # # var.PLUGIN_TYPE.PLUGIN_NAME.KEY # # modules: # # ------------ Queuing Settings -------------- # # Internal queuing model, \"memory\" for legacy in-memory based queuing and # \"persisted\" for disk-based acked queueing. Defaults is memory # # queue.type: memory # # If using queue.type: persisted, the directory path where the data files will be stored. # Default is path.data/queue # # path.queue: # # If using queue.type: persisted, the page data files size. The queue data consists of # append-only data files separated into pages. Default is 250mb # # queue.page_capacity: 250mb # # If using queue.type: persisted, the maximum number of unread events in the queue. # Default is 0 (unlimited) # # queue.max_events: 0 # # If using queue.type: persisted, the total capacity of the queue in number of bytes. # If you would like more unacked events to be buffered in Logstash, you can increase the # capacity using this setting. Please make sure your disk drive has capacity greater than # the size specified here. If both max_bytes and max_events are specified, Logstash will pick # whichever criteria is reached first # Default is 1024mb or 1gb # # queue.max_bytes: 1024mb # # If using queue.type: persisted, the maximum number of acked events before forcing a checkpoint # Default is 1024, 0 for unlimited # # queue.checkpoint.acks: 1024 # # If using queue.type: persisted, the maximum number of written events before forcing a checkpoint # Default is 1024, 0 for unlimited # # queue.checkpoint.writes: 1024 # # If using queue.type: persisted, the interval in milliseconds when a checkpoint is forced on the head page # Default is 1000, 0 for no periodic checkpoint. # # queue.checkpoint.interval: 1000 # # ------------ Dead-Letter Queue Settings -------------- # Flag to turn on dead-letter queue. # # dead_letter_queue.enable: false # If using dead_letter_queue.enable: true, the maximum size of each dead letter queue. Entries # will be dropped if they would increase the size of the dead letter queue beyond this setting. # Default is 1024mb # dead_letter_queue.max_bytes: 1024mb # If using dead_letter_queue.enable: true, the directory path where the data files will be stored. # Default is path.data/dead_letter_queue # # path.dead_letter_queue: # # ------------ Metrics Settings -------------- # # Bind address for the metrics REST endpoint # # http.host: \"127.0.0.1\" # # Bind port for the metrics REST endpoint, this option also accept a range # (9600-9700) and logstash will pick up the first available ports. # # http.port: 9600-9700 # # ------------ Debugging Settings -------------- # # Options for log.level: # * fatal # * error # * warn # * info (default) # * debug # * trace # # log.level: info path.logs: /var/log/logstash # # ------------ Other Settings -------------- # # Where to find custom plugins # path.plugins: []",
    "website_area": "discuss"
  },
  {
    "id": "61a4156c-ced4-4421-a361-56345094401b",
    "url": "https://discuss.elastic.co/t/aggregate-logs-if-value-of-field-is-same/215348",
    "title": "Aggregate logs if value of field is same",
    "category": [
      "Logstash"
    ],
    "author": "mouloudjean",
    "date": "January 16, 2020, 3:56pm",
    "body": "Hello, I'm working on a project in my company, The goal is to implement the ELK stack to have an tracability of our data, and be able to aggregate some logs that containt the same value of a field. I'm trying to aggregate some data but i still don't undersand well how does it work (i've read some documents and see some tutorials on Youtube). Actually logs that we received are like that: Log 1: LIBOR_20Y>>PROCESS1>>OIS_SPREAD/USD/OIS_SPREAD_USD_BSEUR_20Y>0.200625>2020-01-13 OIS_20Y>>PROCESS1>>OIS_SPREAD/USD/OIS_SPREAD_USD_BSEUR_20Y>0.200625>2020-01-13 EUR3M_20Y>>PROCESS1>>OIS_SPREAD/USD/OIS_SPREAD_USD_BSEUR_20Y>0.200625>2020-01-13 EUR3M_USD_20Y>>PROCESS1>>OIS_SPREAD/USD/OIS_SPREAD_USD_BSEUR_20Y>0.200625>2020-01-13 EURIB_20Y>>PROCESS1>>OIS_SPREAD/USD/OIS_SPREAD_USD_BSEUR_20Y>0.200625>2020-01-13 EONIA_20Y>>PROCESS1>>OIS_SPREAD/USD/OIS_SPREAD_USD_BSEUR_20Y>0.200625>2020-01-13 Log 2: APPLICATION1>>>OIS>>PROCESS2>>OIS_20Y>-0.232323>2020-01-13 APPLICATION1>>>EONIA>>PROCESS2>>EONIA_20Y> 0.323232>2020-01-13 We cand see field 1(line 2) of log 1 match with field 4 (line 1) log 2 Same for field 1 (line 6) of log 1 that match with field 4 (line 2) of log 2 How could i aggregate log when the arrived and have same field ? Will be always field 1 of log 1 that can match with field 4 of log 2 For info the conf file is like this : indent preformatted text by 4 spaces ########################################Definition des fichiers de logs a analyser######################################## input { file { path => \"D:/elasticsearch/logstash-7.5.0/matrisk_logs/pyrrhus/*.log\" type => \"pyrrhus\" start_position => \"beginning\" } file { path => \"D:/elasticsearch/logstash-7.5.0/matrisk_logs/brutus/*.log\" type => \"brutus\" start_position => \"beginning\" } } ########################################Definition des differents filtres : log pyrrhus######################################## filter { if [type] == \"pyrrhus\" { grok { match => [ \"message\", \"%{GREEDYDATA:application}>>>%{GREEDYDATA:foldername}>>%{GREEDYDATA:process}>>%{GREEDYDATA:foldernamematurite}>%{GREEDYDATA:value}>%{GREEDYDATA:date}\" ] } ###Supprime les lignes vides if [message] =~ /^\\s*$/ { drop { } } remove_tag => [ \"_grokparsefailure\" ] # remove champ grokparsefailure qui apparait quand ligne n'est pas au bon format remove_field => [ \"@version\", \"host\" ] # remove unused stuff } } ########################################Definition des differents filtres : log brutus######################################## if [type] == \"brutus\" { grok { match => [ \"message\", \">>>%{GREEDYDATA:foldername}>>%{GREEDYDATA:process}>>%{GREEDYDATA:pathfolder}>%{GREEDYDATA:value}>%{GREEDYDATA:date}\" ] } if [message] =~ /^\\s*$/ { drop { } } mutate { remove_field => [ \"@version\", \"host\" ] # remove unused stuff } } } ########################################Definition de l'output pour la consultation sous Kibana######################################## output { stdout {} elasticsearch { hosts => \"http://localhost:9200\" index => \"carto1\" } } Thank you in advance for your help Best regards Julien",
    "website_area": "discuss"
  },
  {
    "id": "7821ff7f-8671-40ba-b3d8-15e5f72ef001",
    "url": "https://discuss.elastic.co/t/install-aggregate-plugin-offline-logstash/215299",
    "title": "Install aggregate plugin (Offline) Logstash",
    "category": [
      "Logstash"
    ],
    "author": "Khalil_Diouri",
    "date": "January 16, 2020, 11:04am January 16, 2020, 3:36pm",
    "body": "Hello, Generally I need to install a plugin aggregate for logstash in a remote server to use it in my file .config to inject data in elasticsearch. the problem is that I can't download directly the aggregate plugin using the command ./logstash-plugin install logstash-filter-aggregate because I can't acces to internet. that's why I choose the offline method in my local computer (windows) and send the plugin to the remote server via FTP. I used the command ./logstash-plugin prepare-offline-pack logstash-filter-aggregate and it generates a zip for me. but when I send it to the remote server and I try to install ans then I got the error. ./logstash-plugin install file:logstash-offline-plugins-5.4.0.zip ERROR: Something went wrong when installing file:logstash-offline-plugins-5.4.0.zip, message: can't convert nil into String",
    "website_area": "discuss"
  },
  {
    "id": "fde164e8-8fb2-44cb-97b2-5d2f40ac585f",
    "url": "https://discuss.elastic.co/t/logstash-7-5-1-flood-syslog-with-messages/214366",
    "title": "Logstash 7.5.1 flood syslog with messages",
    "category": [
      "Logstash"
    ],
    "author": "wga",
    "date": "January 9, 2020, 8:44am January 15, 2020, 11:58pm January 16, 2020, 3:23pm",
    "body": "After upgrade from 7.5 to 7.5.1 except that all messages which receive from filebeat nodes going to elastic, for some reason start bombing /var/log/syslog with the same information without even to have any of output-syslog plugin. Syslog file growing to fast just for minutes is 2-3GB. In log4j.properties I comment console configuration but file still continue to fill itself: rootLogger.level = ${sys:ls.log.level} #rootLogger.appenderRef.console.ref = ${sys:ls.log.format}_console What should to be the different, before upgrade I didn't have issues at all? Configuration of one pipeline: input { beats { port => 5047 } } filter { grok { patterns_dir => [\"./patterns\"] match => { \"message\" => \"%{TIMESTAMP_ISO8601:log.timestamp} *%{LOGLEVEL:log.level} *%{JAVACLASS:log.class}\" } } } output { elasticsearch { hosts => [\"https://********/\"] user => \"*****\" password => \"*****\" index => \"dev-testing\" } stdout { codec => rubydebug } } I have suspicious maybe some of those settings can provoke that action: \"slowlog.logstash.codecs.plain\" : \"TRACE\", \"slowlog.logstash.codecs.rubydebug\" : \"TRACE\", \"slowlog.logstash.filters.grok\" : \"TRACE\", \"slowlog.logstash.filters.json\" : \"TRACE\", \"slowlog.logstash.inputs.beats\" : \"TRACE\", \"slowlog.logstash.outputs.elasticsearch\" : \"TRACE\", \"slowlog.logstash.outputs.stdout\" : \"TRACE\"",
    "website_area": "discuss"
  },
  {
    "id": "af0b24a3-75ea-4aae-865d-177ff30e81d3",
    "url": "https://discuss.elastic.co/t/avaya-ip-office-500-r11-smdr-logstash/215340",
    "title": "Avaya IP Office 500 R11 SMDR -> Logstash",
    "category": [
      "Logstash"
    ],
    "author": "Adrien_Carlyle",
    "date": "January 16, 2020, 3:15pm",
    "body": "Just wanted to share my configuration with the community after refining it for the better part of a week. Being new to the ELK stack it took a bit of time to understand how to get tcp flows into the platform. There was also a bit of a learning curve as some previous examples of how to do this were discussed years ago, and some bugs needed to be worked out as a result. The script consumes the csv lines that come in, maps them to their related field names, creates/caculates/populates a duration field in seconds, and then deletes the original CSV message before sending it on. Hopefully this is helpful to someone else out there looking to log their SMDR data without having to purchase a call accounting software etc. input { tcp { port => \"5500\" } } filter{ csv{ skip_empty_columns => true columns => [\"Call Start\",\"Connected Time\",\"Ring Time\",\"Caller\",\"Direction\",\"Called Number\",\"Dialled Number\",\"Account\",\"Is Internal\",\"Call ID\",\"Continuation\",\"Party1Device\",\"Party1Name\",\"Party2Device\",\"Party2Name\",\"Hold Time\",\"Park Time\",\"AuthValid\",\"AuthCode\",\"User Charged\",\"Call Charge\",\"Currency\",\"Amt at Last User Change\",\"Call Units\",\"Units at Last User Chg\",\"Cost per Unit\",\"Mark Up\",\"Ext Targeting Cause\",\"Ext Targeter Id\",\"Ext Targeted Number\",\"Srv IP of caller extn\",\"Unique call id for the caller extension\",\"Server IP address of the called extension\",\"Unique call id for the called extension\",\"UTC time\"] } mutate{ add_field => { \"Duration\" => 0 } } mutate{ convert => { \"Duration\" => \"integer\" } } ruby { code => \" h, m, s = event.get('Connected Time').split(':').map{|str| str.to_i}; event.set('Duration', h*3600 + m*60 + s) \" } mutate{ remove_field => [ \"message\" ] } } output { elasticsearch { hosts => [\"localhost:9200\"] manage_template => false } }",
    "website_area": "discuss"
  },
  {
    "id": "176c0320-bbb4-414e-9e93-96f1847849ec",
    "url": "https://discuss.elastic.co/t/could-not-find-jruby-in-c-logstash-7-5-1-vendor-jruby/215319",
    "title": "Could not find jruby in C:\\logstash-7.5.1\\vendor\\jruby",
    "category": [
      "Logstash"
    ],
    "author": "martinm1",
    "date": "January 16, 2020, 1:47pm",
    "body": "logstash-7.5.1 Win10/64 jdk1.8.0_241 I'm following the windows installation instructions and have setup the Java paths. I also installed JRuby in case it was needed but when I still get PS C:\\logstash-7.5.1> ./bin/logstash.bat \"could not find jruby in C:\\logstash-7.5.1\\vendor\\jruby\" I didn't have a vendor directory so created one with no change to the error. How do I resolve this? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "1fa9ef05-2e03-4ae3-86d0-dc198e000999",
    "url": "https://discuss.elastic.co/t/different-index-for-different-logs/215317",
    "title": "Different index for different logs",
    "category": [
      "Logstash"
    ],
    "author": "srisum0728",
    "date": "January 16, 2020, 1:33pm",
    "body": "Need different index for each log file path. 1> access log in access.conf file -> same server 2> csv file in task_engine.csv -> same server Need to have task_engine.csv file in different index so that I can pull it in machine learning . Issue: I am not getting task_engine.csv file at all, when I start manually it works but I am seeing acecess logs as well in the same index. I need it to work without manual start, more over need to have task_engine.csv in different index so I can use it in machine learning. OS: RHEL 7 Logstash version: logstash-6.7.1-1.noarch ===================================================================== pwd /etc/logstash/conf.d #cat access.conf input { file { path => \"/var/log/access_log\" path => \"/var/log/access_log**122019\" start_position => \"beginning\" } } filter { if [path] =~ \"access\" { mutate { replace => { \"type\" => \"apache_access\" } } mutate { remove_field => [ \"tags\", \"type\", \"_type\", \"_score\" ] } grok { match => [ \"message\", \"%{IP:client_ip} %{USER:ident} %{USER:auth} [%{HTTPDATE:apache_timestamp}] \"%{WORD:method} /%{NOTSPACE:request_page} HTTP/%{NUMBER:http_version}\" %{NUMBER:server_response} (?:%{NUMBER:bytes}|-)\" ] } } date { match => [ \"timestamp\" , \"dd/MMM/yyyy:HH:mm:ss Z\" ] } geoip { source => \"clientip\" } } output { elasticsearch { hosts => [\"Same_IP:9200\"] index => \"logstash-prod-accesslog-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug } } pwd /etc/logstash/conf.d cat tasks_engine.conf input { file { path => \"/var/log/task_engine.csv\" start_position => \"beginning\" tags => [ \"task\" ] } } filter { csv { separator => \";\" columns => [\"process_id\" , \"task_id\" , \"status\" , \"created_iso\" , \"created\" , \"week\" , \"year\" , \"updated_iso\" , \"user\" , \"project\" , \"region\" , \"process\" , \"hostname\" , \"error_message\" , \"extended_task_name\"] } } output { elasticsearch { hosts => [\"Same_IP:9200\"] index => \"logstash-task-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug } }",
    "website_area": "discuss"
  },
  {
    "id": "a94461c8-c2e3-40c5-95e1-8c978c3a46ed",
    "url": "https://discuss.elastic.co/t/cant-load-installed-filter-plugin/215303",
    "title": "Can't load installed filter plugin",
    "category": [
      "Logstash"
    ],
    "author": "toms130",
    "date": "January 16, 2020, 11:27am",
    "body": "Hi all, I found logstash-filter-jwt-decode plugin to decode jwt tokens, but I am not able to load it despite successfull installation. Below is what i've done. bash-4.2$ logstash-plugin install logstash-filter-jwt-decode ... Validating logstash-filter-jwt-decode Installing logstash-filter-jwt-decode Installation successful bash-4.2$ logstash-plugin list | grep jwt logstash-filter-jwt-decode bash-4.2$ grep jwt /usr/share/logstash/Gemfile gem \"logstash-filter-jwt-decode\" bash-4.2$ grep config_name /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-filter-jwt-decode-0.2.0/ logstash-filter-jwt-decode-0.2.0/lib/logstash/filters/jwt-decode.rb:21: config_name \"jwt-decode\" bash-4.2$ cat /usr/share/logstash/pipeline/logstash.conf input { beats { port => 5044 } } filter { jwt-decode { \"match\" => \"http.request.headers.x-token\" \"signature_alg\" => \"HS512\" \"extract_fields\" => {\"jwt.login\" => \"http.request.jwt.login\"} } } output { stdout { codec => rubydebug { metadata => true } } } bash-4.2$ logstash -t OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by com.headius.backport9.modules.Modules (file:/usr/share/logstash/logstash-core/lib/jars/jruby-complete-9.2.8.0.jar) to field java.io.FileDescriptor.fd WARNING: Please consider reporting this to the maintainers of com.headius.backport9.modules.Modules WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release Thread.exclusive is deprecated, use Thread::Mutex Sending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties [2020-01-16T11:24:04,690][INFO ][org.reflections.Reflections] Reflections took 56 ms to scan 1 urls, producing 20 keys and 40 values [2020-01-16T11:24:05,538][ERROR][logstash.plugins.registry] Tried to load a plugin's code, but failed. {:exception=>#<LoadError: no such file to load -- jwt>, :path=>\"logstash/filters/jwt-decode\", :type=>\"filter\", :name=>\"jwt-decode\"} [2020-01-16T11:24:05,555][FATAL][logstash.runner ] The given configuration is invalid. Reason: Unable to configure plugins: (PluginLoadingError) Couldn't find any filter plugin named 'jwt-decode'. Are you sure this is correct? Trying to load the jwt-decode filter plugin resulted in this error: no such file to load -- jwt [2020-01-16T11:24:05,576][ERROR][org.logstash.Logstash ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit Documentation is not very expansive, so I've checked plugin name into ruby file, I tried too with short documetation examples (jwt-decode) with no success... Any idea what's wrong with this ? regards thomas",
    "website_area": "discuss"
  },
  {
    "id": "d8b13e9e-b195-4104-b4f7-6bb5d4dd3ab2",
    "url": "https://discuss.elastic.co/t/logstash-as-snmp-trap-to-gelf-converter-how-to-split-message-into-seperate-fields/214988",
    "title": "Logstash as SNMP Trap to GELF Converter, how to split message into seperate fields?",
    "category": [
      "Logstash"
    ],
    "author": "Salve",
    "date": "January 14, 2020, 11:41am January 16, 2020, 11:09am January 16, 2020, 11:10am",
    "body": "Hello, I am new to logstash and I want to use it to convert SNMP Traps to GELF. This is my very first (simple) pipeline. input { snmptrap { id => \"snmptrap\" type => \"snmptrap\" port => 1162 } } output { gelf { id => \"gelf\" host => \"mygelfhost\" port => 12201 } } This is working, but I get all Trap OIDs in one message. #<SNMP::SNMPv2_Trap:xx @request_id=xx, @error_index=0, @error_status=0, @source_ip=\"x.x.x.x\", @varbind_list=[ #<SNMP::VarBind:xx @name=[1.3.6.1.2.1.1.3.0], @value=#<SNMP::TimeTicks:0x79d31b31 @value=7920439>>, #<SNMP::VarBind:xx @name=[1.3.6.1.6.3.1.1.4.1.0], @value=[1.3.6.1.4.1.x.2.4.1.1.2.1]>, #<SNMP::VarBind:xx @name=[1.3.6.1.4.1.x.2.4.1.1.1.1.2], @value=#<SNMP::TimeTicks:0x28115c7e @value=1578645869>>, #<SNMP::VarBind:xx @name=[1.3.6.1.4.1.x.2.4.1.1.1.1.3], @value=\"xxx\">, #<SNMP::VarBind:xx @name=[1.3.6.1.4.1.x.2.4.1.1.1.1.4], @value=\"xx\">, #<SNMP::VarBind:xx @name=[1.3.6.1.4.1.x.2.4.1.1.1.1.5], @value=#<SNMP::IpAddress:xx @value=\"xx\">>, #<SNMP::VarBind:xx @name=[1.3.6.1.4.1.x.2.4.1.1.1.1.6], @value=#<SNMP::Integer:0x15364a1e @value=9006>>, #<SNMP::VarBind:xx @name=[1.3.6.1.4.1.x.2.4.1.1.1.1.7], @value=#<SNMP::Integer:0x35b30466 @value=3>>, #<SNMP::VarBind:xx @name=[1.3.6.1.4.1.x.2.4.1.1.1.1.10], @value=\"xxxxx\">]> I want to have each splitted each OID into a seperate field. If I use stdout as output, it looks very close. But with GELF (or syslog) output it does not. As I am very new to logstash I dont know how to proceed here. I think I have to use a filter to seperate this, but I dont know how to start. I played around with kv and split filters but I wasnt able to change this, now I am lost... If anybody could me please point a direction to start?",
    "website_area": "discuss"
  },
  {
    "id": "f73b3a11-237e-4961-8a04-cd7d4c7dc243",
    "url": "https://discuss.elastic.co/t/add-additional-field-count-of-another-field/215008",
    "title": "Add additional field (count of another field)",
    "category": [
      "Logstash"
    ],
    "author": "treknado",
    "date": "January 14, 2020, 3:34pm January 14, 2020, 3:38pm January 14, 2020, 3:53pm January 14, 2020, 3:58pm January 14, 2020, 4:05pm January 14, 2020, 4:08pm January 14, 2020, 4:10pm January 16, 2020, 8:54am January 16, 2020, 10:43am",
    "body": "Hello, i have documents with multiline fields in my.field and need to count the number of received filenames. @Fabio-sama suggested me in another thread (which was in the wrong section) something like that: { \"my_field\":[ { \"filename\": \"test1.json\", \"type\" : \"File\" }, { \"filename\": \"test2.json\", \"type\" : \"File\" } ], \"number_of_filenames\": 2 } How can I do that in logstash in a simple way ? @Fabio-sama: is that my_field field always a list of objects? And is filename present in each of them? I'm asking if you have to make any check on the single objects or if you blindly get the size of the list. It is a list of objects OR is an empty field. So it is empty OR the structure i posted above best regards, treknado",
    "website_area": "discuss"
  },
  {
    "id": "1f8e74b0-5b6d-4114-8bb7-d03fb62c8387",
    "url": "https://discuss.elastic.co/t/how-to-check-if-the-field-exists-before-to-define-him-as-geoppoint/214871",
    "title": "How to check if the field exists before to define him as geoppoint",
    "category": [
      "Logstash"
    ],
    "author": "IneedHelp",
    "date": "January 13, 2020, 4:32pm January 13, 2020, 6:31pm January 14, 2020, 10:40am January 15, 2020, 3:51pm January 16, 2020, 7:29am January 16, 2020, 10:38am",
    "body": "Hello, I use Filebeats, Logstash and Elasticsearch to parse differents types of logs files ( IIS, Syslog, Apache, ... ) and watch the result with Kibana. I want to add a map on my dashboard so I use geoip to get gps coordinates from an ip address but sometimes (to set an example : with syslog log file) I only have the username and not the ip address. In result only a part of my output data have the geoip field but i want to define geoip.location as a geopoint in my template. Can i just set the type of the field:\"location\" on geo_point even if the field is not common to all the output ? Thank you in advance for your answer",
    "website_area": "discuss"
  },
  {
    "id": "d6605970-409f-45d8-b0fd-e81e756c5774",
    "url": "https://discuss.elastic.co/t/error-unable-to-find-driver-class-via-urlclassloader-in-given-driver-jars/215254",
    "title": "Error: Unable to find driver class via URLClassLoader in given driver jars",
    "category": [
      "Logstash"
    ],
    "author": "pyerunka",
    "date": "January 16, 2020, 7:10am January 16, 2020, 10:32am",
    "body": "Hello, I am trying run config file using logstash. It is giving below error: Error: Unable to find driver class via URLClassLoader in given driver jars: Java::oracle.jdbc.driver.OracleDriver and Java::oracle.jdbc.driver.OracleDriver Exception: LogStash::PluginLoadingError My logstash config file look like: input { jdbc { jdbc_driver_library => \"E:\\oracle\\product\\12.2.0\\client_1\\jdbc\\ojdbc8.jar\" jdbc_driver_class => \"Java::oracle.jdbc.driver.OracleDriver\" jdbc_connection_string => \"connection_string_with_hostname_and_password\" jdbc_user => \"username\" jdbc_password => \"******\" jdbc_fetch_size => \"10000\" statement_filepath => \"path for sql file statement\" last_run_metadata_path => \"E:/usr/share/logstash/last_run_metadata/.query.sql\" } } filter{ mutate{ convert=> { contentid=>\"string\" } } } output{ elasticsearch { hosts => \"localhost:9200\" index => \"new_database\" } } Regards, Priyanka",
    "website_area": "discuss"
  },
  {
    "id": "32867e61-c5b9-4090-bdba-e99d23bd75bd",
    "url": "https://discuss.elastic.co/t/field-list/214863",
    "title": "Field list",
    "category": [
      "Logstash"
    ],
    "author": "volcano",
    "date": "January 13, 2020, 3:48pm January 13, 2020, 6:29pm January 14, 2020, 4:21am January 14, 2020, 2:13pm January 14, 2020, 2:33pm January 14, 2020, 2:37pm January 14, 2020, 2:45pm January 14, 2020, 5:26pm January 14, 2020, 4:33pm January 14, 2020, 5:13pm January 14, 2020, 5:27pm January 15, 2020, 2:30pm January 15, 2020, 2:27pm January 15, 2020, 2:33pm January 15, 2020, 3:03pm January 15, 2020, 3:12pm January 15, 2020, 3:19pm January 15, 2020, 3:34pm January 15, 2020, 3:40pm January 15, 2020, 4:02pm",
    "body": "I have this entry in logstash.conf mutate { split => [\"message\",\"Employee\"] add_field => {\"part1\" =>\"%{[message][0]}\"} add_field => {\"part2\" =>\"%{[message][1]}\"} } mutate { split => [\"part2\",\"#\"] add_field => {\"part2_1\" =>\"%{[part2][0]}\"} add_field => {\"part2_2\" =>\"%{[part2][1]}\"} } This adds part1 , part2 , part2_1 and part2_2 fields in Kibana's Available field list. But my requirement is to add only part2_2 field . rest of the fields are not required in Kibana. What changes I should make here so that only part2_2 field is added in Kibana's Available field list.",
    "website_area": "discuss"
  },
  {
    "id": "a19feb70-9c8d-4aba-9de2-9bcb58dc2636",
    "url": "https://discuss.elastic.co/t/geo-lookups-using-canadian-postal-codes/215230",
    "title": "Geo lookups using Canadian postal codes",
    "category": [
      "Logstash"
    ],
    "author": "Rob_wylde",
    "date": "January 16, 2020, 12:32am January 16, 2020, 12:47am January 16, 2020, 1:30am",
    "body": "Has anyone come across a solution for this? I know maxmind mmdb's are not an option. So i think im stuck possibly writing a custom filter to query a google api and to be honest I do not even know where to begin with that task. Hoping to find something already built before I go down that path. Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "8b48e9cb-526f-43fd-a02b-24279b2b7d33",
    "url": "https://discuss.elastic.co/t/skip-on-invalid-json-skipping-all-filters/215195",
    "title": "Skip_on_invalid_json skipping all filters",
    "category": [
      "Logstash"
    ],
    "author": "jfcantu",
    "date": "January 15, 2020, 7:58pm January 16, 2020, 12:07am January 16, 2020, 12:07am January 16, 2020, 12:06am",
    "body": "Hi, I ran across something unexpected yesterday, that doesn't seem to line up with the Logstash documentation, and I was wondering if this was intended behavior (or if I did something wrong.) TL;DR - if you have multiple filter {} blocks, and attempt to use json {} in one of the filters with skip_on_invalid_json set, it seems to skip all filters rather than just the filter where json{} was used. Longer version: Per the JSON filter documentation on skip_on_invalid_json (emphasis added): Allows for skipping the filter on invalid JSON. Based on this, I assumed that if json {} hits invalid JSON, it will skip the rest of the parent filter - but still process other filters in the pipeline. So, I had the following in my pipeline definition: filter { json { skip_on_invalid_json => true source => \"message\" } mutate { remove_field => \"message\" } } filter { mutate { # do some other stuff } } What I found was, when [message] didn't contain valid JSON, the second filter didn't seem to run at all. I added a \"JSON check\" prior to the json {} block by putting if [message] =~ \"^\\s*{.*}\\s*$\" { } around it, and that seemed to solve the problem - if [message] doesn't look like a JSON object, the json {} block doesn't run, skip_on_invalid_json is never encountered, and the second filter runs as expected. Is this expected behavior?",
    "website_area": "discuss"
  },
  {
    "id": "96f2414b-5f88-416c-be1d-b71ac5fd9374",
    "url": "https://discuss.elastic.co/t/ingesting-latest-logs-from-kafka-with-logstash/215209",
    "title": "Ingesting latest logs from kafka with logstash",
    "category": [
      "Logstash"
    ],
    "author": "canaria",
    "date": "January 15, 2020, 8:54pm January 15, 2020, 9:40pm January 15, 2020, 10:20pm",
    "body": "Hi, I searched previous opened topics related that question but none of them have any reply. Im ingesting logs from apache kafka with logstash, everything is fine but I noticed that whenever I started logstash instance it doesn't starts to ingest latest logs. The logs timestamps belong 13-14 hours ago. I was thinking that after I set auto_offset_reset => latest parameter should have solved my issue but I guess It didnt work. This is input part of my logstash config: input { kafka { bootstrap_servers => [\"bootsrap1:39092,bootsrap2:39092,bootsrap3:39092,bootsrap4:39092,bootsrap5:39092\"] topics => \"th-cef\" auto_offset_reset => \"latest\" client_id => \"test10\" type => \"logs9\" group_id => \"sectechlogstash\" consumer_threads => 19 } } Any tips or help would be appreciated Thanks",
    "website_area": "discuss"
  },
  {
    "id": "a57a782f-9dbf-4d1c-8e71-e2f0fbb7a777",
    "url": "https://discuss.elastic.co/t/multiple-nodes-that-get-input-from-a-scheduled-jdbc-database-query/215218",
    "title": "Multiple nodes that get input from a scheduled JDBC database query?",
    "category": [
      "Logstash"
    ],
    "author": "aidanoc15",
    "date": "January 15, 2020, 9:50pm January 15, 2020, 10:17pm",
    "body": "Right now we have one logstash node running in Docker that queries a database every 2 minutes using JDBC and sends it over to a collector. How does something like this work if you want to have this done by a swarm of nodes? Ideally we would like multiple nodes so that if the one fails the whole service doesn't go down. Is it possible to do this? or is the nature of it being a scheduled query prevent such a thing?",
    "website_area": "discuss"
  },
  {
    "id": "82bd4452-6f3a-4889-8795-a124d2b6ed47",
    "url": "https://discuss.elastic.co/t/error-forbidden-8-index-write-api-and-elasticsearch-stop-to-receiving-logstash-bulk-requests/215028",
    "title": "Error FORBIDDEN/8/index write (api)] and Elasticsearch stop to receiving logstash bulk requests",
    "category": [
      "Logstash"
    ],
    "author": "ALEXANDRE_BUNN1",
    "date": "January 14, 2020, 6:16pm January 15, 2020, 4:55pm January 15, 2020, 5:15pm January 15, 2020, 5:38pm January 15, 2020, 5:47pm January 15, 2020, 6:50pm January 15, 2020, 8:57pm January 15, 2020, 10:16pm",
    "body": "I have a server running Logstash, Kibana and ElasticSearch 6.8, all in the same server. I was setting up the ILM for the indexes, just to set a retention and executing force_merge for the old indices (older than 1 day) and \"suddenly\" I've started to receive the error below at logstash log. [2020-01-14T13:20:55,327][INFO ][logstash.outputs.elasticsearch] retrying failed action with response code: 403 ({\"type\"=>\"cluster_block_exception\", \"reason\"=>\"blocked by: [FORBIDDEN/8/index write (api)];\"}) [2020-01-14T13:20:55,327][INFO ][logstash.outputs.elasticsearch] Retrying individual bulk actions that failed or were rejected by the previous bulk request. {:count=>80} I've already removed the ILM from all the indexes, but still receiving this error. POST _all/_ilm/remove I'm not running out of disk space [2020-01-14T13:23:54,864][INFO ][o.e.e.NodeEnvironment ] [I8x9STN] using [1] data paths, mounts [[/var (/dev/mapper/vg00-var)]], net usable_space [196.4gb], net total_space [399.9gb], types [ext4] See below the _cluster/health output: { \"cluster_name\" : \"elasticsearch\", \"status\" : \"yellow\", \"timed_out\" : false, \"number_of_nodes\" : 1, \"number_of_data_nodes\" : 1, \"active_primary_shards\" : 3583, \"active_shards\" : 3583, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 115, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 96.89021092482423 } The server has 64GB of ram, logstash has setup 2GB of heap and elasticsearch with 16GB of heap. I'm able to insert data into a teste indice with kibana. The server has 751 indices and 3698 shards, but it was much more, I'was setting the ILM and Templates for each indice prefix to have the properly number_of_shards and retention to keep the server organized and avoid disk space issues. Following below the json of one ILM Policies, and I'm not using rollover since I have just one server. { \"policy\": { \"phases\": { \"hot\": { \"min_age\": \"0ms\", \"actions\": { \"set_priority\": { \"priority\": 100 } } }, \"warm\": { \"min_age\": \"2d\", \"actions\": { \"forcemerge\": { \"max_num_segments\": 1 }, \"set_priority\": { \"priority\": 50 } } }, \"delete\": { \"min_age\": \"30d\", \"actions\": { \"delete\": {} } } } } } Thanks",
    "website_area": "discuss"
  },
  {
    "id": "83fa4e90-70fe-4183-873c-617550f87bd9",
    "url": "https://discuss.elastic.co/t/different-charset-in-filter-http/215219",
    "title": "Different charset in filter http",
    "category": [
      "Logstash"
    ],
    "author": "Peter_Li",
    "date": "January 15, 2020, 9:56pm",
    "body": "I have a filter http module, but http returns nonUTF8 character sequences. Got this error: invalid byte sequence in utf-8 If one use input http, one can add \"codec plain\" to deal with this. But no such option exist for filter http. Is there a work around ? Thanks,",
    "website_area": "discuss"
  },
  {
    "id": "62756ff5-cd2c-40e9-8689-4d71850ab0d6",
    "url": "https://discuss.elastic.co/t/sqs-input-not-working/215215",
    "title": "Sqs input not working",
    "category": [
      "Logstash"
    ],
    "author": "zozo6015",
    "date": "January 15, 2020, 9:32pm",
    "body": "I am trying to import SES logs from aws using sqs input. I have configured the pipeline. Was working for some time but after moving to 7.x it stopped working. My config looks like this: input { sqs { access_key_id => \"AWS_ACCESS_KEY\" secret_access_key => \"AWS_ACCESS_SECRET\" region => \"us-east-1\" queue => \"ses\" } } filter { json { source => \"Message\" } } output { stdout { codec => rubydebug } elasticsearch { hosts => [\"ip1:9200\", \"ip2:9200\"] user => \"ingest\" password => \"es_password\" manage_template => false index => \"ses-%{+YYYY.MM.dd}\" } } Anything I can do to make this work again? Is there a way to debug the input?",
    "website_area": "discuss"
  },
  {
    "id": "5e0f46a9-a001-4cca-858e-fb6a7445846a",
    "url": "https://discuss.elastic.co/t/conditional-checks-no-longer-work/215201",
    "title": "Conditional checks no longer work",
    "category": [
      "Logstash"
    ],
    "author": "zozo6015",
    "date": "January 15, 2020, 8:29pm January 15, 2020, 8:32pm January 15, 2020, 8:39pm",
    "body": "Hello, I am running logstash 7.5.0 and I am wondering if anything changed on the conditional checks because I have some checks like: if [agent][type] == \"filebeat\" { ... } and it's not matching anymore.",
    "website_area": "discuss"
  },
  {
    "id": "8a54abb6-7326-4889-8df0-492f880c75c5",
    "url": "https://discuss.elastic.co/t/drop-documents-with-empty-field/215161",
    "title": "Drop documents with empty field",
    "category": [
      "Logstash"
    ],
    "author": "JeremyP",
    "date": "January 15, 2020, 3:17pm January 15, 2020, 3:36pm January 15, 2020, 3:41pm January 15, 2020, 4:07pm January 15, 2020, 4:09pm January 15, 2020, 6:07pm",
    "body": "I'm having difficulty finding the right syntax to drop a document with an empty field. I'm using aggregate maps. Here is my code.... filter { aggregate { task_id => \"%{asset_id}%{vulnerability_id}\" code => \" map ['Asset ID'] = event.get('asset_id') map ['Vulnerability ID'] = event.get('vulnerability_id') map ['Asset MAC Address'] = event.get('mac_address') map ['Site Name'] = event.get('sites') map ['Asset Name'] = event.get('host_name') map ['Asset IP Address'] = event.get('ip_address') map ['Asset OS Vendor'] = event.get('os_vendor') map ['Asset OS Name'] = event.get('os_name') map ['Asset OS Version'] = event.get('os_version') map ['Asset OS Family'] = event.get('os_family') map ['Asset Scan Credential Status'] = event.get('credential_status') map ['Last Assessed for Vulnerabilities'] = event.get('last_assessed_for_vulnerabilities') map ['Vulnerability Title'] = event.get('title') map ['Vulnerability Severity'] = event.get('severity') map ['Vulnerability CVSSv3 Score'] = event.get('cvss_v3_score') map ['Vulnerability CVSSv3 Vector'] = event.get('cvss_v3_vector') map ['Vulnerability Description'] = event.get('description') map ['Vulnerability Advisories'] ||= [] var_advisory = {'Source' => event.get('source'),'Reference' => event.get('reference')} if ! map['Vulnerability Advisories'].include?(var_advisory) map['Vulnerability Advisories'] << var_advisory end map ['Vulnerability Advisories CSV'] ||= [] var_advisory_csv = {'Source' => event.get('source'),'Reference' => event.get('reference')} if ! map['Vulnerability Advisories CSV'].include?(var_advisory_csv) map['Vulnerability Advisories CSV'] << var_advisory_csv end map ['Vulnerability Fix'] = event.get('fix') map ['Vulnerability Proof'] = event.get('proof') map ['Service Name'] = event.get('service') map ['Service Port'] = event.get('port') map ['Service Protocol'] = event.get('protocol') map ['type'] = event.get('type') map ['Vulnerability Test Date'] = event.get('test_date') map ['Vulnerable Since'] = event.get('found_date') map ['Vulnerability Published Date'] = event.get('date_published') event.cancel() \" push_previous_map_as_event => true timeout => 3 } if ([Vulnerability ID] == \"\") { drop {} } mutate { convert => {\"Vulnerability Advisories CSV\" => \"string\"} } } I'm trying to drop the document if the \"Vulnerability ID\" is blank. I've also used the pre-aggregate map field \"vulnerability_id\" and no dice. Here is my JSON output for the sample event in question... { \"_index\": \"test\", \"_type\": \"_doc\", \"_id\": \"TEST-336-%{Vulnerability ID}\", \"_version\": 1, \"_score\": 0, \"_source\": { \"Vulnerability CVSSv3 Score\": null, \"Asset MAC Address\": null, \"Vulnerability Title\": null, \"Asset Name\": null, \"Vulnerability Severity\": null, \"Service Port\": null, \"Vulnerability Published Date\": null, \"Vulnerable Since\": null, \"Vulnerability Advisories\": [ { \"Source\": null, \"Reference\": null } ], \"Vulnerability Advisories CSV\": [ \"{\\\"Source\\\"=>nil, \\\"Reference\\\"=>nil}\" ], \"Vulnerability Description\": null, \"Vulnerability ID\": null, \"type\": \"TEST\", \"Asset ID\": 336, \"Asset OS Family\": \"Linux\", \"Vulnerability CVSSv3 Vector\": null, \"Service Protocol\": null, \"Asset IP Address\": \"1.2.3.4/32\", \"Service Name\": null, \"@version\": \"1\", \"Asset Scan Credential Status\": \"N/A\", \"Asset OS Name\": \"Linux\", \"Last Assessed for Vulnerabilities\": \"2020-01-09T06:19:33.142Z\", \"Vulnerability Proof\": null, \"@timestamp\": \"2020-01-15T15:10:53.820Z\", \"Vulnerability Fix\": null, \"Asset OS Vendor\": \"CentOS\", \"Asset OS Version\": \"2.6.9\", \"Site Name\": \"EDC Staging\", \"Vulnerability Test Date\": null }, Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "68f3e7d9-a1c7-4bf6-a552-5ef49fd8c497",
    "url": "https://discuss.elastic.co/t/logstash-filter-date-atypical-failure/215039",
    "title": "Logstash filter date \"atypical\" failure",
    "category": [
      "Logstash"
    ],
    "author": "blenolopes",
    "date": "January 14, 2020, 7:55pm January 14, 2020, 8:10pm January 15, 2020, 6:11pm January 15, 2020, 5:27pm January 15, 2020, 5:38pm",
    "body": "Hello logstashers, I have an \"atypical\" problem with a particular log format using date filter. Follow the tests. bin/logstash --version logstash 7.5.0 Success ~$ echo \"14/01/2020 16:03:00,143\" | bin/logstash -e 'input { stdin {} } filter { date { match => [ \"message\", \"dd/MM/yyyy HH:mm:ss,SSS\" ] } }' { \"message\" => \"14/01/2020 16:03:00,143\", \"@version\" => \"1\", \"@timestamp\" => 2020-01-14T19:03:00.143Z, \"host\" => \"myhost\" } Failure ~$ echo \"[14/01/2020 16:03:00,143 XXX] [XXX] MSG\" | bin/logstash -e 'input { stdin {} } filter { date { match => [ \"message\", \"dd/MM/yyyy HH:mm:ss,SSS\" ] } }' { \"@timestamp\" => 2020-01-14T19:42:51.585Z, \"@version\" => \"1\", \"message\" => \"[14/01/2020 16:03:00,143 XXX] [XXX] MSG\", \"tags\" => [ [0] \"_dateparsefailure\" ], \"host\" => \"myhost\" } Where am i going wrong? Grateful for the help and attention.",
    "website_area": "discuss"
  },
  {
    "id": "93a17315-8317-4184-a462-d25ecd281f4a",
    "url": "https://discuss.elastic.co/t/logstash-parsing-remote-files-from-filebeat/215181",
    "title": "Logstash parsing remote files from filebeat",
    "category": [
      "Logstash"
    ],
    "author": "victor_ide",
    "date": "January 15, 2020, 5:21pm",
    "body": "Hello logstashers! I was wondering if it is possible to read remote files like PDF, docx and import them into elasticsearch using the attachment processor + filebeat, where filebeat would send files to logstash, then logstash insert the base64 encoded data of the files in elasticsearch. My idea was to install filebeat in all my workstation and then send all *.pdf files to elastic to find PII information in the network. Any thoughts if this is possible? Cheers!",
    "website_area": "discuss"
  },
  {
    "id": "cb126fd9-b0df-4544-808f-cdde21d1c63a",
    "url": "https://discuss.elastic.co/t/removing-leading-character-and-trailing-2-characters/214634",
    "title": "Removing leading character and trailing 2 characters",
    "category": [
      "Logstash"
    ],
    "author": "DWbank",
    "date": "January 10, 2020, 4:29pm January 10, 2020, 4:45pm January 10, 2020, 5:05pm January 10, 2020, 6:34pm January 10, 2020, 6:41pm January 10, 2020, 6:44pm January 10, 2020, 7:12pm January 12, 2020, 10:58pm January 12, 2020, 11:31pm January 13, 2020, 12:40pm January 13, 2020, 3:10pm January 14, 2020, 7:48pm January 14, 2020, 8:17pm January 15, 2020, 1:37pm January 15, 2020, 3:58pm",
    "body": "I am using kv to parse my fields due to the fields change depends on the logs that are sent from the siem. kv { value_split => = field_split =>   which work fine but the very first field and the last. due to the extra characters. How do I remove them",
    "website_area": "discuss"
  },
  {
    "id": "37aa7b4a-39b6-45dc-84bf-84e218152a95",
    "url": "https://discuss.elastic.co/t/log-management-for-multiple-application-instances/214940",
    "title": "Log Management for Multiple Application Instances",
    "category": [
      "Logstash"
    ],
    "author": "amin224",
    "date": "January 15, 2020, 7:22am January 14, 2020, 2:41pm January 15, 2020, 3:44pm",
    "body": "Hello, sorry if this has been asked before but i am new to ELK stack and did not find an answer for my question. If i have an application that is running multiple instances for example instance1, instance2, instance3 and i want to do log management for this application using ELK stack. I am using Logstash which takes the logs from a logging.file of the application and displays it on Kibana. My issue is how can i differentiate between the logs of all 3 instances so i know which one sends logs in case one of them is not functioning properly. I tried using spring cloud sleuth but the issue is that i have different traceIDs and i dont want to search in all instances to find the traceID that belongs to the instance. Is there a way to include the IP address and port in the logstash configuration file? This is a sample of the log requests 2020-01-13 23:19:50.078 INFO [microservice1,81bda65d4c16a2d0,81bda65d4c16a2d0,false] 8278 --- [nio-8001-exec-1] c.s.m.c.Microservice1Controller : This is an INFO log 2020-01-13 23:19:50.078 ERROR [microservice1,81bda65d4c16a2d0,81bda65d4c16a2d0,false] 8278 --- [nio-8001-exec-1] c.s.m.c.Microservice1Controller : This is an ERROR log This is the log output information present in kibana |@timestamp| : Jan 13, 2020 @ 06:38:15.986| |@version : 1| |_id : SARXn28B-hgxozi_UC3u| |_index : logstash-2020.01.03-000001| |_score : - | |_type : _doc| |host : ubuntu| |message : 2020-01-13 06:36:39.863 INFO [microservice1,064d4100c385c9f5,064d4100c385c9f5,false] 5791 --- [http-nio-8001-exec-2] c.s.m.c.Microservice1Controller : This is an INFO log| |path : /home/user/microservice1logging/microservice1.log| |type : java| Picture of my Kibana UI with the available fields fields1842956 111 KB",
    "website_area": "discuss"
  },
  {
    "id": "76ed3371-9cfd-440a-8918-58352cc2e0cc",
    "url": "https://discuss.elastic.co/t/s3-dynamic-date-prefix-in-folder-name-by-using-current-date-as-backup-add-prefix/215104",
    "title": "S3 dynamic date prefix in folder name by using current date as backup_add_prefix",
    "category": [
      "Logstash"
    ],
    "author": "Xandr90",
    "date": "January 15, 2020, 3:08pm January 15, 2020, 3:38pm",
    "body": "So what i want to do is use Logstash to rename incoming files in a s3 bucket. I run it via docker and use the following logstash.conf input { s3 { access_key_id => \"some_key\" secret_access_key => \"some_secret\" region => \"some_region\" bucket => \"mybucket\" interval => \"10\" sincedb_path => \"/tmp/sincedb_something\" backup_add_prefix =>'%{+YYYY.MM.dd}/' backup_to_bucket => \"mybucket\" additional_settings => { force_path_style => true follow_redirects => false } } } Basically i want backup_add_prefix =>'%{+YYYY.MM.dd}/' to be the current date, so that logstash builds a structure where incoming files a put into date folders. The syntax does not work though. Is there a way to make backup_add_prefix a changing date?",
    "website_area": "discuss"
  },
  {
    "id": "3135c3bf-9244-4c40-bca3-af0a5ba70fc4",
    "url": "https://discuss.elastic.co/t/using-date-filter-within-split-filter/214402",
    "title": "Using date filter within split filter",
    "category": [
      "Logstash"
    ],
    "author": "maksimize",
    "date": "January 9, 2020, 10:01am January 9, 2020, 4:57pm January 10, 2020, 2:38pm January 10, 2020, 5:14pm January 15, 2020, 2:47pm",
    "body": "Hi All I'm receiving batch event from azure eventhub and I am using split filter to split each event into it's own record the problem I'm facing is that each split event has it's own datetime and I would like to use the as @timestamp here is my filter so far but the records.time is still different from the @timestamp filter { json { source => \"message\" } split { field => \"records\" } date { match => [\"records.time\", \"yyyy-MM-dd'T'HH:mm:ss'.'SSSZ\"] target => \"@timestamp\" } }",
    "website_area": "discuss"
  },
  {
    "id": "17fccf2b-a9c8-488d-a121-401b89f01ebb",
    "url": "https://discuss.elastic.co/t/json-unrecognized-token/215006",
    "title": "Json::ParserError: Unrecognized token",
    "category": [
      "Logstash"
    ],
    "author": "akuninja",
    "date": "January 15, 2020, 1:04pm January 15, 2020, 12:52pm January 15, 2020, 1:11pm January 15, 2020, 1:21pm January 15, 2020, 1:56pm January 15, 2020, 2:06pm January 15, 2020, 2:37pm",
    "body": "//////////////////////////////////THIS PART I SOLVED MYSELF /////////////////////////////////////// I am still getting a broken token within my logstash output, and I do not know where this timestamp (possibly) token comes from. Could someone offer some help? here is my logstash error message: [2020-01-14T16:11:31,022][ERROR][logstash.codecs.json ][main] JSON parse error, original data now in message field {:error=>#<LogStash::Json::ParserError: Unrecognized token 'mestamp': was expecting ('true', 'false' or 'null') Here is what I am sending to logstash via python script: LOGGER = logging.getLogger('python-logstash-logger') LOGGER.setLevel(logging.DEBUG) LOGGER.addHandler(logstash.LogstashHandler(127.0.0.1, 5000, version=1)) print(dir(logstash)) LOGGER.addHandler(logstash.TCPLogstashHandler('127.0.0.1', 5000, version=1)) LOGGER.error('python-logstash: test logstash error message.') LOGGER.info('python-logstash: test logstash info message.') LOGGER.warning('python-logstash: test logstash warning message.') add extra field to logstash message extra = { 'test_string': 'python version: ' + repr(sys.version_info), 'test_boolean': True, 'test_dict': {'a': 1, 'b': 'c'}, 'test_float': 1.23, 'test_integer': 1238888888, 'test_list': [1, 2, '3'], } LOGGER.info(\"python-logstash: test extra fields\", extra=extra) and here is what I see in kibana (as you can see all the fields land in the 'message' due to the error with that token): image1484137 29.8 KB HOW to fix that token??? I tried to filter it out with mutate, but the issue is still there: filter { mutate { add_field => { \"test_string\" => \"Python version 1\" } remove_field => {\"timestamp\"} } } or filter { mutate { add_field => { \"test_string\" => \"Python version 1\" } remove_field => {\"mestamp\"} } } //////////////////////////////////////////////\"solution\"///////////////////////////////////////////////////////////////////////OK I AM PAST THAT PROBLEM NOW, the reason was too long integer in one of the extra fields created in py file, namely: 'test_integer': 1238888888, ///////////////////////////////////////N E W P R O B L E M//////////////////////////////////////// the new issue is with the mapping, when i reduced the length of that 'test_integer' to 12, well yesterday all worked great, but today I am getting logstash error: [2020-01-15T13:25:57,356][WARN ][logstash.outputs.elasticsearch][main] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"mylogstash\", :_type=>\"_doc\", :routing=>nil}, #LogStash::Event:0x5c5416ed], :response=>{\"index\"=>{\"_index\"=>\"mylogstash\", \"_type\"=>\"_doc\", \"_id\"=>\"dycqqW8BjlS_m8VD4TZl\", \"status\"=>400, \"error\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"mapper [test_list] of different type, current_type [text], merged_type [long]\"}}}} I suppose the mapping involved that too long integer already, or? How to solve the current issue? ************ my logstash mapping is: ******************* { \"mapping\": { \"properties\": { \"@timestamp\": { \"type\": \"date\" }, \"@version\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"host\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"level\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"logger_name\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"message\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"path\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"port\": { \"type\": \"long\" }, \"tags\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"test_boolean\": { \"type\": \"boolean\" }, \"test_dict\": { \"properties\": { \"a\": { \"type\": \"long\" }, \"b\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } }, \"test_float\": { \"type\": \"float\" }, \"test_string\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"type\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } }",
    "website_area": "discuss"
  },
  {
    "id": "644b17d4-5ea1-4eb6-8ef2-5688bceecb55",
    "url": "https://discuss.elastic.co/t/logstash-grok-filter-to-generate-priority-number-in-logs/215114",
    "title": "Logstash grok filter to generate priority number in logs",
    "category": [
      "Logstash"
    ],
    "author": "BIs1",
    "date": "January 15, 2020, 2:07pm",
    "body": "Hi Everyone, I am very new to Elastic Stack so I have very less knowledge regarding Logstash grok filter. Basically I am trying to get the logs with priority number so that syslog_pri {} plugin will generate all the necessary fields in ES. Firewall logs already contains priority code and its easy but I am struggling with application logs like kafka, nifi and hadoop which comes with no pri and I don`t know if its even possible as the logs are different. I know its not upto logastash to generate pri code on logs but is it possible or do anyone know how to do that in application ? Currently using version 7.3.1 version logstash, ES and kibana same across. As an example I am listing the logs I am getting from all the components. NIFI LOG 2020-01-14 11:34:48,769 INFO [NiFi Web Server-91] org.apache.nifi.web.filter.RequestLogger Attempting request for (anonymous) GET http://10.X.X.X:9999/nifi-api/flow/cluster/summary (source ip: 10.X.X.X) HADOOP LOG 2020-01-15 10:26:07,021 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 0, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0 KAFKA LOG 2020-01-15 10:25:33,106] DEBUG [Controller id=0] Topics not in preferred replica for broker 0 Map() (kafka.controller.KafkaController) FIREWALL LOG with pri <189> <189>date=2020-01-15 time=10:10:20 devname=\"FIREWALL389\" devid=\"FIREWALL389\" logid=\"0001000014\" type=\"traffic\" subtype=\"local\" level=\"notice\" vd=\"root\" eventtime=1579083020 srcip=fe80::f602:70ff:fe9f:e9aa srcport=5353 srcintf=\"mgmt\" srcintfrole=\"lan\" dstip=ff02::fb dstport=5353 dstintf=unknown-0 dstintfrole=\"undefined\" sessionid=338794 proto=17 action=\"deny\" policyid=0 policytype=\"local-in-policy6\" service=\"udp/5353\" dstcountry=\"Reserved\" srccountry=\"Reserved\" trandisp=\"noop\" app=\"udp/5353\" duration=0 sentbyte=0 rcvdbyte=0 sentpkt=0 rcvdpkt=0 appcat=\"unscanned\" Logstash config for firewall: input { udp { port => 1514 type => \"syslog\" } } filter { if [type] == \"syslog\" { grok { match => { \"message\" => [\"<%{NUMBER:syslog_pri}>%{GREEDYDATA:Log-message}\"] } } syslog_pri {} } } output { elasticsearch { hosts => \"10.X.X.X:9200\" index => \"firewall\" user => elastic password => password document_type => \"syslogs\" } } GROK filter is generating all these fields from firewall logs with pri <189> syslog_facility: local7 syslog_facility_code: 23 syslog_pri: 189 syslog_severity: notice syslog_severity_code: 5 type: syslog",
    "website_area": "discuss"
  },
  {
    "id": "3085641c-c778-46e1-bb6f-6739e188d94f",
    "url": "https://discuss.elastic.co/t/drop-line-rather-than-the-whole-message/215133",
    "title": "Drop line rather than the whole message",
    "category": [
      "Logstash"
    ],
    "author": "whoatemyjam",
    "date": "January 15, 2020, 12:31pm January 15, 2020, 12:57pm January 15, 2020, 1:07pm January 15, 2020, 1:16pm January 15, 2020, 1:51pm January 15, 2020, 1:51pm January 15, 2020, 1:55pm",
    "body": "hi I am trying to drop lines which match a regex filter { if [message] =~ \"\\e\\[8mha.*\\e\\[0m\" { drop{} } } this above one removes the whole message , how do i drop only the line that contains the matching regex Thanks",
    "website_area": "discuss"
  },
  {
    "id": "d3400a14-45af-4b00-91e8-d392219c178f",
    "url": "https://discuss.elastic.co/t/jenkins-logs-via-logstash/215122",
    "title": "Jenkins logs via logstash",
    "category": [
      "Logstash"
    ],
    "author": "whoatemyjam",
    "date": "January 15, 2020, 11:20am January 15, 2020, 11:22am January 15, 2020, 11:40am January 15, 2020, 1:52pm",
    "body": "Hi we are trying to send Jenkins build logs logs to ES via logstash and filebeat. We have tried Jenkins logstash plugin but it doesnot work with pipelines. Our filebeat config is like filebeat.inputs: - type: log enabled: true paths: - /jenkins-builds/jobs/*/builds/*/log multiline.pattern: '^[A-Z]{1}[a-z]{2} {1,2}[0-9]{1,2}, [0-9]{4} {1,2}[0-9]{1,2}:[0-9]{2}:[0-9]{2}' multiline.negate: true multiline.match: after output.logstash: hosts: [\"localhost:5044\"] but jenkins build logs have some metadata which we are trying to remove from the logs so we have tried adding a regex to ignore it input { beats { port => 5044 } } filter { prune { blacklist_names => [ \" \\e\\[8mha.*\\e\\[0m \" ] } } output { stdout { } } but the metadata is still there ` \"message\": \"Started by user \\u001b[8mha:////4Ag0Ig80jDtqRg/RNPPpMGJ+n2VlZliPm5hsa5zI/TRRAAAAmB+LCAAAAAAAAP9b85aBtbiIQTGjNKU4P08vOT+vOD8nVc83PyU1x6OyILUoJzMv2y+/JJUBAhiZGBgqihhk0NSjKDWzXb3RdlLBUSYGJk8GtpzUvPSSDB8G5tKinBIGIZ+sxLJE/ZzEvHT94JKizLx0a6BxUmjGOUNodHsLgAyOEgYe/dLi1CL94pTEnMQcAONGjhrBAAAA\\u001b[0mshantanu\\nRunning in Durability level: MAX_SURVIVABILITY\\n\\u001b[8mha:////4MU1hwwmrQi5WMhiBzYpeSIAvmxMsBgiRIwcP4r0nhFEAAAAoh+LCAAAAAAAAP9tjTEOwjAQBM8BClpKHuFItIiK1krDC0x8GCfWnbEdkooX8TX+gCESFVvtrLSa5wtWKcKBo5UdUu8otU4GP9jS5Mixv3geZcdn2TIl9igbHBs2eJyx4YwwR1SwULBGaj0nRzbDRnX6rmuvydanHMu2V1A5c4MHCFXMWcf8hSnC9jqYxPTz/BXAFEIGsfuclm8zQVqFvQAAAA==\\u001b[0m[Pipeline] Start of Pipeline\\n\\u001b[8mha:////4PvyZR0Eufn8pVijx2W+VC4vO4lzSwW+jYu+Gz5Af9ncAAAApR+LCAAAAAAAAP9tjTEOwjAUQ3+KOrAycohUghExsUZZOEFIQkgb/d8mKe3EibgadyBQiQlLlmxL1nu+oE4RjhQdby12HpP2vA+jK4lPFLtroIm3dOGaMFGwXNpJkrGnpUrKFhaxClYC1hZ1oOTRZdiIVt1VExS65pxj2Q4CKm8GeAAThZxVzN8yR9jeRpMIf5y/AJj7DGxXvP/86jduZBmjwAAAAA==\\u001b[0m[Pipeline] node\\nRunning on \\u001b[8mha:////4HFz2q8kFnRDP54rOdrme553Tm2cBbSad6RlI0bybWm1AAAAnh+LCAAAAAAAAP9b85aBtbiIQTGjNKU4P08vOT+vOD8nVc83PyU1x6OyILUoJzMv2y+/JJUBAhiZGBgqihhk0NSjKDWzXb3RdlLBUSYGJk8GtpzUvPSSDB8G5tKinBIGIZ+sxLJE/ZzEvHT94JKizLx0a6BxUmjGOUNodHsLgAz2EgZh/eT83ILSktQifY3cxGIgrakPAHib2iPIAAAA\\u001b[0mJenkins in C:\\Program Files (x86)\\Jenkins\\workspace\\hello-world-pipeline\\n\\u001b[8mha:////4GZ0r1Sim+oKCuXq81DZfGP5hmWUMYImNxP9daJkty7IAAAApR+LCAAAAAAAAP9tjTEOwjAUQ3+KOrAycoh0gA0xsUZZOEFIQkgb/d8mKe3EibgadyBQiQlLlmxL1nu+oE4RjhQdby12HpP2vA+jK4lPFLtroIm3dOGaMFGwXNpJkrGnpUrKFhaxClYC1hZ1oOTRZdiIVt1VExS65pxj2Q4CKm8GeAAThZxVzN8yR9jeRpMIf5y/AJj7DGxXvP/86jfoP95RwAAAAA==\\u001b[0m[Pipeline] {\\n\\u001b[8mha:////4NJgnKgHJfLGyHX9BVJWEXk8FmH6U/cUZUfkJ68W8IFgAAAApR+LCAAAAAAAAP9tjTEOwjAUQ3+KOrAycoh0gQkxsUZZOEFIQkgb/d8mKe3EibgadyBQiQlLlmxL1nu+oE4RjhQdby12HpP2vA+jK4lPFLtroIm3dOGaMFGwXNpJkrGnpUrKFhaxClYC1hZ1oOTRZdiIVt1VExS65pxj2Q4CKm8GeAAThZxVzN8yR9jeRpMIf5y/AJj7DGxXvP/86jc09154wAAAAA==\\u001b[0m[Pipeline] stage\\n\\u001b[8mha:////4J1SRlpdTkmcCyiVdL5f6mefhdeXHOfnyTs7jmBy+zKPAAAApR+LCAAAAAAAAP9tjTEOwjAUQ3+KOrAycoh0ggUxsUZZOEFIQkgb/d8mKe3EibgadyBQiQlLlmxL1nu+oE4RjhQdby12HpP2vA+jK4lPFLtroIm3dOGaMFGwXNpJkrGnpUrKFhaxClYC1hZ1oOTRZdiIVt1VExS65pxj2Q4CKm8GeAAThZxVzN8yR9jeRpMIf5y/AJj7DGxXvP/86jek7ggRwAAAAA==\\u001b[0m[Pipeline] { (Hello)\\n\\u001b[8mha:////4I57OGEWTMvdmGgd0x28XIAVuBWTg5yDl6c9ouVopWC2AAAAoh+LCAAAAAAAAP9tjTEOAiEURD9rLGwtPQTbaWGsbAmNJ0AWEZb8zwLrbuWJvJp3kLiJlZNMMm+a93rDOic4UbLcG+wdZu14DKOti0+U+lugiXu6ck2YKRguzSSpM+cFJRUDS1gDKwEbgzpQdmgLbIVXD9UGhba9lFS/o4DGdQM8gYlqLiqVL8wJdvexy4Q/z18BzLEA29ce4gfg7KmOvAAAAA==\\u001b[0m[Pipeline] echo\\nHello World\\n\\u001b[8mha:////4BGQRjkxyArSWBqfNvxwFoWmMQturewq5+OY+ft/+6pSAAAAoh+LCAAAAAAAAP9tjTEOAiEURD9rLGwtPQTbGRNjZUtoPAGyiLDkfxZYdytP5NW8g8RNrJxkknnTvNcb1jnBiZLl3mDvMGvHYxhtXXyi1N8CTdzTlWvCTMFwaSZJnTkvKKkYWMIaWAnYGNSBskNbYCu8eqg2KLTtpaT6HQU0rhvgCUxUc1GpfGFOsLuPXSb8ef4KYI6xADvU7j9Dg2gqvAAAAA==\\u001b[0m[Pipeline] }\\n\\u001b[8mha:////4KJLdGONlFYcajFb7HEgAxuMOBEk8OfAEAmpLY6PqYjpAAAAoh+LCAAAAAAAAP9tjTEOAiEURD9rLGwtPQRbWRhjZUtoPAGyiLDkfxZYdytP5NW8g8RNrJxkknnTvNcb1jnBiZLl3mDvMGvHYxhtXXyi1N8CTdzTlWvCTMFwaSZJnTkvKKkYWMIaWAnYGNSBskNbYCu8eqg2KLTtpaT6HQU0rhvgCUxUc1GpfGFOsLuPXSb8ef4KYI6xADvU7j9J+wGOvAAAAA==\\u001b[0m[Pipeline] // stage\\n\\u001b[8mha:////4OaUHmk6C7DGJCMQqpi+M5Ky66dEIIbY3QYtANHMdM1NAAAAox+LCAAAAAAAAP9tjTEOwjAQBDdBFLSUPMIBiQ5R0VppeIFJjHFi3QX7QlLxIr7GH4iIRMVWO9PM641lijhydKqx1HpKlVdd6N301MCxvQYeVMMXVTElDlaVdii5tqcZSxaLeVmOhcbKUhU4eXKCtW7MwxTBkCvOEid30Mh9fccTmZ7KYqJ8YYzY3Po6Mf06fwMYu06Q77aCbP8BhStF0r0AAAA=\\u001b[0m[Pipeline] }\\n\\u001b[8mha:////4NgOLXEXVVO5wLuhyp8l/YrjBFpMpCcvwaobrGA93MeeAAAAoh+LCAAAAAAAAP9tjbEOgjAURS8YB1dHP6KEOBon14bFL6hQa6F5D9uHMPlF/pr/IJHEyTvdc5bzemOdIo4cnWotdZ5S7VUfBjc/NXLsroFH1fJF1UyJg1WVHStu7GnBisViWZZjpbGxVAdOnpxgq1vzMEUw5IqzxNkdNHLf3PFEpueymChfmCJ2t6FJTL/O3wCmvhfkZSnI9h+Wl0FxvQAAAA==\\u001b[0m[Pipeline] // node\\n\\u001b[8mha:////4KY4PpR0M0VKtDOsb5ExOp9v8AJf3pk0dpEoFv+XT3uKAAAAoh+LCAAAAAAAAP9tjTESgjAQRT84FraWHiKMtI6VbYbGE0SIMZDZxWQRKk/k1byDjMxY+av/XvNeb6xTxJGjU62lzlOqverD4OanRo7dNfCoWr6omilxsKqyY8WNPS1YsVgsy3KsNDaW6sDJkxNsdWsepgiGXHGWOLuDRu6bO57I9FwWE+ULU8TuNjSJ6df5G8DU94J8Xwqy8gPQ3eZBvQAAAA==\\u001b[0m[Pipeline] End of Pipeline\\nFinished: SUCCESS\", is there a way to remove the metadata from jenkins logs `",
    "website_area": "discuss"
  },
  {
    "id": "71ee3c2c-70e2-4f28-9ccd-4c4100204821",
    "url": "https://discuss.elastic.co/t/if-statement-not-working-with-gsub/215030",
    "title": "If statement not working with gsub",
    "category": [
      "Logstash"
    ],
    "author": "nino",
    "date": "January 14, 2020, 6:28pm January 15, 2020, 10:33am January 15, 2020, 10:54am January 15, 2020, 11:44am January 15, 2020, 12:23pm",
    "body": "Hello all, i want to remove \"ZC\" in a field and converted in a negative float \"account_value\" => \"767.19ZC\", \"@timestamp\" => 2020-01-06T23:00:00.000Z why this statement is not working? filter{ if \"ZC\" in \"field\" { mutate { gsub => [ \"field\",\"ZC\",\"\" ] } } } without \"if\" statement works filter{ mutate { gsub => [ \"field\",\"ZC\",\"\" ] } }",
    "website_area": "discuss"
  },
  {
    "id": "62d66d4d-156a-4a64-bc3e-5c3f095b881b",
    "url": "https://discuss.elastic.co/t/logstash-dont-delete-files/215127",
    "title": "Logstash don't delete Files",
    "category": [
      "Logstash"
    ],
    "author": "Ramon_Mateo",
    "date": "January 15, 2020, 11:55am",
    "body": "Hi I have added the field \"file_completed_action=> \"delete\" but logstash don't delete files. This is my logstash.conf and docker-compose input { file { path => \"/usr/share/logstash/inputlogs/*.csv\" start_position => \"beginning\" mode => \"read\" sincedb_path => \"/usr/share/logstash/.sincedb\" file_completed_action => \"delete\" } } filter { csv { separator => \";\" columns => [\"Protocol_Version\",\"Site_Producer_Name\",\"Site_Producer_ID\",\"Site_Sender_ skip_header => true } mutate{ remove_field => [\"Protocol_Version\",\"Site_Producer_Name\",\"Site_Producer_ID\", } date{ match => [\"Event_Timestamp\", \"dd/MM/yyyy HH:mm:ss\"] } mutate{ remove_field => [\"Event_Timestamp\", \"message\", \"host\"] } } output { elasticsearch { hosts => \"elasticsearch:9200\" index => \"lab-benchmarking-csv\" } stdout{} } This is my docker-compose version: '3.5' services: elasticsearch: build: context: elasticsearch/ container_name: elasticsearch volumes: - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro - ./elasticsearch/data:/data ports: - \"9200:9200\" - \"9300:9300\" environment: ES_JAVA_OPTS: \"-Xmx256m -Xms256m\" networks: - elk logstash: build: context: logstash/ container_name: logstash user: root volumes: - ./logstash/config:/usr/share/logstash/config - ./logstash/pipeline:/usr/share/logstash/pipeline - logfiles:/usr/share/logstash/inputlogs ports: - \"5000:5000\" environment: LS_JAVA_OPTS: \"-Xmx256m -Xms256m\" networks: - elk depends_on: - elasticsearch kibana: build: context: kibana/ container_name: kibana volumes: - ./kibana/config/:/usr/share/kibana/config:ro ports: - \"5601:5601\" networks: - elk depends_on: - elasticsearch Thanks for advanced.",
    "website_area": "discuss"
  },
  {
    "id": "4d3d2d3e-cf4b-4a49-b182-4df9fdbdda48",
    "url": "https://discuss.elastic.co/t/issues-sending-monitoring-data-from-logstash-to-elasticsearch/215103",
    "title": "Issues sending monitoring data from logstash to elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "asp",
    "date": "January 15, 2020, 9:30am January 15, 2020, 10:28am January 15, 2020, 10:32am January 15, 2020, 10:36am January 15, 2020, 11:04am January 15, 2020, 11:07am January 15, 2020, 11:09am January 15, 2020, 11:43am",
    "body": "Hi, I have some issues shipping internal monitoring data from logstash to elasticsearch. I don't know if i better place it in logstash or elasticsearch forum. So I start here I installed logstash and elasticsearch from rpm on centos. Stack version is 7.4.2. I am using basic license with enabled security. I am struggling with following error message in logstash logs and I don't see any data of logstash process in kibana's monitoring module. Error message in logstash logs: [2020-01-15T10:17:08,598][ERROR][logstash.outputs.elasticsearch][.monitoring-logstash] Encountered a retryable error. Will Retry with exponential backoff {:code=>403, :url=>\"https://es-lb.local:9200/_monitoring/bulk?system_id=logstash&system_api_version=7&interval=1s\"} complete log: [2020-01-15T10:16:09,666][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.4.2\"} [2020-01-15T10:16:11,132][INFO ][logstash.monitoring.internalpipelinesource] Monitoring License OK [2020-01-15T10:16:11,133][INFO ][logstash.monitoring.internalpipelinesource] Validated license for monitoring. Enabling monitoring pipeline. [2020-01-15T10:16:13,648][INFO ][org.reflections.Reflections] Reflections took 45 ms to scan 1 urls, producing 20 keys and 40 values [2020-01-15T10:16:52,579][INFO ][logstash.outputs.elasticsearch][commonOutElasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[https://logstash_ingest:xxxxxx@es-lb.local:9200/]}} [2020-01-15T10:16:53,642][WARN ][logstash.outputs.elasticsearch][commonOutElasticsearch] Restored connection to ES instance {:url=>\"https://logstash_ingest:xxxxxx@es-lb.local:9200/\"} [2020-01-15T10:16:53,650][INFO ][logstash.outputs.elasticsearch][commonOutElasticsearch] ES Output version determined {:es_version=>7} [2020-01-15T10:16:53,650][WARN ][logstash.outputs.elasticsearch][commonOutElasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7} [2020-01-15T10:16:53,764][INFO ][logstash.outputs.elasticsearch][commonOutElasticsearch] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"//es-lb.local:9200\"]} [2020-01-15T10:16:54,635][INFO ][logstash.outputs.elasticsearch][commonOutElasticsearch] Using default mapping template [2020-01-15T10:16:54,773][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][commonOutElasticsearch] A gauge metric of an unknown type (org.jruby.specialized.RubyArrayOneObject) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-01-15T10:16:54,775][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][httpd_access] A gauge metric of an unknown type (org.jruby.RubyArray) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-01-15T10:16:54,775][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][auskunft_json] A gauge metric of an unknown type (org.jruby.RubyArray) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-01-15T10:16:54,781][INFO ][logstash.javapipeline ][httpd_access] Starting pipeline {:pipeline_id=>\"httpd_access\", \"pipeline.workers\"=>8, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>1000, :thread=>\"#<Thread:0x60cf97e9 run>\"} [2020-01-15T10:16:54,782][INFO ][logstash.javapipeline ][auskunft_json] Starting pipeline {:pipeline_id=>\"auskunft_json\", \"pipeline.workers\"=>8, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>1000, :thread=>\"#<Thread:0x9901e19@/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:38 run>\"} [2020-01-15T10:16:54,782][INFO ][logstash.javapipeline ][commonOutElasticsearch] Starting pipeline {:pipeline_id=>\"commonOutElasticsearch\", \"pipeline.workers\"=>8, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>1000, :thread=>\"#<Thread:0x6cffdc69@/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:37 run>\"} [2020-01-15T10:16:54,809][INFO ][logstash.outputs.elasticsearch][commonOutElasticsearch] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-*\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"*\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} [2020-01-15T10:16:56,018][INFO ][logstash.inputs.redis ][httpd_access] Registering Redis {:identity=>\"redis://<password>@redis-lb.local:16379/0 list:httpd_access\"} [2020-01-15T10:16:56,026][INFO ][logstash.inputs.redis ][auskunft_json] Registering Redis {:identity=>\"redis://<password>@redis-lb.local:16379/0 list:auskunft_json\"} [2020-01-15T10:16:56,063][INFO ][logstash.javapipeline ][commonOutElasticsearch] Pipeline started {\"pipeline.id\"=>\"commonOutElasticsearch\"} [2020-01-15T10:16:56,063][INFO ][logstash.javapipeline ][auskunft_json] Pipeline started {\"pipeline.id\"=>\"auskunft_json\"} [2020-01-15T10:16:56,080][INFO ][logstash.javapipeline ][httpd_access] Pipeline started {\"pipeline.id\"=>\"httpd_access\"} [2020-01-15T10:16:56,087][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][generic_json] A gauge metric of an unknown type (org.jruby.RubyArray) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-01-15T10:16:56,088][INFO ][logstash.javapipeline ][generic_json] Starting pipeline {:pipeline_id=>\"generic_json\", \"pipeline.workers\"=>8, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>1000, :thread=>\"#<Thread:0x8eb9d50@/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:37 run>\"} [2020-01-15T10:16:56,154][INFO ][logstash.inputs.redis ][generic_json] Registering Redis {:identity=>\"redis://<password>@redis-lb.local:16379/0 list:generic-json\"} [2020-01-15T10:16:56,155][INFO ][logstash.inputs.redis ][generic_json] Registering Redis {:identity=>\"redis://<password>@redis-lb.local:16379/0 list:generic-json-root\"} [2020-01-15T10:16:56,172][INFO ][logstash.javapipeline ][generic_json] Pipeline started {\"pipeline.id\"=>\"generic_json\"} [2020-01-15T10:16:56,295][INFO ][logstash.agent ] Pipelines running {:count=>5, :running_pipelines=>[:httpd_access, :generic_json, :commonOutElasticsearch, :auskunft_json], :non_running_pipelines=>[]} [2020-01-15T10:16:57,953][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting \"document_type\" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>\"document_type\", :plugin=><LogStash::Outputs::ElasticSearch bulk_path=>\"/_monitoring/bulk?system_id=logstash&system_api_version=7&interval=1s\", ssl_certificate_verification=>false, password=><password>, hosts=>[//es-lb.local:9200], cacert=>\"/etc/logstash/config_sets/plx/certs/ca/elasticsearch/ca.crt\", sniffing=>false, manage_template=>false, id=>\"d2bd83dad561381257de0b203724b86122bc0132449508529f15cb48cc583204\", user=>\"remote_monitoring_user\", ssl=>true, document_type=>\"%{[@metadata][document_type]}\", enable_metric=>true, codec=><LogStash::Codecs::Plain id=>\"plain_3d4aab69-94d0-42a0-a510-e66974c44d90\", enable_metric=>true, charset=>\"UTF-8\">, workers=>1, template_name=>\"logstash\", template_overwrite=>false, doc_as_upsert=>false, script_type=>\"inline\", script_lang=>\"painless\", script_var_name=>\"event\", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, ilm_enabled=>\"auto\", ilm_rollover_alias=>\"logstash\", ilm_pattern=>\"{now/d}-000001\", ilm_policy=>\"logstash-policy\", action=>\"index\", sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>} [2020-01-15T10:16:58,017][WARN ][logstash.outputs.elasticsearch][.monitoring-logstash] ** WARNING ** Detected UNSAFE options in elasticsearch output configuration! ** WARNING ** You have enabled encryption but DISABLED certificate verification. ** WARNING ** To make sure your data is secure change :ssl_certificate_verification to true [2020-01-15T10:16:58,043][INFO ][logstash.outputs.elasticsearch][.monitoring-logstash] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[https://remote_monitoring_user:xxxxxx@es-lb.local:9200/]}} [2020-01-15T10:16:58,140][WARN ][logstash.outputs.elasticsearch][.monitoring-logstash] Restored connection to ES instance {:url=>\"https://remote_monitoring_user:xxxxxx@es-lb.local:9200/\"} [2020-01-15T10:16:58,159][INFO ][logstash.outputs.elasticsearch][.monitoring-logstash] ES Output version determined {:es_version=>7} [2020-01-15T10:16:58,160][WARN ][logstash.outputs.elasticsearch][.monitoring-logstash] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7} [2020-01-15T10:16:58,187][INFO ][logstash.outputs.elasticsearch][.monitoring-logstash] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"//es-lb.local:9200\"]} [2020-01-15T10:16:58,205][INFO ][logstash.javapipeline ][.monitoring-logstash] Starting pipeline {:pipeline_id=>\".monitoring-logstash\", \"pipeline.workers\"=>1, \"pipeline.batch.size\"=>2, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>2, :thread=>\"#<Thread:0x60584987 run>\"} [2020-01-15T10:16:58,250][INFO ][logstash.javapipeline ][.monitoring-logstash] Pipeline started {\"pipeline.id\"=>\".monitoring-logstash\"} [2020-01-15T10:16:58,260][INFO ][logstash.agent ] Pipelines running {:count=>6, :running_pipelines=>[:\".monitoring-logstash\", :httpd_access, :generic_json, :commonOutElasticsearch, :auskunft_json], :non_running_pipelines=>[]} [2020-01-15T10:16:58,534][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2020-01-15T10:17:08,598][ERROR][logstash.outputs.elasticsearch][.monitoring-logstash] Encountered a retryable error. Will Retry with exponential backoff {:code=>403, :url=>\"https://es-lb.local:9200/_monitoring/bulk?system_id=logstash&system_api_version=7&interval=1s\"} [2020-01-15T10:17:10,631][ERROR][logstash.outputs.elasticsearch][.monitoring-logstash] Encountered a retryable error. Will Retry with exponential backoff {:code=>403, :url=>\"https://es-lb.local:9200/_monitoring/bulk?system_id=logstash&system_api_version=7&interval=1s\"} I have no entry in elasticsearch's log .log. For monitoring I define the standard remote-monitoring-user which ships with elasticsearch. I just set the password for it, but no other changes on it or it's roles. My logstash.yml looks like this: node.name: kubernetes03-plx-0 path.data: /var/lib/logstash/plx/0 log.level: info path.logs: /var/log/logstash/plx/0 xpack.monitoring.enabled: ${XPACK_MONITORING_ENABLED} xpack.monitoring.elasticsearch.username: ${monitoring_user} xpack.monitoring.elasticsearch.password: ${monitoring_password} xpack.monitoring.elasticsearch.hosts: ${XPACK_MONITORING_ELASTICSEARCH_HOSTS} xpack.monitoring.elasticsearch.ssl.certificate_authority: \"/etc/logstash/config_sets/plx/certs/ca/elasticsearch/ca.crt\" xpack.monitoring.elasticsearch.ssl.verification_mode: ${XPACK_MONITORING_ELASTICSEARCH_SSL_VERIFICATION_MODE} xpack.monitoring.elasticsearch.sniffing: false xpack.monitoring.collection.interval: ${XPACK_MONITORING_COLLECTION_INTERVAL} xpack.monitoring.collection.pipeline.details.enabled: true Can you please support here? Thanks a lot, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "8d2cea2b-6c21-49ce-9569-7cb95908c60f",
    "url": "https://discuss.elastic.co/t/adding-fields-to-configuration-file/215110",
    "title": "Adding fields to configuration file",
    "category": [
      "Logstash"
    ],
    "author": "amin224",
    "date": "January 15, 2020, 9:51am January 15, 2020, 10:04am January 15, 2020, 10:06am January 15, 2020, 10:12am January 15, 2020, 10:46am January 15, 2020, 10:31am January 15, 2020, 10:42am January 15, 2020, 11:30am",
    "body": "Hi, I am running multiple java applications and creating logging files for each one, so i decided to use elastic stack for Centralised Log Management. My question is how can i add port and IP address fields to my index pattern. should i use mutate filter plugin or do i specify them in my input. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "2a8cc6df-e059-43f0-a13c-66f42c33b088",
    "url": "https://discuss.elastic.co/t/how-to-edit-data-coming-in-from-kafka-within-logstash-and-ingest-into-elasticsearch/215123",
    "title": "How to edit data coming in from kafka within logstash and ingest into elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "fallicious",
    "date": "January 15, 2020, 11:23am",
    "body": "i have two indexes coming in elasticsearch that are being fed through kafka, (indexA) and (indexB), i want to take some fields from the (indexB) and a few from (indexA) and make a third (indexC) of of those fields. i have tried one way of using the elasticsearch input plugin to get the already posted index data out and process it in the config file to post it back into elasticsearch, but apart from makinng copies of the same indexes i could not achive anything. This is the config file i tried with one index input { elasticsearch { hosts => [\"localhost:9200\"] index => \"analytics*\" query => '{ \"query\": { \"query_string\": { \"query\": \"*\" } } }' size => 500 scroll => \"5m\" docinfo => true } } filter { mutate { remove_field => [ \"user.id\"] } } output { elasticsearch { index => \"copy.%{[@metadata][_index]}\" document_type => \"%{[@metadata][_type]}\" document_id => \"%{[@metadata][_id]}\" } } my goal is to take the (username) field, (topic) field, and the (data) field from (indexA), the distinct (username) field using the cardinality aggregation from (indexB) and make a new (indexC) from this data. the Cardinality aggregation used { \"aggs\": { \"user_count\": { \"cardinality\": { \"field\": \"data.user.name.keyword\" } } } } any suggestions on how i can solve this issue?",
    "website_area": "discuss"
  },
  {
    "id": "93b0b357-6630-4937-b600-9b5f2039eb00",
    "url": "https://discuss.elastic.co/t/how-to-get-certifiates-checked-on-elasticsearch-output-plugin/215100",
    "title": "How to get certifiates checked on elasticsearch-output-plugin?",
    "category": [
      "Logstash"
    ],
    "author": "asp",
    "date": "January 15, 2020, 9:08am",
    "body": "Hi, I installed logstash on centos via rpm and installed via system-install as service. When starting logstash I get the following warning: ** WARNING ** You have enabled encryption but DISABLED certificate verification. ** WARNING ** To make sure your data is secure change :ssl_certificate_verification to true My output config looks like this: output { elasticsearch { #hosts => [\"${ES_HOST}:${ES_PORT}\"] hosts => [\"${ES_HOSTS}\"] ssl => \"${USE_ES_SSL}\" cacert => \"${ES_CA_CERT_PATH}\" ssl_certificate_verification => \"${USE_ES_OUTPUT_SSL_CERT_VERIFICATION}\" # credentials are fetched from envrionment or logstash-keystore user => \"${LOGSTASH_USER}\" password => \"${LOGSTASH_PASSWORD}\" index => \"%{[@metadata][indexName]}\" } } The Environment variable USE_ES_OUTPUT_SSL_CERT_VERIFICATION is set via /etc/sysconfig/<service_name>: cat /etc/sysconfig/logstash-plx-0 | grep USE_ES_OUTPUT_SSL_CERT_VERIFICATION USE_ES_OUTPUT_SSL_CERT_VERIFICATION=true Using logstash 7.4.1 Any idea? Since I condigure cacert I would like to have the certificate checked. Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "19b23011-ead3-44fc-a5f2-e884ba257367",
    "url": "https://discuss.elastic.co/t/pipeline-error/214973",
    "title": "Pipeline Error",
    "category": [
      "Logstash"
    ],
    "author": "jogoinar10",
    "date": "January 14, 2020, 10:42am January 14, 2020, 1:11pm January 15, 2020, 8:22am",
    "body": "[2020-01-14T18:39:45,304][INFO ][org.logstash.beats.BeatsHandler][main] [local: 192.168.0.79:5044, remote: 192.168.0.97:55045] Handling exception: Connection reset [2020-01-14T18:39:45,314][WARN ][io.netty.channel.DefaultChannelPipeline][main] An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception. java.net.SocketException: Connection reset at sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:345) ~[?:?] at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:376) ~[?:?] at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1128) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:347) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-all-4.1.30.Final.jar:4.1.30.Final] at java.lang.Thread.run(Thread.java:830) [?:?]",
    "website_area": "discuss"
  },
  {
    "id": "1c14c65a-6145-4d55-8a05-48453726ab7f",
    "url": "https://discuss.elastic.co/t/logstash-ssl-error-with-http-and-https/214759",
    "title": "Logstash SSL Error with http and https",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "January 13, 2020, 5:47am January 15, 2020, 7:34am January 15, 2020, 8:08am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c9aa6465-09bb-4d29-ac6d-7bd3876de9b4",
    "url": "https://discuss.elastic.co/t/problem-sorting-unique-values-in-csv/215057",
    "title": "Problem Sorting Unique Values in CSV",
    "category": [
      "Logstash"
    ],
    "author": "niall",
    "date": "January 14, 2020, 10:49pm January 14, 2020, 11:10pm",
    "body": "Hi, I've been trying to solve this issue and came to a dead end. I will need some advice for this. I do have a CSV file. Below is just sample data, but the file has like up to thousand lines. Below is the sample file: \"timestamp\",\"name\",\"type\",\"value\",\"date\" \"2019-11-27 05:42:45.000\",\"bill.acme.com\",\"ns\",\"ns-1468.awsdns-55.org\",\"201911\" \"2019-11-27 05:42:45.000\",\"bill.acme.com\",\"ns\",\"ns-1777.awsdns-30.co.uk\",\"201911\" \"2019-11-27 05:42:45.000\",\"bill.acme.com\",\"ns\",\"ns-258.awsdns-32.com\",\"201911\" \"2019-11-27 05:42:45.000\",\"bill.acme.com\",\"ns\",\"ns-638.awsdns-15.net\",\"201911\" \"2019-11-27 05:51:28.000\",\"bills-record.global.acme.com\",\"a\",\"10.144.152.62\",\"201911\" \"2019-11-27 05:51:28.000\",\"bills-record.global.acme.com\",\"a\",\"10.144.126.177\",\"201911\" \"2019-11-27 05:51:28.000\",\"bills-record.global.acme.com\",\"a\",\"10.144.135.205\",\"201911\" What I'm trying to achieve here is, if name,type and date has the same value, the output should be a single name value with multiple IP addresses/string as it's value. However the value of IP/string should not be a duplicate. The values can amount from 1 to n. I've tried using IF statements but to no avail. I know by using add_field, it will automatically create an array if the field is not, but I can't get it to grab multiple IP addresses. The sample JSON output I'm hoping is as below: { \"type\" => \"ns\", \"date\" => \"201911\", \"name\" => \"bill.acme.com\", \"@version\" => \"1\", \"timestamp\" => \"2019-11-27 05:42:45.000\", \"value\" => [ [0] \"ns-1468.awsdns-55.org\", [1] \"ns-1777.awsdns-30.co.uk\", [2] \"ns-258.awsdns-32.com\", [3] \"ns-638.awsdns-15.net\" ] } { \"type\" => \"a\", \"date\" => \"201911\", \"name\" => \"bills-record.global.acme.com\", \"@version\" => \"1\", \"timestamp\" => \"2019-11-27 05:51:28.000\", \"value\" => [ [0] \"10.144.152.62\", [1] \"10.144.126.177\", [2] \"10.144.135.205\" ] } Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "03bceb53-74a5-4007-a928-b11f4de1353a",
    "url": "https://discuss.elastic.co/t/jdbc-problem-when-tracking-column-is-fixed-decimal/215051",
    "title": "Jdbc problem when tracking column is fixed decimal",
    "category": [
      "Logstash"
    ],
    "author": "Peter_Li",
    "date": "January 14, 2020, 9:38pm",
    "body": "I have a table where the tracking value has to be constructed from two integers: batch and index. So, I created a fixed decimal \"bistamp\" like 123.0034, 124.0003, etc. using: select ... to_number(to_char ( batch , 'FM999999' ) || '.' || to_char ( index , 'FM0000' ) , '999999.9999') as bistamp, where ... to_number(to_char ( batch , 'FM999999' ) || '.' || to_char ( index , 'FM0000' ) , '999999.9999') > :sql_last_value The problem is that $HOME/.logstash_jdbc_last_run stores this number as: --- !ruby/object:BigDecimal '0:0.70380003e4' And on subsequent runs, the value for :sql_last_value is 0. Is this a bug ? Thanks,",
    "website_area": "discuss"
  },
  {
    "id": "4fd1d2af-9e60-4579-aa04-9a6102aa9f4a",
    "url": "https://discuss.elastic.co/t/plotting-geoip-data-in-kibana-maps-via-beats-output-to-logstash/214673",
    "title": "Plotting geoIP Data in Kibana Maps via Beats Output to Logstash",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "January 10, 2020, 10:07pm January 10, 2020, 10:57pm January 14, 2020, 8:13pm January 14, 2020, 8:12pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a4a412ba-215d-432d-9252-6e7fbdb6a336",
    "url": "https://discuss.elastic.co/t/how-to-make-csv-output-filter-close-file-not-append-when-pipeline-runs/215031",
    "title": "How to make CSV output filter close file (not append) when pipeline runs?",
    "category": [
      "Logstash"
    ],
    "author": "tomj",
    "date": "January 14, 2020, 6:31pm",
    "body": "I have a pipeline set up that I wish to \"refresh\" a CSV file (to be used later in a \"translate\" step) from a database. What I'm seeing is that every time this pipeline runs, the entire CSV output gets appended to the named file. From reading the Ruby code, the CSV output is derived from the File output, and thus it is possible (but undocumented) to pass the \"write_behavior\" option to the CSV output However, the \"write_behavior\" setting appears to have exactly two options: \"append\" => append every event to the file (the file will keep growing) \"overwrite\" => overwrite the file each time an event is processed (the file will always contain at most one line) What I need is for each time the pipeline runs, the CSV file will be replaced with the most recent data obtained from a JDBC source. If you are wondering why I don't just use jdbc_static, I would like to. I am currently trying to figure out why the jdbc_static plugin seems unable to process more than about 200 events per second. I need a single Logstash instance to be able to process about 7500 events per second.",
    "website_area": "discuss"
  },
  {
    "id": "7039e2fc-b219-46f4-a8b6-581d260eeb14",
    "url": "https://discuss.elastic.co/t/cant-install-ruby-gem-into-logstash-environment/214926",
    "title": "Can't install ruby gem into logstash environment",
    "category": [
      "Logstash"
    ],
    "author": "Brian_Seel",
    "date": "January 14, 2020, 4:17am January 14, 2020, 2:16pm January 14, 2020, 5:00pm January 14, 2020, 5:57pm",
    "body": "I need to install a gem into the ruby instance that logstash uses. Specifically, I need to install jdbc-sqlite3 to support database calls that can't be done as an input plugin. I have tried doing it remote or local, without success. bitnami@ip-172-31-64-199:/opt/bitnami/logstash$ sudo logstash-plugin install ~/jdbc-sqlite3-3.8.11.2.gem --no-verify ERROR: Mixed source of plugins, you can't mix local .gem and remote gems See: 'bin/logstash-plugin install --help' bitnami@ip-172-31-64-199:/opt/bitnami/logstash$ sudo logstash-plugin install jdbc-sqlite3 --no-verify Validating jdbc-sqlite3 jdbc-sqlite3 is not a Logstash plugin ERROR: Installation aborted, verification failed for jdbc-sqlite3 Any suggestions?",
    "website_area": "discuss"
  },
  {
    "id": "6430d0d4-e329-4b38-8007-0d699bac24ed",
    "url": "https://discuss.elastic.co/t/logstash-wont-show-output-on-console/214941",
    "title": "Logstash won't show output on console",
    "category": [
      "Logstash"
    ],
    "author": "Anindita_Chavan",
    "date": "January 14, 2020, 7:38am January 14, 2020, 8:52am January 14, 2020, 9:00am January 14, 2020, 5:41pm January 14, 2020, 5:41pm January 14, 2020, 5:41pm January 14, 2020, 10:52am January 14, 2020, 5:41pm January 14, 2020, 5:40pm",
    "body": "This is my logstash configuration file: < input { file { path => \"/home/trimslogs/trims.log\" sincedb_path => \"NUL\" ignore_older => 0 } } filter { grok { match => { \"message\" => \"([(?%{YEAR}-%{MONTHNUM}-%{MONTHDAY:day} %{TIME:time})])([%{LOGLEVEL:type}])([%{USERNAME:application}])([%{IPV4:ip}])([%{HOSTNAME:hostname}])([%{HOSTNAME:class}])([%{USERNAME:method}])([%{BASE10NUM:linenumber}])([null])([null])([Thread[%{GREEDYDATA:threadname}]])([(?%{YEAR}-%{MONTHNUM}-%{MONTHDAY:day} %{TIME:time})])([(?%{YEAR}-%{MONTHNUM}-%{MONTHDAY:day} %{TIME:time})])([%{GREEDYDATA:threadname1}])([%{GREEDYDATA:error_message}])()()()()()()()()()()\"} } } output { stdout { codec => rubydebug } } /> When i run logstash, it won't give me output on console and remains stuck on [INFO ] 2020-01-14 02:37:55.184 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=>9600}",
    "website_area": "discuss"
  },
  {
    "id": "3ad26d22-28e5-4e41-9f30-6dc9817bbca5",
    "url": "https://discuss.elastic.co/t/how-to-monitor-individual-pipelines-for-memory-usage/215024",
    "title": "How to monitor individual pipelines for memory usage",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "January 14, 2020, 5:21pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "145792cc-0ac0-40ef-a952-51bd0881510c",
    "url": "https://discuss.elastic.co/t/wid-extractions-with-logstash/215011",
    "title": "WID Extractions with Logstash",
    "category": [
      "Logstash"
    ],
    "author": "John_Guzman",
    "date": "January 14, 2020, 4:12pm",
    "body": "Hello! I need to extract some data from a Windows internal Database from WSUS. I already tried using the JDBC and Exec input plugins, buy no one worked using this string connection \\.\\pipe\\MICROSOFT##WID\\tsql\\query. This is my exec config file, but it's not retrieving the query results. If i use it directly from shell, it works. input { exec { command => \"sqlcmd -S \\\\.\\pipe\\MICROSOFT##WID\\tsql\\query SELECT [ComputerTargetId],[ParentServerId],[Name],[IPAddress],[LastSyncResult],[LastSyncTime],[LastReportedStatusTime],[LastReportedInventoryTime],[ClientVersion],[OSArchitecture],[Make],[Model],[BiosName],[BiosVersion],[BiosReleaseDate],[OSMajorVersion],[OSMinorVersion],[OSBuildNumber],[OSServicePackMajorNumber],[OSDefaultUILanguage]FROM [SUSDB].[PUBLIC_VIEWS].[vComputerTarget] go\" interval => 30 } } Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "ad898fcc-8e7c-4ea0-a997-03b10cfeedb3",
    "url": "https://discuss.elastic.co/t/azure-module-setup-with-elastic-https-security/215009",
    "title": "Azure module setup with elastic https security",
    "category": [
      "Logstash"
    ],
    "author": "rugenl",
    "date": "January 14, 2020, 3:54pm",
    "body": "I'm trying to run setup for the logstash (7.5.0) azure module. All examples are without security but we are using elasticsearch https. I'm getting errors: [2020-01-14T09:42:45,431][ERROR][logstash.licensechecker.licensereader] Unable to retrieve license information from license server {:message=>\"Elasticsearch Unreachable: [https://met-elk-csg06.missouri.edu:9200/][Manticore::ClientProtocolException] PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\"} [2020-01-14T09:42:45,506][ERROR][logstash.licensechecker.modulelicensechecker] Failed to fetch X-Pack information from Elasticsearch. This is likely due to failure to reach a live Elasticsearch cluster. [2020-01-14T09:42:45,513][WARN ][logstash.config.modulescommon] The azure module is not enabled. Please check the logs for additional information. I think the key is \"unable to find valid certification path\", per tcpdump, no connection is attempted. I added these variables to the module config but they don't seem to make any difference: .... var.elasticsearch.cacerts: \"/etc/logstash/certs/https_interm.cer\" var.elasticsearch.user: userid var.elasticsearch.password: password .... These values work in elasticsearch output plugins (in other logstash instances) Any ideas or any pointers to where this is documented? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "00e96ca0-0c45-4c0c-8363-43405df5763f",
    "url": "https://discuss.elastic.co/t/disable-enable-multiline-codec-when-using-clone-filter/214942",
    "title": "Disable/enable multiline codec when using clone filter",
    "category": [
      "Logstash"
    ],
    "author": "nino",
    "date": "January 14, 2020, 9:31am January 14, 2020, 2:24pm January 14, 2020, 2:39pm January 14, 2020, 2:50pm January 14, 2020, 5:15pm",
    "body": "Hello all, is it posible to disable multiline when using clone filter?. For instance: input{ codec => multiline { pattern => \"^ImposibleMatch\" negate => true what => previous auto_flush_interval => 1 multiline_tag => \"\" } } filter{ clone { clones => [\"multilineOff\"] } if [type] == \"multilineOff\" { ### treat file as no multiline, one line, one event (bypassing multiline code on input) } else{ ### treat file as multiline (input multiline codec takes effect) } } Best regards",
    "website_area": "discuss"
  },
  {
    "id": "5239cc24-3e31-448a-a2db-0fcda53a5798",
    "url": "https://discuss.elastic.co/t/stdout-codec-json-lines-is-empty/214834",
    "title": "Stdout codec json_lines is empty",
    "category": [
      "Logstash"
    ],
    "author": "Kostis95",
    "date": "January 13, 2020, 4:00pm January 14, 2020, 2:57pm",
    "body": "Hi there, I'm still trying to connect my postgresql database to elastic. I've got all the required files: logstash.yml, pipelines.yml and p1.conf (in my local/etc/logstash). However I'm only getting Successfully started Logstash API endpoint and no result, if I run logstash --debug I get: stdout { codec => json_lines } ```] So it's empty... Here are some lines I'm also getting from the debug: config LogStash::Inputs::Jdbc/@statement = \"SELECT * from phpbb_banlist\" config LogStash::Inputs::Jdbc/@jdbc_validation_timeout = 3600 config LogStash::Inputs::Jdbc/@connection_retry_attempts = 1 config LogStash::Inputs::Jdbc/@connection_retry_attempts_wait_time = 0.5 [DEBUG][org.logstash.ackedqueue.io.MmapPageIOV2] PageIO recovery element index:0, readNextElement exception: Element invalid length [DEBUG][logstash.javapipeline ] Starting pipeline {:pipeline_id=>\"my-pipeline_1\"} [WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][my-pipeline_1] A gauge metric of an unknown type (org.jruby.RubyArray) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [INFO ][logstash.javapipeline ][my-pipeline_1] Starting pipeline {:pipeline_id=>\"my-pipeline_1\", \"pipeline.workers\"=>1, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>125, \"pipeline.sources\"=>[\"/usr/local/etc/logstash/p1.conf\"], :thread=>\"#<Thread:0x2dc0c9bc run>\"} [logstash.javapipeline ][my-pipeline_1] Pipeline started {\"pipeline.id\"=>\"my-pipeline_1\"} [DEBUG][logstash.javapipeline ] Pipeline started successfully {:pipeline_id=>\"my-pipeline_1\", :thread=>\"#<Thread:0x2dc0c9bc run>\"} [DEBUG][org.logstash.execution.PeriodicFlush][my-pipeline_1] Pushing flush onto pipeline. [INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:\"my-pipeline_1\"], :non_running_pipelines=>[]} [DEBUG][logstash.agent ] Starting puma [DEBUG][logstash.agent ] Trying to start WebServer {:port=>9600} [DEBUG][logstash.api.service ] [api-service] start [INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [DEBUG][logstash.inputs.jdbc ][my-pipeline_1] loading ~/Desktop/postgresql-42.2.9.jar [DEBUG][logstash.inputs.jdbc ] config LogStash::Inputs::Jdbc/@use_prepared_statements = false [DEBUG][logstash.inputs.jdbc ] config LogStash::Inputs::Jdbc/@prepared_statement_name = \"\" Maybe somebody can help me to sync my postgresql database with elastic (because it's taking forever this way), I've been searching the web for a solution for a long time. I was thinking the problem is the prepared statement being empty but my .conf file is all in order and as simple as can be.",
    "website_area": "discuss"
  },
  {
    "id": "77523325-1b88-4b32-8a23-0c880cf1aa40",
    "url": "https://discuss.elastic.co/t/extracting-multiline-xml-fields-from-a-log/214954",
    "title": "Extracting multiline xml fields from a log",
    "category": [
      "Logstash"
    ],
    "author": "angelcab",
    "date": "January 14, 2020, 8:53am January 14, 2020, 2:34pm",
    "body": "Hello I'm struggling with a problem trying to extract indepent fields from a log which contains a XML embebbed. I'm using logstash for the ingesting into elasticsearch. First, I collect the logs with filebeat and send it to kafka (architecture reasons). So, logstash conf file is read from kafka and ingest into elastic. The format of my message is similar to this: 2019-12-06 14:34:13,620 hostname: [com.ibm.mq.jmqi.remote.impl.RemoteSession[:/13514][connectionId=C328C3D8]] INFO - >>message: { some data in JSON} <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Document xmlns=\"urn:iso:std:iso:20022\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"urn:iso:std:iso:20022\"> <FIToFICstmrCdtTrf> <GrpHdr> <MsgId>Fe9275f33b63794dea4</MsgId> ....some other fields... </Document> 2019-12-06 14:34:13,620 hostname:*** another log line So, the idea is to ingest the log into elasticsearch but extract every single node in xml in sepparated fields. I've tryed with a grok filter to identified the message (this specific structure) and then parse with the xml filter but I haven't succeeded. One of the problems is that I've lines in the logs that are not equal as the showed, there are also lines without the xml.. so I've only had to match this concret log line with the xml inside. Any adviced or help? Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "93c71d3a-d22e-4265-9acb-461eb3b02ef7",
    "url": "https://discuss.elastic.co/t/fluent-codec-and-multline/212504",
    "title": "Fluent codec and multline",
    "category": [
      "Logstash"
    ],
    "author": "bquevat",
    "date": "December 19, 2019, 2:41pm January 9, 2020, 2:42pm January 14, 2020, 1:52pm",
    "body": "Hello, I use the fluent codec in my input. It receives logs from Java applications and I would like to make Java stack trace into a single event. This can be done with the multiline plugin. How can I benefit from the functionnalities of both fluent and multiline codec? My configuration below: input { tcp{ port => 9532 codec => fluent } } filter { grok { # Parsing des logs Tomcat HTTP match => { \"message\" => [ \"%{IP} - - [%{HTTPDATE}] \"%{NOTSPACE:verb} %{NOTSPACE:request} HTTP/%{NUMBER:httpversion}\" %{INT:code} %{INT:size} %{INT:time}\", \"%{TIMESTAMP_ISO8601:timestamp}\\s+%{LOGLEVEL:severity}\\s+[%{DATA:service},%{DATA:trace},%{DATA:span},%{DATA:exportable}]\\s+%{DATA id}\\s+---\\s+[%{DATA:thread}]\\s+%{DATA:class}\\s+:\\s+%{GREEDYDATA:logMessage}\" ]} } date { match => [ \"timestamp\" , \"YY-MM-dd HH:mm:ss.SSS\" ] } } output { elasticsearch { user => \"logstash\" password => \"xxxxx\" hosts => [\"xxxxx:9632\"] index => \"logstash-tap2use-%{+YYYY.MM.dd}\" document_type => \"logs\" } } Thanks for your help.",
    "website_area": "discuss"
  },
  {
    "id": "8a0d2027-96f3-43e6-b036-5d364d9b6a3a",
    "url": "https://discuss.elastic.co/t/created-nested-fields-from-xpath-check-for-existing-documents/214414",
    "title": "Created nested fields from Xpath & check for existing documents",
    "category": [
      "Logstash"
    ],
    "author": "karlm",
    "date": "January 9, 2020, 10:37am January 9, 2020, 6:05pm January 14, 2020, 11:47am",
    "body": "I have two questions; parsing xml data & adding it to an array in a record in an index checking for an existing record in an index and if it exists add the new data of that record to the array of the existing record I have an jdbc input that has an xml column, input { jdbc { .... statement => \"SELECT event_xml.... } } then an xml filter to parse the data, How do i make the the last 3 xpaths to be an array, do i need a mutate or ruby filter? I cant seem to figure it out filter { xml { source => \"event_xml\" remove_namespaces => true store_xml => false force_array => false xpath => [ \"/CaseNumber/text()\", \"case_number\" ] xpath => [ \"/FormName/text()\", \"[conversations][form_name]\" ] xpath => [ \"/EventDate/text()\", \"[conversations][event_date]\" ] xpath => [ \"/CaseNote/text()\", \"[conversations][case_note]\" ] } } so it would something like this look like this in the Elastic search. { \"case_number\" : \"12345\", \"conversations\" : [ { \"form_name\" : \"form1\", \"event_date\" : \"2019-01-09T00:00:00Z\", \"case_note\" : \"this is a case note\" } ] } So second question is, if there is already a case_number of \"12345\" instead of creating a new document for this add the new xml values to the existing record. so it would look like this { \"case_number\" : \"12345\", \"conversations\" : [ { \"form_name\" : \"form1\", \"event_date\" : \"2019-01-09T00:00:00Z\", \"case_note\" : \"this is a case note\" }, { \"form_name\" : \"form2\", \"event_date\" : \"2019-05-09T00:00:00Z\", \"case_note\" : \"this is another case note\" }, ] } my output filter output { elasticsearch { hosts => [\"http://localhost:9200\"] index => \"cases\" manage_template => false } } Is this possible? thanks",
    "website_area": "discuss"
  },
  {
    "id": "91792851-936f-4129-80a6-4b914e89682a",
    "url": "https://discuss.elastic.co/t/parsing-json-file-with-logstash/214080",
    "title": "Parsing json file with logstash",
    "category": [
      "Logstash"
    ],
    "author": "BeMoore",
    "date": "January 7, 2020, 3:15pm January 7, 2020, 4:33pm January 9, 2020, 7:20am January 9, 2020, 11:48am January 9, 2020, 12:03pm January 9, 2020, 12:07pm January 9, 2020, 3:18pm January 9, 2020, 3:58pm January 9, 2020, 4:31pm January 9, 2020, 4:39pm January 9, 2020, 6:08pm January 14, 2020, 10:27am",
    "body": "Happy new year everyone! hoping someone can shed some light on this, i have a weird issue i cant set right with parsing a json file. This is the source json file { \"SHA256\": \"766be5c99ba674f985ce844add4bc5ec423e90811fbceer5ec84efa3cf1624f4\", \"source\": \"localhost\", \"Msg\": \"404 OK\", \"YaraRule\": [ \"no_match\" ], \"URL\": \"http://127.0.0.1\", \"@timestamp\": \"2020-01-07T08:59:04\", \"raw_msg\": \"404.19  Denied by filtering rule\", \"filename\": \"log.txt\", \"syntax\": \"text\", \"log_url\": \"http://127.0.0.1/log.txt\", \"MD5\": \"2c5cddf13ab55a1d4eca955dfa32d245\", \"expire\": \"0\", \"user\": \"user\", \"key\": \"22op3dfe\", \"size\": 107 } when i run this log stash conf against it, the data is ingested but the individual lines are ingested as separate docs and not as a single doc in ES. input { file { path => \"/opt/data/*\" start_position =>\"beginning\" codec => \"json_lines\" sincedb_path => \"/opt/logreader.sincedb\" } } filter { json { source => \"message\" } } output { elasticsearch { hosts => [\"192.168.136.144:9200\"] index => \"log-test-%{+YYYY.MM.dd}\" } } So i whipped this up and ran it and nothing is being ingested at all! Yet the logstash logs show no errors, input { file { path => \"/opt/data/*\" start_position =>\"beginning\" codec => \"json_lines\" sincedb_path => \"/opt/logreader.sincedb\" } } filter{ json { source => \"message\" target => \"doc\" add_field => [ \"Encryption\", \"%{[string]}\" ] add_field => [ \"source\", \"%{[string]}\" ] add_field => [ \"msg\", \"%{[WORD]}\" ] add_field => [ \"YaraRule\", \"%{[WORD]}\" ] add_field => [ \"status\", \"%{[WORD][WORD]}\" ] add_field => [ \"url\", \"%{[URL]}\" ] add_field => [ \"timestamp\", \"%{[TIMESTAMP]}\"] add_field => [ \"rawmsg\", \"%{[raw_msg]}\" ] add_field => [ \"filename\", \"%{[filename]}\" ] add_field => [ \"syntax\", \"%{[word]}\" ] add_field => [ \"log_url\", \"%{[URL]}\" ] add_field => [ \"MD5\", \"%{[MD5]}\" ] add_field => [ \"expire\", \"%{[num]}\" ] add_field => [ \"user\", \"%{[USER]}\" ] add_field => [ \"key\", \"%{[key]}\" ] add_field => [ \"size\", \"%{[num]}\" ] } } output { elasticsearch { hosts => [\"192.168.136.144:9200\"] index => \"log-test-%{+YYYY.MM.dd}\" } } So i guess my question is, is this the right way? or is their an easier way to get the json file ingested as a single document rather than lots of docs per json line ? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "3ce04014-5d89-4fee-a6f3-5574dc7cf767",
    "url": "https://discuss.elastic.co/t/logstash-csv-with-a-json-column-add-a-new-nested-field-from-the-existing-field-if-it-is-integer/214967",
    "title": "Logstash - csv with a json column - Add a new nested field from the existing field if it is integer",
    "category": [
      "Logstash"
    ],
    "author": "Lakshmi_Narasimhan_V",
    "date": "January 14, 2020, 9:34am",
    "body": "Below are the details: - CSV File, visitid,labreport, 11111,\"[{\"\"test\"\":\"\"BUN\"\",\"\"value\"\":\"\"0.5\"\"},{\"\"test\"\":\"\"Creatinie\"\",\"\"value\"\":\"\"0.8\"\"},{\"\"test\"\":\"\"Potassium\"\",\"\"value\"\":\"\"3.5\"\"}]\", 22222,\"[{\"\"test\"\":\"\"BUN\"\",\"\"value\"\":\"\"1\"\"},{\"\"test\"\":\"\"Creatinie\"\",\"\"value\"\":\"\"0.8\"\"},{\"\"test\"\":\"\"Potassium\"\",\"\"value\"\":\"\"3.5\"\"}]\", 33333,\"[{\"\"test\"\":\"\"BUN\"\",\"\"value\"\":\"\"1\"\"},{\"\"test\"\":\"\"Creatinie\"\",\"\"value\"\":\"\"0.8\"\"},{\"\"test\"\":\"\"Potassium\"\",\"\"value\"\":\"\"3.5\"\"}]\", 44444,\"[{\"\"test\"\":\"\"BUN\"\",\"\"value\"\":\"\"1\"\"},{\"\"test\"\":\"\"Creatinie\"\",\"\"value\"\":\"\"0.8\"\"},{\"\"test\"\":\"\"Potassium\"\",\"\"value\"\":\"\"3.5\"\"}]\", 55555,\"[{\"\"test\"\":\"\"BUN\"\",\"\"value\"\":\"\"1\"\"},{\"\"test\"\":\"\"Creatinie\"\",\"\"value\"\":\"\"0.8\"\"},{\"\"test\"\":\"\"Potassium\"\",\"\"value\"\":\"\"3.5\"\"}]\", 66666,\"[{\"\"test\"\":\"\"BUN\"\",\"\"value\"\":\"\"1\"\"},{\"\"test\"\":\"\"Creatinie\"\",\"\"value\"\":\"\"0.8\"\"},{\"\"test\"\":\"\"Potassium\"\",\"\"value\"\":\"\"3.5\"\"}]\", Index Mapping PUT /test-index { \"mappings\" : { \"properties\" : { \"visitid\" : { \"type\" : \"integer\"}, \"labreport\" : { \"type\" : \"nested\", \"properties\" : { \"test\" : { \"type\" : \"text\"}, \"value\" : { \"type\" : \"text\" }, \"numValue\" : { \"type\" : \"text\" } } } } } } Logstash conf input { beats { host => \"0.0.0.0\" port => \"5044\" codec => \"json\" } } filter { csv { skip_header => \"true\" separator => \",\" columns => [\"visitid\",\"labreport\"] } json { source => \"labreport\" target => \"labreport\" } } output { stdout { codec => json } elasticsearch { hosts => \"http://localhost:9200\" index => \"test-index\" action => \"update\" doc_as_upsert => true document_id => \"%{visitid}\" } } What i wanted is i wanted to push the value of value column in the json to numValue in my nested mappings if it is a numeric field. i.e add a new field in the nested json array and copy the value to the new variable numValue only if the value is numeric ? Support please",
    "website_area": "discuss"
  },
  {
    "id": "e25f9b8e-f03f-40f8-8bd7-11698da9df78",
    "url": "https://discuss.elastic.co/t/logstash-nested-data-update-is-not-working-as-expected/214965",
    "title": "Logstash nested data update is not working as expected",
    "category": [
      "Logstash"
    ],
    "author": "Ram_29",
    "date": "January 14, 2020, 9:27am",
    "body": "Hi I'm having a nested field as below \"appdata\": { \"type\":\"nested\", \"include_in_parent\":true, \"properties\": { \"accessType\": { \"type\": \"text\", \"norms\": false, \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"appname\": { \"type\": \"text\", \"norms\": false, \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"eventtime\": { \"type\": \"text\", \"norms\": false, \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } I'm updating the same using logstash in output plugin as below elasticsearch { hosts => [\"localhost:9200\"] document_id => \"%{sid}\" index => \"dashboard_write\" timeout => 30 script => \"if (ctx._source.appdata == null) { ctx._source.appdata = params.event.get('appdata') } else { ctx._source.appdata = ctx._source.appdata + params.event.get('appdata') }\" doc_as_upsert => true action => \"update\" } First time appdata will be null and it should assign that value. For second event, it should append the data to existing appdata But I saw ctx._source.appdata is empty even though data is there Am I doing anything wrong here",
    "website_area": "discuss"
  },
  {
    "id": "8b04d1c5-1112-4803-bba5-2a61a120c0d9",
    "url": "https://discuss.elastic.co/t/i-cant-set-values-in-new-fields/214930",
    "title": "I cant set values in new fields",
    "category": [
      "Logstash"
    ],
    "author": "Jonny3",
    "date": "January 14, 2020, 6:17am January 14, 2020, 8:32am",
    "body": "I use grok to differentiate the content in each line of the log and mutate to create a new field and be able to assign those values but it doesn't work, write the configuration content literally. I don't have compiling problems. I try the first one and the second and nothing change. grok { match => { \"message\" => [\"%{DATE:date} (?:(?:%{TIME:time})|(?:%{TIMEX:timex})) %{WORD} (?:(?:%{PROG:prog})|(?:%{PROGRAM:program})) (?:\\(%{USERNAME:value}\\)) (?:%{WORD:level}\\:) %{GREEDYDATA:text}\"] } pattern_definitions => { \"TIMEX\" => \"(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECONDX})(?![0-9])\" \"SECONDX\" => \"(?:(?:[0-5]?[0-9]|60)(?:[:.,][x]+)?)\" \"PROGRAM\" => \"(?:%{PROG}\\s\\-\\s%{PROG})\" } } mutate { add_field => { \"Host\" => \"%{value}\" } add_field => { \"Level\" => \"%{level}\" } add_field => { \"Text\" => \"%{text}\" } } Is something wrong? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "1b0b293f-9e0c-4440-87dd-152c59b65c25",
    "url": "https://discuss.elastic.co/t/streaming-events-from-elastic/214923",
    "title": "Streaming events from Elastic",
    "category": [
      "Logstash"
    ],
    "author": "Olatunde_Tokun",
    "date": "January 14, 2020, 2:46am",
    "body": "I currently use logstash to stream logs from Elasticsearch to an syslog connector on a schedule (say 5mins). Every 5 mins logstash runs and grabs events in the index then fires them off to the syslog output. Input (Elasticsearch) Output (Syslog TCP) Doing this with this schedule means there might be events missing in between runs (and this has happened a few times). I'm wondering if there is any built in streaming plugin or any other suggestions folks in this community have tried. Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "a284ecba-412a-444e-9ad2-99606a746f15",
    "url": "https://discuss.elastic.co/t/parsing-dns-logs-from-gcp-cloud/214911",
    "title": "Parsing DNS Logs from GCP Cloud",
    "category": [
      "Logstash"
    ],
    "author": "daniel_a",
    "date": "January 13, 2020, 10:10pm January 13, 2020, 11:32pm",
    "body": "Is there a parser for the DNS logs in Logstash? I've noticed some of my logs are not being parsed, the ones with multiple lines. This shows up as a single field. How can I parse it? jsonPayload.rdata google.com. 299 IN a 172.217.214.113 google.com. 299 IN a 172.217.214.101 google.com. 299 IN a 172.217.214.138 google.com. 299 IN a 172.217.214.100 google.com. 299 IN a 172.217.214.102 google.com. 299 IN a 172.217.214.139",
    "website_area": "discuss"
  },
  {
    "id": "6d18252f-1cff-422f-944c-23b7001c4093",
    "url": "https://discuss.elastic.co/t/extracting-parts-of-different-timestamp-fields/214915",
    "title": "Extracting parts of different timestamp fields",
    "category": [
      "Logstash"
    ],
    "author": "Peter_Li",
    "date": "January 13, 2020, 11:12pm January 13, 2020, 11:30pm",
    "body": "I have many timestamp fields: AdmitDateTime DCDateTime etc. I want to pull out the yyyy, MM, dd parts individually from them, eg copy => { \"AdmitDateTime\" => \"@timestamp\" } add_field => { AdmitYear => \"%{+yyyy}\" AdmitMonth => \"%{+MM}\" } copy => { DCDateTime => \"@timestamp\" } add_field => { DCYear => \"%{+yyyy}\" DCMonth => \"%{+MM}\" } etc. Unfortunately, the %{+<format>} like \"%{+yyyy}\" will only work on @timestamp field, not on any other fields. Is there a shorthand, like \"%{<field>+<date format>}\", e.g. \"%{AdmitDateTime+yyyy}\" ? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "d15903d7-eb2d-4324-a6c2-74d52cc806f9",
    "url": "https://discuss.elastic.co/t/drop-filter-is-not-working-for-certain-string/214901",
    "title": "Drop filter is not working for certain string",
    "category": [
      "Logstash"
    ],
    "author": "hadars",
    "date": "January 13, 2020, 9:03pm January 13, 2020, 9:50pm January 13, 2020, 10:20pm January 13, 2020, 11:29pm",
    "body": "Hi, I am trying to apply filter only if message has certain string. For example my message has \"skipping L1 entity store\" and my config looks like, filter { if [msg] == \"skipping L1 entity store\" { drop { } } } i tried - if \"skipping L1 entity store\" in [msg] as well Maybe it's the spaces? i'm sure that the data is in the field called [msg], i have a lot of filters and this is the only one that i can't make it to work, sapces is the only thing i have on this string that i do not have on the others..do i need to escape them in any way? Condition is not working at all. Please help me to use condition to check above mentioned string in Thanks",
    "website_area": "discuss"
  },
  {
    "id": "19da0772-4686-4951-84a4-89829d663f43",
    "url": "https://discuss.elastic.co/t/open-source-parsers/214914",
    "title": "Open source parsers",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "January 13, 2020, 11:02pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "fe8b3235-a649-445e-b874-ed3542a2e198",
    "url": "https://discuss.elastic.co/t/multiple-grok-patterns-or-can-i-use-regex/214910",
    "title": "Multiple Grok patterns or can I use Regex?",
    "category": [
      "Logstash"
    ],
    "author": "IshanP1",
    "date": "January 13, 2020, 10:07pm January 13, 2020, 10:37pm",
    "body": "I am making a centralised syslogging server. Do i have to make a different grok filter for every single different format of syslog or can I use something like regex to pick out certain elements of a log? Dec 16 15:01:13 172.20.x.xx NPF_OLT_LAB05: service \"403 for ONT: \"10002\" - ONT needs restart at 2019/12/16 15:01:13.39 ONT message: \"Backup files exist\" I have the grok pattern for the above log, but my question is - would I have to make a separate pattern layout for each different log layout, or can I use something like Regex to pick out key words ie: ONT \"10002\" and thus save time making separate patterns for everything. Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "cc5073d9-5f80-496f-90ad-89865e766d84",
    "url": "https://discuss.elastic.co/t/timestamp-explanation/214749",
    "title": "Timestamp explanation",
    "category": [
      "Logstash"
    ],
    "author": "amhulli",
    "date": "January 13, 2020, 1:53am January 13, 2020, 12:56pm January 13, 2020, 10:08pm",
    "body": "Hi all, I'm using ELK 7.4.2 I have an issue where Kibana throws up an error when I click on any of the bars on the graph in the discover tab (it complains about: Request to Elasticsearch failed: {\"error\":{\"root_cause\":[{\"type\":\"parse_exception\",\"reason\":\"failed to parse date field [-813227399999] with format [epoch_millis]: [failed to parse date field [-813227399999] with format [epoch_millis]]\"}, (many of these messages are displayed in the popup - presumably once for each message from the search) I can fix it by changing the \"advanced settings => timezone for date formatting\" from \"browser\" to \"GMT\" but I don't want all my dates to be in GMT. I want the display to be in my local timezone as per the timestamp on all of the logs that filebeat-logstash-elasticsearch is ingesting. What am I doing wrong here? regards, Andrew.",
    "website_area": "discuss"
  },
  {
    "id": "c23171b7-5ea6-4bdd-84b1-4c23911655e1",
    "url": "https://discuss.elastic.co/t/how-to-send-data-from-http-input-to-elasticsearch-using-logstash-and-jdbc-streaming-filter/214898",
    "title": "How to send data from HTTP input to ElasticSearch using Logstash and jdbc_streaming filter?",
    "category": [
      "Logstash"
    ],
    "author": "aliabbasifard",
    "date": "January 13, 2020, 9:36pm January 13, 2020, 9:52pm",
    "body": "I want to send data from Http to elasticsearch using logstash and I want to enrich my data using jdbc_streaming filter plugin. This is my logstash config: input { http { id => \"sensor_data_http_input\" user => \"sensor_data\" password => \"sensor_data\" } } filter { jdbc_streaming { jdbc_driver_library => \"E:\\ElasticStack\\Logstash\\logstash-7.4.1\\logstash-core\\lib\\jars\\connector_6.jar\" jdbc_driver_class => \"com.mysql.cj.jdbc.Driver\" jdbc_connection_string => \"jdbc:mysql://localhost:3306/sensor_metadata\" jdbc_user => \"elastic\" jdbc_password => \"hide\" statement => \"select st.sensor_type as sensorType, l.customer as customer, l.department as department, l.building_name as buildingName, l.room as room, l.floor as floor, l.location_on_floor as locationOnFloor, l.latitude, l.longitude from sensors s inner join sensor_type st on s.sensor_type_id=st.sensor_type_id inner join location l on s.location_id=l.location_id where s.sensor_id= :sensor_identifier\" parameters => { \"sensor_identifier\" => \"sensor_id\"} target => lookupResult } mutate { rename => {\"[lookupResult][0][sensorType]\" => \"sensorType\"} rename => {\"[lookupResult][0][customer]\" => \"customer\"} rename => {\"[lookupResult][0][department]\" => \"department\"} rename => {\"[lookupResult][0][buildingName]\" => \"buildingName\"} rename => {\"[lookupResult][0][room]\" => \"room\"} rename => {\"[lookupResult][0][floor]\" => \"floor\"} rename => {\"[lookupResult][0][locationOnFloor]\" => \"locationOnFloor\"} add_field => { \"location\" => \"%{lookupResult[0]latitude},%{lookupResult[0]longitude}\" } remove_field => [\"lookupResult\", \"headers\", \"host\"] } } output { elasticsearch { hosts => [\"localhost:9200\"] index => \"sensor_data-%{+YYYY.MM.dd}\" user => \"elastic\" password => \"hide\" } } But when I start logstash, I see following error: [2020-01-09T22:57:16,260] [ERROR][logstash.javapipeline] [main] Pipeline aborted due to error { :pipeline_id=>\"main\", :exception=>#<TypeError: failed to coerce jdk.internal.loader.ClassLoaders$AppClassLoader to java.net.URLClassLoader>, :backtrace=>[ \"org/jruby/java/addons/KernelJavaAddons.java:29:in `to_java'\", \"E:/ElasticStack/Logstash/logstash-7.4.1/vendor/bundle/jruby/2.5.0/gems/logstash-filter-jdbc_streaming-1.0.7/lib/logstash/plugin_mixins/jdbc_streaming.rb:48:in `prepare_jdbc_connection'\", \"E:/ElasticStack/Logstash/logstash-7.4.1/vendor/bundle/jruby/2.5.0/gems/logstash-filter-jdbc_streaming-1.0.7/lib/logstash/filters/jdbc_streaming.rb:200:in `prepare_connected_jdbc_cache'\", \"E:/ElasticStack/Logstash/logstash-7.4.1/vendor/bundle/jruby/2.5.0/gems/logstash-filter-jdbc_streaming-1.0.7/lib/logstash/filters/jdbc_streaming.rb:116:in `register'\", \"org/logstash/config/ir/compiler/AbstractFilterDelegatorExt.java:56:in `register'\", \"E:/ElasticStack/Logstash/logstash-7.4.1/logstash-core/lib/logstash/java_pipeline.rb:195:in `block in register_plugins'\", \"org/jruby/RubyArray.java:1800:in `each'\", \"E:/ElasticStack/Logstash/logstash-7.4.1/logstash-core/lib/logstash/java_pipeline.rb:194:in `register_plugins'\", \"E:/ElasticStack/Logstash/logstash-7.4.1/logstash-core/lib/logstash/java_pipeline.rb:468:in `maybe_setup_out_plugins'\", \"E:/ElasticStack/Logstash/logstash-7.4.1/logstash-core/lib/logstash/java_pipeline.rb:207:in `start_workers'\", \"E:/ElasticStack/Logstash/logstash-7.4.1/logstash-core/lib/logstash/java_pipeline.rb:149:in `run'\", \"E:/ElasticStack/Logstash/logstash-7.4.1/logstash-core/lib/logstash/java_pipeline.rb:108:in `block in start'\"], :thread=>\"#<Thread:0x17fa8113 run>\" } [2020-01-09T22:57:16,598] [ERROR][logstash.agent] Failed to execute action { :id=>:main, :action_type=>LogStash::ConvergeResult::FailedAction, :message=>\"Could not execute action: PipelineAction::Create<main>, action_result: false\", :backtrace=>nil } I am enriching my http input with some data in my mysql database but it doesn't start logstash at all. every thing is latest version",
    "website_area": "discuss"
  },
  {
    "id": "d4712247-241e-4ad5-a2ff-0025dcde0b3e",
    "url": "https://discuss.elastic.co/t/logstash-jdbc-ssl-error/209631",
    "title": "Logstash JDBC SSL Error",
    "category": [
      "Logstash"
    ],
    "author": "newtologstash",
    "date": "November 27, 2019, 8:27am November 29, 2019, 2:52am December 18, 2019, 9:20pm December 18, 2019, 9:56pm December 18, 2019, 10:50pm December 18, 2019, 11:42pm December 19, 2019, 1:02am December 19, 2019, 3:01am December 19, 2019, 3:08am January 13, 2020, 8:54pm",
    "body": "Hi, I'm trying to use jdbc-input plugin to pull the data from oracle. It is working fine when using TCP connection string, but getting below error when trying to connect using TCP\"S\" connection (SSL). [ERROR][logstash.inputs.jdbc ][main] Unable to connect to database. Tried 1 times {:error_message=>\"Java::JavaSql::SQLRecoverableException: IO Error: Connection reset\"} I'm unable to find option to connect to oracle database using TCPS(SSL) connection. Please let me know if there is any such option available in other plugin.",
    "website_area": "discuss"
  },
  {
    "id": "151b98b6-cc1b-41a4-91b7-ba960e75dced",
    "url": "https://discuss.elastic.co/t/parsing-eventdata-field-correctly-from-json-input/214891",
    "title": "Parsing EventData field correctly from json input",
    "category": [
      "Logstash"
    ],
    "author": "bensonmp",
    "date": "January 13, 2020, 7:00pm January 13, 2020, 7:29pm",
    "body": "Hi, I'm new to using ELK. I'm looking for some help to properly parse the EventData field from my windows logs. Here's an example event: <Event xmlns=\"http://schemas.microsoft.com/win/2004/08/events/event\"> <System> <Provider Name=\"XXXXXX\" /> <EventID Qualifiers=\"0\">20</EventID> <Level>4</Level> <Task>0</Task> <Keywords>0x80000000000000</Keywords> <TimeCreated SystemTime=\"2020-01-09T16:31:32.928992200Z\" /> <EventRecordID>36567</EventRecordID> <Channel>XXX-Log</Channel> <Computer>XXX-XXX-XXX</Computer> <Security /> </System> <EventData> <Data><SUI> <OU>XXX</OU> <UID>XXXXX</UID> <COMP>XXX-XXX-XXX</COMP> </SUI> <PDMessage> <Message>Some message</Message> <DateTime>00:00:05.5298444</DateTime> </PDMessage></Data> </EventData> </Event> I'm using nxlog to ship the events from my system to Logstash. The nxlog conf is set to convert the event logs to json format and ship it out. Is there a way on Logstash to properly extract the fields within EventData? EventData values differ for different events - it may have more/fewer tags within it. Here's the Logstash config I'm using: https://github.com/Security-Onion-Solutions/securityonion-elastic/blob/master/configfiles/6300_windows.conf I'd really appreciate some help with this. Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "047c7528-d326-450e-bdbd-2a60dd45b923",
    "url": "https://discuss.elastic.co/t/append-to-a-property-if-exists/214889",
    "title": "Append to a property if exists",
    "category": [
      "Logstash"
    ],
    "author": "Sbezgoan",
    "date": "January 13, 2020, 6:47pm January 13, 2020, 6:37pm",
    "body": "Hello, Input csv looks like : \"\",\"Code\",\"Date\",\"Time\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\" \"1\",\"3IINFOTECH\",20150703,\"09:16:00\",5.55,4.55,4.55,4.55,835 \"2\",\"3IINFOTECH\",20150703,\"09:17:00\",5.55,4.55,4.55,4.55,390 Logstash config is : input { file { path => \"/Users/sbezgoan/Documents/Elastic/StockData/nse-company-stocks/three.csv\" start_position => \"beginning\" sincedb_path => \"/Users/sbezgoan/Documents/Tools/logstash-7.5.1/temp.log\" } } filter { csv { separator => \",\" columns => [\"SNo\",\"Code\", \"Date\", \"Time\", \"Open\",\"High\", \"Low\", \"Close\",\"Volume\"] skip_header => true } mutate { add_field => {\"DateTime\" => \"%{Date} %{Time}\"} } mutate { remove_field => [\"%{Time}\", \"%{Date}\"] } date{ match => [\"DateTime\" , \"YYYYMMdd HH:mm:ss\"] } } output { elasticsearch { hosts => \"http://localhost:9200\" index => \"nse-stocks2\" action => \"update\" doc_as_upsert => \"true\" document_id => \"%{[SNo]}\" manage_template => \"true\" script => 'if (ctx._source.Open == null) { ctx._source.Open = new ArrayList();} if(ctx._source.Open != null) {ctx._source.Open.add(\"%{[Open]}\");}' } stdout {} } The aim is if a document with the same ID already exists, then append to the Open field new incoming values creating an array of Open values. The script doesn't seem to be doing the intended, appreciate any help on how I can achieve Open field mapping as a multi-valued array. Also, I don't like the fact that I am allowing logstash to manage templates (elastic mappings). I think the issue is the way I am retrieving the existing _source, but not sure how to fix.",
    "website_area": "discuss"
  },
  {
    "id": "adf4c30b-9d77-47b2-93f7-e81a493f1503",
    "url": "https://discuss.elastic.co/t/filebeat-logstash-pipeline-exception-parsing-csv/212896",
    "title": "Filebeat / Logstash pipeline exception parsing csv",
    "category": [
      "Logstash"
    ],
    "author": "galexander87",
    "date": "December 23, 2019, 10:25pm January 13, 2020, 3:14pm",
    "body": "I am trying to send some reports in CSV format to logstash and then Elasticsearch. I am getting this pipeline error: [WARN ] [io.netty.channel.DefaultChannelPipeline] An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means he last handler in the pipeline did not handle the exception. java.net.SocketEception: Connection reset ... sample.csv Title,Author,Subject,Pages,Publication Date Test,John,Shipping,42,16 Dec 2018 filebeat.yml filebeat.inputs: - type: log paths: - /path/to/input.csv fields: report_type: book output.logstash: hosts: [\"10.0.0.1:5044\"] processors: - decode_csv_fields: fields: message: decoded.csv logstast-pipeline.conf input { beats { port => \"5044\" host => \"10.0.0.1\" } } filter { csv { autodetect_column_names => true separator => \",\" } } output { elasticsearch { hosts => [\"http://10.0.0.1:9200\"] index => bookreports-%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYY.MM.dd}\" } }",
    "website_area": "discuss"
  },
  {
    "id": "257d873f-1dbe-47e1-b7c0-2572bbeeb56c",
    "url": "https://discuss.elastic.co/t/extracting-the-last-word-of-an-event-line-grok/214774",
    "title": "Extracting the last word of an event/line (Grok)",
    "category": [
      "Logstash"
    ],
    "author": "nino",
    "date": "January 13, 2020, 8:00am January 13, 2020, 1:38pm January 13, 2020, 2:56pm January 13, 2020, 3:00pm January 13, 2020, 3:04pm",
    "body": "Hello all, i am looking for a Grok pattern to retrieve the last word a matching line with literal. input (file content) THISLINE 1,011.53EUR 12,815.50EUR 2,940.17 100,011.53EUR filter grok{ match => { message => [ \"(?<field>(?<=THISLINE )[^\\r]*)\" ] } } output { \"@version\" => \"1\", \"path\" => \"\", \"message\" => \" 1,011.53EUR 12,815.50EUR 2,940.17 100,011.53EUR\", \"@timestamp\" => 2020-01-06T23:00:00.000Z } I need to extract the last word/number of the event and i've couldn't find a succesful pattern. Best regards",
    "website_area": "discuss"
  },
  {
    "id": "fce7fea4-89fa-42c2-a051-92ea386e2e38",
    "url": "https://discuss.elastic.co/t/import-general-configuration-options/214846",
    "title": "Import general configuration options",
    "category": [
      "Logstash"
    ],
    "author": "YvorL",
    "date": "January 13, 2020, 1:50pm",
    "body": "Hi, Is there a way to import some options (mostly for the input and the output part) from a file to keep the different configuration files more transparent and occasionally the changes easier? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "3ae515b6-c4c3-42c2-9074-ef173014d348",
    "url": "https://discuss.elastic.co/t/how-can-i-setting-that/214758",
    "title": "How can i setting that",
    "category": [
      "Logstash"
    ],
    "author": "pageno11",
    "date": "January 13, 2020, 5:46am January 13, 2020, 1:11pm",
    "body": "How can i setting that Server 1 Input milti files (differnt path) Output logstash tcp Server 2 Input logstash tcp Output also multi files (diffrent path) Only use logstash but in/out is diffrent server. Also, Can i use the input filename as the oupt filename? Pleash help me and thank you.",
    "website_area": "discuss"
  },
  {
    "id": "9721236a-2b74-44c8-ba8e-4b93b5ba892d",
    "url": "https://discuss.elastic.co/t/mutiple-filebeat-servers-to-single-logstash-server/214725",
    "title": "Mutiple filebeat servers to Single logstash server",
    "category": [
      "Logstash"
    ],
    "author": "pathfinder225",
    "date": "January 12, 2020, 11:55am January 12, 2020, 3:05pm January 12, 2020, 4:19pm January 13, 2020, 12:52pm",
    "body": "Hi, New to ELK and I am doing a production setup . I have 3 servers and installed filebeat on each of them. now i am sending these 3 filebeat inputs to single logstash using below config. In all 3 servers Filebeat output is configured as : type: log enabled: true paths: - /wls_domains/Microservices/logs/app/*.log logstash output to x.x.x.x:5044 x.x.x.x is ip of logstash server logstash config: input { beats { port => 5044 } } output { elasticsearch { hosts => [\"http://host:port\"] index => \"service-%{+YYYY.MM.dd}\" #user => \"elastic\" #password => \"changeme\" } stdout { codec => rubydebug } } The problem now is my logstash can listen to beat on one server at any time,. logs sent by filebeat from all 3 servers are not read simultaneously by logstash. can you please help here! Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "c7625bf7-eeea-4c96-9c9c-05b023cf5907",
    "url": "https://discuss.elastic.co/t/successfullystarted-logstash-but-it-cant-read-my-file-even-configuration-is-done-without-error-msg/214731",
    "title": "Successfullystarted logstash but it cant read my file even configuration is done without error msg",
    "category": [
      "Logstash"
    ],
    "author": "saravana_hariharan",
    "date": "January 12, 2020, 3:35pm January 13, 2020, 12:50pm January 13, 2020, 5:14am January 13, 2020, 12:51pm",
    "body": "My config file input { file { path =>\"C:\\Users\\pgt0005\\Desktop\\data\\100-contacts.csv\" start_position => \"beginning\" } } filter { csv{ separator=>\",\" columns=>[\"first_name\",\"last_name\",\"company_name\",\"address\",\"city\",\"county\",\"state\",\"zip\",\"phone1\",\"phone\",\"email\"] } } output { elasticsearch { hosts => [\"localhost:9200\"] index => \"contactdetailchart\" } stdout{codec => rubydebug} } and My cmd prompt shown msg in logstash configuration Thread.exclusive is deprecated, use Thread::Mutex Sending Logstash logs to C:/elk/logstash-7.3.2/logs which is now configured via log4j2.properties [2020-01-12T21:02:01,899][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-01-12T21:02:01,992][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.3.2\"} [2020-01-12T21:02:05,953][INFO ][org.reflections.Reflections] Reflections took 48 ms to scan 1 urls, producing 19 keys and 39 values [2020-01-12T21:02:10,024][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>, :added=>[http://localhost:9200/]}} [2020-01-12T21:02:10,884][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>\"http://localhost:9200/\"} [2020-01-12T21:02:11,088][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>7} [2020-01-12T21:02:11,109][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the type event field won't be used to determine the document _type {:es_version=>7} [2020-01-12T21:02:11,222][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"//localhost:9200\"]} [2020-01-12T21:02:11,521][INFO ][logstash.outputs.elasticsearch] Using default mapping template [2020-01-12T21:02:11,748][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge] A gauge metric of an unknown type (org.jruby.specialized.RubyArrayOneObject) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-01-12T21:02:11,769][INFO ][logstash.javapipeline ] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>4, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>500, :thread=>\"#<Thread:0x281403fc run>\"} [2020-01-12T21:02:11,921][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} [2020-01-12T21:02:13,224][INFO ][logstash.inputs.file ] No sincedb_path set, generating one based on the \"path\" setting {:sincedb_path=>\"C:/elk/logstash-7.3.2/data/plugins/inputs/file/.sincedb_1bf826181e8ad8335fde285764e82cd9\", :path=>[\"C:\\Users\\pgt0005\\Desktop\\data\\100-contacts.csv\"]} [2020-01-12T21:02:13,330][INFO ][logstash.javapipeline ] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-01-12T21:02:13,567][INFO ][filewatch.observingtail ] START, creating Discoverer, Watch with file and sincedb collections [2020-01-12T21:02:13,592][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>} [2020-01-12T21:02:14,795][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} Help me Saravanan.R",
    "website_area": "discuss"
  },
  {
    "id": "79059001-188f-40dd-ad38-d91cff05ecff",
    "url": "https://discuss.elastic.co/t/how-to-fetch-an-attribute-and-create-a-field-with-value/214821",
    "title": "How to fetch an attribute and create a field with value",
    "category": [
      "Logstash"
    ],
    "author": "Rajesh_M",
    "date": "January 13, 2020, 11:50am",
    "body": "Hi, I am parcing a log file and generating the report. The report contains the user and few other fields related to application. I need to find an attribute of the user fetched from log and create a new field with the value. Ex, in the ldap, need to fetch organization and email address of the user identified in logs. Could you please guide me achieve this!! -Thanks",
    "website_area": "discuss"
  },
  {
    "id": "bd7ad841-3385-4deb-a257-04c5172f4d4d",
    "url": "https://discuss.elastic.co/t/logstash-jdbc-connection-string-multiple-entries-in-environment-variable/214801",
    "title": "Logstash JDBC Connection string multiple entries in Environment variable",
    "category": [
      "Logstash"
    ],
    "author": "JP_Smith",
    "date": "January 13, 2020, 11:20am",
    "body": "I want to add multiple entries in my environment variable and then pass them into my logstash jdbc connection string. When I do that it reads all the values in my variable at once and then fails with my connection attempt - is there a way around this? The intention is to have this logstash input section run across multiple servers and the environment variable be dynamically read in. I have about 300+ servers which I need to run this against thus it is vital to have the servername in my connection string populated with a list. Example below: This is how I set my variable, this works fine if I only have one server in the list: export ENV_NAME=\"servername1,servername2\" Config file (not sure why my Dollar signs are being removed): \"jdbc:sqlserver://{ENV_NAME};user=secretuser;password={ES_PWD}\"",
    "website_area": "discuss"
  },
  {
    "id": "1d1e3297-f84c-44c0-baad-8adcd5a80e17",
    "url": "https://discuss.elastic.co/t/how-to-set-multiple-hosts-in-elasticsearch-output-via-environment-variable/214802",
    "title": "How to set multiple hosts in elasticsearch-output via environment variable?",
    "category": [
      "Logstash"
    ],
    "author": "asp",
    "date": "January 13, 2020, 10:48am",
    "body": "Hi, I am trying to configure elasticsearch hosts via environment variable to be used in elasticsearch-output plugin. I want to set them as array for loadbalancing in the multi node cluster without the need for an external loadbalancer. Here is my try: output { elasticsearch { hosts => [${ES_HOSTS}] ssl => \"${USE_ES_SSL}\" cacert => \"${ES_CA_CERT_PATH}\" ssl_certificate_verification => \"${USE_ES_OUTPUT_SSL_CERT_VERIFICATION}\" # credentials are fetched from envrionment or logstash-keystore user => \"${LOGSTASH_USER}\" password => \"${LOGSTASH_PASSWORD}\" index => \"%{[@metadata][indexName]}\" } } I've tried to set ES_HOSTS the following ways: export ES_HOSTS='\"https://server1:9200\"' echo $ES_HOSTS \"https://server1:9200\" export ES_HOSTS='\\\"https://server1:9200\\\"' echo $ES_HOSTS \\\"https://server1:9200\\\" export ES_HOSTS='\\\"\"https://server1:9200\"\\\"' echo $ES_HOSTS \\\"\"https://server1:9200\"\\\" export ES_HOSTS='\"\"https://server1:9200\"\"' echo $ES_HOSTS \"\"https://server1:9200\"\" But I always get this error when I test the configuration with logtash's -t option: [FATAL] 2020-01-13 11:39:25.118 [LogStash::Runner] runner - The given configuration is invalid. Reason: Expected one of #, \", ', -, [, {, ] at line 16, column 15 (byte 239) after output { elasticsearch { hosts => [ [ERROR] 2020-01-13 11:39:25.126 [LogStash::Runner] Logstash - java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit If that one is working for one server, then I want to be able to set something like this: \"https://server1:9200\",\"https://server2:9200\",\"https://server3:9200\" And if there is an external loadbalancer available, then I can just set it to the loadbalancer's \"LB_IP:LB:PORT\". So I would be flexible and more independent from the runtime environment without needing to change the pipeline between different environments. Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "db5ef7e9-61e9-42ac-90a7-6d743e50fce6",
    "url": "https://discuss.elastic.co/t/how-to-install-mutate-filter-in-logstash/214616",
    "title": "How to install mutate filter in logstash?",
    "category": [
      "Logstash"
    ],
    "author": "akuninja",
    "date": "January 10, 2020, 3:10pm January 10, 2020, 5:23pm January 13, 2020, 10:46am",
    "body": "Hi, I have that plugin from gitHub and I'm trying to follow the instructions in README, since I do not use Ruby, mine are (I believe) limited to: Install plugin # Logstash 2.3 and higher bin/logstash-plugin install --no-verify But does is simply mean I should cd to the plugin folder and execute install --no-verify ??? I tried this approach in cmd and powershell, yet with not success. Could anyone please provide some more detailed tutorial on how to do this? Since I could not find any online.",
    "website_area": "discuss"
  },
  {
    "id": "b27c24c2-4445-464b-a8da-726ad3847f04",
    "url": "https://discuss.elastic.co/t/filter-only-on-the-current-date-on-my-sql/214795",
    "title": "Filter only on the current date on my sql",
    "category": [
      "Logstash"
    ],
    "author": "Haalanam",
    "date": "January 13, 2020, 10:29am",
    "body": "Hi, I want to make a stop criteria on my logstash loads. On the sql statement i want Something like select * from mytable where insert_date := current_date Will that work ? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "6b31e263-d98e-4625-a48c-c429a47b33af",
    "url": "https://discuss.elastic.co/t/logstash-split-filter-throwing-split-type-failure/213984",
    "title": "Logstash split filter throwing _split_type_failure",
    "category": [
      "Logstash"
    ],
    "author": "karthiknpy",
    "date": "January 7, 2020, 5:46am January 7, 2020, 2:49pm January 13, 2020, 7:36am",
    "body": "Hi Team, I have a situation to break json using split filter which works fine. But certain scenarios where some subkeys of json are optional. when the split filter is not able to find a specific key to split it is throwing a _split_type_failure in tags.Is there a way to ignore the optional json keys if not found ? My split filter is as follows. filter { split { field => \"[result]\" } # Mandatory split { field => \"[matches]\" } # Optional } Regards Karthik.K",
    "website_area": "discuss"
  },
  {
    "id": "370654c3-fd1c-4137-bbf7-f6aac80a4e9c",
    "url": "https://discuss.elastic.co/t/logstash-block-in-block-in-converge-state/214763",
    "title": "Logstash Block in block in converge_state",
    "category": [
      "Logstash"
    ],
    "author": "katara",
    "date": "January 13, 2020, 6:23am",
    "body": "Hi Below is my Logstash (7.2.0) conf file: input { http_poller { urls => { snowsr => { url => \"https://now.com/api/now/table/u_service_request?sysparm_display_value=true&sysparm_exclude_reference_link=true\" #url => \"https://now.com/api/now/table/u_service_request?sysparm_query=sys_updated_onONLast%2045%20minutes@javascript:gs.beginningOfLast45Minutes()@javascript:gs.endOfLast45Minutes()&sysparm_display_value=true&sysparm_exclude_reference_link=true\" user => \"user\" password => \"C23eeer\" headers => {Accept => \"application/json\"} } } request_timeout => 60 metadata_target => \"http_poller_metadata\" schedule => { cron => \" * * * * * UTC\"} #codec => \"json\" } } filter { json { source => \"result\" } split { field => [\"result\"] } date { match => [\"[result][sys_created_on]\",\"yyyy-MM-dd HH:mm:ss\"] target => \"sys_created_on\" } date { match => [\"[result][sys_updated_on]\",\"yyyy-MM-dd HH:mm:ss\"] target => \"sys_updated_on\" } date { match => [\"[result][opened_at]\",\"yyyy-MM-dd HH:mm:ss\"] target => \"opened_at\" } date { match => [\"[result][closed_at]\",\"yyyy-MM-dd HH:mm:ss\"] target => \"closed_at\" } date { match => [\"[result][u_resolved_at]\",\"yyyy-MM-dd HH:mm:ss\"] target => \"resolved_at\" } } output { elasticsearch { hosts => [\"10.19.85.209:9200\"] index => \"srequestsnow\" action=>update document_id => \"%{[result][number]}\" doc_as_upsert =>true } stdout { codec => rubydebug } } Below is my Error log: > [2020-01-13T00:56:46,722][ERROR][logstash.agent ] Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:snowsr, :exception=>\"LogStash::ConfigurationError\", :message=>\"Expected one of #, {, } at line 7, column 5 (byte 468) after input {\\n http_poller {\\n urls => {\\n snowsr => {\\n url => \\\"https://lsk12dev.service-now.com/api/now/table/u_service_request?sysparm_display_value=true&sysparm_exclude_reference_link=true\\\"\\n #url => \\\"https://lsk12dev.service-now.com/api/now/table/u_service_request?sysparm_query=sys_updated_onONLast%2045%20minutes@javascript:gs.beginningOfLast45Minutes()@javascript:gs.endOfLast45Minutes()&sysparm_display_value=true&sysparm_exclude_reference_link=true\\\"\\n \", :backtrace=>[\"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:41:in compile_imperative'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:49:in compile_graph'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:11:in block in compile_sources'\", \"org/jruby/RubyArray.java:2577:in map'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:10:in compile_sources'\", \"org/logstash/execution/AbstractPipelineExt.java:151:in initialize'\", \"org/logstash/execution/JavaBasePipelineExt.java:47:in initialize'\", \"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:24:in initialize'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:36:in execute'\", \"/usr/share/logstash/logstash-core/lib/logstash/agent.rb:325:in block in converge_state'\"]} Help me with where im missing. Thanks. Regards, Katara",
    "website_area": "discuss"
  },
  {
    "id": "c21f1ae9-ca04-45dc-97a1-75f811f636f9",
    "url": "https://discuss.elastic.co/t/stop-proccesing-events-lines-after-first-grok-match/214696",
    "title": "Stop proccesing events/lines after first grok match",
    "category": [
      "Logstash"
    ],
    "author": "nino",
    "date": "January 11, 2020, 10:54am January 11, 2020, 10:51am January 11, 2020, 4:23pm January 11, 2020, 6:22pm January 11, 2020, 6:42pm January 12, 2020, 3:17pm January 12, 2020, 9:02pm January 12, 2020, 11:41pm January 12, 2020, 11:14pm January 12, 2020, 11:41pm January 12, 2020, 11:41pm",
    "body": "Hello all, i am not be able to avoid logstash continues proccesing events after grok patter matched. I just want to index the first occurence of the entire file. Here is what i have: File content first line content... second line content... third line should match EXTRACT_THIS fourth line content... fifth line not should match EXTRACT_THIS Filter filter { grok { match => { message => \"(?<extraction1>(?<=EXTRACT_THIS).*)\" } } } Output (stdout) - I need retrieve just the first line macth, i don't want second match to output or be processed { \"@version\" => \"1\", \"path\" => \"\", \"extraction1\" => \"\\r\", \"host\" => \"\", \"message\" => \"third line should match EXTRACT_THIS\\r\", \"@timestamp\" => 2020-01-11T08:44:18.561Z } { \"@version\" => \"1\", \"path\" => \"\", \"extraction1\" => \"\\r\", \"host\" => \"\", \"message\" => \"fifth line not should match EXTRACT_THIS\\r\", \"@timestamp\" => 2020-01-11T08:44:18.561Z } Best regards",
    "website_area": "discuss"
  },
  {
    "id": "b167e47c-197c-4978-a7a9-1ae19061c0dd",
    "url": "https://discuss.elastic.co/t/parsing-json-kinesis-stream-to-s3-bucket-how-to-add-new-lines-b-n-records/214742",
    "title": "Parsing json Kinesis stream to S3 bucket - How to add new lines b/n records",
    "category": [
      "Logstash"
    ],
    "author": "stefank",
    "date": "January 12, 2020, 10:00pm",
    "body": "Hi All, I am trying to parse Kinesis stream and store it into S3 buckets. I managed to do that, but I also need to have a newline between each stream record. Is this something that can be done within logstash config? I tried in a couple of ways but didn't work. Can anyone point me in the right direction, I assume this should be an easy thing? thanks!",
    "website_area": "discuss"
  },
  {
    "id": "795c063a-521a-454b-a3da-991f3e58e1b7",
    "url": "https://discuss.elastic.co/t/how-to-use-jdbc-to-import-data-into-nested-objects/52871",
    "title": "How to use jdbc to import data into nested objects?",
    "category": [
      "Logstash"
    ],
    "author": "DaanCreemers",
    "date": "June 15, 2016, 1:53pm January 11, 2017, 12:57pm January 11, 2017, 10:10pm June 23, 2017, 2:11am July 15, 2017, 7:26am June 23, 2017, 1:41pm January 12, 2020, 9:09pm",
    "body": "Hi everybody, I have multiple tables in my database which I would like to import into Elasticsearch using a single document type. One of the tables has a one-to-many relation to another table. I will use the example as provided in the guide to explain the problem, because my problem is similar (https://www.elastic.co/guide/en/elasticsearch/guide/current/nested-objects.html). So we have blogposts (in one table) and each blogpost can have multiple comments (in another table, linked with a one-to-many relation). Representing comments as Arrays of Inner Objects does not work for us, because we would loose the correlation between the different attributes of a comment as stated in the Guide. A solution is the use of Nested Objects, by mapping the comments field as type nested. This would be a valid output to Elasticsearch: { { \"comments.id\": [ 11 ], \"comments.name\": [ john, smith ], \"comments.comment\": [ article, great ], \"comments.age\": [ 28 ], \"comments.stars\": [ 4 ], \"comments.date\": [ 2014-09-01 ] } { \"comments.id\": [ 12 ], \"comments.name\": [ alice, white ], \"comments.comment\": [ like, more, please, this ], \"comments.age\": [ 31 ], \"comments.stars\": [ 5 ], \"comments.date\": [ 2014-10-22 ] } { \"id\": [ 1 ], \"title\": [ eggs, nest ], \"body\": [ making, money, work, your ], \"tags\": [ cash, shares ] } } I used the following nested mapping: (mostly obtained from the guide) PUT /my_index { \"mappings\": { \"blogpost\": { \"properties\": { \"comments\": { \"type\": \"nested\", \"properties\": { \"id\": { \"type\": \"number\" }, \"name\": { \"type\": \"string\" }, \"comment\": { \"type\": \"string\" }, \"age\": { \"type\": \"short\" }, \"stars\": { \"type\": \"short\" }, \"date\": { \"type\": \"date\" } } } } } } } The statement below shows the relevant configuration to import data from jdbc using logstash: input { jdbc { ... statement => \"SELECT * FROM blogpost LEFT JOIN comments ON blogpost.id = comments.id ORDER BY blogpost.id\" ... } } However, when looking what has been imported into ES I see that for each comment another event is generated instead of a single event per blogpost with comments as nested objects. I have not found how to implement this functionality, but here (https://github.com/jprante/elasticsearch-jdbc#structured-objects) this functionality is used. How should I proceed with importing to get a single event per blogpost with its comments as nested objects? Thank you. Daan",
    "website_area": "discuss"
  },
  {
    "id": "71a6a4ce-cf95-4c1d-afab-fffcd5735581",
    "url": "https://discuss.elastic.co/t/how-to-add-a-tag-based-on-input-data/214713",
    "title": "How to add a tag based on input data",
    "category": [
      "Logstash"
    ],
    "author": "debojyoti_bandyopadh",
    "date": "January 11, 2020, 9:48pm January 12, 2020, 1:01am January 12, 2020, 10:52am January 12, 2020, 3:04pm January 12, 2020, 5:15pm",
    "body": "Hello, I am a beginner in logstash and currently learning the basics. I am stuck while solving a problem. Basically my input(to logstash) can be a series of Alphabets like below: a b a c d I want to add a tag \"A\" if the data read is \"a\" also i need to Ensure that the input data is tagged with type as \"test\" , and write the output to the file output.txt in the path usr/share/logstash. I understand that i can tag the input data in stdin and then use the file plugin to output the data to output.txt file but i just cant figure out how i can add the tag \"A\" if the data read is \"a\". Please advise.",
    "website_area": "discuss"
  },
  {
    "id": "8e017237-57e5-47c6-99ba-55d06021b8ee",
    "url": "https://discuss.elastic.co/t/dd-geo-point-from-twitter-geo-field/214716",
    "title": "Dd Geo Point From Twitter geo Field",
    "category": [
      "Logstash"
    ],
    "author": "shakesperoo",
    "date": "January 13, 2020, 1:03pm January 12, 2020, 3:03pm January 12, 2020, 3:09pm",
    "body": "how to add Geo Point From Twitter geo ? i am using twiiter API like this input { twitter { consumer_key => \"\" consumer_secret => \"\" oauth_token => \"\" oauth_token_secret => \"\" keywords => [\"USA\"] full_tweet => true type => \"tweet\" } } and i want to index twitter geo field as geo point ny using mutue i tried this but it didnt work if [geo.coordinates] { mutate { add_field => { \"[location][lat]\" => \"%{XCent}\" \"[location][lon]\" => \"%{YCent}\" } } mutate { convert => { \"[location][lat]\" => \"float\" \"[location][lon]\" => \"float\" } } }",
    "website_area": "discuss"
  },
  {
    "id": "d6b8ca12-822f-43c7-80af-50effe01091a",
    "url": "https://discuss.elastic.co/t/logstash-twitter-dyanmic-keywords-and-search-filter/214697",
    "title": "Logstash Twitter Dyanmic Keywords and Search filter",
    "category": [
      "Logstash"
    ],
    "author": "shakesperoo",
    "date": "January 12, 2020, 1:48am January 12, 2020, 3:00pm January 12, 2020, 3:00pm",
    "body": "Can i use Can i use logstash http plugin to to feed log twitter keywords search ? input { http { host => \"http://localhost/WebApplication3/home/Keywords\" port => 80 } } filter { mutate { add_field => { \"token\" => \"logzioAccountToken\" } } } I need to feed the keywords dyanmic input { twitter { consumer_key => \"\" consumer_secret => \"\" oauth_token => \"\" oauth_token_secret => \"\" keywords => [\"USA\"] full_tweet => true type => \"tweet\" } }",
    "website_area": "discuss"
  },
  {
    "id": "36a8af2c-b910-4f5f-b4b3-3a7676132144",
    "url": "https://discuss.elastic.co/t/all-files-collected-under-on-index-only-is-it-possible-to-have-multiple-indexes-for-multiple-files/212806",
    "title": "All files collected under on index only. Is it possible to have multiple indexes for multiple files?",
    "category": [
      "Logstash"
    ],
    "author": "Mehak_Bhargava",
    "date": "December 23, 2019, 5:14am December 23, 2019, 6:20am December 23, 2019, 7:06am December 23, 2019, 7:21am December 23, 2019, 7:23am December 23, 2019, 7:34am December 23, 2019, 7:40am December 23, 2019, 7:42am December 23, 2019, 7:51am December 23, 2019, 8:52am December 23, 2019, 9:18am December 23, 2019, 9:29am December 23, 2019, 9:33am December 23, 2019, 10:23am December 23, 2019, 10:38am December 23, 2019, 10:52am December 23, 2019, 11:00am December 23, 2019, 2:47pm December 23, 2019, 4:18pm December 23, 2019, 4:24pm",
    "body": "EVery time I search GET cat/index? in kibana, I only get one index under all files are collected. I want to get three indexes for three different log files given in filebeat. This is logstash.conf file input { beats { port => 5044 } } filter { if[log_type] ==\"access\"{ grok { match => {\"message\" => \"%{COMBINEDAPACHELOG}\"} } else if [log_type] == \"errors\" { grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } } }else [log_type] == \"dispatcher\" { grok { match => { \"message\" => \"\\A%{TIMESTAMP_ISO8601:timestamp}%{SPACE}\\[%{DATA:threadId}]%{SPACE}%{LOGLEVEL:logLevel}%{SPACE}%{JAVACLASS:javaClass}%{SPACE}-%{SPACE}?(\\[%{NONNEGINT:incidentId}])%{GREEDYDATA:message}\" } } } } output { elasticsearch { hosts => [\"localhost:9200\"] sniffing => true manage_template => false index => \"%{type}-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug } }",
    "website_area": "discuss"
  },
  {
    "id": "70303058-68c2-4a0f-95f4-6a9f92318b22",
    "url": "https://discuss.elastic.co/t/elasticsearch-unreachable-error-with-logstash-configuration/213043",
    "title": "Elasticsearch unreachable error with logstash configuration",
    "category": [
      "Logstash"
    ],
    "author": "Aniket_Salvi",
    "date": "December 26, 2019, 5:31am January 11, 2020, 7:03am January 11, 2020, 7:47am January 11, 2020, 8:42am",
    "body": "I am getting error when I am trying to access elasticsearch from logstash configuration. elasticsearch and logstash version - 7.4.0 logstash configuration :- input{ } output { elasticsearch { hosts => [\"icici-elk-production.nip.io\"] index => \"winston-logs\" } } error :- error=>\"Elasticsearch Unreachable: [http://icici-elk-production.nip.io:9200/][Manticore::ConnectTimeout] connect timed out\"} You can see in error automatically port is appended to elasticsearch url. My elasticsearch is hosted on server and my logstash conf file is on my local system. Can anyone tell me how this port 9200 is automatically append to my elasticearch url? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "bd1a3102-85c9-4dae-bdb1-b6a11ab84c3d",
    "url": "https://discuss.elastic.co/t/send-daynamic-keywords-to-twitter-api-in-logstash/214693",
    "title": "Send Daynamic keywords to twitter API in logstash",
    "category": [
      "Logstash"
    ],
    "author": "shakesperoo",
    "date": "January 11, 2020, 6:22am January 11, 2020, 7:50am January 11, 2020, 8:10am January 11, 2020, 8:11am",
    "body": "I am using Logstash to stream twittter tweets like below input { twitter { consumer_key => \"\" consumer_secret => \"\" oauth_token => \"\" oauth_token_secret => \"\" keywords => [\"USA\",\"Trump\"] full_tweet => true type => \"tweet\" } } I want to send the keywords dyanmic from elastic index , or API url ! how could i possible do that ? Is there anyway to read the keywords from API or elastic index instead of static keywords like keywords => \"https://Api.com\" keywords => \"https://localhoat:9200/keywords/text\"",
    "website_area": "discuss"
  },
  {
    "id": "6343c6c7-39c7-49df-9ad2-4f828c4e397e",
    "url": "https://discuss.elastic.co/t/error-pushing-logs-to-elasticsearch-using-kafka-as-input-for-logstash/214658",
    "title": "Error pushing logs to elasticsearch using kafka as input for logstash",
    "category": [
      "Logstash"
    ],
    "author": "Monu_Kumar",
    "date": "January 11, 2020, 6:14am",
    "body": "Below is my logstash config file logstash.conf. Lotstash reads logs from Kafka topic 'my_topic' and outputs it to elasticsearch index 'es-index'. input { kafka { bootstrap_servers =>\"kafka1.xxx:9092,kafka2.xxx:9092,kafka3.xxx:9092\" topics => [\"my_topic\"] codec => \"json\" group_id => \"logstashgroup\" } } output { elasticsearch { hosts => [\"es1.myhost:9200\",\"es2.myhost:9200\",\"es3.myhost:9200\"] user => \"user123\" password => \"password\" index => \"es-index\" } } filter { json { source => \"message\" skip_on_invalid_json => true } } It worked fine for a few months but recently it started throwing the following errors: [ERROR][logstash.outputs.elasticsearch] An unknown error occurred sending a bulk request to Elasticsearch. We will retry indefinitely {:error_message=>\"Could not read from stream: Corrupt GZIP trailer\", :error_class=>\"Manticore::StreamClosedException\" and, [FATAL][logstash.runner ] An unexpected error occurred! {:error=>org.apache.kafka.common.KafkaException: Received exception when fetching the next record from rumbl_log-1. If needed, please seek past the record to continue consumption., :backtrace=>[\"org.apache.kafka.clients.consumer.internals.Fetcher$PartitionRecords.fetchRecords(org/apache/kafka/clients/consumer/internals/Fetcher.java:1469)\", \"org.apache.kafka.clients.consumer.internals.Fetcher$PartitionRecords.access$1600(org/apache/kafka/clients/consumer/internals/Fetcher.java:1328)\", then, [ERROR][logstash.javapipeline ] A plugin had an unrecoverable error. Will restart this plugin. Plugin: <LogStash::Inputs::Kafka codec=><LogStash::Codecs::JSON id=>\"json_9a7a9d96-d7be-4292-a3de-67f797d22ab5\" then, Error: Received exception when fetching the next record from my_topic-1. If needed, please seek past the record to continue consumption. Exception: Java::OrgApacheKafkaCommon::KafkaException and then logstash stops [ERROR][org.logstash.Logstash ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit I'm not sure what's the problem. I couldn't find anything relevant to solve the error. .",
    "website_area": "discuss"
  },
  {
    "id": "b944d1e3-327e-4823-8f4c-1309273df761",
    "url": "https://discuss.elastic.co/t/remove-extra-lines-from-output-message/214538",
    "title": "Remove extra lines from output message",
    "category": [
      "Logstash"
    ],
    "author": "Saravana37",
    "date": "January 11, 2020, 4:43am January 11, 2020, 4:45am",
    "body": "Hi , I am using multiline pattern to filter my output message from Exec plugin , Now I am able to get the lines separately according to my required pattern ^D122, But i am getting some extra characters in the message like below. Output : '*************************************************************** <D1220001022A SvrTblCleanup Shutdown Manual 0 1 Server Tables Cleanup 223 rows returned. srvrmgr> list server show SBLSRVR_NAME,SBLSRVR_STATE SBLSRVR_NAME SBLSRVR_STATE ------------ ------------- /> My Logstash config file looks like this : '# Sample Logstash configuration for creating a simple Beats -> Logstash -> Elasticsearch pipeline. input { beats { port => 5044 tags => [\"srvr_logs\"] } exec { command => \"E:\\ELK\\logstash\\scripts\\DEV_Srvrmgr.bat\" interval => 120 #type => \"string\" tags => [\"srvrmgr\"] codec => multiline { pattern => \"^D122\" negate => true what => \"previous\" } } } filter { mutate { remove_field => [ \"host\" ] gsub => [\"message\", \"\\n\", \"\"] } if \"srvr_logs\" in [tags] { grok { match => {\"message\" => \"%{WORD:EventType}%{SPACE}%{WORD:EventSubType}%{SPACE}%{INT:Severity}%{SPACE}%{WORD:SARMID}%{NOTSPACE}%{SPACE}%{PROG:EventDate}%{SPACE}%{TIME:EventTime}%{SPACE}%{GREEDYDATA:LogMessage}\"} } } else { grok { match => { \"message\" => [ #Most specific grok: \"%{WORD:ServerName}%{SPACE}%{WORD:Comp_Alias}%{SPACE}%{WORD:CompStatus}%{SPACE}%{WORD:CompStartMode}%{SPACE}%{WORD:RunningTasks}%{SPACE}%{WORD:MaxTasks}%{SPACE}%{GREEDYDATA:CompName}\", #Less specific: \"%{WORD:SBLSRVR_NAME}%{SPACE}%{WORD:SBLSRVR_STATE}\" ] } } } } output { if \"srvr_logs\" in [tags] { elasticsearch { hosts => [\"http://localhost:9200\"] index => \"srvrlog-%{+YYYY.MM.dd}\" } } else { elasticsearch { hosts => [\"http://localhost:9200\"] index => \"srvrmgr-%{+YYYY.MM.dd}\" } } } ' Here GREEDY DATA Gives me extra lines as well which is not at all required , Checked forums,stackoverflow and all . Not able to get proper solution .Please let me know how can we omit extra lines ? In other words , how can say logstash to process message till new line \\n ?",
    "website_area": "discuss"
  },
  {
    "id": "d3a8cd31-8a97-45ca-a060-3a7b6e0327c6",
    "url": "https://discuss.elastic.co/t/http-poller-trend-micro-deepsecurity/214686",
    "title": "Http_Poller Trend Micro DeepSecurity",
    "category": [
      "Logstash"
    ],
    "author": "Zorkmid",
    "date": "January 10, 2020, 11:36pm",
    "body": "Hello Everyone, I'd like to use the http_poller plugin to pull events from Trend Micro Deep Security using the REST api they have. I have 2 concerns on this - Has anyone done this with LS and if you have what issues if did you run into? The 2nd is how to pull the most recent events up to the previous polling. Does the poller keep a record of the last event pull so that it know where to stop each time? Thanks TimW",
    "website_area": "discuss"
  },
  {
    "id": "12ab1f91-c9b5-4887-85fb-6fbe91ac638c",
    "url": "https://discuss.elastic.co/t/grok-filter-for-log-files-in-logstash/213377",
    "title": "Grok filter for log files in logstash",
    "category": [
      "Logstash"
    ],
    "author": "Mehak_Bhargava",
    "date": "December 30, 2019, 7:03pm December 30, 2019, 8:35pm December 30, 2019, 8:55pm December 30, 2019, 9:39pm December 30, 2019, 10:31pm January 7, 2020, 1:26am January 9, 2020, 10:11pm January 9, 2020, 10:24pm January 9, 2020, 10:26pm January 9, 2020, 10:30pm January 9, 2020, 10:31pm January 9, 2020, 11:30pm January 9, 2020, 11:35pm January 10, 2020, 9:44pm",
    "body": "input { beats { port => 5044 } } filter { if[log_type] ==\"access\"{ grok { match => {\"message\" => \"%{COMBINEDAPACHELOG}\"} } else if [log_type] == \"errors\" { grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } } }else [log_type] == \"dispatcher\" { grok { match => { \"message\" => \"\\A%{TIMESTAMP_ISO8601:timestamp}%{SPACE}\\[%{DATA:threadId}]%{SPACE}%{LOGLEVEL:logLevel}%{SPACE}%{JAVACLASS:javaClass}%{SPACE}-%{SPACE}?(\\[%{NONNEGINT:incidentId}])%{GREEDYDATA:message}\" } } } } output { elasticsearch { hosts => [\"localhost:9200\"] sniffing => true manage_template => false ilm_enabled => false index => \"%{log_type}-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug } } Below are the three log types I want to filter and extract information out of. Above is the file which has the match patterns. Log Type 1: 08/10/2019 12:14:48 599 (null) DEBUG 27 GetUpdatedIncident for Incident Id 24749162 on thread Log type 2: 08/10/2019 12:38:09 742 (null) DEBUG 10 Add activty in cache (152782646) Log type 3: 2019-10-08 12:31:37,767 [pool-5-thread-47] INFO c.e.d.s.ScheduledActionProcessor - [24749750]EDI=NHA CustomFAULTSDEF: RR=NULL DispatchType=FLM RRDelay=0.0 RRThreshold=NULL DispatchWait=3 FaultSource=EMS HoldWhileServicing=false I would like the Log type 3 to come out as below in kibana after the filter provided in filter block in logstash.conf but it isnt. Why? { \"threadId\": \"pool-5-thread-47\", \"logLevel\": \"INFO\", \"javaClass\": \"c.e.d.s.ScheduledActionProcessor\", \"incidentId\": \"24749750\", \"message\": \"EDI=NHA CustomFAULTSDEF: RR=NULL DispatchType=FLM RRDelay=0.0 RRThreshold=NULL DispatchWait=3 FaultSource=EMS HoldWhileServicing=false\", \"timestamp\": \"2019-10-08 12:31:37,767\" }",
    "website_area": "discuss"
  },
  {
    "id": "237b298e-74fd-49b7-812a-3cdc1b3a9a21",
    "url": "https://discuss.elastic.co/t/date-filter-dateparsefailure/214668",
    "title": "Date filter dateparsefailure",
    "category": [
      "Logstash"
    ],
    "author": "DevOP",
    "date": "January 10, 2020, 9:07pm January 10, 2020, 9:44pm",
    "body": "Hello... I've been experimenting with logstash capabilities lately, and no matter which direction I take with the date filter, I seem to end up with _dateparsefailure. So to save myself the frustration, I was wondering if anyone could identify what is wrong, second set of eyes . I am reading from dated logs. therefore my goal was to set @timestamp to the time within the log as opposed to the log insertion time. Here is a sample date i'm working with: \"2020-01-01 18:12:09.746046\" This date is set to a field named \"Logdate\" using grok before the date filter is called in the config. and here is the snippet of my config: date { locale => \"en\" match => [\"Logdate\", \"yyyy-MM-dd HH:mm:ss.SSS\"] target => \"@timestamp\" } Any input would be appreciated!",
    "website_area": "discuss"
  },
  {
    "id": "2301b226-8dcb-4c05-b6fa-7a049566af10",
    "url": "https://discuss.elastic.co/t/a-typical-case-of-sql-relation-aggregating-results-from-jdbc-input-plugin-nested/214597",
    "title": "A typical case of SQL relation: aggregating results from jdbc input plugin (nested)",
    "category": [
      "Logstash"
    ],
    "author": "Ahmed_Ahmed",
    "date": "January 10, 2020, 12:28pm January 10, 2020, 5:25pm January 10, 2020, 6:08pm January 10, 2020, 7:10pm January 10, 2020, 8:40pm January 10, 2020, 9:22pm",
    "body": "I would like to ask With reference to the following where there is a parent-child relation is pipelined Logstash aggregate-example4 I wonder how to achieve a scenario where event elements have a hierarchal relationship like following parent (n) ---child (n) ------ grand-child (n) ------------great-grand-child (n) For events, I am using JDBC and a SQL query. works fine and giving me events as expected I filter aggregate them with the following code. filter { aggregate { task_id => \"%{region_name}\" code => \" map['region_name'] = event.get('region_name') map['countryz_lst'] ||= [] map['countryz'] ||= [] if (event.get('country_name') != nil) if !( map['countryz_lst'].include? event.get('country_name') ) map['countryz_lst'] << event.get('country_name') map['countryz'] << { 'country_name' => event.get('country_name') } end map['cityz_lst'] ||= [] map['cityz'] ||= [] if (event.get('city') != nil) if !( map['cityz_lst'].include? event.get('city') ) map['cityz_lst'] << event.get('city') map['cityz'] << { 'city' => event.get('city'), 'postal_code' => event.get('postal_code') } end end event.cancel() end event.cancel() \" push_map_as_event_on_timeout => true timeout => 123 } mutate { remove_field => [\"countryz_lst\",\"cityz_lst\",\"@version\",\"@timestamp\"] } } output { # Output to the console. stdout { codec => json_lines } } It produces the following json { \"region_name\": \"Europe\", \"countryz\": [ { \"country_name\": \"United Kingdom\" }, { \"country_name\": \"Netherlands\" }, { \"country_name\": \"Italy\" }, { \"country_name\": \"France\" }, { \"country_name\": \"Denmark\" }, { \"country_name\": \"Germany\" }, { \"country_name\": \"Switzerland\" }, { \"country_name\": \"Belgium\" } ], \"cityz\": [ { \"postal_code\": \"09629850293\", \"city\": \"Stretford\" }, { \"postal_code\": \"OX9 9ZB\", \"city\": \"Oxford\" }, { \"postal_code\": null, \"city\": \"London\" }, { \"postal_code\": \"3029SK\", \"city\": \"Utrecht\" }, { \"postal_code\": \"10934\", \"city\": \"Venice\" }, { \"postal_code\": \"00989\", \"city\": \"Roma\" }, { \"postal_code\": \"80925\", \"city\": \"Munich\" }, { \"postal_code\": \"3095\", \"city\": \"Bern\" }, { \"postal_code\": \"1730\", \"city\": \"Geneva\" } ] } REGION COUNTRY CITY I want related cities nested into related countries. Like Region COUNTRY CITY I hope i could explain. Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "e36deee2-2ca6-4adf-b6b3-ead5a2738956",
    "url": "https://discuss.elastic.co/t/regex-in-filter-plugin-not-working/214641",
    "title": "Regex in filter plugin not working",
    "category": [
      "Logstash"
    ],
    "author": "srbhklkrn",
    "date": "January 10, 2020, 7:19pm January 10, 2020, 7:23pm January 10, 2020, 7:26pm January 10, 2020, 7:50pm January 10, 2020, 7:56pm January 10, 2020, 8:21pm January 10, 2020, 8:57pm January 10, 2020, 9:22pm",
    "body": "Hello, I'm trying to tag one host by using syslog5424_host but it's not matching anything. First, can I tag syslog5424_host? if yes can I use it directly in the output section and forward it to the destination instead of first tagging and then by using tag send it to the destination? filter { if [syslog5424_host] = ~ /^sharedservices.prod.authorization-prod(...)/ { mutate { add_tag => [\"Prod_Auth\"] } } } The string I'm trying to match : sharedservices.prod.authorization-prod-16",
    "website_area": "discuss"
  },
  {
    "id": "84dd7004-b7c6-445e-9283-ae9d80d42963",
    "url": "https://discuss.elastic.co/t/index-lifecyle-policy-doesnt-rollover-the-index-automatically/214511",
    "title": "Index lifecyle policy doesn't rollover the index automatically",
    "category": [
      "Logstash"
    ],
    "author": "EZprogramming",
    "date": "January 10, 2020, 1:21am January 10, 2020, 9:10pm",
    "body": "Objective: Write a Logstash configuration that receives logs and sends it to an index using an ILM policy, and rolls over an index after a certain threshold (max 10 documents). Below is my Logstash configuration for the output plugin. Logstash: output { elasticsearch { hosts => [\"${HOST}\"] user => \"${USERNAME}\" password => \"${PASSWORD}\" index => \"cloudwatch-testing\" template_name => \"cloudwatch\" ilm_rollover_alias => \"cloudwatch-testing-alias\" ilm_pattern => \"000001\" ilm_policy => \"cloudwatch-policy\" id => \"cloudwatch\" } stdout { codec => rubydebug } } At first, I assumed this will do the work for me but then I realized that if cloudwatch-testing-000001 doesn't exist, then it will write the logs to cloudwatch-testing-alias instead. Therefore, I found these Elasticsearch queries that can create a template and then roll over an index such as cloudwatch-testing-000001 to cloudwatch-testing-000002. Problem: This process is manual. I need an automated solution where I can just send my logs, and after every 10 logs, the index is rolled over and a new index is created. What suggestions / feedback do you have in mind? Elasticsearch queries: 1- Create a template using an existing policy PUT _template/cloudwatch { \"index_patterns\": [\"cloudwatch-testing*\"], \"settings\": { \"number_of_shards\": 1, \"number_of_replicas\": 1, \"index.lifecycle.name\": \"cloudwatch-policy\", \"index.lifecycle.rollover_alias\": \"cloudwatch-testing-alias\" } } 2- Create a new index for logstash to write to, if not it will write to cloudwatch-testing-alias instead which is not the index I would like to write to. PUT cloudwatch-testing-000001 { \"aliases\": { \"cloudwatch-testing-alias\":{ \"is_write_index\": true, \"rolled_over\" : true } } } 3- The below query command rolls the index over manually, but I would like to do this step automatically. POST /cloudwatch-testing-alias/_rollover/cloudwatch-testing-000002 { \"conditions\": { \"max_docs\": 10 }, \"settings\": { \"index.number_of_shards\": 1 } } Policy Attributes: GET /_ilm/policy/cloudwatch-policy Returns: { \"cloudwatch-policy\" : { \"version\" : 3, \"modified_date\" : \"2020-01-09T23:29:44.942Z\", \"policy\" : { \"phases\" : { \"warm\" : { \"min_age\" : \"30d\", \"actions\" : { \"set_priority\" : { \"priority\" : 50 } } }, \"cold\" : { \"min_age\" : \"30d\", \"actions\" : { \"freeze\" : { }, \"set_priority\" : { \"priority\" : 0 } } }, \"hot\" : { \"min_age\" : \"0ms\", \"actions\" : { \"rollover\" : { \"max_size\" : \"50gb\", \"max_age\" : \"30d\", \"max_docs\" : 10 }, \"set_priority\" : { \"priority\" : 100 } } }, \"delete\" : { \"min_age\" : \"180d\", \"actions\" : { \"delete\" : { } } } } } } }",
    "website_area": "discuss"
  },
  {
    "id": "e6ac4181-31b3-4f23-87b0-cc43e185d8a7",
    "url": "https://discuss.elastic.co/t/process-twice-a-specific-field-created-by-grok/214623",
    "title": "Process twice a specific field created by grok",
    "category": [
      "Logstash"
    ],
    "author": "rschirin",
    "date": "January 10, 2020, 3:29pm January 10, 2020, 5:13pm",
    "body": "Hi guys, I would like to know if there is a way to apply grok filter on a message to obtain several fields and then apply one more time grok filter to extract a subset of information from a field. currently, collecting the ES slowlog data I'm creating the field query that contains the query executed on ES. I would like to process one more time my query field to extract another value. is it possible? regards",
    "website_area": "discuss"
  },
  {
    "id": "30537827-9a25-4b88-b713-7f91bf09abe1",
    "url": "https://discuss.elastic.co/t/routing-one-input-to-diferent-indexes/214423",
    "title": "Routing one input to diferent indexes",
    "category": [
      "Logstash"
    ],
    "author": "nino",
    "date": "January 9, 2020, 6:41pm January 9, 2020, 6:40pm January 11, 2020, 10:00am January 9, 2020, 7:16pm January 9, 2020, 8:41pm January 10, 2020, 5:04pm",
    "body": "Hello all, i don't know if this can be achieve with logstah. I am just starting with logstash. This is the structure of each file that comes in the folder every day: DATE: FEB 10, 2020 ******* section 1 ******** COLUMN_1 COLUMN_2 COLUMN_3 12 mar 20 3421 15 ene 20 1200 40 mar 20 2102 ******* section 2 ******** COLUMN_1 COLUMN_2 COLUMN_3 17 ene 20 3421 18 feb 20 1200 20 mar 20 2102 TOTAL 107,68 .00 7,830 68,123 SUBTOTAL 40,321 I need to route different parts (matches) of the same file in different indexes (csv files) output 1 (routing events/lines to first_index.csv) @timestamp = Extract the date of the file (top date in the content) TOTAL = 107,68 SUBTOTAL = 40,321 output 2 (routing events lines to second_index.csv) @timestamp = Extract the date of the file (top date in the content) COLUMN_1 = 17 COLUMN_2 = ene 20 COLUMN_3 = 3421 @timestamp = Extract the date of the file (top date in the content) COLUMN_1 = 18 COLUMN_2 = feb 20 COLUMN_3 = 1200 How can i accomplish this? Best regards",
    "website_area": "discuss"
  },
  {
    "id": "fb4b8037-2422-4ea5-adff-289baefa25c8",
    "url": "https://discuss.elastic.co/t/added-new-field-to-index-how-to-enable-it-for-all-previous-data/214625",
    "title": "Added new field to index, how to enable it for all previous data?",
    "category": [
      "Logstash"
    ],
    "author": "IndiaSierra",
    "date": "January 10, 2020, 3:33pm",
    "body": "We collect a great amount of logs from our systems. These logs are already stored in a field \"message\" as a normal string message. I now added (via logstash filter) a field which is a very specific log with timing info i.e. \"Process took 23s\". The field will be named ProcessTimeInS and as value an integer with the amount of time it took. This logline existed before so it should be in the \"index.message\" log lines (I can search for it in kibana and I get hits). If I understand correctly, when I reindex (using this method, is this field then enabled / filled for historic data? If not, how do I make use of this new field for old data? Thanks and apologies for the seemingly silly question.",
    "website_area": "discuss"
  },
  {
    "id": "7ce6ce90-83ca-4cbd-bd8b-5cf723f001b1",
    "url": "https://discuss.elastic.co/t/no-default-year-month-day-for-only-hhss-sss-logs/214043",
    "title": "No default Year/month/day for only HH:mm:ss,SSS logs",
    "category": [
      "Logstash"
    ],
    "author": "drubior",
    "date": "January 7, 2020, 11:55am January 7, 2020, 4:12pm January 9, 2020, 1:44pm January 9, 2020, 4:44pm January 10, 2020, 12:05pm",
    "body": "We have some logfiles wich store only the time in format \"HH:mm:ss,SSS\" ( no day, month, year info) 11:38:58,654 INFO [ajp-195.72.2xx.17-8109-722] [lm.PerfilLoginModule] : [TOKEN_OK] authenticated user pepito This has never been a problem until about a month (I haven't been able to find the exact date), we catched the hour, and logstash automatically added the year/mont/date, and added it to the daily index... We use this: if [logname] == \"egovern\" { grok { match => [ #Jboss \"message\", \"(%{TIMESTAMP_ISO8601:timestamp}|%{DATESTAMP:timestamp}|%{TIME:timestamp})%{SPACE}%{LOGLEVEL:level}%{SPACE}([%{DATA:logger}]+) [%{DATA:method}] : %{GREEDYDATA:message}\" ] overwrite => [ \"message\" ] } date { match => [ \"timestamp\" , \"HH:mm:ss,SSS\" ] # target => \"@timestamp\" } And the destination index is: index => \"%{[@metadata][destindex]}-%{+YYYY.MM.dd}\" Since that misterious moment, those logs are being sent to index_name-2020-01-01 instead of for example index_name-2020-01-07 (this was happening too in 2019) I've been reading and it seems that in those cases the date info was provided internally by logstash, or at least it was the same that it was in the @timestamp field, and I think this was the case until some time ago, but actually it's not :_( How could I solve it?",
    "website_area": "discuss"
  },
  {
    "id": "91747e65-8162-4054-b91b-13710d74a506",
    "url": "https://discuss.elastic.co/t/logstash-file-input-stopped-picking-up-file-changes/214586",
    "title": "Logstash file input stopped picking up file changes",
    "category": [
      "Logstash"
    ],
    "author": "shchuka",
    "date": "January 10, 2020, 10:38am",
    "body": "I am using logstash to process a large number of log files (over 1000 files in total). After the initial setup, all log files were processed successfully and logged into elasticsearch. This took just over a day. Then new entries were being picked up for a while - until they just stopped. The service was not restarted, the files were not rolled in any way - seemingly, filewatcher stopped picking up file changes. My config contains the following input: file { path => \"/mnt/logs/**/console-*.log\" type => \"tomcat\" start_position => \"beginning\" ... } I set log level to TRACE for everything and restarted logstash. Among the output, I can see entries from filewatch about opening sincedb and reading its entries - but nothing after that - it's been about 30 minutes since and the log files have thousands of new entries added in this time. The log file (with TRACE log level) has only this for the past 30 minutes (every 5 seconds): [2020-01-10T10:23:38,633][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>\"ParNew\"} [2020-01-10T10:23:38,635][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>\"ConcurrentMarkSweep\"} [2020-01-10T10:23:38,960][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline. How can I make it pick up new changes? I have the latest version of logstash/elasticsearch - version 7.5.1 - running on CentOS 7.7. The log files are in Azure Storage Account file shares, mounted locally using SMB. If I look directly on the ELK server, I do see the new entries in the log files.",
    "website_area": "discuss"
  },
  {
    "id": "1d167324-47c0-4eff-a140-21310b7e5869",
    "url": "https://discuss.elastic.co/t/error-logstash-unable-to-load-plugin-postgresql-jar/214562",
    "title": "Error Logstash: unable to load plugin Postgresql.jar",
    "category": [
      "Logstash"
    ],
    "author": "Kostis95",
    "date": "January 16, 2020, 1:45pm January 10, 2020, 10:07am",
    "body": "Hi there, I'm trying to get started with elasticsearch and to use it for our forum data (which is stored in a postgresql database). I read that it would be possible to sync a postgresql database with elastic by using logstash. So in usr/local/etc/logstash I've got all files in place (logstash.yml, p1.conf, pipelines.yml). I've put the postgresql jar on my Desktop and refer to it in the p1.conf file like so: jdbc_driver_library => \"Users/{myUserName}/Desktop/postgresql-42.2.9.jar\" However when running logstash I get a pluginLoadingError that states that there is no such file to load. I can't see what I'm doing wrong here, so any help would be appreciated!",
    "website_area": "discuss"
  },
  {
    "id": "01187582-64f0-4c74-b28e-5e67dab3a9be",
    "url": "https://discuss.elastic.co/t/geoip-lookup-failure-despite-having-an-ip-address/214574",
    "title": "GeoIP lookup failure despite having an IP address",
    "category": [
      "Logstash"
    ],
    "author": "VeikkoLehmuskorpi",
    "date": "January 10, 2020, 10:06am January 10, 2020, 9:43am January 10, 2020, 10:05am",
    "body": "I can't understand why my geoip filter plugin won't work. I have an ip address as a string but I still get a \"_geoip_lookup_failure\" error message. Log inside Kibana: { \"_index\": \"logstash-2020.01.09-000001\", \"_type\": \"_doc\", \"_id\": \"X\", \"_version\": 1, \"_score\": null, \"_source\": { \"@message\": \"{\"message_type\":\"login\",\"message\":\"failed login\",\"username\":\"user1\",\"ip\":\"XXX.XX.X.X\"}\", \"message\": \"failed login\", \"@timestamp\": \"2020-01-10T09:09:25.608Z\", \"message_type\": \"login\", \"host\": \"X\", \"port\": XXXXX, \"ip\": \"XX.XX.X.X\", \"@version\": \"1\", \"tags\": [ \"_geoip_lookup_failure\" ], \"@fields\": { \"level\": \"INFO\", \"category\": \"default\" }, \"username\": \"user1\" }, \"fields\": { \"@timestamp\": [ \"2020-01-10T09:09:25.608Z\" ] }, \"highlight\": { \"message_type\": [ \"@kibana-highlighted-field@login@/kibana-highlighted-field@\" ], \"message\": [ \"@kibana-highlighted-field@failed@/kibana-highlighted-field@ @kibana-highlighted-field@login@/kibana-highlighted-field@\" ] }, \"sort\": [ 1578647365608 ] } stdout log: { \"@message\" => \"{\"message_type\":\"login\",\"message\":\"failed login\",\"username\":\"user1\",\"ip\":\"XXX.XX.X.X\"}\", \"message\" => \"failed login\", \"@timestamp\" => 2020-01-10T09:09:25.608Z, \"message_type\" => \"login\", \"host\" => \"X\", \"port\" => XXXXX, \"ip\" => \"XXX.XX.X.X\", \"@version\" => \"1\", \"tags\" => [ [0] \"_geoip_lookup_failure\" ], \"@fields\" => { \"level\" => \"INFO\", \"category\" => \"default\" }, \"username\" => \"user1\" } logstash config input { tcp { port => 10514 type => 'syslog' } tcp { port => 5959 codec => json } } filter { json { source => \"@message\" } geoip { source => \"ip\" target => \"geoip\" add_field => [ \"[geoip][coordinates]\", \"%{[geoip][longitude]}\" ] add_field => [ \"[geoip][coordinates]\", \"%{[geoip][latitude]}\" ] } mutate { convert => [ \"[geoip][coordinates]\", \"float\" ] } } output { elasticsearch { hosts => \"elasticsearch:9200\" } stdout { codec => rubydebug } } Versions Logstash: docker.elastic.co/logstash/logstash:7.5.1 Elasticsearch: docker.elastic.co/elasticsearch/elasticsearch:7.5.1 Kibana: docker.elastic.co/kibana/kibana:7.5.1",
    "website_area": "discuss"
  },
  {
    "id": "978592b0-fb6b-40e6-b03e-45318190e9e7",
    "url": "https://discuss.elastic.co/t/parent-child-in-logstash/214577",
    "title": "Parent/child in logstash",
    "category": [
      "Logstash"
    ],
    "author": "Intissar_Ayadi",
    "date": "January 10, 2020, 9:47am",
    "body": "I m receiving data from two Streaming input .I m using logstash to parse data then send them to elasticsearch and evry input has many fields , ipadress in common field with the other input. After searching, i found that in this case it is possible to use parent-child feature when indexing. Is there any support for parent-child indexing in logstash please ? How can i index parents and child using logstash ?",
    "website_area": "discuss"
  },
  {
    "id": "4e828d89-f23b-42b0-8908-fb92875cd579",
    "url": "https://discuss.elastic.co/t/does-logstash-keystore-has-a-force-flag-or-a-quiet-flag/214575",
    "title": "Does logstash-keystore has a force flag? Or a quiet flag?",
    "category": [
      "Logstash"
    ],
    "author": "asp",
    "date": "January 10, 2020, 9:31am",
    "body": "Hi, I want to fill logstash-keystore via ansible and want to pipe the value via stdin. So I need to fill anything from stdin and need to avoid any readings from input during the run like \"do you really want to overwrite?) ,... Currently I am able to add new entries with following block: - name: keystore - add logstash user shell: cmd: \"echo {{ CRED_LOGSTASH_SYSTEM_USER }} | ./logstash-keystore --path.settings {{ logstash.path.config }}/config_sets/{{ CONFIG_SET_NAME }}/{{ item }} add LOGSTASH_USER\" chdir: \"/usr/share/logstash/bin\" But If I want to overwrite an entry keystore is asking if I want to overwrite. -f / --force flag is at least not documented in usage information. Can you please shed some light here? Or is the only workaround to remove and add? I find it really difficult that logstash, kibana and elasticsearch have keystore-feature and all are implemented with different shell interface and different function scope (only logstash has the possibility to secure the keystore with a password)... Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "cec2fcdc-f552-4a4a-8d12-4fe50ef27505",
    "url": "https://discuss.elastic.co/t/how-to-add-a-field-to-kibana-via-logstash/214569",
    "title": "How to add a field to kibana via logstash",
    "category": [
      "Logstash"
    ],
    "author": "akuninja",
    "date": "January 10, 2020, 9:07am",
    "body": "stackoverflow.com How to add a field to kibana via logstash python, elasticsearch, logging, logstash asked by fdf4rt5 on 12:57PM - 09 Jan 20 UTC",
    "website_area": "discuss"
  },
  {
    "id": "572b8c7f-e20a-4b1e-9b58-2ac373e2f8d8",
    "url": "https://discuss.elastic.co/t/how-to-stream-log-file-in-another-instance/214546",
    "title": "How to stream log file in another instance",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "January 10, 2020, 7:11am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c4371ce7-b247-4cd4-862a-28b6e5bd770e",
    "url": "https://discuss.elastic.co/t/log-correlation-with-threat-feeds/214540",
    "title": "Log correlation with threat feeds",
    "category": [
      "Logstash"
    ],
    "author": "yayemail",
    "date": "January 10, 2020, 6:05am",
    "body": "Hi everyone, I am working on ELK currently, using the Apache httpd web server as a sample. I'm trying to match/correlate my apache access logs with signatures ( https://gist.github.com/xsscx/530fa25964f94e74d7c1 ). Can anyone help me figure out the approach to correlate access logs with the threat feed/signatures.",
    "website_area": "discuss"
  },
  {
    "id": "ac626cec-e4f9-47b7-8ed4-c52a7b28e1fc",
    "url": "https://discuss.elastic.co/t/jdbc-static-loader-to-create-local-table-from-index-rater-than-a-database/214536",
    "title": "JDBC static loader to create local table from index rater than a database",
    "category": [
      "Logstash"
    ],
    "author": "ishu.singla",
    "date": "January 10, 2020, 5:50am",
    "body": "I am using JDBC streaming for my local lookups because I need to replace data from another legacy database. This process is very very slow. I cannot create a local table out of it because data is in millions. Is there a way to create local table from an index or directly lookup into an index for replacing data.",
    "website_area": "discuss"
  },
  {
    "id": "7bfa1f83-d8bb-4c45-af0f-d352df4c34b8",
    "url": "https://discuss.elastic.co/t/using-my-own-date-field-instead-of-using-the-default-timestamp-in-index-creation/214479",
    "title": "Using my own date field instead of using the default timestamp in index creation",
    "category": [
      "Logstash"
    ],
    "author": "kumar4",
    "date": "January 9, 2020, 6:59pm January 9, 2020, 8:46pm January 9, 2020, 10:23pm January 9, 2020, 10:27pm January 9, 2020, 11:51pm",
    "body": "hello, i want to use the date in column named time in my csv file instead of timestamp when creating index,i indexed a csv file containing multiple columns, among the columns there is a date field in Y-m-d H:M:S format, when i try to use logstash -f it give me _dateparsefaillure, an example of csv date column is: 2019-12-31 15:22:12, the problem is that i cant use that field instead of timpstamp in order to generate graphical charts and always in kibana the type of the field is sting not date , in logstash i use date { match => [\"time\", \"yyyy-MM-dd HH:mm:ss\" ] target => @timestamp } thank you",
    "website_area": "discuss"
  },
  {
    "id": "33ef4598-ccee-4f52-b24c-f8c1da79b4be",
    "url": "https://discuss.elastic.co/t/logstash-optionally-set-output-options/214504",
    "title": "Logstash optionally set output options",
    "category": [
      "Logstash"
    ],
    "author": "Riddhesh",
    "date": "January 9, 2020, 11:08pm",
    "body": "Suppose I have some documents with join field, so they may have parent(e.g. [join_field][parent]) field in them, so as per docs, I need to pass that to _routing in logstash elasticsearch output: ... output { elasticsearch { ... routing => \"%{[join_field][parent]}\" } } Now if there is no join_field in the doc, above will set its routing to literally %{[join_field][parent]} in ES. Is there anyway I can make it optional so the ES output will have routing set only if [join_field][parent] is there? Or only way here is to have if else condition on the field and have separate output for each(but it feels odd to have multiple ifs for many options)? Also can this have any performance issue? ... output { if [join_field][parent] { elasticsearch { ... routing => \"%{[join_field][parent]}\" } } else { elasticsearch { ... } } }",
    "website_area": "discuss"
  },
  {
    "id": "578b55b7-537c-42a6-abc8-da70b8c3728d",
    "url": "https://discuss.elastic.co/t/updating-the-dictionary-file-used-in-translate-filter-while-logs-are-being-processed/214494",
    "title": "Updating the dictionary file used in translate filter while logs are being processed",
    "category": [
      "Logstash"
    ],
    "author": "dmalhan",
    "date": "January 9, 2020, 9:34pm January 9, 2020, 10:28pm",
    "body": "Currently the Logstash instance that we're running basically runs 24 hours a day (processes millions of logs everyday). It's using a file as a mapping dictionary through a Translate filter with the default refresh interval. The docs here mention a default rate for refreshing the file and that's what we're using in the configuration file. My question is regarding the architecture of the refresh process: If logs are continuously being processed, does it mean the file being used for mapping is under lock and being read continuously or is it read into memory every refresh interval and that's what's used for mapping? My use case is that the file will be updated on a weekly basis but the process will only be able to update the file if it's not being read by another process (i.e. Logstash in this case). Any help is appreciated!",
    "website_area": "discuss"
  },
  {
    "id": "7f91e0fe-2dec-4f1a-b08d-2aec6962286a",
    "url": "https://discuss.elastic.co/t/failed-to-execute-action-action-logstash-create-pipeline-id-test-exception-logstash-configurationerror-message-expected-one-of/214486",
    "title": "Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:test, :exception=>\"LogStash::ConfigurationError\", :message=>\"Expected one of #,",
    "category": [
      "Logstash"
    ],
    "author": "Mehak_Bhargava",
    "date": "January 9, 2020, 8:15pm January 9, 2020, 8:26pm January 9, 2020, 8:41pm January 9, 2020, 9:07pm January 9, 2020, 9:32pm",
    "body": "I am not sure why is the if[fields][log_type] throwing error here Here is the error my pipeline.conf file is giving below [2020-01-09T11:51:31,810][ERROR][logstash.agent ] Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:test, :exception=>\"LogStash::ConfigurationError\", :message=>\"Expected one of #, { at line 12, column 10 (byte 257) after filter {\\n if[fields][log_type] ==\\\"access\\\"{\\n grok {\\n\\tmatch => {\\\"message\\\" => \\\"%{DATESTAMP:timestamp} %{NONNEGINT:code} %{GREEDYDATA} %{LOGLEVEL} %{NONNEGINT:anum} %{GREEDYDATA} %{NONNEGINT:threadId}\\\"}\\n } else \", :backtrace=>[\"/home/mehak/Documents/logstash-7.4.0/logstash-core/lib/logstash/compiler.rb:41:in `compile_imperative'\", \"/home/mehak/Documents/logstash-7.4.0/logstash-core/lib/logstash/compiler.rb:49:in `compile_graph'\", \"/home/mehak/Documents/logstash-7.4.0/logstash-core/lib/logstash/compiler.rb:11:in `block in compile_sources'\", \"org/jruby/RubyArray.java:2584:in `map'\", \"/home/mehak/Documents/logstash-7.4.0/logstash-core/lib/logstash/compiler.rb:10:in `compile_sources'\", \"org/logstash/execution/AbstractPipelineExt.java:153:in `initialize'\", \"org/logstash/execution/JavaBasePipelineExt.java:47:in `initialize'\", \"/home/mehak/Documents/logstash-7.4.0/logstash-core/lib/logstash/java_pipeline.rb:26:in `initialize'\", \"/home/mehak/Documents/logstash-7.4.0/logstash-core/lib/logstash/pipeline_action/create.rb:36:in `execute'\", \"/home/mehak/Documents/logstash-7.4.0/logstash-core/lib/logstash/agent.rb:326:in `block in converge_state'\"]} Pipeline.conf- input { beats { port => 5044 } } filter { if[fields][log_type] ==\"access\"{ grok { match => {\"message\" => \"%{DATESTAMP:timestamp} %{NONNEGINT:code} %{GREEDYDATA} %{LOGLEVEL} %{NONNEGINT:anum} %{GREEDYDATA} %{NONNEGINT:threadId}\"} } else if [fields][log_type] == \"errors\" { grok { match => { \"message\" => \"%{DATESTAMP:timestamp} %{NONNEGINT:code} %{GREEDYDATA} %{LOGLEVEL} %{NONNEGINT:anum} %{GREEDYDATA:message}\" } } }else [fields][log_type] == \"dispatch\" { grok { match => { \"message\" => \"\\A%{TIMESTAMP_ISO8601:timestamp}%{SPACE}\\[%{DATA:threadId}]%{SPACE}%{LOGLEVEL:logLevel}%{SPACE}%{JAVACLASS:javaClass}%{SPACE}-%{SPACE}?(\\[%{NONNEGINT:incidentId}])%{GREEDYDATA:message}\" } } } } output { elasticsearch { hosts => [\"localhost:9200\"] sniffing => true manage_template => false ilm_enabled => false index => \"%{log_type}-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug } }",
    "website_area": "discuss"
  },
  {
    "id": "60a9e582-2ae1-4993-8994-1fc783355570",
    "url": "https://discuss.elastic.co/t/nomethoderror-undefined-method-deep-merge/214367",
    "title": "(NoMethodError) undefined method `deep_merge'?",
    "category": [
      "Logstash"
    ],
    "author": "Kenneth_M_Kolano",
    "date": "January 9, 2020, 8:32am January 9, 2020, 10:35am January 9, 2020, 4:49pm January 9, 2020, 8:02pm January 9, 2020, 8:09pm January 9, 2020, 8:33pm January 9, 2020, 8:37pm",
    "body": "I'm unexpectedly seeing this error when starting my Logstash 7.5.1 service on a new deployment on Ubuntu 19.04, and can find no relevant info online. [ERROR] 2020-01-09 08:18:37.084 [main] Logstash - java.lang.IllegalStateException: Logstash stopped processing because of an error: (NoMethodError) undefined method `deep_merge' for #<Hash:0x6b29771f> None of the standard Logstash startup messaging is seen; no standard log files are created; just that error out to syslog vacillating between a few different hashes every few startups. Same state with all config customization removed.",
    "website_area": "discuss"
  },
  {
    "id": "0087c9d2-9336-428f-8ccb-a89510887c70",
    "url": "https://discuss.elastic.co/t/problem-with-reading-aggregation-map-file/213375",
    "title": "Problem with reading aggregation map file",
    "category": [
      "Logstash"
    ],
    "author": "lens",
    "date": "December 30, 2019, 9:56pm January 9, 2020, 7:48pm",
    "body": "I'm using the aggregate plugin and currently testing to see if using the aggregate_maps_path option works to write the aggregation maps to a file when I stop logstash and read them when I restart it. I'm getting these errors (the first one I've seen and ignored before - don't know if it's related): [2019-12-13T22:04:26,395][INFO ][logstash.filters.aggregate][main] **Aggregate maps loaded** from : /tmp/aggregation-maps.logstash ... [2019-12-13T22:04:26,499][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2019-12-13T22:04:26,502][INFO ][logstash.javapipeline ][main] **Starting pipeline** {:pipeline_id=>\"main\", \"pipeline.workers\"=>1, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>125, :thread=>\"#<Thread:0x6936b532 run>\"} ... [2019-12-13T22:04:26,932][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2019-12-13T22:04:30,025][ERROR][org.logstash.execution.WorkerLoop][main] **Exception in pipelineworker, the pipeline stopped processing new events,** please check your filter configuration and restart Logstash. org.jruby.exceptions.Exception: (GeneratorError) (was java.lang.NullPointerException) (through reference chain: org.logstash.ConvertedMap[ **\"_@timestamp\"]** ) ... I've been trying various things and have seen the ConvertedMap error refer to @timestamp as well as 2 date fields. Anyone ever see anything like this before? Any clues as to what this error means or how to debug it? I've tried everything I can think of. I'm totally stuck at this point.",
    "website_area": "discuss"
  },
  {
    "id": "9d7c42c7-ed86-4ace-b83e-3fdc81445443",
    "url": "https://discuss.elastic.co/t/help-needed-in-grok/213827",
    "title": "Help needed in grok",
    "category": [
      "Logstash"
    ],
    "author": "adeel109",
    "date": "January 6, 2020, 3:17am January 9, 2020, 5:31pm January 7, 2020, 2:29pm January 9, 2020, 5:31pm January 9, 2020, 7:39pm",
    "body": "I've been searching this and am directed to the same page most of the time. I am trying to write grok patterns for my custom logs. For example, below is a log line: 2020/01/02 08:40:16 UUID: 5E82093B:7550_B0092619:01BB_5E0DAC6F_33A27FC:05AD - URL: https://endpoint.point/path/to/api 0.011636824 elapsed(s) Now, most of my searches lead me to this link. But I can't understand it or how to use it. Is there is a tutorial to explain exactly what type of data each defined pattern renders. Like I can't understand what NOTSPACE is for or what GREEDYDATA is for and many more. I can't find out how to grok this 5E82093B:7550_B0092619:01BB_5E0DAC6F_33A27FC:05AD from above log. P.S. I am aware of grok debugger (this). And I wrote below pattern using it. Just to give an idea where I am at. %{IPORHOST:clientip} - %{DATA:somedata} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{URIPATH:request} HTTP/(?<httpversion>[0-9.]*)\" %{NUMBER:response_code} %{NUMBER:bytes_transferred} %{DATA:referrer} \"%{DATA:agent_info}\"rt=%{NUMBER:request_time} uct=\"%{NUMBER:upstream_connect_time}\" uht=\"%{NUMBER:upstream_header_time}\" urt=\"%{NUMBER:upstream_response_time}\" I have specific questions as well. Please answer them as they will be helpful: If space in between two fields is unpredictable, how do I manage that? If a field or two in a log line is not consistent, how do ensure it's index field is created when it does appear in kibana? How do I grok above UUID?",
    "website_area": "discuss"
  },
  {
    "id": "214d08c1-8d65-4198-a57f-bf54b4e7c40d",
    "url": "https://discuss.elastic.co/t/logstash-parsing-problem/214379",
    "title": "Logstash parsing problem",
    "category": [
      "Logstash"
    ],
    "author": "Vladpov",
    "date": "January 9, 2020, 9:12am January 9, 2020, 7:00pm January 9, 2020, 6:39pm January 9, 2020, 7:00pm January 9, 2020, 7:01pm",
    "body": "Hello guys! I'm trying to parse the following type of log message: 111.22.333.444 - - [08/Jan/2020:11:50:15 +0100] [https://awdasfe.asfeaf.cas:111] \"POST /VFQ3P/asfiheasfhe/v2/safiehjafe/check HTTP/1.1\" 204 0 \"-\" \"-\" (rt=0.555 urt=0.555 uct=0.122 uht=0.11) My logstash conf file: input { beats { port => 5044 } } filter { grok { match => { \"message\" => \"%{IPORHOST:clientip} %{HTTPDUSER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \\[%{NOTSPACE:referrer}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)\" } } geoip { source => \"clientip\" } } output { elasticsearch { hosts => [\"localhost:9200\"] index => \"my_index5\" } } I'm using almost the same patterns like in the github pattern library for COMMONAPACHELOG. When I put the code through grok debugger in Kibana it works the way I want but when I try to execute it on machine logstash throws me an error that there is a symbol expected before the \"(?:%{WORD:verb} part and when I add there \\ there is still a problem. Does anyone have any suggestions for solving the problem? Thanks in advance!",
    "website_area": "discuss"
  },
  {
    "id": "997bb789-7dfd-44c9-8248-cb2225ddd7b5",
    "url": "https://discuss.elastic.co/t/tcp-input-plugin-generate-additional-port-field/214459",
    "title": "TCP input plugin generate additional port field",
    "category": [
      "Logstash"
    ],
    "author": "lantern77",
    "date": "January 9, 2020, 3:35pm January 9, 2020, 5:18pm January 9, 2020, 7:00pm",
    "body": "Hello, For some reason, I can't seem to figure out why, but whenever we use the tcp input plugin, an addition \"port\" field gets added to our documents. For example \"port\": 39750. Can anyone confirm if this is default behaviour from the tcp input plugin? We could use filters to remove the field yes, but I would like to confirm if this is behaviour coming from this plugin, as it is no where mentioned in the documentation of: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-tcp.html I could also be overlooking something completely and it could be on our end, but has anyone else encountered this? I also found a possibly related issue here: https://github.com/logstash-plugins/logstash-input-tcp/issues/146 Currently we are running the tcp input plugin with this configuration: # input plugin configuration input { # udp connection for transport udp { id => \"UDP-INPUT\" host => \"0.0.0.0\" port => 19503 codec => \"json\" type => \"udp\" workers => 5 queue_size => 5000 tags => [ \"data-udp\" ] } # tcp connection for transport tcp { id => \"TCP-INPUT\" host => \"0.0.0.0\" port => 19503 codec => \"json_lines\" type => \"tcp\" tags => [ \"data-tcp\" ] } } # output plugin configuration output { # handler for the magnum devices elasticsearch { id => \"ELASTICSEARCH\" index => \"device-%{+YYYY.MM.dd}\" hosts => [ \"host:9200\" ] codec => \"json\" template => \"/parasite/applications/configuration/global/mapping/elasticsearch-template-device.json\" template_name => \"device-template\" template_overwrite => true } } Logstash version: 7.4.2 Regards",
    "website_area": "discuss"
  },
  {
    "id": "1431f5c0-6c51-4abc-b87b-16cf6ef03219",
    "url": "https://discuss.elastic.co/t/using-upsert-across-index/214465",
    "title": "Using Upsert across index",
    "category": [
      "Logstash"
    ],
    "author": "smo",
    "date": "January 9, 2020, 4:30pm January 9, 2020, 4:59pm January 9, 2020, 5:15pm January 9, 2020, 5:19pm January 9, 2020, 6:25pm",
    "body": "I use this config to insert document in ES :``` output { if [type] == \"usage\" { elasticsearch { hosts => [\"elastic4:9204\"] index => \"usage-%{+YYYY-MM}\" document_id => \"%{IDSU}\" action => \"update\" doc_as_upsert => true } } This is fine but the problem i have the duplicates occur within the different month, when the month rolls over, and the document still appears, elastic/logstash thinks it's a new doc, and create new document bu i want to do is to update document wich dos'ent exist in current index but in the old index. Is there a way to make the upsert work cross index? These would all be the same type of doc, they would simply apply change status field for exampl",
    "website_area": "discuss"
  },
  {
    "id": "a9ed1434-5359-4a47-82d0-0d420e12b52d",
    "url": "https://discuss.elastic.co/t/help-with-combining-events-from-a-log/214441",
    "title": "Help with combining events from a log",
    "category": [
      "Logstash"
    ],
    "author": "chapmanjc",
    "date": "January 9, 2020, 2:09pm January 9, 2020, 5:40pm",
    "body": "Hello all, I am a total newbie to the ELK Stack (Please be kind!) and I am hoping to get some assistance. Essentially what I am wanting to do is when I go into Discovery in Kibana, it takes a \"sessionID\" and combines it into one as opposed to separate events. What is happening is when someone goes to our login page. I get 3 log events, one on showing up, one when the login takes place and another when the login is complete. I used a GROK filter to create that will separate out the sessionID from the message field from the logs. What I would like to do is when I go into Kibana and go to Discovery, I would prefer to have those 3 events with the same ID just be pushed into one event. I did follow the example of the aggregate filter (Example 3) and while it does add the fields for hits and several hits true/false. It still doesnt combine it into one. Or should I be looking to NOT use the aggregate filter and be looking more on the Kibana side of things? Here is a sample of the log 1/9/2020 8:06:26 AM: 1076/9: ConnectAutid Information: 1001: Event: Sent SSO request to local IdP, Local IdP application: LoginFormIdentityProvider, Partner SP: emailprovider.com, Target URL: , Session ID: 12345, IP address: 192.168.1.100 1/9/2020 8:06:34 AM: 1076/7: ConnectAudit Information: 1004: Event: Received integration token from local IdP, Local IdP application: LoginFormIdentityProvider, Partner SP: , Target URL: ,Local user name: enduser, Attributes: 3, mail: enduser@ouremail.com, Domain: ouremail, UserName: enduser, Session ID: 12345, IP address: 192.168.1.100 1/9/2020 8:06:34 AM: 1076/7: ConnectAudit Information: 1011: Event: SAML v2.0 assertion to partner SP, Local IdP application: LoginFormIdentityProvider, Partner SP: emailprovider.com, SAML subject name: enduser@ouremail, Session ID: 12345, IP address: 192.168.1.100 And here is my current pipeline.conf input { beats { port => \"5044\" } } filter { grok { break_on_match => false match => { \"message\" => [ \"SAML subject name: %{EMAILADDRESS:email},\", \"mail: %{EMAILADDRESS:email},\", \"Session ID: %{DATA:sessionID},\", \"IP address: %{IP:client}\" ] } } geoip { source => \"client\" } aggregate { task_id => \"%{sessionID}\" code => \"map['hits'] ||= 0; map['hits'] += 1;\" push_map_as_event_on_timeout => true timeout_task_id_field => \"sessionID\" timeout => 3 timeout_tags => ['_sessionidtimeout'] timeout_code => \"event.set('several_hits', event.get('hits') > 1)\" } output { stdout { codec => rubydebug } elasticsearch { hosts => [\"192.168.1.150:9200\"] } }",
    "website_area": "discuss"
  },
  {
    "id": "013756d1-8030-43eb-9abe-ebd2083835b0",
    "url": "https://discuss.elastic.co/t/type-string-in-a-multiple-input-http-poller-for-different-grok-pattern/211481",
    "title": "Type string in a multiple input Http_Poller for different Grok pattern",
    "category": [
      "Logstash"
    ],
    "author": "kamalesh",
    "date": "December 11, 2019, 9:28pm December 11, 2019, 9:52pm December 23, 2019, 10:25am January 9, 2020, 5:10pm",
    "body": "Hi All, I am trying to using multiple Url's for the http_poller and need to use a different Grok patterns for each of the Url's I am aware that i can make use of type string in the Input plugins and make Grok patterns using 'If' condition , if it matches the type. I tried to built with the below sample file ,Provide me help on building it . input { http_poller { urls => { url1 => \"https://hedeededededje\" type[url1] => \"JVM_Memory\" } } http_poller { urls => { url2 => \"https://wwdwjjwhewkjek\" type[url2] => \"GC\" } } request_timeout => 60 user => \"tttttt\" password => \"yyyyyy\" request_timeout => 60 schedule => { cron => \"*/5 * * * *\"} codec => \"json\" metadata_target => \"http_metadata\" } } filter { if type[url1] == \"JVM_Memory\"{ split { terminator => \"\\n\" field => \"message\" remove_field => \"tags\" } } if type[url2] == \"GC\" { split { terminator => \"\\n\" field => \"message\" remove_field => \"tags\" } } }",
    "website_area": "discuss"
  },
  {
    "id": "db0cad8d-e394-48b6-b04a-64e0958900d2",
    "url": "https://discuss.elastic.co/t/not-able-to-write-in-file-with-particular-tag/214440",
    "title": "Not able to write in file with particular tag",
    "category": [
      "Logstash"
    ],
    "author": "srbhklkrn",
    "date": "January 9, 2020, 2:02pm January 9, 2020, 3:48pm January 9, 2020, 3:51pm January 9, 2020, 4:46pm",
    "body": "I'm trying to write all logs with tag Prod to file but it's not working. When I remove tag condition it works but not with tags. Any inputs? Here is my pipeline: input { tcp { port => 10514 type => syslog } udp { port => 10514 type => syslog } } filter { if [type] == \"syslog\" { grok { match => { \"message\" => \"%{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{HOSTNAME:syslog5424_host}|-) +(?:%{NOTSPACE:syslog5424_app}|-) +(?:%{NOTSS PACE:syslog5424_proc}|-) +(?:%{WORD:syslog5424_msgid}|-) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|) +%{GREEDYDATA:syslog5424_msg}\" } } syslog_pri { } date { match => [ \"syslog_timestamp\", \"MMM d HH:mm:ss\", \"MMM dd HH:mm:ss\" ] } if !(\"_grokparsefailure\" in [tags]) { mutate { replace => [ \"@source_host\", \"%{syslog_hostname}\" ] replace => [ \"@message\", \"%{syslog_message}\" ] } } mutate { remove_field => [ \"syslog_hostname\", \"syslog_message\", \"syslog_timestamp\" ] } if [message] =~ /.prod./ { mutate { add_tag => [\"Prod\"] } } } } output { if [type] == \"syslog\" { elasticsearch { hosts => [\"10.31.45.85:9450\", \"10.31.45.86:9450\", \"10.31.45.87:9450\"] index => \"logstash-%{+yyyy.MM.dd}\" } if \"Prod\" in [tags] { file { path => \"/home/myuser/logstash-logs/test-%{+YYYY-MM-dd}.log\" codec => rubydebug } } else { file { path => \"/home/myuser/logstash-logs/test2-%{+YYYY-MM-dd}.log\" codec => rubydebug } } } }",
    "website_area": "discuss"
  },
  {
    "id": "0d417d3f-15b8-4adf-9f32-24fe2c1b878b",
    "url": "https://discuss.elastic.co/t/persisted-queue-and-zabbix-ouput-plugin/214395",
    "title": "Persisted queue and Zabbix Ouput Plugin",
    "category": [
      "Logstash"
    ],
    "author": "Halgrim",
    "date": "January 9, 2020, 9:44am January 9, 2020, 3:58pm",
    "body": "Hello! I'm using logstash with a persisted queue option. And it works fine when I using elasticsearch as output plugin. (If the connection with ES is lost messages goes to persisted queue and when the connection is up again messages successfully goes to ES). But I have a need to send these messages to Zabbix output plugin also. And here is I've got a problem: when the connection is lost - messages are correctly going to the persisted queue but when the connection is up messages are still there. And they're not going to the Zabbix output. So I have two questions: Is it possible to automatically send messages from the persisted queue to the Zabbix output plugin? Or I can't do it due to plugin limitation? If I can't do it - Is there any option to force events from persisted queue to pipeline again (or directly to Zabbix output) manually? (Because messages are still in queue and I can see them))",
    "website_area": "discuss"
  },
  {
    "id": "1f1b578f-c890-44aa-9684-7862d8d28286",
    "url": "https://discuss.elastic.co/t/index-templates-for-older-filebeat-indices-how/214461",
    "title": "Index Templates for older filebeat indices...how?",
    "category": [
      "Logstash"
    ],
    "author": "hueyg",
    "date": "January 9, 2020, 3:53pm",
    "body": "I migrated to a 7.4 cluster from a stand alone 5.x single test instance. I may have carried over some legacy bad habits so to speak. Attempting to implement ILM policies but exposing other issues. I followed the same older example of shipping via filebeat to logstash and creating indices based on an index pattern. I never created any \"index template\" for these various indices and never really understood why it was required in the newer examples of shippers. I see mappings and shard settings so something is taking care of it. Can I simply create a NEW index template for these pre-existing indices based on the index pattern so I can apply ILM policies to it? Will this cause an issue to the existing \"rules\" that are currently being applied to the filebeat indices based on index pattern? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "b6f9dec5-4f31-48f7-9101-d479b661b832",
    "url": "https://discuss.elastic.co/t/logstash-7-5-1-high-cpu-usage/214120",
    "title": "Logstash 7.5.1 High CPU Usage",
    "category": [
      "Logstash"
    ],
    "author": "chris_g",
    "date": "January 7, 2020, 7:43pm January 7, 2020, 8:04pm January 7, 2020, 9:07pm January 7, 2020, 9:26pm January 7, 2020, 9:36pm January 7, 2020, 9:56pm January 9, 2020, 3:46pm",
    "body": "I recently upgraded Elastic from 7.0.1 to 7.5.1. Elasticsearch and Kibana had no issues but I am having some trouble with Logstash. When I run Logstash 7.0.1, no issues. Everything works as normal. Then, I upgraded to 7.5.1 and my CPU went through the roof. My CPU sits at 100% utilization when starting/running Logstash. We are still in pre-prd, so I have a pretty basic setup. We ingest about 15 documents a minute, so it is not ingestion volume that is causing this high utilization. The config directory is exactly the same between 7.0.1 and 7.5.1. The logs for 7.5.1 don't indicate anything fishy going on either. I have attached it below. Any thoughts would be very helpful. Thanks. logstash-plain.log for Logstash 7.5.1 [2020-01-07T14:25:54,661][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-01-07T14:25:54,699][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.5.1\"} [2020-01-07T14:26:03,790][INFO ][org.reflections.Reflections] Reflections took 90 ms to scan 1 urls, producing 20 keys and 40 values [2020-01-07T14:26:07,358][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://elastic:xxxxxx@localhost:9200/]}} [2020-01-07T14:26:08,098][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>\"http://elastic:xxxxxx@localhost:9200/\"} [2020-01-07T14:26:08,221][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>7} [2020-01-07T14:26:08,228][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7} [2020-01-07T14:26:08,384][WARN ][logstash.outputs.elasticsearch] DEPRECATION WARNING: Connecting to an OSS distribution of Elasticsearch using the default distribution of Logstash will stop working in Logstash 8.0.0. Please upgrade to the default distribution of Elasticsearch, or use the OSS distribution of Logstash {:url=>\"http://elastic:xxxxxx@localhost:9200/\"} [2020-01-07T14:26:08,456][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"http://localhost:9200\"]} [2020-01-07T14:26:08,493][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://elastic:xxxxxx@localhost:9200/]}} [2020-01-07T14:26:08,508][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>\"http://elastic:xxxxxx@localhost:9200/\"} [2020-01-07T14:26:08,517][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>7} [2020-01-07T14:26:08,543][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7} [2020-01-07T14:26:08,637][WARN ][logstash.outputs.elasticsearch] DEPRECATION WARNING: Connecting to an OSS distribution of Elasticsearch using the default distribution of Logstash will stop working in Logstash 8.0.0. Please upgrade to the default distribution of Elasticsearch, or use the OSS distribution of Logstash {:url=>\"http://elastic:xxxxxx@localhost:9200/\"} [2020-01-07T14:26:08,674][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"http://localhost:9200\"]} [2020-01-07T14:26:09,181][INFO ][logstash.outputs.elasticsearch] Using default mapping template [2020-01-07T14:26:09,333][INFO ][logstash.outputs.elasticsearch] Index Lifecycle Management is set to 'auto', but will be disabled - Index Lifecycle management is not available in your Elasticsearch cluster [2020-01-07T14:26:09,337][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-*\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"*\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} [2020-01-07T14:26:09,466][INFO ][logstash.outputs.elasticsearch] Using default mapping template [2020-01-07T14:26:09,586][INFO ][logstash.outputs.elasticsearch] Index Lifecycle Management is set to 'auto', but will be disabled - Index Lifecycle management is not available in your Elasticsearch cluster [2020-01-07T14:26:09,587][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-*\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"*\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} [2020-01-07T14:26:10,128][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge] A gauge metric of an unknown type (org.jruby.specialized.RubyArrayOneObject) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-01-07T14:26:10,146][INFO ][logstash.javapipeline ] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>2, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>250, \"pipeline.sources\"=>[\"D:/ELK/logstash-7.5.1/config/logstash.conf\"], :thread=>\"#<Thread:0x677e4b9e run>\"} [2020-01-07T14:26:12,624][INFO ][logstash.inputs.beats ] Beats inputs: Starting input listener {:address=>\"0.0.0.0:5044\"} [2020-01-07T14:26:13,656][INFO ][logstash.javapipeline ] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-01-07T14:26:14,581][INFO ][org.logstash.beats.Server] Starting server on port: 5044 [2020-01-07T14:26:15,580][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]} [2020-01-07T14:26:20,134][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} logstash.conf file input { jms { type => \"jms\" yaml_file => \"D:\\ELK\\logstash-7.0.1\\config\\jms.yml\" yaml_section => \"activemq\" interval => 0 destination => \"preprod.logging.event.q.1\" pub_sub => false include_header => false include_properties => false include_body => true use_jms_timestamp => true } jms { type => \"jms\" yaml_file => \"D:\\ELK\\logstash-7.0.1\\config\\jms.yml\" yaml_section => \"activemq\" interval => 0 destination => \"tst.event.logging.q.1\" pub_sub => false include_header => false include_properties => false include_body => true use_jms_timestamp => true } beats { type => \"beats\" port => 5044 } } filter { if [type] == \"beats\"{ if [log][file][path] =~ /(dcma-all.log)/ or [log][file][path] =~ /(dcma-user.log)/ { grok { match => { \"message\" => \"%{IPV4:ephesoftVersion} %{CISCO_REASON:operatingSystem} %{TIMESTAMP_ISO8601:date} %{LOGLEVEL:logLevel} (?<container>[^\\s]+) %{JAVACLASS:javaClass} \\- %{GREEDYDATA:body}\" } } } else if [log][file][path] =~ /(dcma_report_all.log)/ { grok { match => { \"message\" => \"%{IPV4:ephesoftVersion} %{CISCO_REASON:operatingSystem} \\[%{WORD:logLevel} %{DATESTAMP:date}]\\ %{NOTSPACE:javaClass} \\- %{GREEDYDATA:body}\" } } } else { grok { match => { \"message\" => \"%{TIMESTAMP_ISO8601:date} %{WORD:logLevel} (?<container>[^\\-]+) \\- %{GREEDYDATA:body}\" } } } date { match => [ \"date\", \"dd/MM/yyyy HH:mm:ss,SSS\", \"ISO8601\" ] } } if [type] == \"jms\"{ json{ source => \"message\" remove_field => [\"message\"] } } } output { if [type] == \"jms\"{ elasticsearch { hosts => [\"http://localhost:9200\"] index => \"talendesb-log\" user => \"myUsername\" password => \"myPassword\" } } if [type] == \"beats\"{ elasticsearch { hosts => [\"http://localhost:9200\"] index => \"ephesoft-log\" user => \"myUsername\" password => \"myPassword\" } } }",
    "website_area": "discuss"
  },
  {
    "id": "cbd6af54-a693-4897-83da-cc6772ec146f",
    "url": "https://discuss.elastic.co/t/what-security-layer-logstash-uses/214021",
    "title": "What security layer logstash uses",
    "category": [
      "Logstash"
    ],
    "author": "Gauti",
    "date": "January 7, 2020, 10:00am January 7, 2020, 4:06pm January 9, 2020, 9:52am January 9, 2020, 3:19pm",
    "body": "HI All, We are using logstash to transfer data from our source to elasticsearch, just wondering when data is transferred is there any kind of encryption happens to send the data securely? Thanks Gauti",
    "website_area": "discuss"
  },
  {
    "id": "aae0505e-a7d7-4edd-aedc-2ae64c7b107a",
    "url": "https://discuss.elastic.co/t/logstash-aggregate-jdbc-sql-relations/214424",
    "title": "Logstash aggregate - JDBC SQL Relations",
    "category": [
      "Logstash"
    ],
    "author": "Ahmed_Ahmed",
    "date": "January 9, 2020, 1:13pm",
    "body": "Hi, I have been following your responses and found them always helpful and spot on. I would like to ask With reference to the following where there is a parent-child relation is pipelined Logstash aggregate-example4 I wonder how to achieve a scenario where event elements have a hierarchal relationship like following { \"country_name\": \"France\", \"town_name\": \"Paris\" , \"location-name\": \"Location A in Paris\"} { \"country_name\": \"France\", \"town_name\": \"Paris\" , \"location-name\": \"Location B in Paris\"} { \"country_name\": \"France\", \"town_name\": \"Paris\" , \"location-name\": \"Location C in Paris\"} { \"country_name\": \"France\", \"town_name\": \"Paris\" , \"location-name\": \"Location D in Paris\"} { \"country_name\": \"France\", \"town_name\": \"Marseille\", \"location-name\": \"Location A in Marseille\" } { \"country_name\": \"France\", \"town_name\": \"Marseille\", \"location-name\": \"Location B in Marseille\" } { \"country_name\": \"France\", \"town_name\": \"Marseille\", \"location-name\": \"Location C in Marseille\" } { \"country_name\": \"France\", \"town_name\": \"Marseille\", \"location-name\": \"Location D in Marseille\" } { \"country_name\": \"France\", \"town_name\": \"Marseille\", \"location-name\": \"Location E in Marseille\" } { \"country_name\": \"France\", \"town_name\": \"Marseille\", \"location-name\": \"Location F in Marseille\" } { \"country_name\": \"France\", \"town_name\": \"Marseille\", \"location-name\": \"Location G in Marseille\" } { \"country_name\": \"USA\", \"town_name\": \"New-York\" , \"location-name\": \"Location A in New-York\"} { \"country_name\": \"USA\", \"town_name\": \"New-York\" , \"location-name\": \"Location B in New-York\"} { \"country_name\": \"USA\", \"town_name\": \"New-York\" , \"location-name\": \"Location C in New-York\"} { \"country_name\": \"USA\", \"town_name\": \"New-York\" , \"location-name\": \"Location D in New-York\"} parent (n) ---child (n) ------ grand-child (n) ------------great-grand-child (n) I hope I could ask it right. I am totally new to this and your help in this regard is highly appreciated. Regards.",
    "website_area": "discuss"
  },
  {
    "id": "0997616b-5ae5-4a70-a4e2-91f04e33d3c2",
    "url": "https://discuss.elastic.co/t/change-log-level-logstash-kafka-input-plugin/214426",
    "title": "Change log level Logstash kafka input plugin?",
    "category": [
      "Logstash"
    ],
    "author": "SKumarMN",
    "date": "January 9, 2020, 12:18pm January 9, 2020, 12:20pm",
    "body": "I am using kafka input plugin and logs are getting filled with the below msgs [2020-01-09T17:28:16,750][WARN ][org.apache.kafka.clients.NetworkClient] [Consumer clientId=logstash-0, groupId=logstash] Error while fetching metadata with correlation id 1667 : {devglan-log-test1=UNKNOWN_TOPIC_OR_PARTITION} [2020-01-09T17:28:16,854][WARN ][org.apache.kafka.clients.NetworkClient] [Consumer clientId=logstash-0, groupId=logstash] Error while fetching metadata with correlation id 1668 : {devglan-log-test1=UNKNOWN_TOPIC_OR_PARTITION} I tried to set the log level of kaka input plugin to ERROR as below logger.kafkainput.name = logstash.inputs.kafka logger.kafkainput.level = error but it didnt help. (base) C02VN29BHTD8:bin apple$ curl -XGET 'localhost:9600/_node/logging?pretty' { \"host\" : \"C02VN29BHTD8\", \"version\" : \"6.2.4\", \"http_address\" : \"127.0.0.1:9600\", \"id\" : \"90f1ffa9-bf26-43a9-8b97-1f6edefbc41e\", \"name\" : \"C02VN29BHTD8\", \"loggers\" : { \"logstash.agent\" : \"INFO\", \"logstash.api.service\" : \"INFO\", \"logstash.codecs.plain\" : \"INFO\", \"logstash.codecs.rubydebug\" : \"INFO\", \"logstash.config.source.local.configpathloader\" : \"INFO\", \"logstash.config.source.multilocal\" : \"INFO\", \"logstash.config.sourceloader\" : \"INFO\", \"logstash.inputs.kafka\" : \"ERROR\", \"logstash.instrument.periodicpoller.deadletterqueue\" : \"INFO\", \"logstash.instrument.periodicpoller.jvm\" : \"INFO\", \"logstash.instrument.periodicpoller.os\" : \"INFO\", \"logstash.instrument.periodicpoller.persistentqueue\" : \"INFO\", \"logstash.modules.scaffold\" : \"INFO\", \"logstash.outputs.stdout\" : \"INFO\", \"logstash.pipeline\" : \"INFO\", \"logstash.plugins.registry\" : \"INFO\", \"logstash.runner\" : \"INFO\", \"org.apache.kafka.clients.ClientUtils\" : \"INFO\", \"org.apache.kafka.clients.CommonClientConfigs\" : \"INFO\", \"org.apache.kafka.clients.Metadata\" : \"INFO\", \"org.apache.kafka.clients.NetworkClient\" : \"INFO\", \"org.apache.kafka.clients.consumer.ConsumerConfig\" : \"INFO\", \"org.apache.kafka.clients.consumer.KafkaConsumer\" : \"INFO\", \"org.apache.kafka.clients.consumer.internals.AbstractCoordinator\" : \"INFO\", \"org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread\" : \"INFO\", \"org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor\" : \"INFO\", \"org.apache.kafka.clients.consumer.internals.ConsumerCoordinator\" : \"INFO\", \"org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient\" : \"INFO\", \"org.apache.kafka.clients.consumer.internals.Fetcher\" : \"INFO\", \"org.apache.kafka.common.metrics.JmxReporter\" : \"INFO\", \"org.apache.kafka.common.metrics.Metrics\" : \"INFO\", \"org.apache.kafka.common.network.NetworkReceive\" : \"INFO\", \"org.apache.kafka.common.network.PlaintextChannelBuilder\" : \"INFO\", \"org.apache.kafka.common.network.Selector\" : \"INFO\", \"org.apache.kafka.common.protocol.Errors\" : \"INFO\", \"org.apache.kafka.common.requests.DeleteAclsResponse\" : \"INFO\", \"org.apache.kafka.common.utils.AppInfoParser\" : \"INFO\", \"org.apache.kafka.common.utils.Utils\" : \"INFO\", \"org.logstash.Logstash\" : \"INFO\", \"org.logstash.common.DeadLetterQueueFactory\" : \"INFO\", \"org.logstash.common.io.DeadLetterQueueWriter\" : \"INFO\", \"org.logstash.config.ir.CompiledPipeline\" : \"INFO\", \"org.logstash.instrument.metrics.gauge.LazyDelegatingGauge\" : \"INFO\", \"org.logstash.secret.store.SecretStoreFactory\" : \"INFO\", \"slowlog.logstash.codecs.plain\" : \"TRACE\", \"slowlog.logstash.codecs.rubydebug\" : \"TRACE\", \"slowlog.logstash.inputs.kafka\" : \"TRACE\", \"slowlog.logstash.outputs.stdout\" : \"TRACE\" } } Unless i set the level of \"org.apache.kafka.clients.NetworkClient\" to \"ERROR\" it doesn't work. My question is how to change the log level of org.apache.kafka.clients.NetworkClient in log4j.properties",
    "website_area": "discuss"
  },
  {
    "id": "ff7ef8dc-7dec-4f94-83e5-74137e081ed7",
    "url": "https://discuss.elastic.co/t/indexing-csv-with-header-in-elasticsearch/214229",
    "title": "Indexing csv with header in elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "johnkary",
    "date": "January 8, 2020, 1:31pm January 8, 2020, 12:50pm January 8, 2020, 1:43pm January 8, 2020, 1:31pm January 8, 2020, 1:49pm January 8, 2020, 1:57pm January 8, 2020, 2:04pm January 9, 2020, 11:20am January 9, 2020, 10:57am",
    "body": "Hi all! I' m using Elasticsearch 6.6 and Kibana 6.4 installed on my Google Cloud account. Here is my issue: I have a folder there where csv logs arrive from some IoT devices and with a Logstash pipeline that i created, i injest these csvs into an index in my Elasticsearch instance. The point is that these csv are gonna be changed and will come with headers inside and i want in someway to include this extra information too and correlate it with the body of the corresponding csv. So, in the next step, when i perform a search with some keywords from the header, i want to be able to get results related to the body. How can i do this? Can anyone help me? Thank you in advance!",
    "website_area": "discuss"
  },
  {
    "id": "c8fdcf5f-8b49-4ed2-a309-ae4f1706ebb0",
    "url": "https://discuss.elastic.co/t/multiple-logstash-servers-unique-pipeline-name-per-server/214417",
    "title": "Multiple logstash servers - unique pipeline name per server?",
    "category": [
      "Logstash"
    ],
    "author": "probson",
    "date": "January 9, 2020, 10:39am",
    "body": "Hi, I have 2 logstash servers with the same pipeline, all of the beats are going to 1 server at the moment, if I go into stack monitoring I see 1 pipeline (main) from there I want to see the pipeline and the performance per phase but there are no hits, its as though it is only showing the pipeline for the server with no beats running through it. I was considering setting up a unique pipeline per server to see if that helps with the metrics. Any recommendations? Thanks Phil",
    "website_area": "discuss"
  },
  {
    "id": "bb5be67e-600c-4dd2-bded-00ec30b54fca",
    "url": "https://discuss.elastic.co/t/logstash-7-4-0-grok-filter-not-work-or-might-some-problem-with-logstash/214408",
    "title": "Logstash 7.4.0 \"grok \" filter not work or might some problem with Logstash",
    "category": [
      "Logstash"
    ],
    "author": "Krunal_Patel",
    "date": "January 9, 2020, 10:14am",
    "body": "Here, I have tried to send MySQL's error log data through filebeat 7.4.0 to Logstash 7.4.0 and after filter, the required information, send these all data to Elastic and then viewing through Kibana. There has some problems that error log data did not filter with Logstash's config file and the whole line of error log processed and displayed to Kibana without any error in Logstash and elastic. Software versions: MySQL 8.0.18 GPL ELK - 7.4.0 Filebeat - 7.4.0 The requirement to use Logstash: I need the only word \"NOTE|WARNING|ERRRO\" from error log statement when any problem with MySQL Here I displayed Logstash and Filebeat ##################Logstash config file######### input { beats { port => 5044 host => \"XXXXXXX\" } } filter { if [fileset][module] == \"mysql\" { if [fileset][name] == \"error\" { grok { match => [\"message\", \"(?<errortype>(Note|Warning|ERROR))\"] add_field => { \"errortype\" => \"%{errortype}\" } } } } } output{ elasticsearch { hosts => \"XXXXXXX:9200\" manage_template => false index => \"filebeatindex\" } } ######################################## ######################Filebeat config file############# filebeat.inputs: # Each - is an input. Most options can be set at the input level, so # you can use different inputs for various configurations. # Below are the input specific configurations. - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /file-path-of-mysql-instance-1/err1.log - /file-path-of-mysql-instance-2/err2.log - /file-path-of-mysql-instance-3/err3.log #- c:\\programdata\\elasticsearch\\logs\\* #================================ Outputs ===================================== output.logstash: # The Logstash hosts hosts: [\"XXXXXXX:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" Can anyone suggest to me what I have to do to solve this? Am I used correct version?",
    "website_area": "discuss"
  },
  {
    "id": "d701f87d-f89e-4ddb-9cf8-90d5bbd3ac95",
    "url": "https://discuss.elastic.co/t/logstash-aggregate-filter-plugin-works-in-7-5-0-not-in-6-6-1/214260",
    "title": "Logstash Aggregate filter plugin works in 7.5.0, not in 6.6.1",
    "category": [
      "Logstash"
    ],
    "author": "bjosve",
    "date": "January 8, 2020, 2:51pm January 8, 2020, 3:25pm January 8, 2020, 3:25pm January 8, 2020, 4:00pm January 9, 2020, 8:26am",
    "body": "Hi, I have a problem with Logstash Aggregate filter. The exact same filter works in Logstash 7.5.0 but not in Logstash 6.6.1 and I can't find out why. The Aggregate filter version in 7.5.0 is 2.9.1 and in 6.6.1 it is 2.9.0 - and it doesn't seems as it happened so much between these versions. The input, filters and output, input { file { path => \"/mnt/testlogs/*\" } } # Before cloning: # Generate uuid, in order to match the original and the cloned event. # If there is another field to match the events it can be used as well. filter { ruby { code => \"event.set('uuid', rand(36**10).to_s(36))\" } } # Cloning the event filter { clone { clones => [\"cloned\"] } } filter { # Adding a field to the cloned event if [type] == \"cloned\" { mutate { add_field => { \"origin.hostname\" => \"myhost\" } } mutate { copy => { \"origin.hostname\" => \"hfolder\" } } # In the cloned event, # Creating an aggregate map that is shared between events that have the same task_id = uuid. # Adding the first and second field to the map aggregate { task_id => \"%{uuid}\" code => \"map['hfolder'] ||= event.get('hfolder')\" } # Clean up - remove uuid mutate { remove_field => [\"uuid\"] } # In the original event # Declare the aggregate map with the same task_id as the cloned event. # Copy the fields from the aggregate map to the event. # } else { aggregate { task_id => \"%{uuid}\" code => \"event.set('hfolder', map['hfolder'])\" # Delete the aggregate map from memory, as it is no longer needed. end_of_task => true } # Clean up - remove uuid mutate { remove_field => [\"uuid\"] } # The mutate filter below is here only for debugging purposes. It can be deleted. mutate { add_field => { \"iscloned\" => \"no\" } } } } output { if [type] == \"cloned\" { stdout { codec => rubydebug } } else { stdout { codec => rubydebug } } } Result in Logstash 6.6.1, \"iscloned\" => \"no\", \"@version\" => \"1\", \"message\" => \"(deleted)\", \"path\" => \"/mnt/testlogs/test.log\", \"@timestamp\" => 2020-01-08T14:02:50.743Z, \"hfolder\" => nil \"@version\" => \"1\", \"host\" => \"216d984a3495\", \"message\" => \"(deleted)\", \"origin.hostname\" => \"myhost\", \"type\" => \"cloned\", \"path\" => \"/mnt/testlogs/test.log\", \"@timestamp\" => 2020-01-08T14:02:50.743Z, \"hfolder\" => \"myhost\" Result in Logstash 7.5.0, \"host\" => \"9c76c3424d39\", \"type\" => \"cloned\", \"path\" => \"/mnt/testlogs/test.log\", \"hfolder\" => \"myhost\", \"message\" => \"(deleted)\", \"@version\" => \"1\", \"origin.hostname\" => \"myhost\", \"@timestamp\" => 2020-01-08T13:18:00.424Z \"@timestamp\" => 2020-01-08T13:18:00.380Z, \"path\" => \"/mnt/testlogs/test.log\", \"hfolder\" => \"myhost\", \"message\" => \"(deleted)\", \"iscloned\" => \"no\", \"@version\" => \"1\", \"host\" => \"9c76c3424d39\" pipeline.workers is set to 1 in both environments. In 6.6.1 the original (not clone) event \"hfolder\" gets \"nil\" as value. In 7.5.0 \"myhost\" as expected. I am grateful for all suggestions and input. /Bjorn",
    "website_area": "discuss"
  },
  {
    "id": "f8496b77-0bf7-478f-9e4b-9f380f01d09e",
    "url": "https://discuss.elastic.co/t/read-csv-file-automatically-without-using-logstash-f/214298",
    "title": "Read csv file automatically without using 'logstash -f'",
    "category": [
      "Logstash"
    ],
    "author": "kumar4",
    "date": "January 8, 2020, 7:55pm January 8, 2020, 8:57pm January 8, 2020, 9:00pm January 8, 2020, 9:07pm January 8, 2020, 9:17pm January 9, 2020, 7:09am",
    "body": "hello, i have a csv file that i view on kibana (i have already created an index), the problem is that every time i have to type the command 'logstash -f myfile.csv', I want to automate this task, that is to say that the file is read automatically without typing this command thank you a lot",
    "website_area": "discuss"
  },
  {
    "id": "ba6d6851-5dae-4808-9a90-a950953b770d",
    "url": "https://discuss.elastic.co/t/multiple-logstash-instance-need-to-run-on-window-10/214198",
    "title": "Multiple logstash instance need to run on window 10",
    "category": [
      "Logstash"
    ],
    "author": "Pramod_Thakur",
    "date": "January 8, 2020, 9:35am January 8, 2020, 2:44pm January 9, 2020, 6:59am",
    "body": "Hi Folks, On my local machine logstash is working fine when I am running only one logstash pipeline, but when I am trying to run second logstash pipeline (config file) it says that Logstash could not be started because there is already another instance using the configured data directory. If you wish to run multiple instances, you must change the \"path.data\" setting. but I don't know what I need to make change OS: window 10 enterprise Logstash: Thanks",
    "website_area": "discuss"
  },
  {
    "id": "ce4220d3-88a5-4c08-b1e0-0de9b479b3d7",
    "url": "https://discuss.elastic.co/t/logstash-sql-server/213979",
    "title": "Logstash Sql Server",
    "category": [
      "Logstash"
    ],
    "author": "Syed.Ubaid",
    "date": "January 7, 2020, 5:11am January 7, 2020, 5:12am January 7, 2020, 2:48pm January 8, 2020, 4:40am January 8, 2020, 2:37pm January 9, 2020, 4:37am",
    "body": "I have integrate elk stack with sql server. When i run logstash to parse the data the query parse the data and logstash service shutdown after parsing the records. Is there any way around that the service will run in the background and i have dump data in sql server and the data will get into elasticsearch automatically",
    "website_area": "discuss"
  },
  {
    "id": "961c62a4-400d-4f21-8e4a-4877217a57d0",
    "url": "https://discuss.elastic.co/t/seeing-io-netty-util-internal-outofdirectmemoryerror-in-my-logtash-logs/214334",
    "title": "Seeing io.netty.util.internal.OutOfDirectMemoryError in my logtash logs",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "January 9, 2020, 2:31am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d2e2a742-4173-42b0-91da-7cb0f439f6bf",
    "url": "https://discuss.elastic.co/t/how-to-auto-set-zabbix-host/213961",
    "title": "How to auto-set zabbix_host?",
    "category": [
      "Logstash"
    ],
    "author": "Philip_Brown",
    "date": "January 7, 2020, 1:51am January 7, 2020, 4:08am January 7, 2020, 4:16am January 7, 2020, 4:19am January 7, 2020, 5:18am January 7, 2020, 1:23pm January 7, 2020, 2:51pm January 7, 2020, 3:42pm January 7, 2020, 3:45pm January 7, 2020, 4:07pm January 7, 2020, 4:14pm January 7, 2020, 4:19pm January 8, 2020, 6:17pm January 8, 2020, 7:44pm January 8, 2020, 8:17pm January 8, 2020, 9:09pm January 8, 2020, 9:14pm January 8, 2020, 9:13pm",
    "body": "I'm trying to set up a logstash-> zabbix gateway, that collects inputs from multiple servers running filebeat. I gather that zabbix_host is a mandatory field. It also seems like it would be set to the ACTUAL generating host, not the one running the logstash server. So that zabbix can fire off alerts about the actual relevant host. How can I do this? After much searching, I stumbled across a reference to using zabbix_host => \"%{source_host}\" but apparently that isnt valid either. What can I do here? Kinda in shock that I cant seem to find any working examples of this. Logstash 7.5",
    "website_area": "discuss"
  },
  {
    "id": "49a6c398-6c76-461f-ab1c-755d3a7701b7",
    "url": "https://discuss.elastic.co/t/cannot-write-to-a-field-alias-user-domain/213944",
    "title": "Cannot write to a field alias [user.domain]",
    "category": [
      "Logstash"
    ],
    "author": "Tony_Chirillo",
    "date": "January 6, 2020, 8:36pm January 8, 2020, 9:08pm",
    "body": "I recently installed a single instance of Winlogbeat 7.5.0 on a host to test log ingestion from the 7.5.0 beater version into Logstash and then into our production Elasticsearch environment. Our Elastic environment is on version 7.4.2, and beaters are still on 6.7.1. Log data makes it into Elasticsearch and can be browsed in Kibana under Discover. However, the following warning is repeatedly shown in the Logstash server logs. [2020-01-06T00:14:33,307][WARN ][logstash.outputs.elasticsearch][winlogbeat-dc] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"winlogbeat-7.5.0-dc-um\", :_type=>\"_doc\", :routing=>nil}, #LogStash::Event:0x20a5bb71], :response=>{\"index\"=>{\"_index\"=>\"winlogbeat-7.5.0-dc-um-2020.01.02-000001\", \"_type\"=>\"_doc\", \"_id\"=>\"J2x9eW8BIofUioNWniFB\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse\", \"caused_by\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"Cannot write to a field alias [user.domain].\"}}}}} Is there a way to fix resolve\\clear this warning message or did I stumble across a bug? The shipped Winlogbeat-7.5.0 template is loaded into Elasticsearch, and it does match the index pattern outlined in the provided warning message. The Winlogbeat-7.5.0 template lists user.domain as an alias of winlog.user.domain. For what it is worth, this line is enabled in the Winlogbeat 7.5.0 .yml file: migration.6_to_7.enabled: true",
    "website_area": "discuss"
  },
  {
    "id": "ec5d3094-8e75-4965-a982-c098b6adea7d",
    "url": "https://discuss.elastic.co/t/logstash-file-output/214288",
    "title": "Logstash file output",
    "category": [
      "Logstash"
    ],
    "author": "rahul_cheekoti",
    "date": "January 8, 2020, 5:36pm January 8, 2020, 7:38pm",
    "body": "Hi all My Logstash file generates a output file every day . It will modified for every few minutes. But when I check the last modified date of file it is updating once the file is created and when new file is created. Could anyone help us in resolving this. I need last modified of file updated when ever the file is updated.",
    "website_area": "discuss"
  },
  {
    "id": "d974f583-a175-455d-a63d-ca5e3667e98b",
    "url": "https://discuss.elastic.co/t/logstash-core-plugin-api-gem-compatibility-to-7-x/214295",
    "title": "Logstash-core-plugin-api gem compatibility to 7.x",
    "category": [
      "Logstash"
    ],
    "author": "pawankt",
    "date": "January 8, 2020, 7:38pm",
    "body": "Hi, Issue noticed for api gem compatibility which still looking older logstash core. I was tried to place 5.6.4 gem in vendor/cache still having problem. If we can sort logstash-core-plugin-api to 7.x i can rebuild with latest logstash core gem. Could not find gem 'logstash-core (= 5.6.4)' in source at ./logstash-core or in gems cached in vendor/cache. The source contains 'logstash-core' at: 7.4.0 java https://rubygems.org/gems/logstash-core-plugin-api",
    "website_area": "discuss"
  },
  {
    "id": "ac3eff98-8a11-4a6b-9e03-5774fa8636d6",
    "url": "https://discuss.elastic.co/t/cant-access-fields-from-log-using-query-template-for-elasticsearch-filter-in-logstash/214291",
    "title": "Cant access fields from log using query template for Elasticsearch filter in Logstash",
    "category": [
      "Logstash"
    ],
    "author": "james.garside",
    "date": "January 8, 2020, 6:21pm",
    "body": "Cant access fields from log using query template I have an instance of logstash which I want to use to enrich a field on incomming logs by querying elasticsearch then add the result to the recieved log. I have configured it using the resources available however the field is not being updated. My configurations are below. pipeline.yml mutate { add_field => { \"destination.geo.name\" => \"none\" }} elasticsearch { hosts => [\"https://eshost1\"] #ssl => true (Not used due to a mention there is a bug with using ssl and instead use https) ca_file => \"/path/to/cert.crt\" user => \"logstash\" password => \"password\" index => \"index-ap*\" query_template => \"/path/to/ap_query.json\" fields => { \"Map Location\" => \"destination.geo.name\" } } ap_query.json { \"size\": 1, \"sort\" : [ { \"@timestamp\" : \"desc\" } ], \"query\": { \"match_phrase\": { \"Base Radio MAC Address\": \"%{[destination][address]}\" } }, \"_source\": [\"Map Location\"] } Logstash is opertaing completly as expected apart from this section. Any help would be greatly appreciated.",
    "website_area": "discuss"
  },
  {
    "id": "4b192256-792b-49f9-8e72-00d78fe3216a",
    "url": "https://discuss.elastic.co/t/dmarc-xml-file/214248",
    "title": "DMARC XML FIle",
    "category": [
      "Logstash"
    ],
    "author": "Wilks",
    "date": "January 8, 2020, 2:38pm January 8, 2020, 2:36pm January 8, 2020, 5:09pm",
    "body": "Hi All, I am trying to send dmarc logs to elasticsearch and running into some issues with the logstash config. Logstash only appears to process the first few lines of the xml log file as per below. Screen Shot 2020-01-08 at 9.29.07 AM1219218 22.9 KB It seems to be something with the pattern => \"^<?feedback.*>\" but I could be wrong. Below is the Logstash config input { file { path => \"/var/cache/dmarc-reports/*.xml\" start_position => \"beginning\" discover_interval => \"1\" tags => [\"dmarc-reports\"] codec => multiline { pattern => \"^<?feedback>\" negate => true what => \"previous\" } } } filter { xml { #store_xml => \"false\" target => \"dmarc\" source => \"message\" } } output { elasticsearch { hosts => [\"10.10.10.34:9200\"] http_compression => \"true\" index => \"dmarc-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug }",
    "website_area": "discuss"
  },
  {
    "id": "e388205b-49c2-4392-a14b-4df59d331666",
    "url": "https://discuss.elastic.co/t/how-to-forward-logstash-logs-to-rapid7/214141",
    "title": "How to forward logstash logs to Rapid7?",
    "category": [
      "Logstash"
    ],
    "author": "srbhklkrn",
    "date": "January 7, 2020, 11:34pm January 8, 2020, 12:06am January 8, 2020, 3:23pm January 8, 2020, 3:19pm January 8, 2020, 3:24pm January 8, 2020, 3:58pm January 9, 2020, 1:55pm",
    "body": "I'm trying to forward logs using Logstash to Rapid7 SIEM tool with particular tags, below is the output plugin but it's not working, what am I doing wrong? output { if [type] == \"syslog\" { elasticsearch { hosts => [\"localhost:9200\"] } if [tags] == \"Prod\" { http { hosts => [\"10.10.1.1:10025\"] } } } }",
    "website_area": "discuss"
  },
  {
    "id": "99c99592-2088-4756-9382-e2fe80784b42",
    "url": "https://discuss.elastic.co/t/logtash-ignores-some-lines-randomly/214259",
    "title": "Logtash ignores some lines randomly",
    "category": [
      "Logstash"
    ],
    "author": "JustDevZero",
    "date": "January 8, 2020, 2:50pm January 8, 2020, 3:18pm",
    "body": "Logtash apparently ignores some of my log lines done by nginx. I'm trying to parse my customized nginx logs with logstash, I'm adding the response time by upstream, etc... and a few more fields that aren't normally on the nginx logs but have a lot of interest. But somehow logstash seems to ignore some lines, even the grok debugger (https://grokdebug.herokuapp.com/) doesn't seem do discard them. Following is the ignored line that apparently match with the first posibility: \"08/Jan/2020:14:22:47 +0000\" client=88.217.181.177 method=POST request=\"POST /common/message_count HTTP/2.0\" request_length=402 status=200 bytes_sent=884 body_bytes_sent=90 referer=https://backoffice.andronautic.com/planner user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\" upstream_addr=unix:/var/run/popeye/back.socket upstream_status=200 request_time=0.731 ssl_session_reused=r upstream_response_time=0.732 upstream_connect_time=0.000 upstream_header_time=0.732 And next the logstash.conf that I use, attached on pastebin due to weird pasting here: https://pastebin.com/L5vQzbMU Why is it ignored on logstash/kibana? /tmp/grok_failures is empty for hours... but still, ignoring some lines Many thanks!",
    "website_area": "discuss"
  },
  {
    "id": "b9a5bd5f-c8ed-4361-a1ab-f55a85e0bd1c",
    "url": "https://discuss.elastic.co/t/logstash-issue-with-dropping-rows-with-a-condition-check-on-empty-elements/214132",
    "title": "Logstash issue with dropping rows with a condition check on empty elements",
    "category": [
      "Logstash"
    ],
    "author": "Nick11",
    "date": "January 7, 2020, 9:07pm January 7, 2020, 9:15pm January 7, 2020, 9:44pm January 7, 2020, 9:54pm January 7, 2020, 11:03pm January 8, 2020, 12:09am January 8, 2020, 3:03am January 8, 2020, 2:52pm",
    "body": "I have a CSV file with the following 2 rows (sample) reservation date, reservationID Jan 6th, res:id:adbcj-oksok-gjkk Jan 10th, Mar 10th, res:id:kkbcj-oksok-gjkk My ask is to drop empty rows and apply a grok filter on reservationID to extract the last elements after the \"-\". This is what I did without success csv { separator => \",\" skip_header => \"true\" autodetect_column_names => \"true\" skip_empty_columns => \"true\" skip_empty_rows => \"true\" } if [reservationID] =~ \"\" { grok { reservationID => \"MY GROK PATTERN HERE, WHICH IS WORKING FINE EXTERNALLY THROUGH THE DEBUGGER\" } } I was expecting the first and the third row in the output (not worried about the grok). Instead I see all 3 rows. Am I missing anything. I do not want the 2nd row in my output. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "0ad07bd4-1055-4c18-b58a-f7af62c929de",
    "url": "https://discuss.elastic.co/t/usr-share-logstash-bin-system-install-line-88-command-not-found/214242",
    "title": "/usr/share/logstash/bin/system-install: line 88: #: command not found",
    "category": [
      "Logstash"
    ],
    "author": "leprovokateur",
    "date": "January 8, 2020, 2:01pm January 9, 2020, 11:55am",
    "body": "Hi, On upgrading from 7.5.0 to 7.5.1 I receive the error: Using provided startup.options file: /etc/logstash/startup.options OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x000000075f330000, 2697789440, 0) failed; error='Not enough space' (errno=12) /usr/share/logstash/bin/system-install: line 88: #: command not found. Unable to install system startup script for Logstash. The command in question is ruby_exec, which doesn't exist on my CentOS 7 machine. How do I solve this? Robert",
    "website_area": "discuss"
  },
  {
    "id": "587d49a8-c575-4ee7-9f23-02dcd17408e5",
    "url": "https://discuss.elastic.co/t/syslog-input-configuration-for-logstash/211510",
    "title": "Syslog input configuration for Logstash",
    "category": [
      "Logstash"
    ],
    "author": "Marcell0e",
    "date": "December 11, 2019, 5:54pm December 11, 2019, 5:57pm December 12, 2019, 1:54pm December 13, 2019, 9:05pm December 14, 2019, 2:48pm December 16, 2019, 3:02pm December 18, 2019, 3:09pm January 6, 2020, 3:37pm January 7, 2020, 7:58pm January 7, 2020, 8:42pm January 8, 2020, 2:46pm",
    "body": "I want to send syslog files to my logstash, but not sure how to incorporate it with my beats input. When I have tried adding my beats stop working. Below is my logstash file. input { beats { port => 5044 } } filter { if [source][ip] =~ /^10.3./ { mutate { replace => { \"[source][geo][timezone]\" => \"Eastern/New York\" } } mutate { replace => { \"[source][geo][country_name]\" => \" Office\" } } mutate { replace => { \"[source][geo][country_code2]\" => \"US\" } } mutate { replace => { \"[source][geo][country_code3]\" => \"IMO\" } } mutate { replace => { \"[source][geo][city_name]\" => \"XXX\" } } mutate { replace => { \"[source][geo][region_name]\" => \"XX\" } } mutate { replace => { \"[source][geo][region_code]\" => \"XX\" } } mutate { replace => { \"[source][geo][postal_code]\" => \"XXXXX\" } } mutate { update => { \"[source][geo][ip]\" => \"[source][ip]\" } } mutate { remove_field => [ \"[source][geo][location]\" ] } mutate { add_field => { \"[source][geo][location]\" => \"-XX.XX\" } } mutate { add_field => { \"[source][geo][location]\" => \"XX.XXX\" } } mutate { convert => [ \"[source][geo][location]\", \"float\" ] } mutate { replace => [ \"[source][geo][latitude]\", XX.XX ] } mutate { convert => [ \"[source][geo][latitude]\", \"float\" ] } mutate { replace => [ \"[source][geo][longitude]\", -XX.XXX ] } mutate { convert => [ \"[source][geo][longitude]\", \"float\" ] } } else { geoip { source => \"[source][ip]\" target=>\"[source][geo]\" } } } output { if [@metadata][pipeline] { elasticsearch { ilm_enabled => true hosts => [\"http://10.3.200.45:9200\", \"http://10.3.200.46:9200\"] index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" ilm_pattern => \"000001\" pipeline => \"%{[@metadata][pipeline]}\" } } else { elasticsearch { hosts => [\"http://10.3.200.45:9200\", \"http://10.3.200.46:9200\"] index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" ilm_pattern => \"000001\" } } }",
    "website_area": "discuss"
  },
  {
    "id": "f7aeb99c-8697-4c81-b8f9-9e63210f5247",
    "url": "https://discuss.elastic.co/t/why-stdout-output-shows-message/214079",
    "title": "Why stdout output shows message => \"?\"",
    "category": [
      "Logstash"
    ],
    "author": "nino",
    "date": "January 7, 2020, 5:22pm January 7, 2020, 5:34pm January 7, 2020, 7:27pm January 7, 2020, 7:37pm January 7, 2020, 8:28pm January 7, 2020, 10:26pm January 7, 2020, 8:51pm January 7, 2020, 9:16pm January 7, 2020, 10:07pm January 8, 2020, 12:11am January 8, 2020, 2:35pm",
    "body": "Hello all, i am very new to logstash. I am just testing some filters to extract content from txt files without any structure. Why is this happening? On Windows enviroment logstash config file input { file { path => \"C:/test/*.txt\" start_position => \"beginning\" codec => plain { charset => \"UTF-16\" } } } filter { } output { stdout {} } first text line of the file: echo \"first line text file...\" >> file.txt message content in log output is correct: { \"path\" => \"C:/test/file.txt\", \"message\" => \"first line text file...\\r\", \"host\" => \"DESKTOP\", \"tags\" => [ [0] \"_grokparsefailure\" ], \"@timestamp\" => 2020-01-07T14:55:37.172Z, \"@version\" => \"1\" } a second line added to the file (and next ones): echo \"second line text file...\" >> file.txt why message content in log output shows \"?\" { \"path\" => \"C:/test/file.txt\", \"message\" => \"??????????????????????????\", \"host\" => \"DESKTOP\", \"tags\" => [ [0] \"_grokparsefailure\" ], \"@timestamp\" => 2020-01-07T15:11:49.747Z, \"@version\" => \"1\" } Best regards, nino",
    "website_area": "discuss"
  },
  {
    "id": "0d772a22-70d7-4c23-9822-fc94352e81a0",
    "url": "https://discuss.elastic.co/t/logstash-goes-down-solution/213887",
    "title": "Logstash goes down || Solution",
    "category": [
      "Logstash"
    ],
    "author": "rkcharlie",
    "date": "January 6, 2020, 12:18pm January 8, 2020, 2:03pm",
    "body": "Hello Team, I am trying to push data from oracle to ES and I am able to do that as well. My problem when I am pushing those data and that time my ES goes down then I am not able to figure out that from where I need to push that data because my last_run_metadata file also gets updated. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "b42c2f62-f90b-4703-925c-28909edfc4c0",
    "url": "https://discuss.elastic.co/t/delete-documents-after-sync/214236",
    "title": "Delete documents after sync",
    "category": [
      "Logstash"
    ],
    "author": "tushar_chevulkar",
    "date": "January 8, 2020, 12:55pm",
    "body": "I have a logstash config where I am putting data from one to another input { elasticsearch { hosts => [ \"http://localhost:9200\" ] index => \"node_2\" docinfo => true } } output { elasticsearch { hosts => [\"http://localhost:9200\"] index => \"node_1\" document_type => \"fields\" doc_as_upsert => true manage_template => false document_id => \"%{[@metadata][_id]}\" } stdout { codec => \"dots\" } } filter { mutate { remove_field => [ \"@version\",\"@timestamp\" ] } } The above query is working fine but I just need to delete the documents form node_2 after the sync is done. Is there a way to do it same configuration ?",
    "website_area": "discuss"
  },
  {
    "id": "1e6e8ff3-9749-4742-837c-30fcbec7dfb5",
    "url": "https://discuss.elastic.co/t/issue-to-connect-aws-rds-mysql-from-logstash/214233",
    "title": "Issue to connect AWS RDS mysql from Logstash",
    "category": [
      "Logstash"
    ],
    "author": "vijoyem",
    "date": "January 8, 2020, 12:23pm",
    "body": "Hello, I am setting up logstash conf file to fetch the logs from my AWS RDS MYSQL database. Please let me know what needs to be corrected. ELK is configured in the AWS ec2 single machine. All services are up and running. I used the debugging command sudo bin/logstash -f <path_to_config_file> this is the conf file settings and error below: input { jdbc { jdbc_connection_string => \"jdbc:mysql://database-1.abcdef.ap-south-1.rds.amazonaws.com:3306/testdb\" # The user we wish to execute our statement as jdbc_user => \"abcd\" jdbc_password => \"abcd\" # The path to our downloaded jdbc driver jdbc_driver_library => \"\\etc\\logstash\\mysql-connector-java-5.1.30.jar\" jdbc_driver_class => \"com.mysql.jdbc.Driver\" # our query schedule => \"* * * *\" statement => \"SELECT * FROM users\" } } output { stdout { codec => json_lines } elasticsearch { \"hosts\" => \"localhost:9200\" \"index\" => \"test\" \"document_type\" => \"data\" } } LogStash::Error: Don't know how to handle Java::JavaLang::IllegalStateException for PipelineAction::Create<main>",
    "website_area": "discuss"
  },
  {
    "id": "8fe97a3a-78ae-4502-84da-a87610c8aa4f",
    "url": "https://discuss.elastic.co/t/logstash-parsing-with-combinedapachelog-pattern-for-nginx/214232",
    "title": "Logstash parsing with combinedapachelog pattern for nginx",
    "category": [
      "Logstash"
    ],
    "author": "Vladpov",
    "date": "January 8, 2020, 12:23pm",
    "body": "Hello everyone!! I ran into a problem with parsing my log messages with logstash. I have set ELK stack for geoip monitoring and my logstash config file looks like that: input { beats { port => 5044 } } filter { grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } } geoip { source => \"clientip\" } } output { elasticsearch { hosts => [\"localhost:9200\"] index => \"geoip\" } } It works good for messages like : 111.22.333.444 - - [15/Oct/2019:07:30:20 +0200] \"POST /asifuheahsfeasd/asfef-sasde HTTP/1.1\" 200 428 \"-\" \"-\" My goal is to be able to monitor status codes like 200 in this case, IP adresses and \"end points\" that are here located in last (\"-\" \"-\") fields. I have logs coming from nginx now and configurated like: log_format extended_access_log '$remote_addr - $remote_user [$time_local] ' '[$scheme://$server_name:$server_port] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ' '(rt=$request_time urt=$upstream_response_time uct=$upstream_connect_time uht=$upstream_header_time)'; So all messages have the following format: 111.22.333.444 - - [08/Jan/2020:11:50:15 +0100] [https://domain.point.name:111] \"POST /sadead/asdeadeade/v2/asdeadead/asdead HTTP/1.1\" 204 0 \"-\" \"-\" (rt=0.111 urt=0.222 uct=0.000 uht=0.444) How can I parse logs in this format like there is gonna be always IP address at the first place, date, domain , message, status code etc. I would like to additionaly parse the domain part [https://domain.point.name:111] like protocol:http, name: domain, name2: point, name3: name, port: 111 etc. is it possible to do? Thank you very much for any help!!!",
    "website_area": "discuss"
  },
  {
    "id": "26a6edf0-0cbd-4ad8-a3f9-a81ba8ab1d03",
    "url": "https://discuss.elastic.co/t/logstash-jdbc-string-column-value-in-sql-how-to-split-individual-field-and-value/213890",
    "title": "Logstash jdbc string column value in sql. how to split individual field and value",
    "category": [
      "Logstash"
    ],
    "author": "saravana_hariharan",
    "date": "January 6, 2020, 1:04pm January 6, 2020, 2:58pm January 7, 2020, 6:48am January 7, 2020, 2:53pm January 8, 2020, 10:52am",
    "body": "sql column value concatenated in {\"ID\":\"P0001\",\"Name\":\"sam\",\"Createdby:124\",\"Createdtime\":\"157312334321\"} In the above line is my sample data in one clolumn value of sql table how to split this column to individual field in logstash and i like individualy ID ,Name,Createdby,Createdtime in discover dashboard in kibana Help me, Saravanan R",
    "website_area": "discuss"
  },
  {
    "id": "b47a1407-f3ae-4f45-980a-5d6dd17d01f0",
    "url": "https://discuss.elastic.co/t/logstash-throws-config-error-even-though-input-is-listed-as-the-first-parameter-on-config-file/214154",
    "title": "Logstash throws config error, even though 'input' is listed as the first parameter on config file",
    "category": [
      "Logstash"
    ],
    "author": "sg_shreyas",
    "date": "January 8, 2020, 8:41am January 8, 2020, 8:59am",
    "body": "I am getting config error while launching Logstash. I am using 7.5.1 version. Same config file worked earlier. I had to setup ELK stash on another VM since older VM was decommissioned, and I am facing this error here. I'm instantiating logstash using - logstash-7.5.1\\bin\\logstash -f logstash-config.conf config file #config file input{ jdbc{ jdbc_driver_library=>\"C:\\software\\ojdbc7.jar\" jdbc_driver_class=>\"Java::oracle.jdbc.driver.OracleDriver\" jdbc_connection_string=>\"jdbc:oracle:thin:@<host>:1528:TSTIM\" jdbc_user=>\"UName\" jdbc_password=>\"PWD\" statement=>\"SELECT * from TABLE where run_id > 4500\" } } output{ elasticsearch{ hosts=>\"http://localhost:9200\" index=>\"run_id\" document_type=>\"TABLEname\" document_id=>\"run_id\" user=>\"elastic\" password=>\"changeme\" } stdout{ codec=>rubydebug } } Error: image1898243 84.8 KB C:\\software>logstash-7.5.1\\bin\\logstash -f logstash-config.conf Thread.exclusive is deprecated, use Thread::Mutex Sending Logstash logs to C:/software/logstash-7.5.1/logs which is now configured via log4j2.properties [2020-01-08T05:04:33,207][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-01-08T05:04:33,426][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.5.1\"} [2020-01-08T05:04:34,869][ERROR][logstash.agent ] Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>\"LogStash::ConfigurationError\", :message=>\"Expected one of [ \\t\\r\\n], \"#\", \"input\", \"filter\", \"output\" at line 1, column 1 (byte 1)\", :backtrace=>[ \"C:/software/logstash-7.5.1/logstash-core/lib/logstash/compiler.rb:41:in compile_imperative'\", \"C:/software/logstash-7.5.1/logstash-core/lib/logstash/compiler.rb:49:in compile_graph'\", \"C:/software/logstash-7.5.1/logstash-core/lib/logstash/compiler.rb:11:in block in compile_sources'\", \"org/jruby/RubyArray.java:2584:in map'\", \"C:/software/logstash-7.5.1/logstash-core/lib/logstash/compiler.rb:10:in compile_sources'\", \"org/logstash/execution/AbstractPipelineExt.java:156:in initialize'\", \"org/logstash/execution/JavaBasePipelineExt.java:47:in initialize'\", \"C:/software/logstash-7.5.1/logstash-core/lib/logstash/java_pipeline.rb:27:in initialize'\", \"C:/software/logstash-7.5.1/logstash-core/lib/logstash/pipeline_action/create.rb:36:in execute'\", \"C:/software/logstash-7.5.1/logstash-core/lib/logstash/agent.rb:326:in block in converge_state'\" ]} [2020-01-08T05:04:35,510][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2020-01-08T05:04:40,325][INFO ][logstash.runner ] Logstash shut down.",
    "website_area": "discuss"
  },
  {
    "id": "41222746-c74a-4c52-95fd-03b4edf9608e",
    "url": "https://discuss.elastic.co/t/logstash-pipeline-for-microsoft-sql-server-to-elasticsearch/212649",
    "title": "Logstash Pipeline for Microsoft SQL Server to Elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "December 20, 2019, 10:37am December 20, 2019, 10:51am December 20, 2019, 1:45pm December 20, 2019, 1:56pm December 23, 2019, 1:17pm December 23, 2019, 2:43pm December 23, 2019, 4:06pm December 26, 2019, 2:16pm January 2, 2020, 11:27am January 8, 2020, 8:01am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ea031fc2-48bd-4b55-a6ce-1072b150762e",
    "url": "https://discuss.elastic.co/t/grok-pattern-works-fine-in-debugger-but-in-the-logtstash/214026",
    "title": "Grok Pattern works fine in Debugger but in the Logtstash",
    "category": [
      "Logstash"
    ],
    "author": "sabyasachi",
    "date": "January 7, 2020, 10:36am January 7, 2020, 4:31pm January 8, 2020, 12:27am January 8, 2020, 12:27am",
    "body": "My Pattern: %{MONTH:FP_Month} %{NUMBER:FP_Date} %{TIME} %{IP:FP_LogSource} %{WORD}:%{INT}.%{INT}|%{WORD}|%{WORD}|%{WORD}.%{WORD}.%{WORD}|%{WORD}:%{WORD:FP_Transaction_Status}|%{WORD}=%{NUMBER:FP_Severity}\\%{WORD}=%{NUMBER:FP_Category}\\tusrName=LDAP://%{IP:FP_DomainControllerIP} \\OU\\%{GREEDYDATA}=com/%{GREEDYDATA:FP_UserName}\\tloginID=%{GREEDYDATA:FP_UserID}\\tsrc=%{IP:FP_ClientIP}\\tsrcPort=%{NUMBER:FP_ClientPort}\\tsrcBytes=%{NUMBER:FP_SourceBytes}\\tdstBytes=%{NUMBER:FP_DestinationBytes}\\tdst=%{IP:FP_DestinationIP}\\tdstPort=%{NUMBER:FP_DestinationPort}\\tproxyStatus-code=%{NUMBER:FP_ProxyStatus}\\tserverStatus-code=%{NUMBER:FP_ServerStatus}\\tduration=%{NUMBER:FP_Duration}\\tmethod=%{WORD:FP_METHOD}\\tdisposition=%{NUMBER:FP_Disposition}\\tcontentType=%{GREEDYDATA:FP_ContentType}\\treason=%{GREEDYDATA:FP_Reason}\\tpolicy=%{GREEDYDATA:FP_Policy}\\trole=%{NUMBER:FP_Role}\\tuserAgent=%{GREEDYDATA:FP_UserAgent}\\turl=%{URI} The filter throws Grokerror. On investigating i found the filter throws error the moment it sees a \"\\\"",
    "website_area": "discuss"
  },
  {
    "id": "9be9b6c4-499b-4c48-ae3b-5986fa040bbd",
    "url": "https://discuss.elastic.co/t/looking-for-best-practice-recommendations-for-syslog-data-retrieval/214139",
    "title": "Looking for best practice recommendations for SysLog data retrieval?",
    "category": [
      "Logstash"
    ],
    "author": "zombo",
    "date": "January 7, 2020, 10:34pm",
    "body": "I'm planning to receive SysLog data from various network devices that I'm not able to directly install beats on and trying to figure out the best way to go about it. I know Beats is being leveraged more and see that it supports receiving SysLog data, but haven't found a diagram or explanation of which configuration would be best practice moving forward. FileBeat looks appealing due to the Cisco modules, which some of the network devices are. To break it down to the simplest questions, should the configuration be one of the below or some other model? Network Device > LogStash > Elastic Network Device > LogStash > FileBeat > Elastic Network Device > FileBeat > Elastic Network Device > FileBeat > LogStash > Elastic We want to have the network data arrive in Elastic, of course, but there are some other external uses we're considering as well, such as possibly sending the SysLog data to a separate SIEM solution. I know we could configure LogStash to output to a SIEM but can you output from FileBeat in the same way or would this be a reason to ultimately send to LogStash at some point? Thank you in advance!",
    "website_area": "discuss"
  },
  {
    "id": "5f3ea122-22a6-4c55-9160-48efd1d6f0e1",
    "url": "https://discuss.elastic.co/t/fields-in-documents-are-using-a-literal-instead-of-hierarchical-json/214122",
    "title": "Fields in documents are using a literal '.' instead of hierarchical JSON",
    "category": [
      "Logstash"
    ],
    "author": "kdp494",
    "date": "January 7, 2020, 7:55pm January 7, 2020, 8:02pm January 7, 2020, 8:11pm",
    "body": "I am having an issue where the output from grok and other filters are producing fields with a literal '.' separator as opposed to the expected hierarchical JSON. For example, I use the following pattern in my grok filter: %{NUMBER:destination.port:int} It will match successfully, but the JSON output is the following: \"destination.port\": 22 Where I would expect the following: \"destination\": { \"port\": 22 } This is one example of many. I have other log sources NOT going through logstash to the same index with the same fields and the output IS how I would expect. Any idea what I may be doing wrong?",
    "website_area": "discuss"
  },
  {
    "id": "80701768-da2d-4a94-9bc1-c48f7315daea",
    "url": "https://discuss.elastic.co/t/not-able-to-see-logs-in-the-index/213399",
    "title": "Not able to see logs in the index",
    "category": [
      "Logstash"
    ],
    "author": "dustlesspuma",
    "date": "December 31, 2019, 12:34am January 3, 2020, 7:20pm January 4, 2020, 2:56am January 6, 2020, 7:46pm January 7, 2020, 7:45pm",
    "body": "I have two mutate filters created one to get all the /var/log/messages to type > security and other mutate filter to get all the logs from one kind of hosts to type > host_type. I am not able to see the /var/log/messages in the host_type index. Here is the filters code I am using, please help me understand what's going on here. why am I not able to see /var/log/messages in my apihost index? I have filebeat setup on the hosts to send logs to logstash. fileter-security.conf filter { if [source] =~ //var/log/(secure|syslog|auth.log|messages|kern.log)$/ { mutate { replace => { \"type\" => \"security\" } } } } filter-apihost.conf filter { if (([host.name] =~ /(?i)apihost-/) or ([host] =~ /(?i)apihost-/)) { mutate { replace => { \"type\" => \"apihost\" } } } }",
    "website_area": "discuss"
  },
  {
    "id": "e2315fa3-85e7-446b-9667-924021b4be81",
    "url": "https://discuss.elastic.co/t/removing-special-characters-from-field-name/213788",
    "title": "Removing special characters from field name",
    "category": [
      "Logstash"
    ],
    "author": "Rob_wylde",
    "date": "January 5, 2020, 7:26pm January 6, 2020, 12:39am January 6, 2020, 12:40am January 7, 2020, 5:12pm",
    "body": "if \"Holding Dealer Organic ID\" { mutate { add_tag => [\"holding_dealer\"] } mutate { lowercase => [\"Holding Dealer Organic ID\"] } } lowercase is not working nor is the rename function but the tag gets added so the 'if' matches. I believe it is because of the special characters only viewable via cat -e filename. This is what the field name looks like in the raw file. M-oM-;M-?Holding Dealer Organic I've tried using 'sed' to remove these characters but it is making a mess of other fields. My question is how can i remove these characters in logstash so that i can actually make us of mutate functionality on this field? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "843a0af5-e5da-4491-8742-3a27d6e7511f",
    "url": "https://discuss.elastic.co/t/jdbc-issue-after-upgrade-to-7-5/214096",
    "title": "JDBC issue after upgrade to 7.5",
    "category": [
      "Logstash"
    ],
    "author": "ginu",
    "date": "January 7, 2020, 4:14pm January 7, 2020, 4:47pm January 7, 2020, 4:48pm",
    "body": "Hi all, We have a logstash pipeline which is configured to collect data from Vertica Analytic v9.0.1-20190201 (https://www.vertica.com/)... it all worked well with the logstash version 7.4. with the configureation jdbc { jdbc_validate_connection => true jdbc_connection_string => \"jdbc:vertica://xx.xx.xx.xx:5433/myDB\" jdbc_user => \"xxxx\" jdbc_password => \"xxxx\" jdbc_driver_library => \"/opt/vertica/java/lib/vertica-jdbc-9.3.0-0.jar\" jdbc_driver_class => \"com.vertica.jdbc.Driver\" statement => \"SELECT * FROM FOO WHERE event_start > TIMESTAMPADD (n, -15, CURRENT_TIMESTAMP);\" } But since we upgraded to version 7.5 we are getting errors: Unable to find driver class via URLClassLoader in given driver jcom.vertica.jdbc.Driver and com.vertica.jdbc.Driver Any Idea how can we solve this?",
    "website_area": "discuss"
  },
  {
    "id": "66d3c865-c89b-43e2-b6c9-7799bb14bf7c",
    "url": "https://discuss.elastic.co/t/sql-last-value-store-value-even-no-document-indexed/213695",
    "title": "Sql_last_value, store value even no document indexed",
    "category": [
      "Logstash"
    ],
    "author": "pilo",
    "date": "January 3, 2020, 12:23pm January 3, 2020, 3:49pm January 4, 2020, 1:40am January 6, 2020, 5:56pm January 7, 2020, 1:42pm January 7, 2020, 4:34pm",
    "body": "Hello everyone, I'm using logstash jdbc to transform my data in postgresql to Elasticsearch. Because my sql query is too large, about 1 millions records, i have to use schedule to transform about 5 row each time, and it repeats every second. I use tracking_column to track ID column. The problem is that sometime there are two consecutive rows that the gap is bigger than 100 and logstash stops at that point and :sql_last_value too. My config file is like following input { jdbc { jdbc_connection_string => \"jdbc:postgresql://${POSTGRES_HOST}:${POSTGRES_PORT}/${DB_NAME}\" jdbc_driver_class => \"org.postgresql.Driver\" jdbc_user => \"${JDBC_USER}\" jdbc_password => \"${JDBC_PASSWORD}\" jdbc_paging_enabled => true use_column_value => true tracking_column_type => \"numeric\" tracking_column => \"decision_id\" last_run_metadata_path => \"/usr/share/logstash/config/decision_last_value.yml\" record_last_run => true statement => \" select d.id as decision_id, daet.content as element_content from decision d left join decision_element dae on dae.decision_id = d.id WHERE decision_id > :sql_last_value AND decision_id < :sql_last_value + 5 \" type => \"decision\" schedule => \"* * * * * *\" } } filter { mutate { add_field => { \"[my_join_field][name]\" => \"decision\" \"[my_join_field][parent]\" => \"%{case_id}\" } } } output { elasticsearch { hosts => \"elasticsearch:9200\" user => \"elastic\" password => \"changeme\" index => \"case\" } } For example, i have decision_id in two consecutive row is 14 and 120. Logstash doesn't move on at decision_id = 14 and :sql_last_value stay at 14 because there is no document between 14 < decision_id < 19. Are there anyway to update :sql_last_value or maybe another way ? Thank alot.",
    "website_area": "discuss"
  },
  {
    "id": "52ab541e-ca6c-43fd-a6f2-e346e28edad0",
    "url": "https://discuss.elastic.co/t/capture-multiple-information-for-one-match-grok/214069",
    "title": "Capture multiple information for one match grok",
    "category": [
      "Logstash"
    ],
    "author": "focheur91300",
    "date": "January 7, 2020, 2:09pm January 7, 2020, 3:43pm January 7, 2020, 3:43pm",
    "body": "Hello, I am currently stuck on a parsing topic. I am looking with a single match to recover several information from the same information to already capture. I'm working on DNS log: Jan 7 13:49:02 192.168.10.254 unbound: [33534:0] info: 192.168.1.122 proxy.gamestream.nvidia.com. A IN I need to retrieve the following information from the same log: FQDN : proxy.gamestream.nvidia.com Domaine : nvidia.com TLD : com I made 3 matches to capture each one of the information but only the first match. Testing it individually works. Below the current configuration : if \"pfsense\" in [tags] and [program] == \"unbound\" { grok { patterns_dir => [\"/usr/share/logstash/pipeline/patterns/pfsense\"] match => { \"message\" => \".*?info:\\s%{IP:IP_SOURCE}\\s%{HOSTNAME:FQDN}\\.\\s\" } match => { \"message\" => \".*?info:\\s%{IP:IP_SOURCE}\\s.*?\\.%{DOMAIN:DOMAIN}\\.\\s\" } match => { \"message\" => \".*?info:\\s%{IP:IP_SOURCE}\\s.*?\\.%{TLD:TLD}\\.\\s\" } add_tag => \"pfsense-unbound\" } } Pattern : DOMAIN [\\w-]+.[\\w-]+ TLD [\\w-]+ Thank you in advance",
    "website_area": "discuss"
  },
  {
    "id": "6da51b93-4d90-4e99-a98b-557768894cb5",
    "url": "https://discuss.elastic.co/t/how-to-loop-through-array-in-logstash/213852",
    "title": "How to loop through array in Logstash?",
    "category": [
      "Logstash"
    ],
    "author": "Ahmet_Kartal",
    "date": "January 6, 2020, 8:41am January 6, 2020, 5:59pm January 7, 2020, 6:28am January 7, 2020, 2:52pm",
    "body": "Hello, I have this data: \"url\" => [ [0] [ [0] \"\", [1] \"data\", [2] \"data1\" [3] \"9462\" ], What I want is that I need to get last element of list as assignment_1 ( I need to assing last data into this variable) and the other things need to placed in assignment_2 as \"data/data1\". I though it would be useful if have used loop over array list but could not figured it out. Think that my array lenght will be changed all the time.But again, I need last data into assignmet_1 and the other into assingment_2.",
    "website_area": "discuss"
  },
  {
    "id": "02e305bc-72ba-4b60-b120-5e7fecaf4414",
    "url": "https://discuss.elastic.co/t/logstash-seconds-part-is-missing-for-es-plugin/214048",
    "title": "Logstash - seconds part is missing for ES plugin",
    "category": [
      "Logstash"
    ],
    "author": "psiva017",
    "date": "January 7, 2020, 12:27pm",
    "body": "I need to monitor the ES. I tries with following logstash config. input{ elasticsearch { hosts => [\"http://localhost:9200\"] index => \"my_index\" query => '{\"query\":{\"match_all\": {}}}' schedule => \"0-59 * * * *\" } } output { stdout { codec => rubydebug } file { path => \"C:\\Users\\log\" } But it is not running every second. Also there is no seconds part in CRON exp. It is starting from min. Is there anything that I need to change here ? Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "7652f46b-bf15-4d02-8695-8f911d78e66c",
    "url": "https://discuss.elastic.co/t/logstash-get-index-name-from-filename/213891",
    "title": "Logstash - Get index name from filename",
    "category": [
      "Logstash"
    ],
    "author": "Lubos_Marek",
    "date": "January 6, 2020, 1:07pm January 6, 2020, 2:26pm January 7, 2020, 10:13am January 7, 2020, 10:13am",
    "body": "Hello, I am loading files by logstash and I would like to know if it possible to set index from filename. For example my files are: system1-yyyyMMdd.csv (system1-20200106.csv) system2-yyyyMMdd.csv (system2-20200106.csv) and I would like to create indexes with \"system1\" and \"system2\", so I need to separate the first part from the filename. input { file { path => [\"/usr/share/logstash/data1/*.csv\"] start_position => \"beginning\" } } filter { csv { separator => \";\" columns => [\"datetime\", \"level\", \"statuscode\", \"message\", \"endpoint\"] } } output { elasticsearch { hosts => [\"http://host.docker.internal:9200\"] index => \"index\" } }",
    "website_area": "discuss"
  },
  {
    "id": "1b90fa6a-0103-4c6a-b9cb-0e219b1193e9",
    "url": "https://discuss.elastic.co/t/cosmosdb-configuration/213997",
    "title": "CosmosDB configuration",
    "category": [
      "Logstash"
    ],
    "author": "Croos",
    "date": "January 7, 2020, 7:43am",
    "body": "Hi, Currently, I have the following config file for logstash. input { jdbc { #https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html#plugins-inputs-jdbc-record_last_run jdbc_connection_string => \"jdbc:sqlserver://localhost:1433;database=CatalogDb;user=croos;password=12345\" jdbc_driver_class => \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" jdbc_user => nil # The path to our downloaded jdbc driver jdbc_driver_library => \"C:\\Program Files (x86)\\sqljdbc6.2\\enu\\sqljdbc4-3.0.jar\" # The name of the driver class for SqlServer jdbc_driver_class => \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" # Query for testing purpose schedule => \"* * * * *\" #last_run_metadata_path => \"C:\\Software\\ElasticSearch\\logstash-6.4.0\\.logstash_jdbc_last_run\" #record_last_run => true #clean_run => true statement => \" select * from Todos\" } } output { elasticsearch { hosts => [\"localhost:9200\"] index => \"croos_test\" #document_id is a unique id, this has to be provided during syn, else we may get duplicate entry in ElasticSearch index. document_id => \"%{id}\" } stdout { codec => rubydebug } } Now I planed to replace Azure cosmos DB for SqlServer. Any sample configuration for CosmosDB connection?",
    "website_area": "discuss"
  },
  {
    "id": "867ba79e-8d30-434f-808d-0caa65041505",
    "url": "https://discuss.elastic.co/t/what-does-this-error-mean/213971",
    "title": "What does this error mean",
    "category": [
      "Logstash"
    ],
    "author": "volcano",
    "date": "January 7, 2020, 7:20am",
    "body": "what does this logstash error mean \"status\"=>400, \"error\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"Can't merge a non object mapping [batch-upload-usage.costcalculation.elapsed-time] with an object mapping [batch-upload-usage.costcalculation.elapsed-time]\"}}}} this is my logstash filter mutate { split => [\"batch-upload-usage.costcalculation_4\",\"=\"] add_field => {\"batch-upload-usage.costcalculation.elapsed-time\" =>\"%{[batch-upload-usage.costcalculation_4][0]}\"} add_field => {\"batch-upload-usage.costcalculation.elapsed-time.value\" =>\"%{[batch-upload-usage.costcalculation_4][1]}\"} } How to fix this ?",
    "website_area": "discuss"
  },
  {
    "id": "0fca9041-f19b-4f64-8f5f-af04e62707f2",
    "url": "https://discuss.elastic.co/t/default-index-lifecycle-management-period/213977",
    "title": "Default index lifecycle management period",
    "category": [
      "Logstash"
    ],
    "author": "suraj_kumar3",
    "date": "January 7, 2020, 5:08am",
    "body": "What is the default lifecycle of indexes of .monitoring-kibana and .monitoring-es?",
    "website_area": "discuss"
  },
  {
    "id": "7f953277-3576-4e24-bfd1-3f6ccd1cb5eb",
    "url": "https://discuss.elastic.co/t/watcher-to-monitor-logstash-pipeline-events-emitted-rate/213958",
    "title": "Watcher to monitor Logstash Pipeline \"Events Emitted Rate\"",
    "category": [
      "Logstash"
    ],
    "author": "SSS1",
    "date": "January 7, 2020, 12:10am January 7, 2020, 4:16am",
    "body": "Hi, I need to create a Watcher to alert when a pipeline's \"Events Emitted Rate\" value drops below a certain threshold. Is this possible to do in the current monitoring tools? I've tried to run querying myself, but can only find the total emitted over the lifetime duration.",
    "website_area": "discuss"
  },
  {
    "id": "6267cb73-df63-47ac-aa4d-def58c8ee4f4",
    "url": "https://discuss.elastic.co/t/logstash-jdbc-and-firebird/213393",
    "title": "Logstash JDBC and Firebird",
    "category": [
      "Logstash"
    ],
    "author": "Osiel",
    "date": "December 30, 2019, 11:53pm January 6, 2020, 6:09pm January 6, 2020, 6:36pm January 7, 2020, 2:57am",
    "body": "I need some help setting up logstash with jdbc and firebird 2.5 database, I'm using version 7.5 of elastic and logstash. What happens is that I always need to overwrite the information in my indexes, ie will always have the same amount of record but always updated because my filter is by date, I'm doing this because I'm testing on a dashboard. The problem is that every time he enters the information he mixes the records already tried several configurations as the manual does not evolve below my test setup. In summary the result of each select is mixed with the subsequent one. *** I used google translator so I'm sorry anything in english input{ jdbc { jdbc_driver_library => \"C:\\Users\\Siamaco\\Downloads\\Jaybird-3.0.8-JDK_1.8\\jaybird-full-3.0.8.jar\" jdbc_driver_class => \"org.firebirdsql.jdbc.FBDriver\" jdbc_connection_string => \"jdbc:firebirdsql:127.0.0.1/3050:C:\\_Developer\\_SVN\\Siamaco\\ATIVOS\\Hypar\\hotFix-4000\\Projetos\\exec\\BASEDADOS.FDB\" jdbc_user => \"SYSDBA\" jdbc_password => \"masterkey\" statement_filepath => \"C:\\_Developer\\Elastic\\SQL\\ContasReceber\\cheques_devolvidos.sql\" jdbc_paging_enabled => false jdbc_page_size => 0 use_column_value => false # tracking_column => \"total_cheque_devolvido\" # last_run_metadata_path => \"C:\\_Developer\\Elastic\\logstash-7.5.0\\bin\\last_run_cheques_devolvidos.txt\" record_last_run => false } } output{ elasticsearch{ hosts => [\"localhost:9200\"] index => \"cheques_devolvidos\" document_id => \"%{total_cheque_devolvido}\" } } input{ jdbc { jdbc_driver_library => \"C:\\Users\\Siamaco\\Downloads\\Jaybird-3.0.8-JDK_1.8\\jaybird-full-3.0.8.jar\" jdbc_driver_class => \"org.firebirdsql.jdbc.FBDriver\" jdbc_connection_string => \"jdbc:firebirdsql:127.0.0.1/3050:C:\\_Developer\\_SVN\\Siamaco\\ATIVOS\\Hypar\\hotFix-4000\\Projetos\\exec\\BASEDADOS.FDB\" jdbc_user => \"SYSDBA\" jdbc_password => \"masterkey\" statement_filepath => \"C:\\_Developer\\Elastic\\SQL\\ContasReceber\\ranking_clientes.sql\" jdbc_paging_enabled => false jdbc_page_size => 0 use_column_value => false # tracking_column => \"nr_cliente\" # last_run_metadata_path => \"C:\\_Developer\\Elastic\\logstash-7.5.0\\bin\\last_run_ranking_clientes.txt\" record_last_run => false } } output{ elasticsearch{ hosts => [\"localhost:9200\"] index => \"ranking_clientes\" document_id => \"%{nr_cliente}\" } } input{ jdbc { jdbc_driver_library => \"C:\\Users\\Siamaco\\Downloads\\Jaybird-3.0.8-JDK_1.8\\jaybird-full-3.0.8.jar\" jdbc_driver_class => \"org.firebirdsql.jdbc.FBDriver\" jdbc_connection_string => \"jdbc:firebirdsql:127.0.0.1/3050:C:\\_Developer\\_SVN\\Siamaco\\ATIVOS\\Hypar\\hotFix-4000\\Projetos\\exec\\BASEDADOS.FDB\" jdbc_user => \"SYSDBA\" jdbc_password => \"masterkey\" statement_filepath => \"C:\\_Developer\\Elastic\\SQL\\ContasReceber\\valor_todos_titulos_abertos.sql\" jdbc_paging_enabled => false jdbc_page_size => 0 use_column_value => false # tracking_column => \"total_titulos_abertos\" # last_run_metadata_path => \"C:\\_Developer\\Elastic\\logstash-7.5.0\\bin\\last_run_valor_todos_titulos_abertos.txt\" record_last_run => false } } output{ elasticsearch{ hosts => [\"localhost:9200\"] index => \"valor_todos_titulos_abertos\" document_id => \"%{total_titulos_abertos}\" } }",
    "website_area": "discuss"
  },
  {
    "id": "1fdadfe5-0427-47f7-bbba-c83bbaf115b6",
    "url": "https://discuss.elastic.co/t/multiple-grok-parsing-not-extracting-all-the-fields/213764",
    "title": "Multiple grok parsing not extracting all the fields",
    "category": [
      "Logstash"
    ],
    "author": "nmoham",
    "date": "January 4, 2020, 1:45am January 4, 2020, 5:29am January 6, 2020, 4:26pm January 7, 2020, 2:27am January 7, 2020, 2:28am",
    "body": "Trying to extract some fields from the msgbody field using grok , but only the first field in the gork gets extracted. interested Fields - corId, controller, httpStatusText and uri Sample Data - 2020-01-03 10:44:17,025 [93] ERROR MedServFileLogger corId=cf25b00d-1e37-4eb7-ab75-82ceeec7fdab - Exception controller= Loan action= Getmethod= GET uri= http://xxxxxxxxxx/v2/media/instance/xxxx/loans/cdb79433-32fa-4df8-b73a-e87aa89f2007/files/images-178ee8d0-fa48-4b9f-a8df-abcc9cfb1ac7.zip/entries/0b3e99f8-8af8-49a5-95b1-1537c715eb43.png?tokencreator=Encompass&tokenexpires=1578076775&token=pLCvT%2F1pBPhuFXiHKDIlB5F9feocqeq7Wxx%2FyhAz7B6DCcKeOP3YjO%2FnalfjTgXdieAmyFHEiW72Soym14oBuw%3D%3D System.UnauthorizedAccessException: MediaTokenInvalid - A valid Token must be provided for accessing Media 2020-01-03 03:58:12,822 [37] ERROR MedServFileLogger corId=5aa9b90b-9fe6-4700-aa8f-c08be2d3f0ea - Returning controller= Health action= Getmethod= GET uri= http://localhost/v2/media/healthhttpStatusCode=503 httpStatusText=ServiceUnavailable Logstash Filter - filter { if [project] == \"media_server\" { grok { match => [ \"message\", \"(?m)%{TIMESTAMP_ISO8601:logtime} [(?[\\d.]+)] +%{LOGLEVEL:loglevel} %{GREEDYDATA:msgbody}\" ] } grok { match => { break_on_match => \"false\" \"msgbody\" => [ \"corId=%{UUID:corId}\", \"controller=%{SPACE}%{WORD:controller}\", \"httpStatusText=%{WORD:httpStatusText}\", \"uri=%{SPACE}%{URI:uri}\" ] } } date { locale => \"en\" match => [\"logtime\", \"YYYY-MM-dd HH:mm:ss,SSS\"] timezone => \"America/Los_Angeles\" target => \"@timestamp\" } mutate { remove_field => [ \"msgbody\" ] } } } Using the above configuration, only the corId field is getting extracted and all other fields are dropped. I don't see any parsing errors/failures in the logstash logs. Appreciate any help or guidance with this. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "89d66653-d590-4b89-85bd-c69f969f6120",
    "url": "https://discuss.elastic.co/t/hardware-requirement-for-logstash/213831",
    "title": "Hardware requirement for logstash",
    "category": [
      "Logstash"
    ],
    "author": "Jayanth_S_N",
    "date": "January 6, 2020, 5:44am January 6, 2020, 3:15pm January 6, 2020, 11:22pm",
    "body": "What will be the best suitable hardware configuration for logstash, we are reading the data from Apache Kafka and adding a few filters in logstash config file on the different sources of data and then data will be sent to elasticsearch. We are using around 7 different sources mapped to 7 topics in Kafka and all are parsing through the logstash. We will be reading 4TB of data per day. As I am new to this, I need help in understanding how many servers do I need to allocate for logstash for stable transformation of data without any interruptions.",
    "website_area": "discuss"
  },
  {
    "id": "e8eda757-0e71-4369-9d5b-d5caf4b135f4",
    "url": "https://discuss.elastic.co/t/error-with-outside-gem-for-custom-plugin-bug-in-pipeline/213915",
    "title": "Error with outside gem for custom plugin, bug in pipeline",
    "category": [
      "Logstash"
    ],
    "author": "rkhapre",
    "date": "January 6, 2020, 6:40pm",
    "body": "Hi All I am developing a very small plugin, in which in rb file, i am using a ruby gem require 'crack' i installed this crack gem, but still i get this error I installed the external gem like this /opt/logstash/bin/ruby -S gem install crack after running the pipeline i am getting this error LoadError: no such file to load -- crack require at org/jruby/RubyKernel.java:987 require at /vendor/bundle/jruby/2.5.0/gems/pol yglot-0.3.5/lib/polyglot.rb:65 Anyone knows how to install external gems also getting this error ` java.lang.IllegalStateException: Logstash stopped processing because of an error:(LoadError) no such file to load --crack `",
    "website_area": "discuss"
  },
  {
    "id": "c969592f-1ba7-4763-a589-b5e5e80f8dcf",
    "url": "https://discuss.elastic.co/t/date-parse-failure-message-strange-behavior/213862",
    "title": "Date parse failure message strange behavior",
    "category": [
      "Logstash"
    ],
    "author": "Travis",
    "date": "January 6, 2020, 9:40am January 6, 2020, 6:14pm January 6, 2020, 6:14pm",
    "body": "Hello, Using the following filter : #log timestamp filter { date { match => [ \"timestamp\",\"UNIX\" ] target => \"log_timestamp\" timezone => \"Europe/Paris\" } } I have a [0] \"_dateparsefailure\", but target field (log_timestamp) is well created : { \"sentpkt\" => \"3\", \"date\" => \"2020-01-06\", \"srcintfrole\" => \"undefined\", \"policytype\" => \"policy\", \"severity_label\" => \"Notice\", \"service\" => \"tcp/44814\", \"dstip\" => \"1.2.3.4\", \"logid\" => \"0000000013\", \"proto\" => \"6\", \"duration\" => \"12\", \"logsource\" => \"4.5.6.7\", \"craction\" => \"262144\", \"sessionid\" => \"696656618\", \"@version\" => \"1\", \"logver\" => \"60\", \"srcintf\" => \"LAN_P1\", \"trandisp\" => \"snat\", \"srcgeoip\" => {}, \"srcport\" => \"50026\", \"type\" => \"traffic\", \"srcip\" => \"192.168.1.1\", \"eventtime\" => \"1578300095\", \"transip\" => \"1.2.3.4\", \"@timestamp\" => 2020-01-06T08:41:40.000Z, \"priority\" => 189, \"severity\" => 5, \"devname\" => \"FGT\", \"tz\" => \"UTC+1\", \"time\" => \"09:41:35\", \"log_timestamp\" => 2020-01-06T08:41:35.000Z, \"rcvdpkt\" => \"2\", \"facility\" => 23, \"tags\" => [ [0] \"_dateparsefailure\", [1] \"_geoip_lookup_failure\" ], \"action\" => \"timeout\", \"dstport\" => \"44814\", \"srccountry\" => \"Reserved\", \"crlevel\" => \"low\", \"dstintfrole\" => \"undefined\", \"appcat\" => \"unscanned\", \"level\" => \"notice\", \"dstintf\" => \"WAN_2\", \"subtype\" => \"forward\", \"crscore\" => \"5\", \"devid\" => \"FG123456\", \"rcvdbyte\" => \"176\", \"sentbyte\" => \"180\", \"facility_label\" => \"local7\", \"poluuid\" => \"d8ff503c-cdca-51e7-d709-e625957be00d\", \"vd\" => \"PRT\", \"transport\" => \"50026\", \"message\" => \"logver=60 timestamp=1578300095 tz=\\\"UTC+1\\\" devname=\\\"FGT\\\" devid=\\\"FG20\\\" vd=\\\"PORTAIL\\\" date=2020-01-06 time=09:41:35 logid=\\\"0000000013\\\" type=\\\"traffic\\\" subtype=\\\"forward\\\" level=\\\"notice\\\" eventtime=1578300095 srcip=1.2.3.4 srcport=50026 srcintf=\\\"LAN_1\\\" srcintfrole=\\\"undefined\\\" dstip=4.5.6.7 dstport=44814 dstintf=\\\"WAN_2\\\" dstintfrole=\\\"undefined\\\" poluuid=\\\"d8ff503c-cdca-51e7-d709-e625957be00d\\\" sessionid=696656618 proto=6 action=\\\"timeout\\\" policyid=63 policytype=\\\"policy\\\" service=\\\"tcp/44814\\\" dstcountry=\\\"France\\\" srccountry=\\\"Reserved\\\" trandisp=\\\"snat\\\" transip=4.5.6.7 transport=50026 duration=12 sentbyte=180 rcvdbyte=176 sentpkt=3 rcvdpkt=2 appcat=\\\"unscanned\\\" crscore=5 craction=262144 crlevel=\\\"low\\\"\\n\", \"policyid\" => \"63\", \"dstcountry\" => \"France\", \"host\" => \"1.2.3.4\", \"timestamp\" => \"1578300095\" } So I don't understand this message. What is strange is I have other logs from a similar network device with the same filter and it works without any issue : { \"time\" => \"10:06:16.665\", \"log_timestamp\" => 2020-01-06T09:06:16.000Z, \"facility\" => 23, \"session_id\" => \"00696GKc016415-00696GKd016415\", \"severity_label\" => \"Informational\", \"log_id\" => \"0300016416\", \"logsource\" => \"1.2.3.4\", \"@version\" => \"1\", \"to\" => \"'undisclosed-recipients:'@webmail.bonanga.com\", \"pri\" => \"information\", \"device_id\" => \"FE-3KD3R15000024\", \"subject\" => \"RE: Update yolo - 15/10/2019_1\", \"nested\" => \"File name: image001.png scanned by Antivirus Scannerclean\", \"type\" => \"spam\", \"dst_ip\" => \"10.255.50.205\", \"facility_label\" => \"local7\", \"@timestamp\" => 2020-01-06T09:06:16.000Z, \"client_name\" => \"webmail.bonanga.com\", \"priority\" => 190, \"client_ip\" => \"4.5.6.7\", \"vd\" => \"root\", \"message\" => \"timestamp=1578301576 date=2020-01-06 time=10:06:16.665 devname=\\\"FE-AZERTY\\\" device_id=\\\"FE-AZERTY\\\" log_id=\\\"0300016416\\\" type=\\\"spam\\\" pri=\\\"information\\\" session_id=\\\"00696GKc016415-00696GKd016415\\\" client_name=\\\"webmail.bonanga.com\\\" client_ip=\\\"4.5.6.7\\\" dst_ip=\\\"10.255.50.205\\\" from=\\\"alex@piou.fr\\\" to=\\\"'undisclosed-recipients:'@webmail.bonanga.com\\\" subject=\\\"RE: Update yolo - 15/10/2019_1\\\" nested=\\\"File name: image001.png scanned by Antivirus Scannerclean\\\" vd=\\\"root\\\"\\n\", \"severity\" => 6, \"devname\" => \"FE-AZERTY\", \"host\" => \"192.168.160.5\", \"timestamp\" => \"1578301576\", \"from\" => \"alex@piou.fr\" } Thanks for your feedback !",
    "website_area": "discuss"
  },
  {
    "id": "9cbe733e-fa78-4598-874a-e7debabc5ef1",
    "url": "https://discuss.elastic.co/t/logstash-is-getting-terminated-after-succesfully-running-3-4-hours-but-not-completing-its-job/213735",
    "title": "Logstash is getting terminated after succesfully running 3-4 hours but not completing its job",
    "category": [
      "Logstash"
    ],
    "author": "Avinaba_Ghosh_Hajra",
    "date": "January 3, 2020, 5:56pm January 3, 2020, 8:04pm January 4, 2020, 5:05pm January 5, 2020, 11:48am January 6, 2020, 5:54pm",
    "body": "We are running a pipeline that will fetch the data from oracle db table and send it to elasticsearch. We are using jdbc input plugin for that. As it is a one time activity we are running it in the following way nohup logstash-6.8.2/bin/logstash -w 1 -b 1000 -f pipelines/pipeline-oracle_db.conf & (We are using aggregation filter so that worker count is 1) As it is an one time job, expectation is logstash will be getting closed once process is completed. But after running successfully for a couple of minutes (like 2 - 3 hours ) pipeline certainly got terminated without completing the job but logstash is running in background. Not outputs are going to ElasticSearch from Logstash. [2019-12-04T15:13:43,318][INFO ][logstash.pipeline ] Pipeline has terminated {:pipeline_id=>\"main\", :thread=>\"#<Thread:0x14184562 run>\" input { jdbc { jdbc_connection_string => \"{{ db_connection_string }}\" jdbc_user => \"{{ db_user }}\" jdbc_password => \"{{ db_password }}\" jdbc_driver_class => \"Java::oracle.jdbc.driver.OracleDriver\" jdbc_driver_library => \"{{ data_volume }}/logstash-repo/lib/ojdbc6-11.2.0.3.jar\" statement => \"SELECT * from table\" jdbc_fetch_size => 100000 } } filter { ruby { code => \" hash = event.to_hash hash.each do |k,v| if v == nil event.remove(k) end end \" } aggregate { task_id => \"%{id}\" code => \"\" push_previous_map_as_event => true inactivity_timeout => 600 } ruby { path => \"{{ data_volume }}/logstash-repo/pipelines/filter/SubscriptionTypeFilter.rb\" } } output { elasticsearch { document_id => \"%{id}\" document_type => \"subscription\" index => \"index_name\" hosts => [\"https://{{ es_node }}:{{ es_http_port }}\"] user => \"{{ es_user }}\" password => \"{{ es_password }}\" template => \"{{ data_volume }}/logstash-repo/templates/template.json\" template_name => \"template\" template_overwrite => true } stdout { codec => rubydebug } }",
    "website_area": "discuss"
  },
  {
    "id": "47d013e7-89d6-4a9e-9b37-bb3f07ab27ed",
    "url": "https://discuss.elastic.co/t/unusually-rare-error-with-regard-to-logstash/213913",
    "title": "Unusually rare error with regard to logstash",
    "category": [
      "Logstash"
    ],
    "author": "PhobosDthorga",
    "date": "January 6, 2020, 5:41pm January 6, 2020, 5:53pm",
    "body": "Hello everyone! We at GekkoFyre Networks are encountering an issue with logstash where we keep experiencing the same error over and over [ 1 ], causing the service to restart repeatedly until eventually, systemd terminates the misbehaving daemon. [ 1 ] - https://paste.gekkofyre.io/view/a16eb1f2 Below you will find the configuration files relating to our Logstash setup within our metrics oriented server as well. [ 2 ] - https://paste.gekkofyre.io/view/51e0207c The virtual private server we've created for this particular workload also has plenty of resources itself, so we're pretty confident that this is not the cause at hand, with regard to the error experienced by logstash. root@metrics:~# cat /proc/cpuinfo | grep processor | wc -l 6 root@metrics:~# free -h total used free shared buff/cache available Mem: 11G 4.4G 2.5G 39M 4.9G 7.1G Swap: 2.0G 0B 2.0G [root@norwaro ~]# inxi -C CPU: Topology: Quad Core model: Intel Xeon E3-1270 v6 bits: 64 type: MT MCP L2 cache: 8192 KiB Speed: 1108 MHz min/max: 800/4200 MHz Core speeds (MHz): 1: 1108 2: 908 3: 1150 4: 1056 5: 1398 6: 1137 7: 1361 8: 2218 [root@norwaro ~]# We've probably allocated too much RAM at this point in time actually, so I might have to reduce it by around 4 GB. But we'd appreciate any and all help on this matter, thank you. We are ultimately trying to setup snesis Lite for Suricata under Kibana and ElasticSearch.",
    "website_area": "discuss"
  },
  {
    "id": "ee60eb75-e2f2-457f-896d-544db5e4ce43",
    "url": "https://discuss.elastic.co/t/dlq-rotate-or-retention/213860",
    "title": "DLQ rotate or retention",
    "category": [
      "Logstash"
    ],
    "author": "Aviad_Bitton",
    "date": "January 6, 2020, 9:29am January 6, 2020, 5:40pm",
    "body": "Hi, I am using Logstash 6.8 and i intend to use dlq for this purpose: If an event is not entered the Elastic cluster because of mapping conflict or something else, i want to show that log in a specific index that tells why, I am able to get the reason and show it in Kibana. So this is great. What i am struggling about is the retention of those dlq logs in the logstash server. It does not clear itself, and only got a limit on the size of it. Is there any option to do retention or rotate to those logs so that the dlq can accept events and not limit it and ignore them? Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "d82f7005-c300-4f2d-babe-f2dc83c73bb5",
    "url": "https://discuss.elastic.co/t/combining-two-documents-by-aggregating-common-field/213622",
    "title": "Combining two documents by aggregating common field",
    "category": [
      "Logstash"
    ],
    "author": "JeremyP",
    "date": "January 2, 2020, 8:13pm January 6, 2020, 3:40pm January 2, 2020, 11:58pm January 3, 2020, 12:22am January 3, 2020, 1:09am January 3, 2020, 4:07am January 3, 2020, 4:23pm January 6, 2020, 4:14am January 6, 2020, 3:15pm January 6, 2020, 3:40pm",
    "body": "Hello, I'm using the JDBC input plugin to ingest vulnerability data from a Rapid7 postgres database. Everything is fine, but one of the tables I'm referencing has many rows which is causing significant duplication of my data. My goal is to have one IP address mapped to one unique vulnerability. Unfortunately the field \"Vulnerability Reference IDs\" contains the multiple rows and I\"m hoping to have them combined. Below are two examples which I'm hoping to combine into a single record. I can't do anything on the database side unfortunately as it's proprietary. I'd appreciate any help!!! Record #1 { \"_index\": \"idx_test\", \"_type\": \"_doc\", \"_id\": \"KgegNG8BKC_Gpwy8RA2U\", \"_version\": 1, \"_score\": 0, \"_source\": { \"Vulnerability CVSSv3 Score\": null, \"Asset IP Address\": \"1.2.3.4/32\", \"Vulnerability Reference IDs\": \"CVE-2016-3262\", \"Asset OS Version\": \"SP1\", \"Vulnerability Severity\": \"Critical\", \"Vulnerability Description\": \"\\n \\n A remote code execution vulnerability exists due to the way the Windows GDI component handles objects in the memory. An attacker who successfully exploited this vulnerability could take control of the affected system. An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights. Users whose accounts are configured to have fewer user rights on the system could be less impacted than users who operate with administrative user rights.\\n \", \"service\": null, \"credential_status\": \"All credentials successful\", \"type\": \"TEST\", \"@timestamp\": \"2019-12-23T21:18:35.850Z\", \"asset_id\": 1090, \"port\": null, \"tag\": \"TEST\", \"Asset OS Family\": \"Windows\", \"fix\": \"\\nDownload and apply the patch from: \\n<a href=\"http://support.microsoft.com/kb/4019108\">http://support.microsoft.com/kb/4019108\", \"protocol\": null, \"Site Name\": \"TEST-SITE\", \"mac_address\": \"00:50:56:a7:c2:a1\", \"last_assessed_for_vulnerabilities\": \"2019-12-12T08:31:07.107Z\", \"Vulnerability CVSSv3 Vector\": null, \"Asset Names\": \"TARGET-WIN764\", \"Vulnerability Proof\": \" Vulnerable OS: Microsoft Windows 7 Professional Edition SP1 Based on the following 2 results: Found an applicable package: HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Component Based Servicing\\Packages\\Microsoft-Windows-Client-Features-Package~31bf3856ad364e35~amd64~~6.1.7601.17514. HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\SideBySide\\Winners\\amd64_Microsoft-Windows-WebDAVRedir-ClientOnly_31bf3856ad364e35_none_d672e50a093eb855 - key exists The above CBS component is currently version 6.1.7600.16385, expected version 6.1.7601.23542 or higher Fix for KB3192391 is applicable for this CBS component HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersionUBR - defaults to 0 \", \"@version\": \"1\", \"Asset OS Name\": \"Windows 7 Professional Edition\", \"Vulnerability Title\": \"MS16-120: Security Update for Microsoft Graphics Component (3192884)\" }, \"fields\": { \"last_assessed_for_vulnerabilities\": [ \"2019-12-12T08:31:07.107Z\" ], \"@timestamp\": [ \"2019-12-23T21:18:35.850Z\" ] }, } Record #2 { \"_index\": \"idx_test\", \"_type\": \"_doc\", \"_id\": \"LAegNG8BKC_Gpwy8RA2U\", \"_version\": 1, \"_score\": 0, \"_source\": { \"Vulnerability CVSSv3 Score\": null, \"Asset IP Address\": \"1.2.3.4/32\", \"Vulnerability Reference IDs\": \"CVE-2016-3270\", \"Asset OS Version\": \"SP1\", \"Vulnerability Severity\": \"Critical\", \"Vulnerability Description\": \"\\n \\n A remote code execution vulnerability exists due to the way the Windows GDI component handles objects in the memory. An attacker who successfully exploited this vulnerability could take control of the affected system. An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights. Users whose accounts are configured to have fewer user rights on the system could be less impacted than users who operate with administrative user rights.\\n \", \"service\": null, \"credential_status\": \"All credentials successful\", \"type\": \"TEST\", \"@timestamp\": \"2019-12-23T21:18:35.850Z\", \"asset_id\": 1090, \"port\": null, \"tag\": \"TEST\", \"Asset OS Family\": \"Windows\", \"fix\": \"\\nDownload and apply the patch from: \\n<a href=\"http://support.microsoft.com/kb/4019108\">http://support.microsoft.com/kb/4019108\", \"protocol\": null, \"Site Name\": \"TEST-SITE\", \"mac_address\": \"00:50:56:a7:c2:a1\", \"last_assessed_for_vulnerabilities\": \"2019-12-12T08:31:07.107Z\", \"Vulnerability CVSSv3 Vector\": null, \"Asset Names\": \"TARGET-WIN764\", \"Vulnerability Proof\": \" Vulnerable OS: Microsoft Windows 7 Professional Edition SP1 Based on the following 2 results: Found an applicable package: HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Component Based Servicing\\Packages\\Microsoft-Windows-Client-Features-Package~31bf3856ad364e35~amd64~~6.1.7601.17514. HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\SideBySide\\Winners\\amd64_Microsoft-Windows-WebDAVRedir-ClientOnly_31bf3856ad364e35_none_d672e50a093eb855 - key exists The above CBS component is currently version 6.1.7600.16385, expected version 6.1.7601.23542 or higher Fix for KB3192391 is applicable for this CBS component HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersionUBR - defaults to 0 \", \"@version\": \"1\", \"Asset OS Name\": \"Windows 7 Professional Edition\", \"Vulnerability Title\": \"MS16-120: Security Update for Microsoft Graphics Component (3192884)\" }, \"fields\": { \"last_assessed_for_vulnerabilities\": [ \"2019-12-12T08:31:07.107Z\" ], \"@timestamp\": [ \"2019-12-23T21:18:35.850Z\" ] }, } What I'd like to do is combine the strings in \"Vulnerability Reference IDs\" so I end up with a single record. In doing a compare of both records only this field as well as the document ID are unique. Some vendors like Microsoft link multiple KB's to the same vulnerability for different version of Windows - I have instances where I have over 100 records for the same IP to Vulnerability record. The only downfall that I can see in combining this, it may be more taxing to the db to perform queries against this string if it's concatenated with another. Combined Record... { \"_index\": \"idx_test\", \"_type\": \"_doc\", \"_id\": \"KgegNG8BKC_Gpwy8RA2U\", \"_version\": 1, \"_score\": 0, \"_source\": { \"Vulnerability CVSSv3 Score\": null, \"Asset IP Address\": \"1.2.3.4/32\", \"Vulnerability Reference IDs\": \"CVE-2016-3262, CVE-2016-3270\" \"Asset OS Version\": \"SP1\", \"Vulnerability Severity\": \"Critical\", \"Vulnerability Description\": \"\\n \\n A remote code execution vulnerability exists due to the way the Windows GDI component handles objects in the memory. An attacker who successfully exploited this vulnerability could take control of the affected system. An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights. Users whose accounts are configured to have fewer user rights on the system could be less impacted than users who operate with administrative user rights. \\n \", \"service\": null, \"credential_status\": \"All credentials successful\", \"type\": \"TEST\", \"@timestamp\": \"2019-12-23T21:18:35.850Z\", \"asset_id\": 1090, \"port\": null, \"tag\": \"TEST\", \"Asset OS Family\": \"Windows\", \"fix\": \"\\n Download and apply the patch from: \\nhttp://support.microsoft.com/kb/4019108 \", \"protocol\": null, \"Site Name\": \"TEST-SITE\", \"mac_address\": \"00:50:56:a7:c2:a1\", \"last_assessed_for_vulnerabilities\": \"2019-12-12T08:31:07.107Z\", \"Vulnerability CVSSv3 Vector\": null, \"Asset Names\": \"TARGET-WIN764\", \"Vulnerability Proof\": \" Vulnerable OS: Microsoft Windows 7 Professional Edition SP1 Based on the following 2 results: Found an applicable package: HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Component Based Servicing\\Packages\\Microsoft-Windows-Client-Features-Package~31bf3856ad364e35~amd64~~6.1.7601.17514. HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\SideBySide\\Winners\\amd64_Microsoft-Windows-WebDAVRedir-ClientOnly_31bf3856ad364e35_none_d672e50a093eb855 - key exists The above CBS component is currently version 6.1.7600.16385, expected version 6.1.7601.23542 or higher Fix for KB3192391 is applicable for this CBS component HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion UBR - defaults to 0 \", \"@version\": \"1\", \"Asset OS Name\": \"Windows 7 Professional Edition\", \"Vulnerability Title\": \"MS16-120: Security Update for Microsoft Graphics Component (3192884)\" }, \"fields\": { \"last_assessed_for_vulnerabilities\": [ \"2019-12-12T08:31:07.107Z\" ], \"@timestamp\": [ \"2019-12-23T21:18:35.850Z\" ] }, } Below is my logstash configuration (lab config). \"F.reference\" is the column in question. input { jdbc { jdbc_connection_string => \"jdbc:postgresql://192.168.65.240:5432/nexpose\" jdbc_user => \"nexpose\" jdbc_password => \"mysecretpassword\" jdbc_driver_class => \"org.postgresql.Driver\" #jdbc_paging_enabled => true #jdbc_page_size => \"5\" #jdbc_fetch_size => 10 clean_run => true statement => \"SELECT A.asset_id, CAST(mac_address AS varchar), A.sites, A.host_name, CAST(ip_address AS varchar), A.os_name, A.os_version, A.os_family, A.credential_status, A.last_assessed_for_vulnerabilities, C.name AS Tag, E.title, E.severity, E.cvss_v3_score, E.cvss_v3_vector, E.description, F.reference, K.fix, I.proof, I.service, I.port, I.protocol from dim_asset A LEFT JOIN dim_asset_tag B ON A.asset_id = B.asset_id LEFT JOIN dim_tag C on B.tag_id = C.tag_id LEFT JOIN fact_asset_vulnerability_finding_exploit_remediation D on A.asset_id = D.asset_id LEFT JOIN dim_vulnerability E ON D.vulnerability_id = E.vulnerability_id LEFT JOIN dim_vulnerability_reference F ON D.vulnerability_id = F.vulnerability_id LEFT JOIN fact_asset_vulnerability_instance I ON E.vulnerability_id = I.vulnerability_id AND A.asset_id = I.asset_id LEFT JOIN dim_asset_vulnerability_finding_rollup_solution J ON A.asset_id = J.asset_id AND E.vulnerability_id = J.vulnerability_id LEFT JOIN dim_solution K ON J.solution_id = K.solution_id where C.name = 'TEST'\" type => \"TEST\" } } filter { mutate { rename => [\"sites\", \"Site Name\" ] rename => [\"host_name\", \"Asset Names\"] rename => [\"ip_address\", \"Asset IP Address\" ] rename => [\"os_name\", \"Asset OS Name\" ] rename => [\"os_version\", \"Asset OS Version\" ] rename => [\"os_family\", \"Asset OS Family\" ] rename => [\"vulnerability_id\", \"Vulnerability ID\"] rename => [\"title\", \"Vulnerability Title\"] rename => [\"severity\", \"Vulnerability Severity\"] rename => [\"cvss_v3_score\", \"Vulnerability CVSSv3 Score\"] rename => [\"cvss_v3_vector\", \"Vulnerability CVSSv3 Vector\"] rename => [\"description\", \"Vulnerability Description\"] rename => [\"source\", \"Vulnerability Source\"] rename => [\"reference\", \"Vulnerability Reference IDs\"] rename => [\"proof\", \"Vulnerability Proof\"] } } output { if [type] == \"TEST\" { elasticsearch { hosts => [\"192.168.65.240:9200\"] index => \"idx_test\" } } stdout {} }",
    "website_area": "discuss"
  },
  {
    "id": "9556b0cb-2673-49dc-9f27-eef07c8a84df",
    "url": "https://discuss.elastic.co/t/parse-a-gmail-file/213889",
    "title": "Parse a gmail file",
    "category": [
      "Logstash"
    ],
    "author": "Yasso",
    "date": "January 6, 2020, 12:43pm",
    "body": "hello guys .. can i parse a gmail file (i have download my gmail count(mygmailfile.mbox) ) with logstrash?.. what i need exactly is: what is the structure of the gmail .. how my cofig file should look like ..what patterns to use i would be so glad if there is some codes example thank you in advance",
    "website_area": "discuss"
  },
  {
    "id": "36c6932f-4769-4a5d-9ef7-6d8204a1d231",
    "url": "https://discuss.elastic.co/t/use-case-last-login-time-for-user/213700",
    "title": "[Use Case] Last Login Time for User",
    "category": [
      "Logstash"
    ],
    "author": "rohus24",
    "date": "January 3, 2020, 12:16pm January 3, 2020, 3:55pm January 6, 2020, 12:35pm",
    "body": "Hi, I have created an index which is populated with event logs from my application. I would like to attempt to do the following using logstash: Determine all user login events (using pre-defined tags in logstash configuration file) and move them to a separate index. This index should contain only unique login events for any given user (username is parsed to the field \"username\") For any new login event received for the same user, update the @timestamp field of the login event received for that user in the 2nd index which is storing login events only. The purpose of this activity is to identify the last login time for a given user and hence identify dormant users. This would be a multi-document index whose data can be viewed as a saved search with key fields user name and @timestamp. I read some topics in the forum related to this and noted that the document_id field can be used to update the existing document but this is limited to single document indices only. Can anyone please let me know if the above requirement can be achieved ? If yes, please guide me on how to achieve the same. Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "7ae205a3-32db-4c0c-8166-16bd072776d6",
    "url": "https://discuss.elastic.co/t/use-the-data-provide-jdbc-input-to-send-output-email-and-sns/213880",
    "title": "USE the data provide JDBC INPUT to send output email and SNS",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "January 6, 2020, 11:29am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "09c06ad2-1370-4e08-b65c-40a5fc7fcf3d",
    "url": "https://discuss.elastic.co/t/logstash-rabbitmq-input-with-sac/213866",
    "title": "Logstash RabbitMQ input with SAC",
    "category": [
      "Logstash"
    ],
    "author": "cenemoi",
    "date": "January 6, 2020, 9:53am",
    "body": "Hello, I need to add SAC on a queue. My conf : input { rabbitmq { ... arguments => {\"x-single-active-consumer\" => true} #passive => true (SAC working by another way) } } The warn : [WARN ] 2020-01-06 09:44:50.092 <rabbitmq] rabbitmq - Error while setting up connection for rabbitmq input! Will retry. {:message=>\"#method<channel.close>(reply-code=406, reply-text=PRECONDITION_FAILED - invalid arg 'x-single-active-consumer' for queue 'xxx' in vhost 'xxx': {unacceptable_type,longstr}, class-id=50, method-id=10)\", :class=>\"MarchHare::ChannelAlreadyClosed\", :location=>\"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/march_hare-4.1.1-java/lib/march_hare/exceptions.rb:121:in `convert_and_reraise'\"} And the queue is not created. Input version : logstash-input-rabbitmq (6.0.4) Any advice ? Thx !",
    "website_area": "discuss"
  },
  {
    "id": "b219e14f-737e-49c2-aee7-01ca5e341884",
    "url": "https://discuss.elastic.co/t/how-to-edit-logs-with-gork/213850",
    "title": "How to Edit logs with gork?",
    "category": [
      "Logstash"
    ],
    "author": "Coder_HK",
    "date": "January 6, 2020, 8:32am January 6, 2020, 9:25am",
    "body": "I have a log winlog.evend_data.RuleName : technique_Id=T2343,techniqueName=asdfdasd I want to change it to winlog.evend_data.RuleName : technique_Id : T4343 , techniquw_name : asdfdasd I just need the regular expression for this. NOTE: Please don't share useless links be specific .. I don't need generic solutions. And Don't post Gork tutorials videos etc",
    "website_area": "discuss"
  },
  {
    "id": "299bea34-9296-4cb4-bf63-3e9bfbe20d0a",
    "url": "https://discuss.elastic.co/t/too-many-socket-connection-to-coordinating-nodes/211689",
    "title": "Too many socket connection to coordinating nodes?",
    "category": [
      "Logstash"
    ],
    "author": "Travis",
    "date": "December 12, 2019, 4:28pm December 16, 2019, 8:04am December 17, 2019, 9:30pm January 6, 2020, 9:20am December 20, 2019, 8:20am January 6, 2020, 9:22am",
    "body": "Hello, While investigating on \"[main] Marking url as dead.\" errors, I noticed that I have lots of socket connection established from my logstash to the two coordinating nodes : netstat -nat | grep 9200 | wc -l 93 Logstash output conf : output { if [identifiant] == \"CUSTOMER1\" { elasticsearch { ilm_enabled => true hosts => [\"https://192.168.145.7:9200\", \"https://192.168.145.8:9200\"] user => \"elastic\" password => \"pasword\" ssl => true cacert => \"/etc/logstash/certs/ca.pem\" index => \"ims_customer1_logs\" } } } output { if [identifiant] == \"CUSTOMER2\" { elasticsearch { ilm_enabled => true hosts => [\"https://192.168.145.7:9200\", \"https://192.168.145.8:9200\"] user => \"elastic\" password => \"pasword\" ssl => true cacert => \"/etc/logstash/certs/ca.pem\" index => \"ims_customer2_logs\" } } } output { if [identifiant] == \"CUSTOMER3\" { elasticsearch { ilm_enabled => true hosts => [\"https://192.168.145.7:9200\", \"https://192.168.145.8:9200\"] user => \"elastic\" password => \"pasword\" ssl => true cacert => \"/etc/logstash/certs/ca.pem\" index => \"ims_customer3_logs\" } } } output { if [identifiant] == \"CUSTOMER4\" { elasticsearch { ilm_enabled => true hosts => [\"https://192.168.145.7:9200\", \"https://192.168.145.8:9200\"] user => \"elastic\" password => \"pasword\" ssl => true cacert => \"/etc/logstash/certs/ca.pem\" index => \"ims_customer4_logs\" } } } Do you think this is a normal behavior ? maybe this is related to the lost connection to elasticsearch errors ? Thanks for your help !",
    "website_area": "discuss"
  },
  {
    "id": "eca2240b-0f07-466b-aa5d-57838b7b1333",
    "url": "https://discuss.elastic.co/t/logstash-keystore-permisson-denied/213590",
    "title": "Logstash keystore Permisson denied",
    "category": [
      "Logstash"
    ],
    "author": "zeleznypa",
    "date": "January 2, 2020, 3:06pm January 5, 2020, 4:47am January 6, 2020, 6:56am January 6, 2020, 7:01am January 6, 2020, 7:20am January 6, 2020, 7:41am",
    "body": "We have configured smb shared folder between elastic stack servers, that contain the logstash.keystore file. This storage is mounted in for example /mnt/elastic folder the file /mnt/elastic/logstash.keystore is symlinked into /etc/logstash/logstash.keystore and for sure also into /usr/share/logstash/config/logstash.keystore. The environment variables are setuped. When i run following command, It works well: ./bin/logstash-keystore list When I try to remove not existing key, it also properly detect, that key does not exists But when I try to add a new key, it fail for following error: ERROR] 2020-01-02 15:58:33.245 [main] secretstorecli - Error while trying to store secret urn:logstash:secret:v1:new_key {:cause=>java.io.IOException: Permission denied, :backtrace=>[\"org.logstash.secret.store.backend.JavaKeyStore.persistSecret(org/logstash/secret/store/backend/JavaKeyStore.java:318)\", \"org.logstash.secret.cli.SecretStoreCli.add(org/logstash/secret/cli/SecretStoreCli.java:169)\", \"org.logstash.secret.cli.SecretStoreCli.command(org/logstash/secret/cli/SecretStoreCli.java:104)\", \"java.lang.reflect.Method.invoke(java/lang/reflect/Method.java:498)\", \"org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(org/jruby/javasupport/JavaMethod.java:425)\", \"org.jruby.javasupport.JavaMethod.invokeDirect(org/jruby/javasupport/JavaMethod.java:292)\", \"RUBY.<class:SecretStoreCli>(/usr/share/logstash/lib/secretstore/cli.rb:35)\", \"RUBY.<main>(/usr/share/logstash/lib/secretstore/cli.rb:16)\", \"org.jruby.Ruby.runInterpreter(org/jruby/Ruby.java:889)\", \"org.jruby.Ruby.runInterpreter(org/jruby/Ruby.java:893)\", \"org.jruby.Ruby.runNormally(org/jruby/Ruby.java:782)\", \"org.jruby.Ruby.runNormally(org/jruby/Ruby.java:795)\", \"org.jruby.Ruby.runFromMain(org/jruby/Ruby.java:607)\", \"org.jruby.Main.doRunFromMain(org/jruby/Main.java:412)\", \"org.jruby.Main.internalRun(org/jruby/Main.java:304)\", \"org.jruby.Main.run(org/jruby/Main.java:234)\", \"org.jruby.Main.main(org/jruby/Main.java:206)\"]} I have tried almost everything: User: logstash or elastic Group: elastic mask: 600, 660, 666 What can I do more?",
    "website_area": "discuss"
  },
  {
    "id": "a1c23b6b-1366-4418-9459-151e0353fac3",
    "url": "https://discuss.elastic.co/t/duplicate-logs-in-kibana/213562",
    "title": "Duplicate logs In Kibana",
    "category": [
      "Logstash"
    ],
    "author": "Shlok_Srivastava1",
    "date": "January 3, 2020, 6:37am January 6, 2020, 6:46am January 6, 2020, 7:20am",
    "body": "I am getting Double logs for every api hits in Kibana. Following is my logstash configuration assuming the issue is related to logstash input { beats { port => 5044 ssl => false } } output { elasticsearch { hosts => [\"localhost:9200\"] manage_template => false index => \"%{[@metadata][type]}-%{+YYYY.MM.dd}\" document_type => \"log\" } } I fixed the above issue using fingerprints and now onwards I am not getting any duplicates. However old logs are the same i.e, they are having duplicates. Is there any way to reset them?",
    "website_area": "discuss"
  },
  {
    "id": "be28a003-b298-4351-8a5b-0702f3cbe82e",
    "url": "https://discuss.elastic.co/t/aws-s3-subfolder-structure-data-pull/213661",
    "title": "Aws S3 subfolder structure data pull",
    "category": [
      "Logstash"
    ],
    "author": "suraj_kumar3",
    "date": "January 3, 2020, 6:36am January 6, 2020, 5:52am",
    "body": "Hi there, I want to pull data from aws bucket having multiple subfolders but wildcard entry is not working. please help me with this one. Regards, Suraj",
    "website_area": "discuss"
  },
  {
    "id": "924de5fa-02c3-4b83-8a59-dd50d0908556",
    "url": "https://discuss.elastic.co/t/how-to-configure-logstash-and-syslog-ng-to-send-and-recieve-logs/213773",
    "title": "How to configure Logstash and Syslog ng to send and recieve logs?",
    "category": [
      "Logstash"
    ],
    "author": "Coder_HK",
    "date": "January 4, 2020, 11:40am January 5, 2020, 3:59am January 5, 2020, 7:54am January 5, 2020, 8:22am January 5, 2020, 3:14pm January 5, 2020, 7:20pm",
    "body": "I want to send syslog ng logs to logstash, I have installed security onion as a VM and want to send those logs to logstash which is on Ubuntu (another VM) kindly tell me how to configure both syslog-ng.log and logstash.conf to send and recieve logs.",
    "website_area": "discuss"
  },
  {
    "id": "d486b234-b183-476f-8a1e-26e3c226a61e",
    "url": "https://discuss.elastic.co/t/beats-not-parsed/213815",
    "title": "Beats not parsed",
    "category": [
      "Logstash"
    ],
    "author": "omrip",
    "date": "January 5, 2020, 5:31pm",
    "body": "when using beats as input the data show constantly to the console and seems all scrambled, what can be the reason? input { beats { port => 5044 } } filter { if \"beats_input_codec_plain_applied\" in [tags] { mutate { remove_tag => [\"beats_input_codec_plain_applied\"] } } } output { azure_loganalytics { customer_id => \"xxxxxx\" shared_key => \"xxxxxx\" log_type => \"Syslog\" time_generated_field => \"iso8610timestamp\" key_names => ['cloud','message','winlog','instance','agent','host','tags'] key_types => {'cloud'=>'string' 'message'=>'string' 'winlog'=>'string' 'instance'=>'string' 'tags'=>'string' agent=>'string' host=>'string'} flush_items => 10 flush_interval_time => 5 } if \"machine1\" in [tags] { azure_loganalytics { customer_id => \"xxxxxx\" shared_key => \"xxxxxx\" log_type => \"Syslog\" time_generated_field => \"iso8610timestamp\" key_names => ['cloud','message','winlog','instance','agent','host','tags'] key_types => {'cloud'=>'string' 'message'=>'string' 'winlog'=>'string' 'instance'=>'string' 'tags'=>'string' agent=>'string' host=>'string'} flush_items => 10 flush_interval_time => 5 } } else if \"Machine2\" in [tags]{ azure_loganalytics { customer_id => \"xxxxxx\" shared_key => \"xxxxxx\" log_type => \"Syslog\" time_generated_field => \"iso8610timestamp\" key_names => ['cloud','message','instance','winlog','agent','host','tags'] key_types => {'cloud'=>'string' 'message'=>'string' 'winlog'=>'string' 'instance'=>'string' 'tags'=>'string' agent=>'string' host=>'string'} flush_items => 10 flush_interval_time => 5 } } else { azure_loganalytics { customer_id => \"xxxxxx\" shared_key => \"xxxxxx\" log_type => \"Syslog\" time_generated_field => \"iso8610timestamp\" key_names => ['cloud','message','instance','winlog','agent','host','tags'] key_types => {'cloud'=>'string' 'message'=>'string' 'winlog'=>'string' 'instance'=>'string' 'tags'=>'string' agent=>'string' host=>'string'} flush_items => 10 flush_interval_time => 5 } } } example log file: \"instance\":{\"name\":\"Opwindows-machine\",\"id\":\"5xxxxxxx\"},\"region\":\"westeurope\",\"machine\":{\"type\":\"Standard_D2_v2\"},\"provider\":\"az\"},\"message\":\"The handle to an object was closed.\\n\\nSubjec t :\\n\\tSecurity ID:\\t\\tS-1-5-18\\n\\tAccount Name:\\t\\tDomain-WIN10-VM$\\n\\tAccount Domain:\\t\\tDomain\\n\\tLogon ID:\\t\\t0x3E7\\n\\nObject:\\n\\tObject Server:\\t\\tSecurity\\n\\tHandle ID:\\t\\t0x1d2c\\n\\nProc ess Information:\\n\\tProcess ID:\\t\\t0x214\\n\\tProcess Name:\\t\\tC:\\WindowsAzure\\GuestAgent_2.7.41491.949_2019-12-11_215454\\CollectGuestLogs.exe\",\"winlog\":{\"keywords\":[\"Audit Success\"],\"compu ter_name\":\"Domain-Win10-VM.Domain.local\",\"opcode\":\"Info\",\"api\":\"wineventlog\",\"event_id\":4658,\"process\":{\"pid\":4,\"thread\":{\"id\":2672}},\"channel\":\"Security\",\"task\":\"File System\",\"provider_guid\": \"{xxxxxxxx}\",\"record_id\":421390702,\"event_data\":{\"SubjectDomainName\":\"Domain\",\"ProcessId\":\"0x214\",\"ProcessName\":\"C:\\WindowsAzure\\GuestAgent_2.7.41491.949_2019-12 -11_215454\\CollectGuestLogs.exe\",\"HandleId\":\"0x1d2c\",\"ObjectServer\":\"Security\",\"SubjectUserName\":\"Domain-WIN10-VM$\",\"SubjectLogonId\":\"0x3e7\",\"SubjectUserSid\":\"S-1-5-18\"},\"provider_name\":\"Mic rosoft-Windows-Security-Auditing\"},\"agent\":{\"hostname\":\"Domain-Win10-VM\",\"id\":\"1f9a2496-2410-48bd-beba-927f4e21139c\",\"type\":\"winlogbeat\",\"version\":\"7.5.0\",\"ephemeral_id\":\"06dc3765-4886-4af5-b 47b-8652742d0d9f\"},\"host\":{\"architecture\":\"x86_64\",\"hostname\":\"Domain-Win10-VM\",\"name\":\"Domain-Win10-VM\",\"os\":{\"kernel\":\"10.0.18362.535 (WinBuild.160101.0800)\",\"family\":\"windows\",\"platform\":\"w indows\",\"name\":\"Windows 10 Pro\",\"version\":\"10.0\",\"build\":\"18362.535\"},\"id\":\"xxxxxxx\"},\"tags\":},{\"cloud\":{\"instance\":{\"name\":\"Domain-Domain-02\",\"id\":\"ad1e311f-9f0 f-438f-b9fc-2463bd849185\"},\"region\":\"westeurope\",\"machine\":{\"type\":\"Standard_DS2_v2\"},\"provider\":\"az\"},\"message\":\"A handle to an object was requested.\\n\\nSubject:\\n\\tSecurity ID:\\t\\tS-1-5-18 \\n\\tAccount Name:\\t\\tDomain-Domain-02$\\n\\tAccount Domain:\\t\\tDomain\\n\\tLogon ID:\\t\\t0x3E7\\n\\nObject:\\n\\tObject Server:\\t\\tSecurity\\n\\tObject Type:\\t\\tKey\\n\\tObject Name:\\t\\t\\REGISTRY\\USER\\.D EFAULT\\n\\tHandle ID:\\t\\t0x714\\n\\tResource Attributes:\\t-\\n\\nProcess Information:\\n\\tProcess ID:\\t\\t0x11dc\\n\\tProcess Name:\\t\\tC:\\Windows\\System32\\wbem\\WmiPrvSE.exe\\n\\nAccess Request Info rmation:\\n\\tTransaction ID:\\t\\t{00000000-0000-0000-0000-000000000000}\\n\\tAccesses:\\t\\tDELETE\\n\\t\\t\\t\\tREAD_CONTROL\\n\\t\\t\\t\\tWRITE_DAC\\n\\t\\t\\t\\tWRITE_OWNER\\n\\t\\t\\t\\tQuery key value\\n\\t\\t\\t\\tS et key value\\n\\t\\t\\t\\tCreate sub-key\\n\\t\\t\\t\\tEnumerate sub-keys\\n\\t\\t\\t\\tNotify about changes to keys\\n\\t\\t\\t\\tCreate Link\\n\\t\\t\\t\\t\\n\\tAccess Reasons:\\t\\t-\\n\\tAccess Mask:\\t\\t0xF003F\\n\\tPr ivileges Used for Access Check:\\t-\\n\\tRestricted SID Count:\\t0\",\"winlog\":{\"keywords\":[\"Audit Success\"],\"computer_name\":\"Domain-Domain-02.Domain.local\",\"opcode\":\"Info\",\"api\":\"wineventlog\",\"proce ss\":{\"pid\":4,\"thread\":{\"id\":3308}},\"event_id\":4656,\"task\":\"Registry\",\"channel\":\"Security\",\"provider_guid\":\"{xxxxxxxx}\",\"record_id\":xxxxxxxx,\"version\":1,\"event_da ta\":{\"AccessList\":\"%%1537\\n\\t\\t\\t\\t%%1538\\n\\t\\t\\t\\t%%1539\\n\\t\\t\\t\\t%%1540\\n\\t\\t\\t\\t%%4432\\n\\t\\t\\t\\t%%4433\\n\\t\\t\\t\\t%%4434\\n\\t\\t\\t\\t%%4435\\n\\t\\t\\t\\t%%4436\\n\\t\\t\\t\\t%%4437\\n\\t\\t\\t\\t\",\"AccessRe ason\":\"-\",\"ResourceAttributes\":\"-\",\"ObjectServer\":\"Security\",\"HandleId\":\"0x714\",\"ObjectType\":\"Key\",\"RestrictedSidCount\":\"0\",\"SubjectUserSid\":\"S-1-5-18\",\"TransactionId\":\"{00000000-0000-0000-0 000-000000000000}\",\"PrivilegeList\":\"-\",\"SubjectDomainName\":\"Domain\",\"ProcessId\":\"0x11dc\",\"ProcessName\":\"C:\\Windows\\System32\\wbem\\WmiPrvSE.exe\",\"SubjectUserName\":\"Domain-Domain-02$\",\"AccessM ask\":\"0xf003f\",\"ObjectName\":\"\\REGISTRY\\USER\\.DEFAULT\",\"SubjectLogonId\":\"0x3e7\"},\"provider_name\":\"Microsoft-Windows-Security-Auditing\"},\"agent\":{\"hostname\":\"Domain-Domain-02\",\"id\":\"f45cd3dd- 3afb-4d6d-9ed8-b82074876719\",\"type\":\"winlogbeat\",\"version\":\"7.5.0\",\"ephemeral_id\":\"42ec19ba-6d1c-4dad-ba2b-d334107ef836\"},\"host\":{\"architecture\":\"x86_64\",\"name\":\"Domain-Domain-02\",\"hostname\":\" Domain-Domain-02\",\"os\":{\"kernel\":\"10.0.14393.3383",
    "website_area": "discuss"
  },
  {
    "id": "be488ccb-a1f2-4774-b575-2cf6e9fc0ff8",
    "url": "https://discuss.elastic.co/t/about-the-elasticsearch-category/21",
    "title": "About the Elasticsearch category",
    "category": [
      "Elasticsearch"
    ],
    "author": "dadoonet",
    "date": "May 9, 2019, 8:49am",
    "body": "Any questions related to Elasticsearch, including specific features, language clients and plugins. READ THIS SECTION IF IT'S YOUR FIRST POST Some useful links: elasticsearch reference guide elasticsearch user guide elasticsearch plugins elasticsearch clients other documentation If you have any trouble, please tell us as many information as possible like your technical environment, sizing, architecture, nodes... Providing a script to reproduce locally is definitely helping a lot to get quicker and more accurate responses. Please format your code using </> icon and produce full scripts like this one: DELETE index PUT index/_doc/1 { \"foo\": \"bar\" } GET index/_search { \"query\": { \"match\": { \"foo\": \"bar\" } } } A typical script like this one can be copied and pasted in Kibana Dev Console by any reader. It will definitely help to play with your example and provide a fix for your script. If you don't provide it, there is a chance that nobody can help. This is the icon to use if you are not using markdown format: image.jpg76492 16.4 KB Also be patient when waiting for an answer to your questions. This is a community forum and as such it may take some time before someone replies to your question. Not everyone on the forum is an expert in every area so you may need to wait for someone who knows about the area you are asking about to come online and have the time to look into your problem. Please see the code of conduct for more details on our code of conduct (in particular the \"be patient\" section). There are no SLAs on responses to questions posted on this forum, if you require help with an SLA on responses you should look into purchasing a subscription package that includes support with an SLA such as those offered by Elastic.",
    "website_area": "discuss"
  },
  {
    "id": "6a288aa2-e5e5-4df6-8d1e-1d6d6d00add7",
    "url": "https://discuss.elastic.co/t/es-shards-optimisation-and-improvement-to-cluster-health/216102",
    "title": "ES shards optimisation and improvement to cluster health",
    "category": [
      "Elasticsearch"
    ],
    "author": "Norman_Khine",
    "date": "January 22, 2020, 4:47pm",
    "body": "Hello, On my cluster,(version 7.1), I keep getting a Yellow state from time to time and an increase in x400 errors, currently, the cluster has 3 master nodes and 4 data nodes with 640Gb of disk space per data node. curl -X GET 'https://es_domain/_cluster/health?pretty' { \"cluster_name\" : \"elk\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 7, \"number_of_data_nodes\" : 4, \"discovered_master\" : true, \"active_primary_shards\" : 12989, \"active_shards\" : 25978, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } with 114,520,959 searchable documents the OldCollectionTime has jumped from 300,000 to 600,000 and MaxMemoryUtilisation has jumped from an average of 80% to 95% i have one node with a CPU of 80% whereas the rest are between 5% to 10% how do i further investigate these spikes and improve the overall ES cluster? any advice is much appreciated",
    "website_area": "discuss"
  },
  {
    "id": "d47dc0c4-daa2-4fd8-b9b3-369246202c58",
    "url": "https://discuss.elastic.co/t/elasticsearch-6-7-big-difference-between-parent-circuit-breaker-and-jvm-used-heap/215967",
    "title": "ElasticSearch 6.7: Big difference between parent circuit breaker and jvm used heap",
    "category": [
      "Elasticsearch"
    ],
    "author": "musfiqur",
    "date": "January 22, 2020, 3:45pm January 22, 2020, 4:19pm January 22, 2020, 4:43pm",
    "body": "Hi, We are using ES 6.7. We are looking at 2 metrics: Parent Circuit Breaker Used memory JVM used heap We see that ES circuit breaker are way off. We see that for the same timestamp: Used heap returns 17.85 GB whereas parent circuit breaker for the same node shows only 178 MB. Is not the parent circuit breaker breaker be roughly equal to JVM used heap? Why are they way off? In that case, isn't the circuit breaker will always fail to trigger? -Musfiqur",
    "website_area": "discuss"
  },
  {
    "id": "2a881806-fecf-4ef2-9b75-08db1eb77ff6",
    "url": "https://discuss.elastic.co/t/date-parse-error-in-template/216101",
    "title": "Date parse error in template",
    "category": [
      "Elasticsearch"
    ],
    "author": "vishnuvardhan",
    "date": "January 22, 2020, 4:43pm",
    "body": "after creation of template i am getting error , \"reason\"=>\"failed to parse date field [06-APR-18 12:58:13] with format [dd-MMM-yy HH:mm:ss]\", \"caused_by\"=>{\"type\"=>\"date_time_parse_exception\", \"reason\"=>\"Text '06-APR-18 12:58:13' could not be parsed at index 3\" i tried with dd-MMM-yy HH:mm:ss and dd-MMM-y HH:mm:ss same issue is there how to write specific date time match format of \"06-APR-18 12:58:13\" Please help me .",
    "website_area": "discuss"
  },
  {
    "id": "513204be-64d2-4da1-a97b-8865832281e6",
    "url": "https://discuss.elastic.co/t/search-engine-and-elasticsearch/215930",
    "title": "Search Engine and Elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "Serhii_Oleksenko",
    "date": "January 21, 2020, 5:26pm January 21, 2020, 5:42pm January 21, 2020, 6:00pm January 22, 2020, 12:42pm January 22, 2020, 4:22pm January 22, 2020, 4:40pm",
    "body": "Hello! Are there search engines that use elastic? Search Engines that are completely independent of the search giants (Google, Bing).",
    "website_area": "discuss"
  },
  {
    "id": "dab2cef8-cca4-4eea-9990-a90ce1c91f34",
    "url": "https://discuss.elastic.co/t/ansible-role-elastic-elasticsearch-how-to-set-listen-address-of-http-and-api-endpoints/216096",
    "title": "Ansible role elastic.elasticsearch - how to set listen address of http and api endpoints",
    "category": [
      "Elasticsearch"
    ],
    "author": "jdelvecchio",
    "date": "January 22, 2020, 4:13pm",
    "body": "Hi, I'm using https://github.com/elastic/ansible-elasticsearch and cannot find the variables to set the listen address of the elasticsearch endpoints. By default, they are set to listen on 'localhost' which is a problem since I cannot create a cluster since the nodes don't communicate with each others. Thanks a lot",
    "website_area": "discuss"
  },
  {
    "id": "0c4fa03e-463e-41ca-8a59-2ee35d0340de",
    "url": "https://discuss.elastic.co/t/search-elements-of-an-array-in-nested-documents/216057",
    "title": "Search elements of an array in nested documents",
    "category": [
      "Elasticsearch"
    ],
    "author": "styks90",
    "date": "January 22, 2020, 4:36pm January 22, 2020, 2:16pm January 22, 2020, 3:58pm",
    "body": "Suppose I have an index which looks like this: # dummy index PUT nested { \"mappings\": { \"properties\": { \"address\": { \"type\": \"nested\", \"properties\": { \"city\": { \"type\": \"text\" }, \"street\": { \"type\": \"text\" } } } } } } and some documents in there # dummy data POST _bulk {\"index\": {\"_index\": \"nested\", \"_id\": \"1\"}} {\"address\":[{\"city\": \"Berlin\", \"street\": \"Goethestrasse\"}]} {\"index\": {\"_index\": \"nested\", \"_id\": \"2\"}} {\"address\":[{\"city\": \"Berlin\", \"street\": \"Schillerallee\"}]} {\"index\": {\"_index\": \"nested\", \"_id\": \"3\"}} {\"address\":[{\"city\": \"Frankfurt\", \"street\": \"Goethestrasse\"}]} {\"index\": {\"_index\": \"nested\", \"_id\": \"4\"}} {\"address\":[{\"city\": \"Frankfurt\", \"street\": \"Schillerallee\"}]} {\"index\": {\"_index\": \"nested\", \"_id\": \"5\"}} {\"address\":[{\"city\": \"Berlin\", \"street\": \"Goethestrasse\"}, {\"city\": \"Frankfurt\", \"street\": \"Goethestrasse\"}]} {\"index\": {\"_index\": \"nested\", \"_id\": \"6\"}} {\"address\":[{\"city\": \"Berlin\", \"street\": \"Goethestrasse\"}, {\"city\": \"Frankfurt\", \"street\": \"Schillerallee\"}]} {\"index\": {\"_index\": \"nested\", \"_id\": \"7\"}} {\"address\":[{\"city\": \"Berlin\", \"street\": \"Schillerallee\"}, {\"city\": \"Frankfurt\", \"street\": \"Goethestrasse\"}]} {\"index\": {\"_index\": \"nested\", \"_id\": \"8\"}} {\"address\":[{\"city\": \"Berlin\", \"street\": \"Schillerallee\"}, {\"city\": \"Frankfurt\", \"street\": \"Schillerallee\"}]} How can I use a nested query on this index to find all documents with one or more addresses from an array of addresses? So if I'm inputting {\"address\":[{\"city\": \"Berlin\", \"street\": \"Goethestrasse\"}, {\"city\": \"Frankfurt\", \"street\": \"Schillerallee\"}]} I'm looking for docs with addresses that match either or both of the input addresses: 1, 4 and 6 (if scoring is possible, obviously 6 would be ranked higher). One approach would be to use a query builder, but if a solution with mustache templates is possible, we'd prefer that. Edit: As it seems this is very similar to Matching by array elements",
    "website_area": "discuss"
  },
  {
    "id": "4f017ecc-390b-43d0-8d91-40b3f04b0e26",
    "url": "https://discuss.elastic.co/t/query-cache-gets-invalidated/216032",
    "title": "Query cache gets invalidated",
    "category": [
      "Elasticsearch"
    ],
    "author": "royal_man",
    "date": "January 22, 2020, 10:30am January 22, 2020, 3:35pm January 22, 2020, 2:30pm January 22, 2020, 2:31pm January 22, 2020, 2:39pm January 22, 2020, 2:44pm January 22, 2020, 3:01pm",
    "body": "I am making this query with request_cache=true { \"query\": { \"bool\": { \"should\": [ { \"term\": { \"status\": \"online\" } }, { \"term\": { \"status\": \"available\" } }, { \"term\": { \"status\": \"admin\" } } ] } } } I have refresh interval of 1 minute. Even if I make the same query repeatedly, the cache hit count increases and sometimes miss_count increase. There are no evictions yet. Can someone help what is the issue ?",
    "website_area": "discuss"
  },
  {
    "id": "6edc094d-a725-451e-be4d-369650346a7e",
    "url": "https://discuss.elastic.co/t/logs-are-not-coming-in-logs-folder-in-windows/216005",
    "title": "Logs are not coming in logs folder in Windows",
    "category": [
      "Elasticsearch"
    ],
    "author": "naveen_kumar2",
    "date": "January 22, 2020, 7:56am January 22, 2020, 2:19pm January 22, 2020, 2:56pm",
    "body": "After Installing Elastic search in Windows 10, logs are not coming in logs folder , I have also changed the elasticsearch.yml file and update the path, still logs are not coming, Can anyone help here?",
    "website_area": "discuss"
  },
  {
    "id": "2e8c1aa6-7f1f-4a23-b25e-06b7f0e7fd57",
    "url": "https://discuss.elastic.co/t/recommendation-on-very-large-text-fields/216079",
    "title": "Recommendation on very large text fields",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jimmy",
    "date": "January 22, 2020, 2:45pm",
    "body": "Hi, I have an index which one of the fields can contain thousands of characters, it's a field which an user can input some html information, and in some cases a base64 encoded image. This field does not need to be searched, however it needs to be returned on request. What's the recommended approach when working with such large fields? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "54fcec23-0ac7-4d5c-8113-d4dc51532373",
    "url": "https://discuss.elastic.co/t/rolling-upgrade-from-5-6-to-6-8-fails-to-start/215918",
    "title": "Rolling upgrade from 5.6 to 6.8 fails to start",
    "category": [
      "Elasticsearch"
    ],
    "author": "pmadley",
    "date": "January 22, 2020, 2:40pm January 22, 2020, 2:28pm January 22, 2020, 2:29pm",
    "body": "Pretty sure I exactly followed the instructions here: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/rpm.html However tailing journalctl says the following. Anybody got any suggestions. I've very much a Dev, not Ops kind of guy : Jan 21 16:02:18 elasticsearch5.llgc.org.uk polkitd[1841]: Registered Authentication Agent for unix-process:7194:353072264 (system bus name :1.2535 [/usr/bin/pkttyagent --notify-fd 5 --fallback], object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale en_GB.UTF-8) Jan 21 16:02:18 elasticsearch5.llgc.org.uk systemd[1]: Started Elasticsearch. Jan 21 16:02:18 elasticsearch5.llgc.org.uk polkitd[1841]: Unregistered Authentication Agent for unix-process:7194:353072264 (system bus name :1.2535, object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale en_GB.UTF-8) (disconnected from bus) [... stuff ...] Jan 21 16:02:36 elasticsearch5.llgc.org.uk systemd[1]: elasticsearch.service: main process exited, code=exited, status=1/FAILURE Jan 21 16:02:36 elasticsearch5.llgc.org.uk systemd[1]: Unit elasticsearch.service entered failed state. Jan 21 16:02:36 elasticsearch5.llgc.org.uk systemd[1]: elasticsearch.service failed.",
    "website_area": "discuss"
  },
  {
    "id": "2a0fdfa8-c2b8-44a5-bbc4-6cffdae2e21b",
    "url": "https://discuss.elastic.co/t/search-for-string-starting-with-s-1-5-21/215924",
    "title": "Search for string starting with S-1-5-21",
    "category": [
      "Elasticsearch"
    ],
    "author": "agonzalez",
    "date": "January 21, 2020, 5:01pm January 22, 2020, 2:27pm",
    "body": "I am storing users SID and ACLs in elastic but when I try to search for group Users (SID: S-1-5-31-*) it returns all users that contain S or 1 or 5. If used SID.keyword you have you put the exact match SID you cant sepecify SID.keyword:S-1-5-31-* How can I do a search of users that starts with an specific SID string? and not threat - like OR ?",
    "website_area": "discuss"
  },
  {
    "id": "829668a5-996f-4e83-8fa0-6db5856a859a",
    "url": "https://discuss.elastic.co/t/query-for-automplete-feature/215949",
    "title": "Query for automplete feature?",
    "category": [
      "Elasticsearch"
    ],
    "author": "rishabh_jain",
    "date": "January 21, 2020, 8:40pm January 22, 2020, 2:21pm",
    "body": "should we use edge_ngram for autocomplete? If yes will it have any any impact on query execution? If no then what is the use of ngram if one can search using query_string query? Also let suppose want to create ngram of a field whose value is like \"fname lname email phone_number\" and all the documents will have value with this format is it feasible to create of ngrams of this field consider total documents > 1000 and each document will have this field?",
    "website_area": "discuss"
  },
  {
    "id": "bb970214-09ce-49c5-af24-8c8c6ac094a5",
    "url": "https://discuss.elastic.co/t/disk-i-o-monitor-alert-from-new-relic-for-elastic-search-nodes/215981",
    "title": "DISK I/O Monitor alert from new relic for Elastic search nodes",
    "category": [
      "Elasticsearch"
    ],
    "author": "Ajay16",
    "date": "January 22, 2020, 5:06am January 22, 2020, 6:29am January 22, 2020, 6:56am January 22, 2020, 7:28am January 22, 2020, 7:34am January 22, 2020, 9:04am January 22, 2020, 11:39am January 22, 2020, 1:39pm",
    "body": "i am getting disk i/o monitor alert from new relic for the elastic search nodes(2 nodes)., how can i resolve this issue., help me out of this.,new to elastic search",
    "website_area": "discuss"
  },
  {
    "id": "476149d5-49d4-41b4-9779-191f9e876de6",
    "url": "https://discuss.elastic.co/t/get-certain-fields-from-children-in-has-child-query/216031",
    "title": "Get certain fields from children in has-child query",
    "category": [
      "Elasticsearch"
    ],
    "author": "johnkary",
    "date": "January 22, 2020, 1:38pm",
    "body": "Hi all! I have an index with parent-child capability. I managed the has-child query to bring me specific fields from the parent with the command: _source\": [\"parent_field_1\",\"parent_field_2\"...] and it worked. Now i want to do the same for the children fields putting the same command again, but i receive the error message: \"type\": \"parsing_exception\", \"reason\": \"[has_child] query does not support [_source]\", My query: GET npk/_search { \"_source\": [\"parent_field_1\",\"parent_field_2\"...], \"query\": { \"has_child\": { \"type\": \"body\", \"min_children\": 1, \"_source\": [\"child_field_1\",\"child_field_2\"...] \"query\": { \"range\": { \"child_field_1\": { \"gte\": 5, \"lte\": 10 } } }, \"inner_hits\": {} } } } How can i choose which fields from children i receive?",
    "website_area": "discuss"
  },
  {
    "id": "8e2bf52e-5ff8-4b71-ab4f-99632c14e0be",
    "url": "https://discuss.elastic.co/t/master-node-not-discovered-exception-while-executing-cat-indices-query-in-elasticsearch/216072",
    "title": "Master node not discovered exception while executing /_cat/indices query in elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "VIGNESHkumar",
    "date": "January 22, 2020, 1:36pm",
    "body": "cannot able to execute any query in elasticsearch. Masternode not discoverd exception occurs",
    "website_area": "discuss"
  },
  {
    "id": "1365d5b7-f1be-4aeb-b004-eec67c51d9ab",
    "url": "https://discuss.elastic.co/t/build-elasticsearch-from-source-docker-problem/216062",
    "title": "Build elasticsearch from source, docker problem",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 22, 2020, 1:01pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b1563f05-79f5-4b71-8843-75574856844b",
    "url": "https://discuss.elastic.co/t/run-cluster-in-docker-on-2-hosts/216019",
    "title": "Run cluster in docker, on 2 hosts",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 22, 2020, 9:31am January 22, 2020, 10:26am January 22, 2020, 10:38am January 22, 2020, 10:40am January 22, 2020, 10:43am January 22, 2020, 10:49am January 22, 2020, 10:52am January 22, 2020, 11:04am January 22, 2020, 12:43pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a843275d-4cf2-432d-9016-26ac4320db11",
    "url": "https://discuss.elastic.co/t/consul-discovery-plugin/215960",
    "title": "Consul discovery plugin?",
    "category": [
      "Elasticsearch"
    ],
    "author": "rgarrigue",
    "date": "January 22, 2020, 12:34am January 22, 2020, 12:44am January 22, 2020, 12:26pm January 22, 2020, 12:26pm January 22, 2020, 12:31pm",
    "body": "Hi there I'm wondering if there's any plan for a discovery plugin based on Consul ? Best regards, Rmy",
    "website_area": "discuss"
  },
  {
    "id": "f91d1eab-0bbd-4112-bdb1-e4066aa367f0",
    "url": "https://discuss.elastic.co/t/throughput/215649",
    "title": "Throughput",
    "category": [
      "Elasticsearch",
      "Rally"
    ],
    "author": "kinlee",
    "date": "January 19, 2020, 10:10pm January 22, 2020, 12:30pm",
    "body": "Is it possible to find a maximum throughput (indexing, term,phase) using esrally? if so what value do we have to assign in challenges? Thanking you",
    "website_area": "discuss"
  },
  {
    "id": "396a61ee-eba3-49eb-b675-4061f877eced",
    "url": "https://discuss.elastic.co/t/replace-string-with-multiple-patterns-on-ingest-node/216039",
    "title": "Replace string with multiple patterns on ingest node",
    "category": [
      "Elasticsearch"
    ],
    "author": "AndresL",
    "date": "January 22, 2020, 10:45am January 22, 2020, 11:28am",
    "body": "Hi, i have certain numbers in format xxxx-xxxx or format xxxxxxxx that i want to replace on the ingest-node. i will also like to verify if the message contains some credit card info. It seems i cant use multiple gsubs",
    "website_area": "discuss"
  },
  {
    "id": "255b9789-ceb0-4458-8131-d8d76fcbd331",
    "url": "https://discuss.elastic.co/t/elasticsearch-needed-for-1-hour/215807",
    "title": "Elasticsearch needed for 1 hour",
    "category": [
      "Elasticsearch"
    ],
    "author": "royal_man",
    "date": "January 21, 2020, 1:29am January 21, 2020, 3:02am January 22, 2020, 10:25am January 22, 2020, 10:25am",
    "body": "I want to use elasticsearch only for 1 hour twice a day (I know specific time). The indexing can be done once per day anytime we want. Is there anyway I can design the system so that it can handle such request and not waste resources for the rest of the day. I have checked if I can achieve this using scaling of Elasticsearch but it seems that it is not possible to autoscale in Elasticsearch.",
    "website_area": "discuss"
  },
  {
    "id": "1c72f9eb-ab2a-409e-9f01-a7a577c9f25d",
    "url": "https://discuss.elastic.co/t/elk-red-status/216015",
    "title": "ELK Red status",
    "category": [
      "Elasticsearch"
    ],
    "author": "antonopo",
    "date": "January 22, 2020, 9:23am January 22, 2020, 10:06am",
    "body": "Hi, Every day i am facing an issue on my ELK cluster with red status. Some Indexes everyday are red so i am deleting them and they are recreating and after this the ELK Status is becoming green again. Here is the log of GET /_cluster/allocation/explain?pretty { \"index\" : \"metricbeat-2019.12.31\", \"shard\" : 0, \"primary\" : false, \"current_state\" : \"unassigned\", \"unassigned_info\" : { \"reason\" : \"PRIMARY_FAILED\", \"at\" : \"2020-01-22T08:28:18.081Z\", \"details\" : \"primary failed while replica initializing\", \"last_allocation_status\" : \"no_attempt\" }, \"can_allocate\" : \"no\", \"allocate_explanation\" : \"cannot allocate because allocation is not permitted to any of the nodes\", \"node_allocation_decisions\" : [ { \"node_id\" : \"46jaKQPSRC2drwy-bAfhxw\", \"node_name\" : \"xh-gr-elastic-2\", \"transport_address\" : \"10.159.166.9:9300\", \"node_attributes\" : { \"ml.machine_memory\" : \"269930561536\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" }, \"node_decision\" : \"no\", \"deciders\" : [ { \"decider\" : \"replica_after_primary_active\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" }, { \"decider\" : \"throttling\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" } ] }, { \"node_id\" : \"CBiU9s_BSai2oBfVeZjwiQ\", \"node_name\" : \"xh-fr-elastic-1\", \"transport_address\" : \"135.238.239.48:9300\", \"node_attributes\" : { \"ml.machine_memory\" : \"16654974976\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" }, \"node_decision\" : \"no\", \"deciders\" : [ { \"decider\" : \"replica_after_primary_active\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" }, { \"decider\" : \"throttling\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" } ] }, { \"node_id\" : \"EmyFlH3zS3KkvJptV02pMQ\", \"node_name\" : \"xh-gr-elastic-1\", \"transport_address\" : \"10.158.67.175:9300\", \"node_attributes\" : { \"ml.machine_memory\" : \"16654729216\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" }, \"node_decision\" : \"no\", \"deciders\" : [ { \"decider\" : \"replica_after_primary_active\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" }, { \"decider\" : \"throttling\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" } ] }, { \"node_id\" : \"WNfbhnzgTIi4y3PWM4Yg0A\", \"node_name\" : \"xh-it-elastic-2\", \"transport_address\" : \"151.98.17.61:9300\", \"node_attributes\" : { \"ml.machine_memory\" : \"269939691520\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" }, \"node_decision\" : \"no\", \"deciders\" : [ { \"decider\" : \"replica_after_primary_active\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" }, { \"decider\" : \"throttling\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" } ] }, Do you have any idea what is wrong?? Best Regards, Thanos \"node_id\" : \"bLV-av6fSAWBTep4FKAqWA\", \"node_name\" : \"xh-gr-elastic-3\", \"transport_address\" : \"10.158.67.107:9300\", \"node_attributes\" : { \"ml.machine_memory\" : \"17179332608\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" }, \"node_decision\" : \"no\", \"deciders\" : [ { \"decider\" : \"replica_after_primary_active\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" }, { \"decider\" : \"throttling\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" } ] }, { \"node_id\" : \"g9M0MIndTBW62pSrEdktzA\", \"node_name\" : \"xh-it-elastic-1\", \"transport_address\" : \"151.98.17.60:9300\", \"node_attributes\" : { \"ml.machine_memory\" : \"34359738368\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" }, \"node_decision\" : \"no\", \"deciders\" : [ { \"decider\" : \"replica_after_primary_active\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" }, { \"decider\" : \"throttling\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" } ] }, { \"node_id\" : \"obT4-IMdTU6kOAHKVFNxzA\", \"node_name\" : \"xh-fr-elastic-2\", \"transport_address\" : \"135.238.239.132:9300\", \"node_attributes\" : { \"ml.machine_memory\" : \"269930713088\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" }, \"node_decision\" : \"no\", \"deciders\" : [ { \"decider\" : \"replica_after_primary_active\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" }, { \"decider\" : \"throttling\", \"decision\" : \"NO\", \"explanation\" : \"primary shard for this replica is not yet active\" } ] } ] }",
    "website_area": "discuss"
  },
  {
    "id": "e98389d0-ba9a-46f8-bdbc-37a2bcf5c9ee",
    "url": "https://discuss.elastic.co/t/configuration-file-path-to-data-path-error/216010",
    "title": "Configuration file. Path to data: path error",
    "category": [
      "Elasticsearch"
    ],
    "author": "VIGNESHkumar",
    "date": "January 22, 2020, 9:15am January 22, 2020, 9:20am January 22, 2020, 9:30am January 22, 2020, 9:58am",
    "body": "while configuring the latest version of elasticsearch.yml file. the path where we need to store the index cannot be accessed throws an error Masternode not discovered exception. But the same elasticsearch.yml file is working in elasticsearch version 7.3.0 but not in version 7.5.1. should we need to give the path location or it will take any default location",
    "website_area": "discuss"
  },
  {
    "id": "c666c4b0-ba7c-4f0d-a9cd-dd4defb43b1e",
    "url": "https://discuss.elastic.co/t/elasticsearch-1-7-logrotation-crashes-for-clusters-with-low-logging/216017",
    "title": "Elasticsearch 1.7 logrotation crashes for clusters with low logging",
    "category": [
      "Elasticsearch"
    ],
    "author": "workaround",
    "date": "January 22, 2020, 9:48am",
    "body": "Hello, We have specific active Elasticsearch clusters whose logrotation crushes frequently. We have more than 100 elasticsearch clusters and just some specific keep crushing their logrotation. After investigation, I have come to conclude that it has something to do with the small amount of logging to those clusters. Some days there are no logs to those clusters. I have two suspects: either log4j.properties minimum size, which I could not locate ( as a file ) somewhere in the installation. Thus I wonder wether it is overriden by something or has some kind of defaults if not present. The conversion pattern of the logs which looks something like this: file: type: extrasRollingFile file: ${path.logs}/${cluster.name}.log rollingPolicy: timeBased rollingPolicy.FileNamePattern: ${path.logs}/${cluster.name}.log.%d{yyyy-MM-dd}.gz layout: type: pattern conversionPattern: \"[%d{ISO8601}][%-5p][%-25c] %m%n\" That, because some days there are no logs, so maybe if it goes through a day that cannot rotate. itremains crushed for the rest of the days as well, and needs a service restart. So do you believe it is the small size of the logs to blame or the absense of logs that blocks log rotation? log4j or logging.yml? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "45e3966d-2320-46f5-af5b-0352ed69c8f7",
    "url": "https://discuss.elastic.co/t/does-es-store-id-field-in-lucene-as-its-docid/215977",
    "title": "Does ES store _id field in Lucene as its docid?",
    "category": [
      "Elasticsearch"
    ],
    "author": "DeeeFOX",
    "date": "January 22, 2020, 4:07am January 22, 2020, 9:05am January 22, 2020, 9:32am",
    "body": "As the title asked, Does ES store _id field in Lucene as its docid? Or ES just store _id field as an index-field and each time ES write a doc, lucene create an unique docid and return to ES?",
    "website_area": "discuss"
  },
  {
    "id": "85a68b6a-06f5-4aa7-8080-5a2df22b0b88",
    "url": "https://discuss.elastic.co/t/conditional-function-score-boost-based-on-used-search-term/216016",
    "title": "Conditional function score boost based on used search term",
    "category": [
      "Elasticsearch"
    ],
    "author": "DerrickXp",
    "date": "January 22, 2020, 9:24am",
    "body": "I have client that requested a feature to apply certain boosts only when a certain search term is used. My first thought is that this should not be necessary with the right configuration and doc content. But after a demo of a view cases i can see why the client asks for this feature. My escape is to implement this feature at query build time but i'm wondering what the best practice within Elasticsearch is.",
    "website_area": "discuss"
  },
  {
    "id": "f09d4685-14fe-4046-bc8d-60fdb6c1456e",
    "url": "https://discuss.elastic.co/t/wildcard-search-with-space-in-the-text/215797",
    "title": "Wildcard search with space in the text",
    "category": [
      "Elasticsearch"
    ],
    "author": "jwlee",
    "date": "January 21, 2020, 5:02pm January 20, 2020, 9:28pm January 20, 2020, 9:37pm January 21, 2020, 5:03am January 21, 2020, 5:00pm January 21, 2020, 5:59pm January 21, 2020, 8:46pm January 22, 2020, 1:21am January 22, 2020, 1:37am January 22, 2020, 6:02am January 22, 2020, 9:15am January 22, 2020, 4:53pm",
    "body": "Wildcard search seems not working if a value contains space. For example, if a value is \"hello world\" and if you do search hel* it does not return anything. Below is the query that I used: { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"name\": \"hel*\" } } ] } } } It works if I do hello* but anything that is partial string of hello is not working. Is there a way that can make it work if I search a substring of hello such as he*, hel*, hell*?",
    "website_area": "discuss"
  },
  {
    "id": "7c5137fa-b1dc-4e5c-b1ac-be27fa13e060",
    "url": "https://discuss.elastic.co/t/search-for-non-indexed-fields-is-not-working-please-help/215996",
    "title": "Search for non indexed fields is not working Please help",
    "category": [
      "Elasticsearch"
    ],
    "author": "srivatsalya",
    "date": "January 22, 2020, 6:51am January 22, 2020, 8:58am",
    "body": "Hello, I want to filter the search results using non indexed field. { \"_index\" : \"shipmentdata_v1\", \"_type\" : \"_doc\", \"_id\" : \"128Q711U\", \"_score\" : 1.0, \"_source\" : { \"bundleId\" : 13663, \"customerId\" : 101456 } } for example customer Id. I used below and it gives me an exception: { \"query\": { \"match\": { \"customerId\": 129759 } } } ' { \"error\" : { \"root_cause\" : [ { \"type\" : \"query_shard_exception\", \"reason\" : \"failed to create query: {\\n \"match\" : {\\n \"customerId\" : {\\n \"query\" : 129759,\\n \"operator\" : \"OR\",\\n \"prefix_length\" : 0,\\n \"max_expansions\" : 50,\\n \"fuzzy_transpositions\" : true,\\n \"lenient\" : false,\\n \"zero_terms_query\" : \"NONE\",\\n \"auto_generate_synonyms_phrase_query\" : true,\\n \"boost\" : 1.0\\n }\\n }\\n}\", \"index_uuid\" : \"2lkJMMvNQju8HmwMvPeo3A\", \"index\" : \"shipmentdata_v1\" } ], \"type\" : \"search_phase_execution_exception\", \"reason\" : \"all shards failed\", \"phase\" : \"query\", \"grouped\" : true, \"failed_shards\" : [ { \"shard\" : 0, \"index\" : \"shipmentdata_v1\", \"node\" : \"uGsbj81VRGmgHjgG4bgkKA\", \"reason\" : { \"type\" : \"query_shard_exception\", \"reason\" : \"failed to create query: {\\n \"match\" : {\\n \"customerId\" : {\\n \"query\" : 129759,\\n \"operator\" : \"OR\",\\n \"prefix_length\" : 0,\\n \"max_expansions\" : 50,\\n \"fuzzy_transpositions\" : true,\\n \"lenient\" : false,\\n \"zero_terms_query\" : \"NONE\",\\n \"auto_generate_synonyms_phrase_query\" : true,\\n \"boost\" : 1.0\\n }\\n }\\n}\", \"index_uuid\" : \"2lkJMMvNQju8HmwMvPeo3A\", \"index\" : \"shipmentdata_v1\", \"caused_by\" : { \"type\" : \"illegal_argument_exception\", \"reason\" : \"Cannot search on field [customerId] since it is not indexed.\" } } } ] }, \"status\" : 400 } I also tried to use { \"filter\": { \"script\": { \"script\": \"_source['customerId'].value == 129759\" } } } but it gave below exception: { \"error\" : { \"root_cause\" : [ { \"type\" : \"parsing_exception\", \"reason\" : \"Unknown key for a START_OBJECT in [filter].\", \"line\" : 3, \"col\" : 11 } ], \"type\" : \"parsing_exception\", \"reason\" : \"Unknown key for a START_OBJECT in [filter].\", \"line\" : 3, \"col\" : 11 }, \"status\" : 400 }",
    "website_area": "discuss"
  },
  {
    "id": "64c682e5-a613-4c23-a4d7-050be01a129f",
    "url": "https://discuss.elastic.co/t/thread-pool-configuration/215683",
    "title": "Thread pool configuration",
    "category": [
      "Elasticsearch"
    ],
    "author": "Abhishek_Tiwari",
    "date": "January 20, 2020, 8:00am January 20, 2020, 8:21am January 20, 2020, 9:22am January 21, 2020, 5:11am January 22, 2020, 5:34am January 22, 2020, 6:25am January 22, 2020, 6:38am January 22, 2020, 6:47am January 22, 2020, 6:57am January 22, 2020, 8:46am",
    "body": "Hi , I am new in elastic search . here i want to know that how one should configure thread pool. My current configuration is following thread_pool: search: size: 64 queue_size: 500 min_queue_size: 10 max_queue_size: 1000 auto_queue_frame_size: 2000 target_response_time: 1s http.enabled: true I am getting rejection on it . so what would be the best way configure it . Regrads , Abhishek",
    "website_area": "discuss"
  },
  {
    "id": "3767a144-8d49-4243-a5db-78ee89adc51b",
    "url": "https://discuss.elastic.co/t/shard-has-exceeded-the-maximum-number-of-retries/215954",
    "title": "Shard has exceeded the maximum number of retries",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jonathan_Mendenhall",
    "date": "January 21, 2020, 9:40pm January 22, 2020, 8:37am",
    "body": "After moving one of our clusters from ES 6.4 to 7.5,we have been seeing frequent instances of shards failing to allocate because they hit the max of 5 retries. \"explanation\" : \"shard has exceeded the maximum number of retries [5] on failed allocation attempts - manually call [/_cluster/reroute?retry_failed=true] to retry, [unassigned_info[[reason=ALLOCATION_FAILED], at[2020-01-19T04:02:46.344Z], failed_attempts[5]... The reason given for the failure generally ends up being circuit breaker related (since the change to include real memory usage in the parent circuit breaker, we have been making adjustments, but circuits are still tripped occasionally). The circuit breakers are a separate issue, my question for now is: Is there something I can do to remove the need for manual intervention (calling /_cluster/reroute?retry_failed=true) here? I appreciate that the cluster is trying to protect itself to maintain stability, which is why I have been trying to avoid turning off the new parent circuit breaker logic; however, there's not much the cluster could do that would be worse than just silently, permanently giving up on recovering a replica. That sets us up to lose data if someone is not closely paying attention and is there to force the retry. Increasing the retry count does not seem like a real fix. I do not know how quickly the retries happen, but whenever I see this issue it generally is affecting several shards at once. I would much prefer it if I could tell the cluster to wait a while and retry again later. Or something along those lines. Edit: I should note that calling \"/_cluster/reroute?retry_failed=true\" always seems to do the trick on the first try.",
    "website_area": "discuss"
  },
  {
    "id": "6cae2fe1-604c-403c-b326-286c8ef6b42e",
    "url": "https://discuss.elastic.co/t/example-predicting-flight-delays-but-per-carrier/216007",
    "title": "Example: predicting flight delays, but per carrier",
    "category": [
      "Elasticsearch"
    ],
    "author": "Sven_Beauprez",
    "date": "January 22, 2020, 8:26am",
    "body": "I am learning about the way ES uses regression analysis and looking at the example of flight carriers, I wonder if it is possible to group on a certain field such as carrier and have predictions per carrier. In the example, I would then be able to ask for a prediction for 'ES-Air' only. Is that possible? If so what would need to be done on creating the regression model and how does the query looks like? (update, adding link to example: https://www.elastic.co/guide/en/machine-learning/7.5/flightdata-regression.html ) thnx Sven",
    "website_area": "discuss"
  },
  {
    "id": "8ad9e193-f5a4-4fbe-8f5a-f5fa72667ced",
    "url": "https://discuss.elastic.co/t/named-query-within-nested-query/215923",
    "title": "Named query within nested query",
    "category": [
      "Elasticsearch"
    ],
    "author": "meirshal",
    "date": "January 21, 2020, 4:57pm January 22, 2020, 8:17am",
    "body": "Hey folks! Ive encountered something that looked to me like a bug, involving named queries inside nested queries. Ive also found this issue on Github: https://github.com/elastic/elasticsearch/issues/46231 It looks very much like what we encounter. Does anyone know if thats supposed to be addressed in any near version? Is there something I can do to help prioritise this issue against other issues?",
    "website_area": "discuss"
  },
  {
    "id": "a11ea94c-fc40-42d0-a606-9ce10f91e8cc",
    "url": "https://discuss.elastic.co/t/unable-to-login-to-kibana/215410",
    "title": "Unable to login to Kibana",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 17, 2020, 3:23pm January 17, 2020, 3:25pm January 20, 2020, 12:46am January 20, 2020, 7:16am January 21, 2020, 12:09am January 21, 2020, 6:28am January 22, 2020, 5:30am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6ea51d58-6dd5-4c56-8334-a9236cdc093c",
    "url": "https://discuss.elastic.co/t/kibana-circuitbreakingexception/215668",
    "title": "Kibana CircuitBreakingException",
    "category": [
      "Elasticsearch"
    ],
    "author": "DG13",
    "date": "January 20, 2020, 6:09am January 22, 2020, 3:52am",
    "body": "Hi , Version - ELK 7.1.0 cluster - Single node ELK cluster installed in AWS VM Have added the date filter in Kibana dashboard , If select more than 2 dates of data getting below error All shards failed for phase: [query] org.elasticsearch.common.breaker.CircuitBreakingException: [parent] Data too large, data for [<reused_arrays>] would be [493459960/470.6mb], which is larger than the limit of [493030604/470.1mb], real usage: [493459872/470.6mb], new bytes reserved: [88/88b] at org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService.checkParentLimit(HierarchyCircuitBreakerService.java:343) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreaker.java:128) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.common.util.BigArrays.adjustBreaker(BigArrays.java:399) ~[elasticsearch-7.1.0.jar:7.1.0] When browsed I could find below options to resolve the exception Increase the JVM heap size (Where should I increase the heap size ) Update the indices.breaker.total.limit in elastic search.yml , however I couldn't find the this property in elasticsearch.yml file . https://www.elastic.co/guide/en/elasticsearch/reference/7.2/circuit-breaker.html Appreciate your help to guide me to the right direction . Thanks, Divya",
    "website_area": "discuss"
  },
  {
    "id": "36eed379-21f2-484c-be81-8ac9103eb0be",
    "url": "https://discuss.elastic.co/t/es-7-5-translog-recovery-is-extremely-slow/215505",
    "title": "ES 7.5 translog recovery is extremely slow",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jonathan_Mendenhall",
    "date": "January 17, 2020, 6:48pm January 17, 2020, 9:28pm January 17, 2020, 10:07pm January 18, 2020, 6:37am January 18, 2020, 10:33pm January 18, 2020, 6:49pm January 21, 2020, 9:10pm January 21, 2020, 9:14pm January 21, 2020, 9:47pm January 21, 2020, 10:56pm January 21, 2020, 11:41pm January 21, 2020, 11:49pm January 22, 2020, 12:25am January 22, 2020, 1:53am January 22, 2020, 2:45am",
    "body": "A sample of the response from the recovery API: \"translog\" : { \"recovered\" : 17201, \"total\" : 4686825, \"percent\" : \"0.4%\", \"total_on_start\" : -1, \"total_time_in_millis\" : 1790702 } This is on an index that is no longer being written to. The size is well below the cap, but to get 0.4% for the way through it has taken 30 minutes. At that rate, letting it complete would take far too long (although, admittedly, they seem to eventually go away on their own log before they would it that pace was maintained). The shards for this index are all around 6GB. And when I get shard stats for the index \"translog\" : { \"operations\" : 0, \"size_in_bytes\" : 55, \"uncommitted_operations\" : 0, \"uncommitted_size_in_bytes\" : 55, \"earliest_last_modified_age\" : 4316474 } Nothing about this index is special (every since moving to 7.5, I have seen this sort of translog slowness on many different indices). And I have not changed any translog settings away from their defaults. To get rid of these parasitic translog operations, I have been reducing replica counts on the index to 0 (I thought reducing from 1 to 2 would work, but it always seems to keep the replica around that is being recovered from the translog), allow the recovery operations to be abandoned, and then I increase the replica count back to its original value. At that point, the translog size is reported as 0, and recovery happens very quickly. From my perspective, we would be better off if the translog did not exist, and we instead just recovered by copying the raw data. Since, translog recovery has always been slow for us, but never this bad. Is there something about the new \"soft deletes\" (our old cluster was on ES 6.4) that I am not understanding that could be causing this? Assuming that might be to blame, I am tempted to either disable them, or dramatically reduce the lease period.",
    "website_area": "discuss"
  },
  {
    "id": "0dc6a318-7059-4738-b84f-27995e7e09bb",
    "url": "https://discuss.elastic.co/t/compare-2-indexes-with-many-fields-and-same-structure-and-set-alert-watcher-on-threshold/215965",
    "title": "Compare 2 indexes with many fields and same structure and set alert watcher on threshold",
    "category": [
      "Elasticsearch"
    ],
    "author": "adifucan",
    "date": "January 22, 2020, 12:38am",
    "body": "Hi guys! I'm looking for an advice. I have 2 indexes with many identical fields. I just want to compare values of these fields and send an email notification if these values are beyond some threshold. So is it possible to compare all values in a bulk operation and not to compare 2 indexes by each field as there are huge amount of fields so there will be a huge amount of watchers. Here is an example of my indexer: { \"index_1\":{ \"mappings\":{ \"ttfb\":{ \"properties\":{ \"Checkout Billing/Shipping Information\":{ \"type\":\"long\" }, \"Checkout Estimate Shipping Methods\":{ \"type\":\"long\" }, \"Checkout Payment Info/Place Order\":{ \"type\":\"long\" }, \"Checkout success\":{ \"type\":\"long\" }, \"Clear Shopping Cart\":{ \"type\":\"long\" }, \"Configurable Product 1 Add To Cart\":{ \"type\":\"long\" }, \"Configurable Product 1 View\":{ \"type\":\"long\" }, \"Configurable Product 2 Add To Cart\":{ \"type\":\"long\" }, \"Configurable Product 2 View\":{ \"type\":\"long\" }, \"Configurable Product 3 Add To Cart\":{ \"type\":\"long\" }, \"Configurable Product 3 View\":{ \"type\":\"long\" }, \"Get Configurable Product Options\":{ \"type\":\"long\" }, \"Load Cart Section 1\":{ \"type\":\"long\" }, \"Load Cart Section 10\":{ \"type\":\"long\" }, \"Load Cart Section 2\":{ \"type\":\"long\" }, \"Load Cart Section 3\":{ \"type\":\"long\" }, \"Load Cart Section 4\":{ \"type\":\"long\" }, \"Load Cart Section 5\":{ \"type\":\"long\" }, \"Load Cart Section 6\":{ \"type\":\"long\" }, \"Load Cart Section 7\":{ \"type\":\"long\" }, \"Load Cart Section 8\":{ \"type\":\"long\" }, \"Load Cart Section 9\":{ \"type\":\"long\" }, \"Load Customer Private Data\":{ \"type\":\"long\" }, \"Login\":{ \"type\":\"long\" }, \"Logout\":{ \"type\":\"long\" }, \"Open Cart\":{ \"type\":\"long\" }, \"Open Category\":{ \"type\":\"long\" }, \"Open Login Page\":{ \"type\":\"long\" }, \"Open Quote\":{ \"type\":\"long\" }, \"Open Quotes\":{ \"type\":\"long\" }, \"Open Quotes Grid\":{ \"type\":\"long\" }, \"Process categories ids\":{ \"type\":\"long\" }, \"Quote Checkout Start\":{ \"type\":\"long\" }, \"Quote Recalculate\":{ \"type\":\"long\" }, \"Request Quote\":{ \"type\":\"long\" }, \"Send Quote\":{ \"type\":\"long\" }, \"Simple Product 1 Add To Cart\":{ \"type\":\"long\" }, \"Simple Product 1 View\":{ \"type\":\"long\" }, \"Simple Product 10 Add To Cart\":{ \"type\":\"long\" }, \"Simple Product 10 View\":{ \"type\":\"long\" }, \"Simple Product 2 Add To Cart\":{ \"type\":\"long\" }, \"Simple Product 2 View\":{ \"type\":\"long\" }, \"Simple Product 3 Add To Cart\":{ \"type\":\"long\" }, \"Simple Product 3 View\":{ \"type\":\"long\" }, \"Simple Product 4 Add To Cart\":{ \"type\":\"long\" }, \"Simple Product 4 View\":{ \"type\":\"long\" }, \"Simple Product 5 Add To Cart\":{ \"type\":\"long\" }, \"Simple Product 5 View\":{ \"type\":\"long\" }, \"Simple Product 6 Add To Cart\":{ \"type\":\"long\" }, \"Simple Product 6 View\":{ \"type\":\"long\" }, \"Simple Product 7 Add To Cart\":{ \"type\":\"long\" }, \"Simple Product 7 View\":{ \"type\":\"long\" }, \"Simple Product 8 Add To Cart\":{ \"type\":\"long\" }, \"Simple Product 8 View\":{ \"type\":\"long\" }, \"Simple Product 9 Add To Cart\":{ \"type\":\"long\" }, \"Simple Product 9 View\":{ \"type\":\"long\" }, \"Update Quote On Open\":{ \"type\":\"long\" } } } } } } I'm using Elasticsearch 5.5.1",
    "website_area": "discuss"
  },
  {
    "id": "4fc4e653-94b6-4736-8de7-a7fcd0f2281c",
    "url": "https://discuss.elastic.co/t/match-only-if-query-is-found-in-a-field-found-within-a-specific-nested-object-and-nothing-else/215539",
    "title": "Match only if query is found in a field found within a specific nested object, and nothing else",
    "category": [
      "Elasticsearch"
    ],
    "author": "RedactedProfile",
    "date": "January 18, 2020, 1:23am January 21, 2020, 9:42pm",
    "body": "Let's assume my index's data looks something like this: [ { \"_index\": \"doc_pages\", \"_type\": \"pages\", \"_id\": \"264b406593732ffbd15a5ed4b4e3b5af\", \"_score\": 1, \"_source\": { \"record_id\": \"264b406593732ffbd15a5ed4b4e3b5af\", \"title\": \"Page Title 1\", \"title_exact\": \"Page Title 1\", \"versions\": [ { \"page_id\": \"fbaf1d5c-dff0-42bb-a41b-3248f0d115d0\", \"page_slug\": \"page-title-1\", \"page_content\": \"Page Title 1 You can test whether the instances were configured properly. Refer to Page Title 1.\", \"document_id\": \"0cfc5f29-ad3e-11e8-8784-00505692583a\", \"document_title\": \"Document Title 1\", \"document_slug\": \"document-title-1\", \"software_version\": \"5.6.3\" }, { \"page_id\": \"d6d717e2-1868-4f6d-b93d-e42d15f68058\", \"page_slug\": \"connectivity-test\", \"page_content\": \"Page Title 1 You can test whether the instances were configured properly. Refer to Page Title 1.\", \"document_id\": \"012fca84-1911-11e9-b86b-00505692583a\", \"document_title\": \"Document Title 1\", \"document_slug\": \"document-title-1\", \"software_version\": \"6.0.0\" } ] } }, { \"_index\": \"doc_pages\", \"_type\": \"pages\", \"_id\": \"c47b24da3e68e2c854ed8bc0436e7384\", \"_score\": 1, \"_source\": { \"record_id\": \"c47b24da3e68e2c854ed8bc0436e7384\", \"title\": \"Introduction\", \"title_exact\": \"Introduction\", \"versions\": [ { \"page_id\": \"6329c7fb-a1f1-4fd9-ae78-0db702c411f5\", \"page_slug\": \"introduction\", \"page_content\": \"Introduction You can configure with two instances using Highly Available Virtual IP, which is configurable on the platform.\", \"document_id\": \"0cfc5f29-ad3e-11e8-8784-00505692583a\", \"document_title\": \"Document Title 1\", \"document_slug\": \"document-title-1\", \"software_version\": \"5.6.3\" }, { \"page_id\": \"c091675c-7e82-4533-a78e-688770be7ce7\", \"page_slug\": \"introduction\", \"page_permanent_id\": \"647589\", \"page_content\": \"Introduction You can configure with two instances using Highly Available Virtual IP (HAVIP), which is configurable on the platform.\", \"document_id\": \"012fca84-1911-11e9-b86b-00505692583a\", \"document_title\": \"Document Title 1\", \"document_slug\": \"document-title-1\", \"software_version\": \"6.0.0\" } ] } } ] And my query currently looks like this { \"size\": 30, \"query\": { \"bool\": { \"minimum_should_match\": 1, \"filter\": { \"nested\": { \"path\": \"versions\", \"query\": [ { \"term\": { \"versions.document_id\": \"0cfc5f29-ad3e-11e8-8784-00505692583a\" } } ] } }, \"should\": [ { \"dis_max\": { \"queries\": [ { \"term\": { \"title_exact\": \"HAVIP\" } }, { \"query_string\": { \"fields\": [ \"title\" ], \"query\": \"HAVIP\" } }, { \"nested\": { \"path\": \"versions\", \"query\": { \"query_string\": { \"fields\": [ \"versions.page_content\" ], \"query\": \"HAVIP\" } } } } ] } } ] } } } The problem that I face right now is that record id c47b24da3e68e2c854ed8bc0436e7384 will show up in my results. That's the complaint. ES found the string reliably, and the filter is correct in that this Document ID was apart of it. What we're trying to resolve now is to make the search a bit less forgiving, and make things a bit more explicit. So now, the situation is that the query can only exist inside the version object that matches the provided document id, while performing a similar query. At this point, I'm not quite sure how to get ElasticSearch to care about only looking into a very specific sub object based off of a term match Any help on this matter would be very much appreciated. ES version is 6.3.2 Cheers!",
    "website_area": "discuss"
  },
  {
    "id": "d4f5c79b-406a-49aa-a0a4-c914a4badc52",
    "url": "https://discuss.elastic.co/t/negative-key-value-for-date-histogram-in-epoch-milli/215953",
    "title": "Negative key value for date_histogram in epoch_milli",
    "category": [
      "Elasticsearch"
    ],
    "author": "jz1603",
    "date": "January 21, 2020, 9:26pm",
    "body": "Hi ES Dev Team, One of the breaking changes in version 7.0 documentation says that negative epoch timestamps are no longer supported. In our application, we use negative epoch timestamps as a lot of our documents have dates earlier than 1970. We use the date_histogram aggregation on these dates, and the \"key\" field in the response from date_histogram aggregation is formatted in epoch_milli. The response keys still come in negative epoch timestamps. Does that mean it is still supported? Will the key format be changed down the road as well? What would be your suggestion for us moving forward? Thanks in advance, Jenna",
    "website_area": "discuss"
  },
  {
    "id": "53908c99-4148-497e-9dfe-70516bbc4656",
    "url": "https://discuss.elastic.co/t/snapshot-retention-task-not-deleting-expired-snapshots/215952",
    "title": "Snapshot Retention Task not deleting expired snapshots",
    "category": [
      "Elasticsearch"
    ],
    "author": "Micah_Hunsberger",
    "date": "January 21, 2020, 9:20pm",
    "body": "For some reason, I cannot get the snapshot retention policy to delete old snapshots in one of my clusters. I successfully set up SLM on a test, standalone node ingesting a regular but small amount of data. The retention task was deleting the expired snapshots as expected with normal-looking logs. $ grep SnapshotRetentionTask /var/log/elasticsearch/elasticsearch.log -b4 ... 1365314:[2020-01-21T16:00:00,002][INFO ][o.e.x.s.SnapshotRetentionTask] [node-1] starting SLM retention snapshot cleanup task 1365432-[2020-01-21T16:00:00,003][INFO ][o.e.x.s.SnapshotLifecycleTask] [node-1] snapshot lifecycle policy [fast-snapshots] issuing create snapshot [test-snap-aaphnvyiqvu680b4no2oqg] 1365607-[2020-01-21T16:00:00,006][INFO ][o.e.x.s.SnapshotLifecycleTask] [node-1] snapshot lifecycle policy job [fast-snapshots-1] issued new snapshot creation for [test-snap-aaphnvyiqvu680b4no2oqg] successfully 1365810-[2020-01-21T16:00:00,094][INFO ][o.e.s.SnapshotsService ] [node-1] snapshot [my_backup:test-snap-aaphnvyiqvu680b4no2oqg/rfUmLqQDQbmFlreHqYT_aA] started 1365964-[2020-01-21T16:00:01,905][INFO ][o.e.s.SnapshotsService ] [node-1] snapshot [my_backup:test-snap-aaphnvyiqvu680b4no2oqg/rfUmLqQDQbmFlreHqYT_aA] completed with state [SUCCESS] 1366141:[2020-01-21T16:00:01,938][INFO ][o.e.x.s.SnapshotRetentionTask] [node-1] starting snapshot retention deletion for [10] snapshots 1366270:[2020-01-21T16:00:01,943][INFO ][o.e.x.s.SnapshotRetentionTask] [node-1] [my_backup] snapshot retention deleting snapshot [test-snap-lywc9-ecrcygdlbz9kbkyw/9SGpTm-VR4KITk8sSFjGNQ] 1366450-[2020-01-21T16:00:02,179][INFO ][o.e.r.b.BlobStoreRepository] [node-1] [my_backup] Found stale root level blobs [meta-9SGpTm-VR4KITk8sSFjGNQ.dat, snap-9SGpTm-VR4KITk8sSFjGNQ.dat]. Cleaning them up 1366647-[2020-01-21T16:00:02,940][INFO ][o.e.s.SnapshotsService ] [node-1] snapshot [my_backup:test-snap-lywc9-ecrcygdlbz9kbkyw/9SGpTm-VR4KITk8sSFjGNQ] deleted 1366801:[2020-01-21T16:00:02,967][INFO ][o.e.x.s.SnapshotRetentionTask] [node-1] [my_backup] snapshot retention deleting snapshot [test-snap-4st8t8xkt3u_y3bmlbdr6a/-GFKixKCQWaXup3KUA_3eQ] ... and if I run GET _slm/stats the response shows that the retention is running and deleting snapshots as expected. { \"retention_runs\" : 158, \"retention_failed\" : 0, \"retention_timed_out\" : 0, \"retention_deletion_time\" : \"20.8m\", \"retention_deletion_time_millis\" : 1250628, \"total_snapshots_taken\" : 1572, \"total_snapshots_failed\" : 1, \"total_snapshots_deleted\" : 1553, \"total_snapshot_deletion_failures\" : 0, \"policy_stats\" : [ { \"policy\" : \"fast-snapshots\", \"snapshots_taken\" : 1572, \"snapshots_failed\" : 1, \"snapshots_deleted\" : 1553, \"snapshot_deletion_failures\" : 0 } ] } However, when I tried setting up the same policy in a 3-node cluster that had slightly more data, the snapshots would get created, but never deleted. The logs look like this: 588069:[2020-01-21T13:30:00,001][INFO ][o.e.x.s.SnapshotRetentionTask] [sbxelkes-p11b] starting SLM retention snapshot cleanup task 588194-[2020-01-21T13:30:07,147][DEBUG][o.e.a.s.m.TransportMasterNodeAction] [sbxelkes-p11b] Get stats for datafeed '_all' 588310-[2020-01-21T13:30:17,081][DEBUG][o.e.a.s.m.TransportMasterNodeAction] [sbxelkes-p11b] Get stats for datafeed '_all' 588426-[2020-01-21T13:30:27,288][DEBUG][o.e.a.s.m.TransportMasterNodeAction] [sbxelkes-p11b] Get stats for datafeed '_all' 588542-[2020-01-21T13:30:37,229][DEBUG][o.e.a.s.m.TransportMasterNodeAction] [sbxelkes-p11b] Get stats for datafeed '_all' -- 629872-[2020-01-21T14:29:47,805][DEBUG][o.e.a.s.m.TransportMasterNodeAction] [sbxelkes-p11b] Get stats for datafeed '_all' 629988-[2020-01-21T14:29:57,975][DEBUG][o.e.a.s.m.TransportMasterNodeAction] [sbxelkes-p11b] Get stats for datafeed '_all' 630104-[2020-01-21T14:30:00,039][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [sbxelkes-p11b] uncaught exception in thread [elasticsearch[sbxelkes-p11b][generic][T#3]] 630272-org.elasticsearch.ElasticsearchException: java.lang.IllegalStateException: slm retention snapshot deletion out while waiting for ongoing snapshot operations to complete 630441: at org.elasticsearch.xpack.slm.SnapshotRetentionTask.lambda$maybeDeleteSnapshots$15(SnapshotRetentionTask.java:328) ~[?:?] 630565: at org.elasticsearch.xpack.slm.SnapshotRetentionTask$NoSnapshotRunningListener.onTimeout(SnapshotRetentionTask.java:515) ~[?:?] 630694- at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:325) ~[elasticsearch-7.5.1.jar:7.5.1] 630845- at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:252) ~[elasticsearch-7.5.1.jar:7.5.1] 630999- at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:598) ~[elasticsearch-7.5.1.jar:7.5.1] 631142- at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703) ~[elasticsearch-7.5.1.jar:7.5.1] -- 673130-[2020-01-21T15:29:28,440][DEBUG][o.e.a.s.m.TransportMasterNodeAction] [sbxelkes-p11b] Get stats for datafeed '_all' 673246-[2020-01-21T15:29:38,178][DEBUG][o.e.a.s.m.TransportMasterNodeAction] [sbxelkes-p11b] Get stats for datafeed '_all' 673362-[2020-01-21T15:29:48,125][DEBUG][o.e.a.s.m.TransportMasterNodeAction] [sbxelkes-p11b] Get stats for datafeed '_all' 673478-[2020-01-21T15:29:58,300][DEBUG][o.e.a.s.m.TransportMasterNodeAction] [sbxelkes-p11b] Get stats for datafeed '_all' 673594:[2020-01-21T15:30:00,000][INFO ][o.e.x.s.SnapshotRetentionTask] [sbxelkes-p11b] starting SLM retention snapshot cleanup task ... Originally I believed it was because I was trying to take snapshots too quickly, and so there was no time to get any snapshot deletions in, but I changed the SLM schedule to 0 * * * * ? with an expiry of 6h and the retention schedule to 0 30 * * * ?, so there shouldn't have been any snapshot creations running while the retention task was running, since none of the snapshots took longer than 5 minutes to create. GET _slm/stats shows that no snapshots have been deleted. { \"retention_runs\" : 103, \"retention_failed\" : 0, \"retention_timed_out\" : 0, \"retention_deletion_time\" : \"0s\", \"retention_deletion_time_millis\" : 0, \"total_snapshots_taken\" : 21, \"total_snapshots_failed\" : 1, \"total_snapshots_deleted\" : 0, \"total_snapshot_deletion_failures\" : 0, \"policy_stats\" : [ { \"policy\" : \"fast-snapshots\", \"snapshots_taken\" : 21, \"snapshots_failed\" : 1, \"snapshots_deleted\" : 0, \"snapshot_deletion_failures\" : 0 } ] } even though _cat/snapshots/sbx-es-7 clearly shows hourly snapshots well past expiration: test-snap-fssc87zgtqkqdfiopkeljq SUCCESS 1579550399 19:59:59 1579550666 20:04:26 4.4m 82 123 0 123 test-snap-taurpr7rrd2jjxtiivoc6g SUCCESS 1579553999 20:59:59 1579554027 21:00:27 27s 82 123 0 123 test-snap-cfszpk8ctm2jce43tzm2aw SUCCESS 1579557599 21:59:59 1579557627 22:00:27 27.4s 82 123 0 123 test-snap-re5chymnrws6ah4ziiwvbg SUCCESS 1579561199 22:59:59 1579561229 23:00:29 29.1s 80 121 0 121 test-snap-miyrv-7eqiqgh5y4inxbma SUCCESS 1579564799 23:59:59 1579564834 00:00:34 34.4s 80 121 0 121 test-snap-xs6tcg6gsdyv7xxbgaorhg SUCCESS 1579568399 00:59:59 1579568421 01:00:21 21.9s 85 126 0 126 test-snap-tdavsajiqn6vnzafmy2dnw SUCCESS 1579571999 01:59:59 1579572020 02:00:20 20.9s 85 126 0 126 test-snap-j_2qetm6tlm3cxymxsq7ww SUCCESS 1579575599 02:59:59 1579575619 03:00:19 19.7s 85 126 0 126 test-snap-uxi-lwsuskwz7ryaqck8ng SUCCESS 1579579199 03:59:59 1579579222 04:00:22 23.1s 85 126 0 126 test-snap-4koycujkslizvcx4x7vptg SUCCESS 1579582800 05:00:00 1579582824 05:00:24 24.1s 85 126 0 126 test-snap-lzz9onjeqooqpzopr-74uw SUCCESS 1579586400 06:00:00 1579586422 06:00:22 21.5s 77 118 0 118 test-snap-6xyqtazzsf-ejjwoszbj0a SUCCESS 1579590000 07:00:00 1579590025 07:00:25 25.7s 77 118 0 118 test-snap-wp6clxi2rk-t3vngl5ynig SUCCESS 1579593600 08:00:00 1579593622 08:00:22 22.9s 77 118 0 118 test-snap-ee2bjiunrd6yggf5usc4pa SUCCESS 1579597199 08:59:59 1579597223 09:00:23 23.3s 77 118 0 118 test-snap-62_zf-d9rdqrhbfb2rx_dw SUCCESS 1579600799 09:59:59 1579600821 10:00:21 21.9s 77 118 0 118 test-snap-si8nptl9sb-bg7_eqg_xda SUCCESS 1579604400 11:00:00 1579604421 11:00:21 21.8s 77 118 0 118 test-snap-fchx6nf8rgulgl4x3ppbbq SUCCESS 1579607999 11:59:59 1579608023 12:00:23 23.8s 77 118 0 118 test-snap-neihu2tyqj2lglnzgur27q SUCCESS 1579611599 12:59:59 1579611622 13:00:22 22.7s 77 118 0 118 test-snap-xtfinmrsq_mrnekjlijbha SUCCESS 1579615200 14:00:00 1579615239 14:00:39 39.3s 77 118 0 118 test-snap-i5gjidxgqskkjrxjfceoda SUCCESS 1579622399 15:59:59 1579622428 16:00:28 28.3s 77 118 0 118 test-snap-jsmjngbwqzsobqqmd1otda SUCCESS 1579625999 16:59:59 1579626024 17:00:24 24.6s 77 118 0 118",
    "website_area": "discuss"
  },
  {
    "id": "b1a96317-09fa-427f-a80f-aa6a6c52e953",
    "url": "https://discuss.elastic.co/t/elasticsearch-settings-do-not-seem-to-be-applied-via-docker-env-variables/213384",
    "title": "Elasticsearch settings do not seem to be applied via Docker env variables?",
    "category": [
      "Elasticsearch"
    ],
    "author": "jerrac",
    "date": "December 30, 2019, 9:43pm January 21, 2020, 8:14pm January 21, 2020, 8:28pm",
    "body": "I'm running a small test cluster via docker-compose on a desktop in my office, I've configured each node via environment variables. Not by using elasticsearch.yml. My main node looks like: version: '3.3' services: esnode1: image: docker.elastic.co/elasticsearch/elasticsearch:7.5.1 container_name: esnode1 environment: - cluster.name=reagan3-cluster - node.name=esnode1 - node.master=true - discovery.seed_hosts=esnode1,esnode2,esnode3 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms6000m -Xmx6000m\" - http.cors.enabled=true - http.cors.allow-origin=\"*\" - http.cors.allow-headers=X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization - http.cors.allow-credentials=true ulimits: memlock: soft: -1 hard: -1 volumes: - r3_cluster_esdata1:/usr/share/elasticsearch/data - r3_cluster_snapshots:/opt/elasticsearch/snapshots ports: - 127.0.0.1:9201:9200 healthcheck: test: [\"CMD\", \"curl\",\"-s\" ,\"-f\", \"http://localhost:9200/_cat/health\"] networks: - elknet restart: always I have Apache running as a reverse proxy in front of ES so I can use LDAP auth. I've been trying to test Mirage and Dejavu from my laptop, a simple curl call with my username and password works just fine, so the proxy is working. I can also access my Kibana instance just fine, and my cluster health is green. But when I try to connect to the cluster using Mirage or Dejavu, Firefox's console spits out errors like this: Mirage: This page uses the non standard property zoom. Consider using calc() in the relevant property values, or using transform along with transform-origin: 0 0. localhost:3030 Source map error: Error: request failed with status 404 Resource URL: http://localhost:3030/dist/css/vendor.min.css Source Map URL: bootstrap.min.css.map Angular 2 is running in the development mode. Call enableProdMode() to enable the production mode. core.umd.js:201:17 Object { url: \"http://username:password@internalhostname:9200\", appname: \"metricbeat-7.5.1-2019.12.29\", username: \"\", password: \"\", host: \"\" } Object { username: \"username\", password: \"password\", url: \"http://internalhostname:9200\" } app.component.ts:336:12 setting up appbase appbase.service.ts:73:12 http://internalhostname:9200/metricbeat-7.5.1-2019.12.29/_settings appbase.service.ts:182:12 http://internalhostname:9200/metricbeat-7.5.1-2019.12.29/_mapping/ appbase.service.ts:163:14 Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at http://internalhostname:9200/metricbeat-7.5.1-2019.12.29/_mapping/. (Reason: CORS header Access-Control-Allow-Origin missing). Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at http://internalhostname:9200/metricbeat-7.5.1-2019.12.29/_settings. (Reason: CORS header Access-Control-Allow-Origin missing). Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at http://internalhostname:9200/metricbeat-7.5.1-2019.12.29/_mapping/. (Reason: CORS request did not succeed). Object { _body: error, status: 0, ok: false, statusText: \"\", headers: {}, type: 3, url: null } app.component.ts:446:16 Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at http://internalhostname:9200/metricbeat-7.5.1-2019.12.29/_settings. (Reason: CORS request did not succeed). Not able to get the version. app.component.ts:382:16 Dejavu: Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at http://internalhostname:9200/metricbeat-7.5.1-2019.12.29. (Reason: CORS request did not succeed). Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at http://internalhostname:9200/metricbeat-7.5.1-2019.12.29. (Reason: CORS header Access-Control-Allow-Origin missing). Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at http://internalhostname:9200/metricbeat-7.5.1-2019.12.29. (Reason: CORS request did not succeed). I also tried a curl request on my desktop that set the Origin header. (Found the idea here ) $ curl --head http://localhost:9201 -H 'Origin: http://foo.com' HTTP/1.1 403 Forbidden Judging from the docs, http.cors.allow-origin=\"*\" should open my cluster up to any origin, correct? That, combined with the \"node attributes\" error I mention here, makes me wonder if my settings are just not being applied correctly. Is that the case? Do I have to use elasticsearch.yml to get those settings applied? Any other ideas?",
    "website_area": "discuss"
  },
  {
    "id": "eb7ad165-d292-4a33-8c3b-486213ea2975",
    "url": "https://discuss.elastic.co/t/tool-for-ingesting-saas-audit-logs-into-elasticsearch/215948",
    "title": "Tool for ingesting SaaS audit logs into Elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "arkadiy",
    "date": "January 21, 2020, 8:27pm",
    "body": "Hi all, I've been building a product to ingest audit logs from all your SaaS tools (think G Suite, Okta, Github, Dropbox, Zendesk, Salesforce, etc) and push them into Elasticsearch. The product is in a beta state and I'm currently looking for testers who can provide feedback and help guide my roadmap. If this product sounds useful I would love to hear from you, either directly on this thread or via email to arkadiy@logsnitch.com. Thanks very much!",
    "website_area": "discuss"
  },
  {
    "id": "b9708359-0509-4063-af97-9225c100c5ab",
    "url": "https://discuss.elastic.co/t/elasticsearch-old-gen-heap-filling-up-quickly/215944",
    "title": "ElasticSearch Old-Gen Heap filling up quickly",
    "category": [
      "Elasticsearch"
    ],
    "author": "musfiqur",
    "date": "January 21, 2020, 7:55pm",
    "body": "Hi, From our grafana logs, we see that our old-gen heap is filling up quickly (in 10 minutes). We are using ES 6.7 with with 11 nodes and 28 shards. We have seen multiple outages when the old-gen heap is filling quickly. What does it mean? Is our heap too big/ number of shards too low/number of nodes too low/load too high? What does it indicate? -Musfiqur",
    "website_area": "discuss"
  },
  {
    "id": "ca0166ac-4870-421e-9ba0-b0b7f28a4211",
    "url": "https://discuss.elastic.co/t/hung-tasks-in-es-that-cannot-be-cancelled-using-the-task-api/215942",
    "title": "Hung tasks in ES, that cannot be cancelled using the Task API",
    "category": [
      "Elasticsearch"
    ],
    "author": "Lasse_Vang_Gravesen",
    "date": "January 21, 2020, 7:42pm",
    "body": "I have an on-going problem with accumulating read/search tasks that appear to be stuck. The included picture shows the situation, where the 02 index has accumulated ~70 search tasks that never go away. image1909299 84.5 KB The tasks are of type netty or transform, and the action is reads/search. { \"aDhN0FiNQnW4q7UiY8qAfw:8363529459\" : { \"node\" : \"aDhN0FiNQnW4q7UiY8qAfw\", \"id\" : 8363529459, \"type\" : \"netty\", \"action\" : \"indices:data/read/search[phase/query]\", \"description\" : \"shardId[[02][33]]\", \"start_time_in_millis\" : 1579507610392, \"running_time_in_nanos\" : 45661052782486, \"cancellable\" : true, \"parent_task_id\" : \"fvp4CpvsSl6cvb6IeTxlag:4187545907\" } } { \"C2sZvd9MQM-trg7ShfzAig:3580000762\" : { \"node\" : \"C2sZvd9MQM-trg7ShfzAig\", \"id\" : 3580000762, \"type\" : \"transport\", \"action\" : \"indices:data/read/search\", \"description\" : \"indices[02], types[], search_type[QUERY_THEN_FETCH], source[...]\", \"start_time_in_millis\" : 1579248693084, \"running_time_in_nanos\" : 178537370554429, \"cancellable\" : true } } Using the Task Management API to \"cancel\" these tasks, does nothing. The master nodes and coordinating nodes have been restarted, and that didn't do anything either. This is happening on Elasticsearch 5.6.x It appears that the source of the problem is that the query took too long to respond, and the client timed out. Does anyone have ideas on how this could be resolved?",
    "website_area": "discuss"
  },
  {
    "id": "37a41c65-3607-423a-b6bf-353bf751df57",
    "url": "https://discuss.elastic.co/t/sparse-vector-vs-rank-features-which-one/214772",
    "title": "Sparse vector vs rank features. Which one?",
    "category": [
      "Elasticsearch"
    ],
    "author": "snakeztc",
    "date": "January 13, 2020, 7:32am January 13, 2020, 10:07pm January 14, 2020, 9:34pm January 15, 2020, 5:29am January 15, 2020, 9:25pm January 17, 2020, 6:46pm January 20, 2020, 12:02pm January 21, 2020, 7:36pm",
    "body": "I am trying to implement a customized search function that will rank documents based on sparse features. Concretely, each document will have a list of sparse features, e.g. doc_1 -> {a: 1.1, b: 0.2} doc_2 -> {a: 0.2, z: 0.3, zz: 1.2} ... Now at the query stage, the query will have a list of the sparse dimension that appears, e.g. [a, zz, b]. My scoring function is for each doc is simply added up all the value of terms that appear in both the query and the document. Take the above example, doc_1_score = 0.2 + 1.2 doc_2_score = 1.1 + 0.2 My question is what is the most efficient way to implement this, I have two ideas now. using rank features to save all the values in documents, and then using a list of \"should\" to get the score save the features as sparse vectors, then using a dot_product to get the final score. Which one do you think will be more efficient (memory & speed). Is there better way to accomplish this, e.g. using inverted index? Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "fa0e0a00-3d7a-4a00-b2de-15fd2551fa2e",
    "url": "https://discuss.elastic.co/t/serilog-elasticsearch-logs-not-making-it-to-the-server-but-buffering-correctly/215466",
    "title": "Serilog Elasticsearch logs not making it to the server (but buffering correctly)",
    "category": [
      "Elasticsearch"
    ],
    "author": "ljh33txm",
    "date": "January 17, 2020, 1:33pm January 21, 2020, 6:53pm",
    "body": "I have an ELK stack locally hosted (v7.0) on a Windows IIS web server and the logs are not making it to the server. Server is running, I can reach the reserved URL and get back the generic json package saying Elasticsearch is running and I can log into Kibana just fine, there's just no logs to see. I have a bufferBaseFilename set in the apps that are logging, and when I go to that location the logs are actually there, properly indexed and all. I'm wondering why it never gets synced back to the server? It seems like a connection issue, but all the network stuff checks out. I'm probably missing something simple. Any thoughts? Let me know if you need more information!",
    "website_area": "discuss"
  },
  {
    "id": "b4f9f7da-5f9d-4a53-bbb7-f03ccfef1b9d",
    "url": "https://discuss.elastic.co/t/index-cleanup/215476",
    "title": "Index cleanup",
    "category": [
      "Elasticsearch"
    ],
    "author": "andymcinnes",
    "date": "January 17, 2020, 2:07pm January 17, 2020, 2:31pm January 17, 2020, 3:28pm January 17, 2020, 4:22pm January 20, 2020, 9:51am January 21, 2020, 2:34pm January 21, 2020, 2:58pm January 21, 2020, 3:01pm January 21, 2020, 3:15pm January 21, 2020, 3:19pm January 21, 2020, 5:07pm January 21, 2020, 5:30pm January 21, 2020, 5:36pm January 21, 2020, 6:27pm",
    "body": "Hi, I've got Serilog pumping logs in to elastic search from various applications, using the Serilog ElasticSearch Sink. I use a daily indexing pattern per environment and serilog manages the initial creation of these indexes. e.g. logs-development-2020.01.17-00001 We are using elastic cloud, so I've tried creating an index template and rollup policy, with an alias as per the docs and basically made sure they were the same as the apm ones that are there as standard. But I get policy errors which I assume are because serilog is creating the index and not letting ilm do its thing. I tried flipping it so serilog didn't create index and published to the alias, but serilog wouldn't write to the alias, possibly as it generates the index template when serilog creates the index itself. Regardless of the above, the rollup policy still didn't apply probably as a result of serilog not being able to write to the index and their being nothing in it. So I'm a little stumped, I can't figure out how best to approach this. I saw something about creating a cron that deletes the index using a pattern after x days or something, but there was little information about how to do that. I also keep seeing references to curator, but I've not used that before and I don't know if it's available in elastic cloud. Any input would be greatly appreciated Andy",
    "website_area": "discuss"
  },
  {
    "id": "9516bc68-607e-49d3-b601-3c39d47f4276",
    "url": "https://discuss.elastic.co/t/elasticsearch-query-with-timezone-conversion/215220",
    "title": "Elasticsearch query with timezone conversion",
    "category": [
      "Elasticsearch"
    ],
    "author": "elasticforme",
    "date": "January 15, 2020, 10:13pm January 15, 2020, 10:40pm January 16, 2020, 1:59pm January 16, 2020, 5:44pm January 16, 2020, 5:57pm January 16, 2020, 5:58pm January 16, 2020, 6:11pm January 16, 2020, 10:41pm January 17, 2020, 2:19pm January 21, 2020, 6:02pm",
    "body": "I have put a record in elk input message => '{\"submitted_date\": \"2020-01-14 10:05:11.11\"}' it got saved as UTC, 8 hour earlier (Singapore timezone) { \"submitted_date\" => 2020-01-14T02:05:11.110Z } in kibana dev tool when I run search I get this, which is correct. GET /sachin_quick_test/_search { \"submitted_date\" : \"2020-01-14T02:05:11.110Z\" } Now I want this to display 8 hour+ but it still displays same. how do I fix this? am I using wrong parameter? GET /sachin_quick_test/_search { \"query\": { \"range\" : { \"submitted_date\": { \"gte\": \"2020-01-13\", \"lte\": \"2020-01-15\", \"time_zone\": \"+08:00\" } } } }",
    "website_area": "discuss"
  },
  {
    "id": "709c304d-fc3f-4246-969d-6745d3ab1d88",
    "url": "https://discuss.elastic.co/t/how-to-retrieve-the-length-of-an-array-in-a-nested-document-using-painless/215934",
    "title": "How to retrieve the length of an array in a nested document using painless",
    "category": [
      "Elasticsearch"
    ],
    "author": "QuincyBowers",
    "date": "January 21, 2020, 5:48pm",
    "body": "My question is, how do I get the length of an array field in a nested document using a painless script. Here is the problem I am trying to solve. We have documents which can have a set of labels assigned to them. These labels are updated by users. Users need to be able to filter documents by the existence of these labels. One use case is to find documents which have a specific set of labels and no other labels. Example: A user filters for documents containing foo, bar, and baz. A document which contains [\"foo\", \"bar\", \"baz\"] should match. A document which contains [\"foo\", \"bar\", \"baz\", \"qux\"] should not match. A document which contains [\"foo\", \"bar\"] should not match. My research indicates the the best way to accomplish this is to write a query which finds the documents which have the labels I want AND to find the documents which also have the same number of labels as I am searching for. It is often suggested that you should index an additional field which has the count to make this search fast. However, my documents are updated frequently, and from various sources, so ensuring this extra field is updated correctly whenever the array changes is complicated. We may eventually do that, but right now I am hoping a script query will handle things for us. Here is my mappings: { \"mappings\": { \"_doc\": { \"properties\": { \"metadata\": { \"properties\": { \"labels\": { \"type\": \"nested\", \"properties\": { \"name\": { \"type\": \"keyword\" } } } } } } } } } The actual documents are much larger and there are many other sections similar to the metadata section above. An example document for the above mapping would look like this: { \"metadata\": { \"labels\": [ { \"name\": \"foo\" }, { \"name\": \"bar\" }, { \"name\": \"baz\" } ] } } Here is my attempt at a query to find this document: { \"query\": { \"bool\": { \"must\": [ { \"bool\": { \"adjust_pure_negative\": true, \"boost\": 1 } } ], \"filter\": [ { \"bool\": { \"must\": [ { \"bool\": { \"must\": [ { \"nested\": { \"query\": { \"terms\": { \"metadata.labels.name\": [ \"Another?\" ], \"boost\": 1 } }, \"path\": \"metadata.labels\", \"ignore_unmapped\": false, \"score_mode\": \"none\", \"boost\": 1 } }, { \"nested\": { \"query\": { \"terms\": { \"metadata.labels.name\": [ \"another label\" ], \"boost\": 1 } }, \"path\": \"metadata.labels\", \"ignore_unmapped\": false, \"score_mode\": \"none\", \"boost\": 1 } }, { \"nested\": { \"query\": { \"script\": { \"script\": { \"source\": \"if (doc['metadata'] == null || doc['metadata.labels'] == null) { false;} else { doc['metadata.labels'].length == params.termCount;}\", \"lang\": \"painless\", \"params\": { \"termCount\": 2 } }, \"boost\": 1 } }, \"path\": \"metadata.labels\", \"ignore_unmapped\": true, \"score_mode\": \"none\", \"boost\": 1 } } ], \"adjust_pure_negative\": true, \"boost\": 1 } } ], \"adjust_pure_negative\": true, \"boost\": 1 } } ], \"adjust_pure_negative\": true, \"boost\": 1 } } } { \"reason\": { \"type\": \"script_exception\", \"reason\": \"runtime error\", \"script_stack\": [ \"org.elasticsearch.search.lookup.LeafDocLookup.get(LeafDocLookup.java:81)\", \"org.elasticsearch.search.lookup.LeafDocLookup.get(LeafDocLookup.java:39)\", \"if (doc['metadata'] == null || doc['metadata.labels'] == null) { \", \" ^---- HERE\" ], \"script\": \"if (doc['metadata'] == null || doc['metadata.labels'] == null) { false;} else { doc['metadata.labels'].length == params.termCount;}\", \"lang\": \"painless\", \"caused_by\": { \"type\": \"illegal_argument_exception\", \"reason\": \"No field found for [metadata] in mapping with types [_doc]\" } } } This results in the following error: { \"reason\": { \"type\": \"script_exception\", \"reason\": \"runtime error\", \"script_stack\": [ \"org.elasticsearch.search.lookup.LeafDocLookup.get(LeafDocLookup.java:81)\", \"org.elasticsearch.search.lookup.LeafDocLookup.get(LeafDocLookup.java:39)\", \"if (doc['metadata'] == null || doc['metadata.labels'] == null) { \", \" ^---- HERE\" ], \"script\": \"if (doc['metadata'] == null || doc['metadata.labels'] == null) { false;} else { doc['metadata.labels'].length == params.termCount;}\", \"lang\": \"painless\", \"caused_by\": { \"type\": \"illegal_argument_exception\", \"reason\": \"No field found for [metadata] in mapping with types [_doc]\" } } } I have also tried to replace the script with this: { \"nested\": { \"query\": { \"script\": { \"script\": { \"source\": \"if (params._source.metadata == null || params._source.metadata.labels == null) { false;} else { params._source.metadata.labels.length == params.termCount;}\", \"lang\": \"painless\", \"params\": { \"termCount\": 2 } }, \"boost\": 1 } }, \"path\": \"metadata.labels\", \"ignore_unmapped\": true, \"score_mode\": \"none\", \"boost\": 1 } } But this results in this error: { \"reason\": { \"type\": \"script_exception\", \"reason\": \"runtime error\", \"script_stack\": [ \"if (params._source.metadata == null || params._source.metadata.labels == null) { \", \" ^---- HERE\" ], \"script\": \"if (params._source.metadata == null || params._source.metadata.labels == null) { false;} else { params._source.metadata.labels.length == params.termCount;}\", \"lang\": \"painless\", \"caused_by\": { \"type\": \"null_pointer_exception\", \"reason\": null } } } How do I correctly access the length of the array field within a nested document? Thanks, Quincy",
    "website_area": "discuss"
  },
  {
    "id": "003c374f-78e3-40ad-b949-990aa4d86c09",
    "url": "https://discuss.elastic.co/t/dealing-with-highly-scored-sequences-of-words/215927",
    "title": "Dealing with highly scored sequences of words",
    "category": [
      "Elasticsearch"
    ],
    "author": "ivanibash",
    "date": "January 21, 2020, 5:18pm",
    "body": "I have an elastic search filter which for the sake of argument looks like this \"french_company_synonyms\": { \"expand\": \"true\", \"type\": \"synonym_graph\", \"synonyms\": [ \"llp, limited liability partnership\", \"llc, limited liability company\", \"plc, public limited company\", \"sarl, societe a responsabilite limitee\", \"sa, societe anonyme\" ] } As a result, when someone types in \"sarl\", it expands to \"societe a responsabilite limitee\". So far so good. Now, in my index I have records that use both \"societe a responsabilite limitee\" and \"sarl\". Let's say I'm looking for \"sarl dream\". In my db it's stored exactly like that, \"sarl dream\". However, even though there is an exact match, instead it would first return LOTS of \"Societe a responsabilite limitee *\" companies, because I suspect it expands the query, sees that there are records that match 4 words, and scores those higher than the exact match \"sarl dream\" (only 2 words match). If I'm understanding the problem correctly, I'd formalise it something like this. I have many records in the index with the same combination of words (\"societe a responsabilite limitee\"). Usually ES is good at penalising words that appear often in the index through tf/idf. However in this case this seems to be offset by the fact that it's a phrase with multiple words. And even though they are common, the matches still score high. How do you deal with the cases where it's not simply a word that's very common in a db but a phrase/word sequence? Now I see 2 potential solutions. I can maybe use some kind of downscoring thing where I downscore the matches that have a phrase \"Societe a responsabilite limitee\" (I think I need to use boosting query for that). This is relatively easy but seems a bit dirty, I think the boosting score would need to change as the index grows. Another way is to ensure that my index doesn't have any \"societe a responsabilite limitee\", and all these are normalised to a word \"sarl\"). Before I go down the rabbit hole of trying things out, can someone tell me if they encountered a similar problem, and also whether my problem definition is even correct? Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "8dd14fd8-609c-4418-9c41-ec367587c9c2",
    "url": "https://discuss.elastic.co/t/max-lines-in-a-message/215925",
    "title": "Max lines in a message",
    "category": [
      "Elasticsearch"
    ],
    "author": "whoatemyjam",
    "date": "January 21, 2020, 5:05pm",
    "body": "hi I was trying to parse a some logs (using filebeat and logstash to elastic)and put them as multiline which came to be around 800 lines for each message , but the message in in discover in kibana and logUI in kibana was truncated after 500 lines , so can someone please confirm if the length of the message can be max of 500 lines ? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "0d4b419c-fb2d-42af-bdb4-13cf9007096e",
    "url": "https://discuss.elastic.co/t/compare-two-same-filed-value-from-two-different-indexes/215910",
    "title": "Compare two same filed value from two different indexes",
    "category": [
      "Elasticsearch"
    ],
    "author": "elkparser",
    "date": "January 21, 2020, 3:52pm January 21, 2020, 3:57pm January 21, 2020, 4:10pm",
    "body": "Hi, I have a data like this: index1: sample_2020_01_21 in this index i have data: { \"jobName\":\"ABC\" \"jobduration\": 20 } index1: sample_2020_01_20 in this index i have data: { \"jobName\":\"ABC\" \"jobduration\": 40 } Like that I have daily indexes with some amount of documents, form that indexes I need to query the data based on jobduration to check current day job running is more time on not. if that current jobs are running more time then we need to index those documents into other index. Actually it can possible by using watcher with chain input format, but problem here is no watcher for us, i need plain query language to get this output. Can you please help me",
    "website_area": "discuss"
  },
  {
    "id": "fbbb82f6-2282-4bac-b57d-4a170d54d453",
    "url": "https://discuss.elastic.co/t/shared-filesystem/215013",
    "title": "Shared filesystem",
    "category": [
      "Elasticsearch"
    ],
    "author": "elasticus",
    "date": "January 14, 2020, 4:18pm January 14, 2020, 4:57pm January 15, 2020, 12:42pm January 15, 2020, 1:06pm January 20, 2020, 9:36am January 20, 2020, 10:05am January 21, 2020, 11:25am January 21, 2020, 1:01pm January 21, 2020, 4:01pm",
    "body": "I'm looking for the Elastic best practises/supported configurations for shared filesystem with specific regard to performing snapshot backups.",
    "website_area": "discuss"
  },
  {
    "id": "2067907a-7bfb-49fd-b3aa-8aa0d1e4a808",
    "url": "https://discuss.elastic.co/t/how-to-configure-multiple-slack-accounts-in-watcher/212560",
    "title": "How to configure multiple slack accounts in watcher",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "December 20, 2019, 9:22am December 20, 2019, 9:23am December 20, 2019, 9:15pm December 29, 2019, 10:07pm January 8, 2020, 10:12am January 21, 2020, 3:59pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ebc03d2e-43ad-404b-9f5d-5ebebe7a3233",
    "url": "https://discuss.elastic.co/t/high-query-indexing-time-one-node-cpu-ok/215907",
    "title": "High query/indexing time one node (cpu ok)",
    "category": [
      "Elasticsearch"
    ],
    "author": "bruno_z",
    "date": "January 21, 2020, 3:50pm",
    "body": "I'm facing high query/indexing time for just one node. My cluster has 8 nodes My index has 5 primary shards and 3 replicas I checked the shards using the _cat/shards api, and everything looks as expected. image1839401 75.5 KB The cpu is ok. I already tried rebooting the node, but it just moves the problem to another one. Any idea? Elasticsearch version: 5.5.1",
    "website_area": "discuss"
  },
  {
    "id": "160541cd-12f7-4b6a-8881-ff81a9c59c23",
    "url": "https://discuss.elastic.co/t/plugin-that-can-intersect-an-automaton-with-the-completion-suggester-fsts/215908",
    "title": "Plugin that can intersect an automaton with the completion suggester FSTs",
    "category": [
      "Elasticsearch"
    ],
    "author": "luk.morbee",
    "date": "January 21, 2020, 3:35pm",
    "body": "If I understood it correctly an FST is created per segment for every completion suggester. Suppose we would want to write an Elasticsearch plugin that would allow us to intersect a given automaton (i.e. search logic you cant express with a regular expression) with these FSTs, where can I (having no knowledge of the inner workings of Elasticsearch) find some sample code (or other information) to access these FSTs created by Elasticsearch.",
    "website_area": "discuss"
  },
  {
    "id": "9ac0d5f7-cb21-43aa-9e9b-22c3bf217a54",
    "url": "https://discuss.elastic.co/t/index-sorting-and-terminate-after-combination/198233",
    "title": "Index Sorting and terminate_after combination",
    "category": [
      "Elasticsearch"
    ],
    "author": "HenriqueLimao",
    "date": "September 5, 2019, 11:58am September 5, 2019, 11:48am October 3, 2019, 11:48am January 21, 2020, 3:25pm",
    "body": "I have an index in ES 6.6 used to search regular content that should be scored differently based on the user. Today we have more than 50 fields that can be used to calculate this score. In order to speed up the query, I decided to use early termination like described here I created a field based on a combination of all individual fields used for scoring and sorted the index with this value. However, I can't sort the results of the query based on this single field (since the sort order will depend on each user), so I started using the terminate_after parameter in the query. After doing some evaluations I noticed that terminating the query after 1000 elements was good enough to have a set of 10 results with the optimal score for each user. This seemed to work well and I got the speed up that I wanted, however later I realized that this only works if each shard in the index has only one segment. I call force_merge with value 1 during index creation, but I can't do that for index updates during normal traffic. After updating the index, new segments are created and they are completely ignored by the query if we reach enough matches for the terminate_after parameter in the original segment. This happen even if in the new segment, the field that defines the index sorting would position the document as the top result. My questions now: Is that a bug in ES or this is a mix of features that shouldn't really not work together? Is there an alternative to get the benefits of early termination but to be able to resort the results by different fields in a later stage?",
    "website_area": "discuss"
  },
  {
    "id": "087f3cb5-2380-46b4-8b2d-4e7834cd9469",
    "url": "https://discuss.elastic.co/t/separate-documents-from-api-request/215902",
    "title": "Separate documents from API request",
    "category": [
      "Elasticsearch"
    ],
    "author": "victor.nilsson",
    "date": "January 21, 2020, 3:06pm",
    "body": "Hi, We're using logstash with the HTTP_input plugin, to index events from a ReST API. The problem is that for every API request, we effectively index duplicates. If we run a GET request the output looks something like this: { \"_index\": \"randomindex\", \"_type\": \"_doc\", \"_id\": \"XTqUyG8BFEo1Fcv79Ns-\", \"_version\": 1, \"_score\": null, \"_source\": { \"@timestamp\": \"2020-01-21T14:50:02.087Z\", \"@version\": \"1\", \"requests\": [ { \"internalIp\": \"127.0.0.1\", \"categories\": [ \"Ecommerce/Shopping\", \"Business Services\", \"Financial Institutions\", \"Phishing\", \"Infrastructure\", \"Application\" ], \"datetime\": \"2020-01-21T12:26:36.000Z\", \"originType\": \"Roaming Computers\", \"originId\": 1231231, \"tags\": [], \"externalIp\": \"127.0.0.1\", }, { \"internalIp\": \"127.0.0.1\", \"categories\": [ \"Ecommerce/Shopping\", \"Business Services\", \"Financial Institutions\", \"Phishing\", \"Infrastructure\", \"Application\" ], \"datetime\": \"2020-01-21T06:02:42.000Z\", \"originType\": \"Roaming Computers\", \"originId\": 1231231, \"tags\": [], \"externalIp\": \"127.0.0.1\", }, { \"internalIp\": \"127.0.0.1\", \"categories\": [ \"Sports\", \"Malware\" ], \"datetime\": \"2020-01-21T10:52:54.000Z\", \"originType\": \"Roaming Computers\", \"originId\": 123123, \"tags\": [], }, The next time we GET from the API, we get the same logs back, effectively indexing duplicates. Could we somehow use some field in the event to sort out duplicates? Maybe using the \"datetime\" field?",
    "website_area": "discuss"
  },
  {
    "id": "88b746fa-af23-4509-b07f-1b4609d2f233",
    "url": "https://discuss.elastic.co/t/es-cluster-stops-working-when-enabling-too-many-logstash-workers-to-send-bulk-requests-to-it/215898",
    "title": "ES cluster stops working when enabling too many Logstash workers to send bulk requests to it",
    "category": [
      "Elasticsearch"
    ],
    "author": "viniciof",
    "date": "January 21, 2020, 2:45pm",
    "body": "I've noticed that when I scale-out the number of Logstash apps I have in containers writing to same ES cluster it stops all activity (index & search) But as soon as I go back to the reduced number of workers (from 20 to 8) the cluster works back again. The problem is that # of apps is not enough to cope with the load and I want to send more data to ES. image1747592 64.8 KB why is this happening ? I though it was because I didn't have enough ingest nodes (my Logstash apps are writing to 2 of them) but then I no longer think that because it is \"bulk\" thread pools in many data nodes (not ingest) who are completely loaded and rejecting requests. appreciate any inputs, Regards.",
    "website_area": "discuss"
  },
  {
    "id": "133f5090-1ae5-481f-8ff2-4c03bafcc08e",
    "url": "https://discuss.elastic.co/t/noise-word-handling-on-ingest/215871",
    "title": "Noise Word handling on ingest",
    "category": [
      "Elasticsearch"
    ],
    "author": "TWhitehouse",
    "date": "January 21, 2020, 11:30am January 21, 2020, 11:41am January 21, 2020, 1:38pm January 21, 2020, 1:58pm January 21, 2020, 2:01pm January 21, 2020, 2:19pm January 21, 2020, 2:21pm January 21, 2020, 2:35pm",
    "body": "Hi All One of my current use cases is to take input from a SQL DB of service desk ticket subjects and descriptions, and be able to show words that are trending (meaning we have a potential problem). So far I've imported using Logstash, and used some mutate filters to lowercase the data and concatenate it into a single field, then remove special characters, copy the text into another field and then split that into an array. However where I'm falling down is indexing that data into ES and getting my required output. My ideal output is a count of terms with all the noise words filtered out so that, over time, we can track which words are trending on our tickets. I'm also looking to visualise this information within Kibana (which I think limits the APIs I can use in ES) What I've got so far is indexing the full thing into a text field, and indexing the same (but split into an array via logstash) as a keyword field. However, I end up with a load of noise words in there. I'm looking for the best way to get rid of those noise words. I tried using an analyzer, and a normalizer, but im not sure I've done it right, and normalizers dont allow you to use the stop word filter. I appreciate this is a bit rambly, but any advice anyone can give would be well received. Many Thanks",
    "website_area": "discuss"
  },
  {
    "id": "f64a5739-e469-4a4f-8a7a-5fcec70d1f08",
    "url": "https://discuss.elastic.co/t/return-matching-analyzed-field-when-querying/215889",
    "title": "Return matching analyzed field when querying",
    "category": [
      "Elasticsearch"
    ],
    "author": "nogias",
    "date": "January 21, 2020, 2:28pm",
    "body": "Hi everybody, Context: My real product is a bit complex, so I'm explaining it as blog post, then. I have blog posts which are stored in ES. I have an analyzer to analyze posts' content into tokens. The analyzer will be changed and might become more complex as the product grows up. Everytime a user search for posts, say he's searching \"multi thread programming java\", I would like to know which analyzed tokens (token, start_offset, end_offset) of field \"post.content.analyzed_my_way\" match user's search string, So that I can make these words bold (like Google search). This field ideally should be return along with each hits when request ES query. Problems: \"post.content.analyzed_my_way\" is not stored by default, so it's not returned through \"store_fields\". Is it a good practice to store the analyzed field? Logic of the analyzer is complex and will be changed often, so I cannot mimic it outside of ES. That violates DRY, too. Calling POST /post/_analyze to analyze a content on fly doesn't seems a good choice, because it repeats the task of querying (ES already analyzed/indexed post documents). GET /post/_doc/post-id/_termvectors seems a bit better but still not solve my #4 problem. While querying, ES doesn't return position of matching token in it explanations If \"post.content\" becomes \"post.contents\" (array), #3 solution doesn't point out which content a analyzed token belong to. Any helps will be helpful and appreciated. Thank you for reading.",
    "website_area": "discuss"
  },
  {
    "id": "f2ca2651-da1e-4242-87d6-f025d4ddd789",
    "url": "https://discuss.elastic.co/t/search-as-you-type-datatype-text-array-not-working/215888",
    "title": "Search-as-you-type datatype + text array = not working",
    "category": [
      "Elasticsearch"
    ],
    "author": "traed",
    "date": "January 21, 2020, 1:47pm",
    "body": "I'm trying out the search_as_you_type datatype instead of implementing a custom analyser for autosuggestions. My old solution worked when searching a text-field such as: 'text' => [ 'type' => 'text', 'fields' => [ 'suggest' => [ 'type' => 'text', 'analyzer' => 'autosuggest', 'search_analyzer' => 'standard', ] ] ] Where a document could be: { \"_index\": \"someindex\", \"_type\": \"_doc\", \"_id\": \"1\", \"_source\": { \"text\": [ \"some text\", \"other text\" ], \"original_text\": null, \"status\": \"good\" } } And the search query would a bool query containing: bool: { must: [ { multi_match: { query: \"some\", fields: ['text.suggest^2', 'original_text.suggest'], fuzziness: 'AUTO' } } ], filter: [ terms: { status: \"good\" } ] } The text field can be either a string or an array of strings. When changing the text field mapping to: 'text' => [ 'type' => 'search_as_you_type' ] ... and the query to: bool: { must: [ { multi_match: { query: \"some\", fields: [ 'text^10', 'text._index_prefix^2', 'original_text', 'original_text._index_prefix', ], fuzziness: 'AUTO' } } ], filter: [ terms: { status: \"good\" } ] } ... i still get results where text is a single string, but documents where text is an array of strings don't show up. How come?",
    "website_area": "discuss"
  },
  {
    "id": "b3a06483-d06d-478b-9455-510aa1e71c24",
    "url": "https://discuss.elastic.co/t/which-is-the-right-tool-to-save-and-digest-user-search-queries/215648",
    "title": "Which is the right tool to save and digest user search queries?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Aaron_Pitts",
    "date": "January 19, 2020, 10:07pm January 21, 2020, 1:02pm",
    "body": "Hi guys, I have a wordpress site which uses Elastic search. It is a site to search many thousands of watches. The user can filter by many things such as dial/strap/case materials, sizes, colours, brand etc. What would be the best tool to allow us to digest this data in order to see what users are searching for the most? Is Kibana the correct tool or logstash? It seems a bit unclear. Many thanks",
    "website_area": "discuss"
  },
  {
    "id": "f7a7de80-6748-49d3-8dc0-aad3db3d8b3e",
    "url": "https://discuss.elastic.co/t/null-value-in-mapping-for-individual-components-of-date-range/215632",
    "title": "Null_value in mapping for individual components of date_range",
    "category": [
      "Elasticsearch"
    ],
    "author": "dougbergh",
    "date": "January 19, 2020, 5:06pm January 21, 2020, 12:54pm",
    "body": "Mapping below. Docs need to be able to have non-null start date and null end date. How do I specify null_value for each date in date_range individually? { \"de-mr-j-2019.06\" : { \"mappings\" : { \"date_detection\" : false, \"numeric_detection\" : false, \"properties\" : { \"@effective_time\" : { \"type\" : \"date\", \"null_value\" : \"1970-01-01T00:00:00+00:00\" }, \"@effective_time_end\" : { \"type\" : \"date\", \"null_value\" : \"1970-01-01T00:00:00+00:00\" }, \"@effective_time_range\" : { \"type\" : \"date_range\" },",
    "website_area": "discuss"
  },
  {
    "id": "1ba8e5e4-4cfb-466c-910d-b4b54280c83b",
    "url": "https://discuss.elastic.co/t/how-to-check-if-query-applies-to-new-document/215873",
    "title": "How to check if query applies to new document",
    "category": [
      "Elasticsearch"
    ],
    "author": "Marcin_Kukielka",
    "date": "January 21, 2020, 11:35am",
    "body": "Hi, Is there an efficient way to check whether document will be in result set of given query? I have a set of queries that I need to iterate and check if the new document applies to them, in terms of filtering and searching by field value. I know that I can run the query and iterate through result set and check id but this seems a little bit lame. Cheers Marcin",
    "website_area": "discuss"
  },
  {
    "id": "41aa57f6-ebef-4139-9754-9bd53da6e0a2",
    "url": "https://discuss.elastic.co/t/all-index-deleting-error/215396",
    "title": "All Index deleting Error",
    "category": [
      "Elasticsearch"
    ],
    "author": "eogredici",
    "date": "January 16, 2020, 11:57pm January 17, 2020, 6:02am January 17, 2020, 4:53pm January 20, 2020, 10:20am January 20, 2020, 11:10am January 20, 2020, 11:13am January 21, 2020, 8:37am January 21, 2020, 11:31am",
    "body": "Hi, i am using elasticSearch 7.3 version. My All index were deleted even though there was no intervention. Even i checked disk usage, memory usage, gc.log but i found errors.  attached my elastic log. Could you please chechk this log file? error_11586774 40 KB error_21586774 894 KB error_31586774 902 KB error_41591780 896 KB error_51649780 911 KB error_61827780 902 KB",
    "website_area": "discuss"
  },
  {
    "id": "d91054e2-0f12-4d7f-895e-9f343d25c3f0",
    "url": "https://discuss.elastic.co/t/resthighlevelclient-elastic-listener-timeout-after-waiting-for-300000-ms/215869",
    "title": "RestHighLevelClient elastic listener timeout after waiting for [300000] ms",
    "category": [
      "Elasticsearch"
    ],
    "author": "ANGI2019",
    "date": "January 21, 2020, 11:16am",
    "body": "we're running a 3 node cluster of Elasticsearch, version 6.2.4 we launch an integration of new data by batch every morning. we have listener timeout after waiting for [300000] ms every first request restHighLevelClient.bulk(request); we have this error after a moment of inactivity of the culster in this case All night inactivity Is there a way to fix it?",
    "website_area": "discuss"
  },
  {
    "id": "b0ddd19c-4540-4708-a5c1-bb86f3e91e3d",
    "url": "https://discuss.elastic.co/t/how-to-query-for-changes/215812",
    "title": "How to query for changes",
    "category": [
      "Elasticsearch"
    ],
    "author": "uniquename",
    "date": "January 21, 2020, 2:20am January 21, 2020, 7:05am January 21, 2020, 11:07am",
    "body": "Hi, I am looking for a way to query for changes in my elasticsearch documents. I have devices which periodically check in with their on/off status so my document will look something like this: @timestamp - January 21st 2020, 15:15:33.459 on_status - \"on\" error - 2 How can I write a query which will return all the documents where the values for on_status or error have changed from their previous values (ie. the device turned off)? What about if I want to list every time error went up from 0/back down to 0?",
    "website_area": "discuss"
  },
  {
    "id": "dd487e51-6a17-47c5-9de4-e6574f232366",
    "url": "https://discuss.elastic.co/t/how-to-compare-two-fields-of-same-name-on-different-doc-type/215860",
    "title": "How to compare two fields of same name on different doc_type",
    "category": [
      "Elasticsearch"
    ],
    "author": "vimal",
    "date": "January 21, 2020, 10:36am January 21, 2020, 10:59am",
    "body": "Hi, I have two oracle tables containing millions of rows in each table. I have index all records into elasticsearch using logstash jdbc plugin. The Idea is to compare difference between two tables using Elasticsearch in fast manner. Faster than SQL. I have index called myindex and two documents type table1 and table2. I have to compare column1 of table1 document type with column1 of table2 document type. How can it be achieved in elastic search?",
    "website_area": "discuss"
  },
  {
    "id": "5f6ff376-5370-49fe-bdaf-82884a36cb63",
    "url": "https://discuss.elastic.co/t/backup-of-elasticsearch-cluster-running-in-kubernetes-gke/215858",
    "title": "Backup of Elasticsearch cluster running in Kubernetes (GKE)",
    "category": [
      "Elasticsearch"
    ],
    "author": "jagan1333",
    "date": "January 21, 2020, 10:28am January 21, 2020, 10:50am",
    "body": "Hi How can I take backup of Elasticsearch cluster running in Kubernetes as a statefulsets (Google Kubernetes Engine). Thanks in advance!",
    "website_area": "discuss"
  },
  {
    "id": "ffce006c-19e6-4043-a4b5-8432b2f972b0",
    "url": "https://discuss.elastic.co/t/no-parameter-named-configdir-when-using-secrets-on-puppet-module/215847",
    "title": "No parameter named configdir when using secrets on puppet module",
    "category": [
      "Elasticsearch"
    ],
    "author": "lofic",
    "date": "January 21, 2020, 9:18am January 21, 2020, 10:43am",
    "body": "Hello, this is working : elasticsearch::instance { 'es-01': config => { 'network.host' => '0.0.0.0' }, } But as soon as I add some secrets secrets => ... I get : no parameter named 'configdir' on Elasticsearch_keystore[es-01] (...) elasticsearch/manifests/instance.pp, line: 502 Any idea ? Note : may be you could add a tag 'puppet' for topics. Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "a7fe5eb6-61e4-4d41-bfe7-2c1ee9586088",
    "url": "https://discuss.elastic.co/t/elasticsearch-ingest-pipeline-on-failure-field-removal-problem/215861",
    "title": "Elasticsearch ingest pipeline - on failure field removal problem",
    "category": [
      "Elasticsearch"
    ],
    "author": "pastechecker",
    "date": "January 21, 2020, 10:40am",
    "body": "Hello. I am trying to test if the field is a valid IP address. If not, I want to remove it. I also stick to the ECS convention. My ingest pipeline: PUT _ingest/pipeline/ingestion_pipeline_geo { \"description\": \"geoenrichment\", \"processors\": [ { \"dot_expander\": { \"field\": \"host.ip\", \"ignore_failure\": true } }, { \"grok\": { \"field\": \"host.ip\", \"patterns\": [ \"^%{IP:host.ip}$\" ], \"on_failure\": [ { \"remove\": { \"field\": \"host.ip\", \"ignore_failure\": true, \"ignore_missing\": true } } ] } } ] } My data: PUT my_index/_doc/1?pipeline=ingestion_pipeline_geo { \"host.ip\": \"1.2.3.433\", \"host.name\": \"h0012\", \"host.id\": \"prod server\" } My result: GET my_index/_doc/1 { \"_index\" : \"my_index\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 13, \"_seq_no\" : 12, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"host\" : { }, \"host.name\" : \"h0012\", \"host.id\" : \"prod server\" } } When the IP is valid all works fine: PUT my_index/_doc/2?pipeline=ingestion_pipeline_geo { \"host.ip\": \"1.2.3.42\", \"host.name\": \"h0012\", \"host.id\": \"prod server\" } GET my_index/_doc/2 { \"_index\" : \"my_index\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_version\" : 1, \"_seq_no\" : 13, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"host\" : { \"ip\" : \"1.2.3.42\" }, \"host.name\" : \"h0012\", \"host.id\" : \"prod server\" } } Why I do fail to remove the field host.ip and being left with the \"host\": {}? Is there any other workaround than removing \"host\" in the on_failure section? (Which is not accurate tbh). eg. PUT _ingest/pipeline/ingestion_pipeline_geo { \"description\": \"geoenrichment\", \"processors\": [ { \"dot_expander\": { \"field\": \"host.ip\", \"ignore_failure\": true } }, { \"grok\": { \"field\": \"host.ip\", \"patterns\": [ \"^%{IP:host.ip}$\" ], \"on_failure\": [ { \"remove\": { \"field\": \"host\", \"ignore_failure\": true, \"ignore_missing\": true } } ] } } ] } PUT my_index/_doc/2?pipeline=ingestion_pipeline_geo { \"host.ip\": \"1.2.3.42s\", \"host.name\": \"h0012\", \"host.id\": \"prod server\" } GET my_index/_doc/2 { \"_index\" : \"my_index\", \"_type\" : \"_doc\", \"_id\" : \"2\", \"_version\" : 2, \"_seq_no\" : 14, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"host.name\" : \"h0012\", \"host.id\" : \"prod server\" } } Observed behavior is on ES 7.2.0. Thanks for any hints.",
    "website_area": "discuss"
  },
  {
    "id": "9a3ce995-9715-4082-ab6f-348a10c1f3e0",
    "url": "https://discuss.elastic.co/t/data-node-1-not-enough-master-nodes-discovered-during-pinging-found-but-needed-1-pinging-again/215291",
    "title": "] [data_node_1] not enough master nodes discovered during pinging (found [[]], but needed [1]), pinging again",
    "category": [
      "Elasticsearch"
    ],
    "author": "nidhi1",
    "date": "January 16, 2020, 10:21am January 16, 2020, 10:55am January 17, 2020, 7:17am January 17, 2020, 7:20am January 17, 2020, 7:41am January 17, 2020, 9:45am January 17, 2020, 10:13am January 17, 2020, 10:19am January 17, 2020, 11:08am January 17, 2020, 12:02pm January 20, 2020, 3:46am January 20, 2020, 6:49am January 20, 2020, 7:01am January 20, 2020, 7:30am January 20, 2020, 7:44am January 20, 2020, 8:02am January 20, 2020, 8:11am January 20, 2020, 9:04am January 20, 2020, 12:23pm January 20, 2020, 12:25pm",
    "body": "Hi I have two two node cluster on red hat Enterprise Linux server . Both nodes are running on different different server. when i run elastic search for the server having data node ,getting such error",
    "website_area": "discuss"
  },
  {
    "id": "b94c5e1a-adf0-413d-8557-8bf8fd8e46d9",
    "url": "https://discuss.elastic.co/t/document-index-error-index-blocked-by-forbidden-12-index-read-only-allow-delete-api/215833",
    "title": "Document Index error:- index [....] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)]",
    "category": [
      "Elasticsearch"
    ],
    "author": "krushnakantB",
    "date": "January 21, 2020, 8:19am January 21, 2020, 8:23am January 21, 2020, 9:19am January 21, 2020, 9:41am January 21, 2020, 9:58am",
    "body": "Hi All, As I mentioned in the subject we got an error like blocked by: [FORBIDDEN/12/index read-only / allow delete (api)]. For this I have checked that it might be low disk space. But in our case we have free space of 56GB is available out of 100GB in ES Index drive and rootfs have 38GB free space out of 50GB and in the configuration we haven't mention any low disk watermark settings yet. We are using Elasticsearch 7.2.0 and Developer is inserting data through REST API. Please help to resolve this issue.",
    "website_area": "discuss"
  },
  {
    "id": "154a59bd-03fc-4ea4-ae34-0de63454f7c1",
    "url": "https://discuss.elastic.co/t/operating-system-interoperability/215850",
    "title": "Operating System Interoperability",
    "category": [
      "Elasticsearch"
    ],
    "author": "BenBell",
    "date": "January 21, 2020, 9:39am",
    "body": "Hi, Does Elastic Search support multiple operating systems? I am in a position where I want to spin up an ES cluster - preferably on Linux. We do not yet have a corporate image for Linux (although one is being developed now). If I were to spin up a 3master, 2client, 4*data cluster on Windows Server 2016 and then \"swap\" nodes one-by-one once we have a working Linux Image - would this be supported? My intention would be to: stand up a 5th data node on Linux and then decommission the 1st Windows Server stand up a 6th, decommission 2nd 7th - 3rd 8th - 4th (then client nodes) (then master nodes) Given that Elastic Search works on both Windows and Linux - is there any reason why this wouldn't work or wouldn't be supported? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "ecc56b00-0b46-4526-a20d-15008492fdc9",
    "url": "https://discuss.elastic.co/t/kibana-query-match-and-not-eixsts-on-same-field-in-a-single-query/215765",
    "title": "Kibana Query - match and not eixsts on same field in a single query",
    "category": [
      "Elasticsearch"
    ],
    "author": "suren-raymond",
    "date": "January 20, 2020, 3:32pm January 20, 2020, 3:50pm January 21, 2020, 5:59am January 21, 2020, 6:23am January 21, 2020, 7:30am January 21, 2020, 7:39am January 21, 2020, 9:10am January 21, 2020, 9:34am January 21, 2020, 9:28am",
    "body": "i've a index document field called \"historicalData\". Usually it has a value set [true,false]. But few documents of an index are not having the above said field \"historicalData\" at all. Wanted to query the documents with the below conditions. Please help out: documents with historicalData='false' and no field \"historicalData\". Index has 5 documents: 2 documents -> with historicalData='true' 2 documents -> with historicalData ='false' 1 document -> no field exists with historicalData my query should return 3 documents.",
    "website_area": "discuss"
  },
  {
    "id": "c8e9edb2-bea4-477d-8e25-fa77e9755917",
    "url": "https://discuss.elastic.co/t/limiting-repetitive-log-messages/215844",
    "title": "Limiting Repetitive Log Messages",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 21, 2020, 9:03am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "cdb1c6c3-9394-4591-9346-fb3fae8981b9",
    "url": "https://discuss.elastic.co/t/update-using-elasticsearch-restlevel-client-7-5-1-is-failing-on-elasticsearch-6-8-aws-cluster/214231",
    "title": "Update using Elasticsearch restlevel client 7.5.1 is failing on Elasticsearch 6.8 AWS cluster",
    "category": [
      "Elasticsearch"
    ],
    "author": "Aishwarya_Gopikrishn",
    "date": "January 8, 2020, 12:14pm January 8, 2020, 12:43pm January 9, 2020, 12:59pm January 9, 2020, 1:01pm January 9, 2020, 2:50pm January 21, 2020, 7:21am January 21, 2020, 8:19am",
    "body": "Hi Team, We do own 2 AWS Elasticsearch clusters, out of which one got upgraded to 7.1 version while the latter is still on 6.8 version. We did scala code changes to accommodate this upgradation(as our existing jobs were failing with unsupported version error with the new 7.x cluster). Post the code change, we were planning the code built using new maven dependency against the cluster running with 6.8 version. But the update jobs on index failed with the **\"{\"error\":{\"root_cause\":[{\"type\":\"invalid_type_name_exception\",\"reason\":\"Document mapping type name can't start with '_', found: [_update]\"}],\"type\":\"invalid_type_name_exception\",\"reason\":\"Document mapping type name can't start with '_', found: [_update]\"},\"status\":400}\"** Maven Dependency org.elasticsearch.client elasticsearch-rest-high-level-client 7.5.1 The error is obvious as the POST calls made using 7.5.1 and 6.8 versions are quite different as follows: W.r.t ES 7.5.1 => POST test/_update/1 W.r.t ES 6.8 => POST test/_doc/1/_update ES 6.8 cluster is expecting _doc keyword in the URL which is not supported in 7.5.1. Is there any way we can reuse the code by switching the api version while making the call?",
    "website_area": "discuss"
  },
  {
    "id": "b6ea943b-f9da-406e-aa18-f6a73242ef60",
    "url": "https://discuss.elastic.co/t/security-is-enabled-by-default-when-debug/215811",
    "title": "Security is enabled by default when debug",
    "category": [
      "Elasticsearch"
    ],
    "author": "Yanchun",
    "date": "January 21, 2020, 2:16am January 21, 2020, 6:53am",
    "body": "Hi, I compiled es 6.8.5 from source code with the command ./gradlew run --debug-jvm , and I want to debug with IDEA, but the xpack.security.enabled is default true, it's so inconvenient for debug. I ever compiled 6.4, it's default false. The generated package is in distribution/build/cluster/run\\ node0/elasticsearch-6.8.5-SNAPSHOT, and the generate config is below: cluster.name: distribution_run node.name: node-0 pidfile: /Volumes/Files/Study/Elastic/ES/source/elasticsearch/distribution/build/cluster/run node0/es.pid path.repo: /Volumes/Files/Study/Elastic/ES/source/elasticsearch/distribution/build/cluster/shared/repo path.shared_data: /Volumes/Files/Study/Elastic/ES/source/elasticsearch/distribution/build/cluster/shared/ node.attr.testattr: test discovery.zen.master_election.wait_for_joins_timeout: 5s node.max_local_storage_nodes: 1 http.port: 9200 transport.tcp.port: 9300 cluster.routing.allocation.disk.watermark.low: 1b cluster.routing.allocation.disk.watermark.high: 1b cluster.routing.allocation.disk.watermark.flood_stage: 1b script.max_compilations_rate: 2048/1m xpack.monitoring.enabled: true xpack.sql.enabled: true xpack.rollup.enabled: true xpack.security.enabled: true discovery.zen.hosts_provider: file discovery.zen.ping.unicast.hosts: [] All these files are re-generated when I build. How can I disable the xpack.security. And I remember the security is disabled in basic license, but now it's enabled.",
    "website_area": "discuss"
  },
  {
    "id": "61aacf93-1dec-48a3-be22-cf8d78205a46",
    "url": "https://discuss.elastic.co/t/how-to-take-indexes-backup-node-wise-in-cluster/215742",
    "title": "How to take indexes backup node wise in cluster",
    "category": [
      "Elasticsearch"
    ],
    "author": "vishnuvardhan",
    "date": "January 20, 2020, 1:34pm January 20, 2020, 3:17pm January 20, 2020, 3:51pm January 20, 2020, 4:20pm January 20, 2020, 4:25pm January 20, 2020, 4:28pm January 20, 2020, 4:37pm January 21, 2020, 3:14am January 21, 2020, 5:01am January 21, 2020, 6:26am January 21, 2020, 6:39am",
    "body": "We need to take indexes backup node wise or there would be cluster . how to take the backup please help me .",
    "website_area": "discuss"
  },
  {
    "id": "5c984ab0-3d84-4296-8294-055b4498cc61",
    "url": "https://discuss.elastic.co/t/wildcard-in-the-field-name-in-suggest-pharse-query/215824",
    "title": "Wildcard in the field name in suggest pharse query",
    "category": [
      "Elasticsearch"
    ],
    "author": "avinash_kumawat",
    "date": "January 21, 2020, 6:31am",
    "body": "Hello All, I am trying to implement did you mean functionality using elasticsearch, I wanted to search the text in all the fields using wildcard without knowing the field name. Is there any way to do that I know query string support wildcard in the field name like this GET /my_index_name/_search { \"highlight\": { \"fields\": { \"\":{} } }, \"query\": { \"query_string\": { \"fields\": [\"\"], \"lenient\": true, \"query\": \"wireframes\" } } } I want the same funcationality in sugges phrase query where i can pass wildcard character in the field. GET /my_index_name/_search { \"suggest\": { \"text\": \"wirefames dyng\", \"simple_phrase\": { \"phrase\": { \"field\": \"title.shingle\", \"size\":10, \"max_errors\":2, \"confidence\": 0.0, \"direct_generator\": [ { \"field\": \"title.shingle\" } ], \"collate\": { \"query\": { \"source\": { \"match\": { \"title\": { \"query\": \"{{suggestion}}\", \"operator\": \"and\" } } } }, \"prune\": \"true\" } } } } }",
    "website_area": "discuss"
  },
  {
    "id": "fc0d6acd-ff9a-40e5-bb57-8c0b51dea99e",
    "url": "https://discuss.elastic.co/t/index-only-collecting-upto-215kb-data-then-stops/215547",
    "title": "Index only collecting upto 215Kb data, then stops",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mehak_Bhargava",
    "date": "January 18, 2020, 6:22am January 18, 2020, 7:56am January 20, 2020, 6:23pm January 21, 2020, 5:26am",
    "body": "I have multiple indexes creating which gathers logs from different files. After collecting 215 Kb and 149kb worth of data, the index stops collecting more logs. Why is that? will it stop collecting when I have real time files too in future?",
    "website_area": "discuss"
  },
  {
    "id": "6cb1b4cb-6d3e-418f-a3ad-37740f83a517",
    "url": "https://discuss.elastic.co/t/getting-throttled-by-msearch/215752",
    "title": "Getting throttled by _msearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "doron",
    "date": "January 20, 2020, 2:30pm January 20, 2020, 2:55pm January 21, 2020, 5:17am",
    "body": "My scenario is as follows: I want to run a very large number of queries and I want to fully utilize my cluster (40 data nodes X 16 CPUs). I am batching my queries (400 per batch) and sending them via _msearch, however it seems I'm getting throttled. The cluster CPUs hardly get utilized, and I simply cannot get past ~10 seconds for 400 queries, no matter how I play with the max_concurrent_searches and max_concurrent_shard_requests parameters. The took values per each query simply increase as I increase the concurrency, but the total time remains the same. Any idea what I'm doing wrong here? I am using ES 6.5.3. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "eb5b9b02-7d3a-45b7-bc9d-4bf011b50cea",
    "url": "https://discuss.elastic.co/t/how-to-run-multiple-esrally-instances-from-same-server/215818",
    "title": "How to run multiple esrally instances from same server",
    "category": [
      "Elasticsearch"
    ],
    "author": "suvarna",
    "date": "January 21, 2020, 4:43am",
    "body": "Hi currently we are not able to run two esrally instances from same system .. can you please let me know there its feasible or not ? If its feasible please let know about the configuration . I am using eventdata track. Thanks and Regards, Sevanthi.S",
    "website_area": "discuss"
  },
  {
    "id": "f1b9a6e7-caa9-49b1-9843-c66c9397f69c",
    "url": "https://discuss.elastic.co/t/elasticsearch-6-8-does-not-support-searches-against-all-field-still-present-in-es-5-6-created-index/215806",
    "title": "Elasticsearch 6.8 does not support searches against _all field still present in ES 5.6 created index",
    "category": [
      "Elasticsearch"
    ],
    "author": "blindwitness",
    "date": "January 20, 2020, 11:35pm",
    "body": "Hello, In production, we use Elasticsearch to index our data for search related purposes. To interact with our current ES 5.6 cluster, we use the Java transport client. We are in the process of preparing for an upgrade from ES 5.6 to ES 6.8. We plan on switching to the Java high level REST client as part of this process. According to the ES reference manual, each major version of ES should be able to read and support indices created within its same major version or one prior. So, if I understand this correctly, a cluster of nodes running ES 6.8 should be able to read and support indices created in any minor version of ES 6.x as well as any minor version of ES 5.x. If this is true, then ES 6.8 should have no problem reading and supporting our indices created with ES 5.6. So far, this has been true except for when it comes to searches against the _all field. We understand that the _all field is deprecated and that we will be unable to create new indices in ES 6.x with this field enabled or referenced in any form. However, if we simply remove our usage of the _all field in one fell swoop and switch over to our own special catch all field that we write to through the use of the new copy_to parameter, then this means that all users would be forced to reindex before searches that utilize the new catch all field are successful. If we do not enforce a rebuild in this case, then users would be making searches against an old index (one created in 5.6) that does not yet have knowledge of the new catch all field. For example, let's assume that we have an index built in ES 5.6 that has the _all field enabled and used. Searches made against this index that specify usage of the _all field will be successful. Now, let's assume that we upgrade to ES 6.8 and release a new version of our code where searches are made against the index specifying the usage of our new catch all field. Well, until we reindex the index built during 5.6 so that it has knowledge of the new catch all field, these searches will fail. As a result, what we would like to do is have a transition period where we temporarily still search against the _all field in ES 6.8 until all indexes are rebuilt without the _all field, thus eliminating the need for it. However, during our testing of executing searches against ES 6.8 that specify the _all field as the target field for querying against, we are seeing inconsistent behavior when compared to how the same searches worked in ES 5.6. These issues are specifically around usages of the wildcard operator, * , within the query strings that are targeting the _all field. For example, the following query string search payload against the _all field for an index in ES 5.6 expectedly gives us all documents that have any string that ends with \"super\" that is part of the _all field: GET index/_search { \"query\": { \"query_string\" : { \"query\" : \"*super\", \"default_field\" : \"_all\", \"fields\" : [ ], \"use_dis_max\" : true, \"tie_breaker\" : 0.0, \"default_operator\" : \"or\", \"auto_generate_phrase_queries\" : false, \"max_determinized_states\" : 10000, \"enable_position_increments\" : true, \"fuzziness\" : \"AUTO\", \"fuzzy_prefix_length\" : 0, \"fuzzy_max_expansions\" : 50, \"phrase_slop\" : 0, \"escape\" : false, \"split_on_whitespace\" : true, \"boost\" : 1.0 } } } While the same query string search payload (adjusted for unrelated deprecated parameters) against the _all field for the same index that is now running in ES 6.8 instead gives us every single document in the index as if it makes no attempt to see if super is in the _all field: GET index/_search { \"query\": { \"query_string\" : { \"query\" : \"*super\", \"default_field\" : \"_all\", \"fields\" : [ ], \"type\" : \"best_fields\", \"default_operator\" : \"or\", \"max_determinized_states\" : 10000, \"enable_position_increments\" : true, \"fuzziness\" : \"AUTO\", \"fuzzy_prefix_length\" : 0, \"fuzzy_max_expansions\" : 50, \"phrase_slop\" : 0, \"escape\" : false, \"auto_generate_synonyms_phrase_query\" : true, \"fuzzy_transpositions\" : true, \"boost\" : 1.0 } } } It's almost as if the query string \"*super\" is now being tokenized at some point to be separated into \"*\" and \"super\", thus giving back every single document in the index rather than only those that have a string that in some way ends with super in the _all field. So, my question is, what has changed to cause this behavior when searching against the _all field? Are wildcard operators now treated/tokenized differently? Is this a known bug? This is an issue whether or not the _all field is specified as the default_field or in the \"fields\":[] collection. Interestingly, putting the * operator at the end does not result in the same behavior. It's as if the * is ignored in this case. Additionally, the wildcard operator, * , when used in queries against all other fields besides the _all field seems to work as expected. It is only when we have a * in the query string for a query against the _all field that we start to get unexpected results. i.e. every document in the index Thank you for your time and any help Evan",
    "website_area": "discuss"
  },
  {
    "id": "f5a7495a-b164-4a4a-b56a-c0ca97a9c441",
    "url": "https://discuss.elastic.co/t/suggesters-sort-for-weight/215798",
    "title": "Suggesters Sort for Weight?",
    "category": [
      "Elasticsearch"
    ],
    "author": "EliuFlorez",
    "date": "January 20, 2020, 10:10pm",
    "body": "Hi guys, can a query bring the result of my suggestion by weight? that is to say to weight I want to place the total of listings existing then suggesters would be ordered from the weight it contains? Ready PUT test { \"mappings\": { \"properties\" : { \"suggest\" : { \"type\" : \"completion\" }, \"title\" : { \"type\": \"keyword\" } } } } PUT test/_doc/1?refresh { \"suggest\" : [ { \"input\": \"test-10\", \"weight\" : 10 } ] } PUT test/_doc/2?refresh { \"suggest\" : [ { \"input\": \"test-2\", \"weight\" : 2 } ] } PUT test/_doc/3?refresh { \"suggest\" : [ { \"input\": \"test-3\", \"weight\" : 3 } ] } PUT test/_doc/4?refresh { \"suggest\" : [ { \"input\": \"test-100\", \"weight\" : 100 } ] } POST test/_search?pretty { \"suggest\": { \"song-suggest\" : { \"prefix\" : \"test\", \"completion\" : { \"field\" : \"suggest\" } } } }",
    "website_area": "discuss"
  },
  {
    "id": "eb81ea51-9e5c-4b5f-aec8-59a8dce89ae5",
    "url": "https://discuss.elastic.co/t/how-to-index-dropbox-documents/215791",
    "title": "How to index Dropbox documents?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Lisahtwy",
    "date": "January 20, 2020, 7:26pm January 20, 2020, 7:54pm January 20, 2020, 9:30pm January 20, 2020, 9:31pm",
    "body": "Hi, I have been using FScrawler to index files from directories. But I have files (around 2GB) in Dropbox which I want to index and use them with elastic search. Is there anyway to use FScrawler for Dropbox? I saw the Dropbox river plugin github page, but the commands mentioned there are not working with Elastic search 7.2.1. Could you please help me with indexing Dropbox files? Thanks, Lisa.",
    "website_area": "discuss"
  },
  {
    "id": "356f0297-e8de-4993-bcb3-50e058cfc855",
    "url": "https://discuss.elastic.co/t/elk-application-running-on-my-vm-can-i-send-a-link-to-someone-outside-my-network-to-access-the-localhost-5601-webpage/214681",
    "title": "ELK application running on my VM. Can I send a link to someone outside my network to access the localhost:5601 webpage?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mehak_Bhargava",
    "date": "January 10, 2020, 10:48pm January 11, 2020, 12:58am January 11, 2020, 1:23am January 11, 2020, 1:34pm January 13, 2020, 6:49pm January 13, 2020, 6:52pm January 13, 2020, 6:59pm January 13, 2020, 6:59pm January 13, 2020, 8:36pm January 13, 2020, 9:04pm January 13, 2020, 9:24pm January 13, 2020, 9:25pm January 15, 2020, 11:49pm January 16, 2020, 12:48am January 16, 2020, 7:58pm January 16, 2020, 8:35pm January 16, 2020, 10:27pm January 16, 2020, 11:33pm January 17, 2020, 4:11pm January 20, 2020, 6:24pm",
    "body": "I have ES, Logstasha nd Kibana running on my Virtual machine. I want to send a link accessible by someone outside my network to see the discover tab in Kibana I have running on localhost:5601. is it possible and how? In past, I have tried ways that disturbed my pipeline setting and I want to avoid breaking everything i have been building by messing the port forwarding or something.",
    "website_area": "discuss"
  },
  {
    "id": "24ae6f7f-d763-461a-82cc-3adb03f5de81",
    "url": "https://discuss.elastic.co/t/which-nodes-to-install-plugin/215783",
    "title": "Which nodes to install plugin",
    "category": [
      "Elasticsearch"
    ],
    "author": "jdcohen220",
    "date": "January 20, 2020, 5:42pm",
    "body": "Do plugins need to be installed on all nodes, or only data nodes?",
    "website_area": "discuss"
  },
  {
    "id": "b1f88c3c-8ade-416a-af7f-8b28539a1030",
    "url": "https://discuss.elastic.co/t/query-shard-exception-failed-to-execute-query-on-datetime-field/215355",
    "title": "Query_shard_exception failed to execute query on datetime field",
    "category": [
      "Elasticsearch"
    ],
    "author": "Giovanni_Gasola",
    "date": "January 16, 2020, 4:26pm January 20, 2020, 5:04pm",
    "body": "Hi guys, I have this node: { \"_index\" : \"impasti\", \"_type\" : \"impasti\", \"_id\" : \"2019-01-02T15:25:20\", \"_score\" : 1.9806902, \"_source\" : { \"sensor\" : \"Temperature\", \"mac_address\" : \"\", \"time\" : \"2019-01-02T14:25:19.728709Z\", \"unit\" : \"'C\", \"value\" : 20.937 }}, I try this query: POST /impasti/impasti/_search { 'query':{ \"query_string\": { \"default_field\": \"time\", \"query\": \"2019-01-02T14:25:19.728709Z\" } } } But the response is: \"type\": \"query_shard_exception\", \"reason\": \"Failed to parse query [2019-01-02T14:25:19.728709Z]\", Where is the mistake? Thanks guys",
    "website_area": "discuss"
  },
  {
    "id": "10a39d4e-e98f-4c84-b698-108f7a834ced",
    "url": "https://discuss.elastic.co/t/need-optimization-suggestions/215681",
    "title": "Need optimization suggestions",
    "category": [
      "Elasticsearch"
    ],
    "author": "Abhishek_Tiwari",
    "date": "January 20, 2020, 7:47am January 20, 2020, 4:57pm",
    "body": "Hi, My cluster shows us randome behaviour and load varies randomly with normal traffic Please suggest how should i debug it . Regards, Abhishek Tiwari",
    "website_area": "discuss"
  },
  {
    "id": "70d8c0a5-c383-43c0-a015-1ae5f9122cd1",
    "url": "https://discuss.elastic.co/t/update-mappings-takes-30-hours-for-9m-documents/215694",
    "title": "Update mappings takes 30 hours for 9M documents",
    "category": [
      "Elasticsearch"
    ],
    "author": "orenco",
    "date": "January 20, 2020, 9:20am January 20, 2020, 4:50pm",
    "body": "I'm using Elastic 2.4 . I'm trying to update mappings, but it takes 30 hours for Elastic with 9M documents. I'm running on a machine with 360 GB RAM, and Elastic is running with 30 GB. Is there a quick option to run the update mappings much faster ? maybe in parallel processes ? Is it possible to read documents from Elastic when update mapping is running ? Thanks !",
    "website_area": "discuss"
  },
  {
    "id": "83e75a0c-3e83-4e1d-a1cd-c830713d5885",
    "url": "https://discuss.elastic.co/t/after-update-the-7-4-2-too-7-5-1-dont-work/215685",
    "title": "After update the 7.4.2 too 7.5.1 , don't work",
    "category": [
      "Elasticsearch"
    ],
    "author": "econom",
    "date": "January 20, 2020, 9:45am January 20, 2020, 4:49pm",
    "body": "[2020-01-20T11:13:39,384][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [c7.lan] uncaught exception in thread [main] org.elasticsearch.bootstrap.StartupException: ElasticsearchException[failed to bind service]; nested: IOException[failed to test writes in data directory [/var/lib/elasticsearch/nodes/0/indices/Yxp8HoL3RiObmps0LACtBg/_state] write permission is required]; nested: AccessDeniedException[/var/lib/elasticsearch/nodes/0/indices/Yxp8HoL3RiObmps0LACtBg/_state/.es_temp_file]; at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:125) ~[elasticsearch-cli-7.5.1.jar:7.5.1] at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) ~[elasticsearch-7.5.1.jar:7.5.1] Caused by: org.elasticsearch.ElasticsearchException: failed to bind service at org.elasticsearch.node.Node.<init>(Node.java:607) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.node.Node.<init>(Node.java:253) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:221) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:221) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:349) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.5.1.jar:7.5.1] ... 6 more Caused by: java.io.IOException: failed to test writes in data directory [/var/lib/elasticsearch/nodes/0/indices/Yxp8HoL3RiObmps0LACtBg/_state] write permission is required at org.elasticsearch.env.NodeEnvironment.tryWriteTempFile(NodeEnvironment.java:1256) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.env.NodeEnvironment.assertCanWrite(NodeEnvironment.java:1224) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.env.NodeEnvironment.<init>(NodeEnvironment.java:314) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.node.Node.<init>(Node.java:273) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.node.Node.<init>(Node.java:253) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:221) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:221) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:349) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.5.1.jar:7.5.1] ... 6 more Caused by: java.nio.file.AccessDeniedException: /var/lib/elasticsearch/nodes/0/indices/Yxp8HoL3RiObmps0LACtBg/_state/.es_temp_file at sun.nio.fs.UnixException.translateToIOException(UnixException.java:90) ~[?:?] at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) ~[?:?] at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) ~[?:?] at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219) ~[?:?] at java.nio.file.Files.newByteChannel(Files.java:374) ~[?:?] at java.nio.file.Files.createFile(Files.java:651) ~[?:?] at org.elasticsearch.env.NodeEnvironment.tryWriteTempFile(NodeEnvironment.java:1253) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.env.NodeEnvironment.assertCanWrite(NodeEnvironment.java:1224) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.env.NodeEnvironment.<init>(NodeEnvironment.java:314) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.node.Node.<init>(Node.java:273) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.node.Node.<init>(Node.java:253) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:221) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:221) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:349) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.5.1.jar:7.5.1] ... 6 more",
    "website_area": "discuss"
  },
  {
    "id": "93fce20c-0fe0-41a3-b4b4-0d4b542f35d4",
    "url": "https://discuss.elastic.co/t/index-per-organization-or-one-index-for-all-organizations-to-scale/215302",
    "title": "Index per organization or one index for all organizations to scale",
    "category": [
      "Elasticsearch"
    ],
    "author": "agonzalez",
    "date": "January 16, 2020, 11:25am January 20, 2020, 1:36pm January 20, 2020, 4:43pm",
    "body": "I have one multitenant application that is storing lof of data like events in elasticsearch, each organization will search events only from their organization. My question is about query/search perfomance and speed to scale up to hundreds of organizations, it is better to have one index per organization like: events_orgA events_orgB events_orgC Or the performance is the same if we only have one index events for all organizations and we search filtering by Org",
    "website_area": "discuss"
  },
  {
    "id": "51f70b67-64ea-4e13-aaea-05244daafd58",
    "url": "https://discuss.elastic.co/t/statuscode-429-high-fielddata-memory-usage/215774",
    "title": "statusCode:429 - High fielddata memory usage",
    "category": [
      "Elasticsearch"
    ],
    "author": "Djaswan",
    "date": "January 20, 2020, 4:14pm January 20, 2020, 4:13pm January 20, 2020, 4:35pm",
    "body": "Hi, We experienced some strange behavior from Kibana, while opening Kibana we got the following error: Error: {\"statusCode\":429,\"error\":\"Too Many Requests\",\"message\":\"[circuit_breaking_exception] [parent] Data too large, data for [<http_request>] would be [4093269000/3.8gb], which is larger than the limit of [4063657984/3.7gb], real usage: [4093269000/3.8gb], new bytes reserved: [0/0b], with { bytes_wanted=4093269000 & bytes_limit=4063657984 & durability=\\\"PERMANENT\\\" }\"} We ran the command: \"GET /_cat/fielddata?v&fields=*\" and discovered that the problem was caused by a fielddata with high memory usage (3GB). This activated the circuit breaker and the errors in Kibana. The related node was also operating with high memory (95%). Clearing the fielddata memory solved the errors in Kibana and stabilized the node. We still dont know why the fielddata memory was that high and why its being used. We thought that by default the usage of fielddata was disabled. We also checked the index that contains the problem field, but I dont see any mappings with text type fields. Can someone help me out and clarify how the fielddata usage works and which factors can lead to the high memory usage.",
    "website_area": "discuss"
  },
  {
    "id": "fcc523aa-272a-482c-896c-feb34ebf181f",
    "url": "https://discuss.elastic.co/t/analyzer-in-ingest-node/214619",
    "title": "Analyzer in ingest node",
    "category": [
      "Elasticsearch"
    ],
    "author": "rvanegmond",
    "date": "January 10, 2020, 3:13pm January 13, 2020, 1:45pm January 16, 2020, 3:06pm January 20, 2020, 4:10pm",
    "body": "I'm looking for a way to use the (for example path_hierarchy) tokenizer(s) in a ingest pipeline. Is there anyway of doing this?",
    "website_area": "discuss"
  },
  {
    "id": "a0ac3bfb-ac8f-4af7-89e7-cfbd7436c2c7",
    "url": "https://discuss.elastic.co/t/elasticsearch-7-5-cluster-is-failed-to-setup/215243",
    "title": "[ElasticSearch 7.5] Cluster is failed to setup",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 16, 2020, 6:08am January 20, 2020, 4:09pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7672de3b-e7e1-4dee-8af1-b5e03bac9d33",
    "url": "https://discuss.elastic.co/t/publishing-the-elasticsearch-search-apis-to-a-api-gateway/214836",
    "title": "Publishing the elasticsearch Search APIs to a API gateway",
    "category": [
      "Elasticsearch"
    ],
    "author": "mruthyu",
    "date": "January 13, 2020, 1:14pm January 13, 2020, 1:27pm January 16, 2020, 5:40am January 20, 2020, 4:07pm",
    "body": "Is it possible to publish the Elasticsearch Search APIs to a API gateway like axway.",
    "website_area": "discuss"
  },
  {
    "id": "5c45ab75-b113-4722-8eec-0e3067764a00",
    "url": "https://discuss.elastic.co/t/date-formatting-for-import/215063",
    "title": "Date formatting for import",
    "category": [
      "Elasticsearch"
    ],
    "author": "dedicatedmanagers",
    "date": "January 15, 2020, 2:05am January 15, 2020, 10:45am January 15, 2020, 5:31pm January 20, 2020, 4:03pm",
    "body": "I have a date field that is in the following format: 01-JAN-20 When I map the field to dd-MMM-yy it won't accept the date above but it will accept the following: 01-Jan-20 Is there a way to get Elasticsearch to accept the former? PUT tctestindex { \"mappings\": { \"properties\": { \"date\": { \"type\": \"date\", \"format\": \"d-MMM-yy\" } } } } # works! PUT tctestindex/_doc/1 { \"date\": \"01-Jan-20\" } #fails! PUT tctestindex/_doc/2 { \"date\": \"01-JAN-20\" }",
    "website_area": "discuss"
  },
  {
    "id": "bdf5a851-4acd-4cf5-9cb3-4faba974cdeb",
    "url": "https://discuss.elastic.co/t/do-sharding-flush-hot-threads-indicate-slow-indexing/214663",
    "title": "Do sharding & flush hot threads indicate slow indexing?",
    "category": [
      "Elasticsearch"
    ],
    "author": "viniciof",
    "date": "January 10, 2020, 8:50pm January 13, 2020, 1:52pm January 15, 2020, 4:35pm January 20, 2020, 3:37pm",
    "body": "Hello everyone, I notice in my hot threads that it's spending a lot of time in write related sections of code. Also, I'm experiencing slower indexing rate than usual (45% decrease) Here's the output: https://gist.github.com/vinicioflores/fef2cd267224b1532c933eed57b4831c Please help, Regards,",
    "website_area": "discuss"
  },
  {
    "id": "5acefde6-86ff-4715-940f-7e92c7865e90",
    "url": "https://discuss.elastic.co/t/pagination-10000-document-total-limit/214897",
    "title": "Pagination 10000 document total limit",
    "category": [
      "Elasticsearch"
    ],
    "author": "agonzalez",
    "date": "January 13, 2020, 8:22pm January 13, 2020, 8:59pm January 16, 2020, 11:16am January 20, 2020, 1:34pm January 20, 2020, 3:16pm",
    "body": "I am using pagination from my .net app with nest client to just get 50 docs on each page of the grid and that works fine but the response to the search response.HitsMetadata.Total.Value is always 10000 event there are more documents. even i am paging 50 docs I want to show the real number of documents for that search in the app so user can redefine the filters. what is wrong? var query = new SearchDescriptor(); query.Index(FileEvent); query.From(skip); query.Size(take); query.Sort(s => s.Descending(f => f.Date)); var response = await _context.ElasticClient.SearchAsync(query); return new Tuple<long, List>(response.HitsMetadata.Total.Value, response.Documents.ToList());",
    "website_area": "discuss"
  },
  {
    "id": "09f20ac0-4432-4c11-979d-8676c033f620",
    "url": "https://discuss.elastic.co/t/elastic-cloud-custom-domain/214853",
    "title": "Elastic Cloud custom domain?",
    "category": [
      "Elasticsearch"
    ],
    "author": "agonzalez",
    "date": "January 13, 2020, 2:52pm January 20, 2020, 1:35pm January 20, 2020, 3:10pm",
    "body": "When it is going to be possible to have a custom domain for elastic in cloud? like elastic.mydomain.com so that we can embed url in applications and doesnt change and can enable fw rules for that.",
    "website_area": "discuss"
  },
  {
    "id": "966a9a34-c11a-47eb-9485-d620e7e74fcd",
    "url": "https://discuss.elastic.co/t/shards-based-on-specific-nodes/215763",
    "title": "Shards based on specific nodes",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 20, 2020, 3:09pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a01236f6-be5d-4da9-af66-db90887a5fcb",
    "url": "https://discuss.elastic.co/t/replace-in-scripted-field/215703",
    "title": "REPLACE in scripted field",
    "category": [
      "Elasticsearch"
    ],
    "author": "Anne_Kim",
    "date": "January 20, 2020, 9:55am January 20, 2020, 2:12pm January 20, 2020, 2:39pm",
    "body": "Hello, I'm trying to convert a string into a number by extracting dots with the help of regexp. Here is the scripted field for this: if (doc['build.keyword'].size() != 0) { def m = /[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/.matcher(doc['build.keyword'].value); if ( m.matches() ) { return m.replace(/./, '') } else { return null } } else { return null } However it raises a following error: 907459 59.9 KB The sub-string cannot be replaced in a scripted field? How can it be fixed? I suppose that I can find all groups consisting of digits and return them all, right?",
    "website_area": "discuss"
  },
  {
    "id": "43f35d97-0ab1-4559-b8de-75e099c3b0b7",
    "url": "https://discuss.elastic.co/t/brute-force-watcher-and-alert/215708",
    "title": "Brute Force Watcher and Alert",
    "category": [
      "Elasticsearch"
    ],
    "author": "mdp",
    "date": "January 20, 2020, 10:14am January 20, 2020, 2:06pm",
    "body": "Hi, I am looking at support/advice on modifying a watcher to accomplish: A single email and alert output for each occurrence where there a user has failed logins x times in x minutes. The alert and email to contain a combination of Username, Source IP, Hostname (all of which are available in the records. (Only 1 alert per combination, if a second username is failing this should be a unique alert/email) Currently we have the following which was created by a 3rd Party which did not satisfy the requirement. { \"trigger\": { \"schedule\": { \"interval\": \"5m\" } }, \"input\": { \"search\": { \"request\": { \"search_type\": \"query_then_fetch\", \"indices\": [ \"acme-dbauth--v2\" ], \"rest_total_hits_as_int\": true, \"body\": { \"size\": 0, \"query\": { \"bool\": { \"filter\": [ { \"match\": { \"event.action\": \"database_login\" } }, { \"match\": { \"event.outcome\": \"failure\" } }, { \"range\": { \"@timestamp\": { \"gte\": \"now-5m\" } } } ] } }, \"aggs\": { \"failed_logins\": { \"terms\": { \"field\": \"user.name\", \"size\": 10 } } } } } } }, \"condition\": { \"script\": { \"source\": \" def users = [];\\n for (def uname : ctx.payload.aggregations.failed_logins.buckets) {\\n if (uname.doc_count>2) {\\n users.add(uname.key);\\n }\\n }\\n \\n ctx.payload.users = users;\\n \\n ctx.payload._doc = [\\n '@timestamp': ctx.execution_time,\\n 'alert_id': ctx.watch_id,\\n 'cause_index': 'acme-dbauth--v2',\\n 'plain_reason': 'Multiple failed logins from same user(s)',\\n 'info1_key': 'user_names',\\n 'info1_val': users\\n ];\\n \\n return users.size()>0;\", \"lang\": \"painless\" } }, \"actions\": { \"send_email\": { \"throttle_period_in_millis\": 1800000, \"email\": { \"account\": \"exchange_account\", \"profile\": \"standard\", \"to\": [ \"coyote@acme.com\" ], \"subject\": \"Elastic Alert: dba_brute_force\", \"body\": { \"text\": \"Detected multiple failed logins from users: {{ctx.payload.users}}\" } } }, \"index_alert\": { \"index\": { \"index\": \"acme-dbauth-alerts\" } } } }",
    "website_area": "discuss"
  },
  {
    "id": "a72fe3a3-e74d-4536-bfa3-1c2bf79ed68a",
    "url": "https://discuss.elastic.co/t/how-to-see-how-long-we-can-retain-the-indexes-in-curator/215523",
    "title": "How to see how long we can retain the indexes in Curator?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mehak_Bhargava",
    "date": "January 17, 2020, 9:12pm January 20, 2020, 1:45pm",
    "body": "How can we see how long indexes can be retained for? Can we make a scheduled task for it? How can we maintain an index by looking into freeing up disk space in intervals? 1: action: delete_indices description: >- Delete indices older than 60 days (based on index name), for metricbeat- prefixed indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly. options: ignore_empty_list: True timeout_override: continue_if_exception: False disable_action: False filters: - filtertype: pattern kind: prefix value: metricbeat- exclude: - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 28 exclude: Here unit_count is 28 which means that it will be the number of days worth of data that we would like to retain. can it be more than 28 days? And if not in curator, how long can we retain data in indexes before it starts affecting performance? At 215kb, my index stopped taking logs.",
    "website_area": "discuss"
  },
  {
    "id": "03684114-751c-496f-a36d-93e15ff17761",
    "url": "https://discuss.elastic.co/t/slowlog-performance-could-it-deteriorate-clusters-performance/215740",
    "title": "Slowlog performance. could it deteriorate cluster's performance?",
    "category": [
      "Elasticsearch"
    ],
    "author": "rschirin",
    "date": "January 20, 2020, 1:11pm",
    "body": "hi, currently I'm using the slowlog feature on cluster (3 master, 10 data, 2 coordinating only). These are my settings: \"search\" : { \"slowlog\" : { \"level\" : \"trace\", \"threshold\" : { \"fetch\" : { \"warn\" : \"1s\", \"trace\" : \"200ms\", \"debug\" : \"500ms\", \"info\" : \"800ms\" }, \"query\" : { \"warn\" : \"2000ms\", \"trace\" : \"500ms\", \"debug\" : \"1000ms\", \"info\" : \"1500ms\" } } } } Do you know anything about potential deterioration of cluster performance (I am referring to the cluster that generates slowlog data, not to my monitoring cluster)? Daily it will create index with docs.count 220k and size 1.2 gb. I removed the truncating character from log4j.properties file, so I do not truncate generated query after 1000 characters. thanks",
    "website_area": "discuss"
  },
  {
    "id": "d58c3a4c-73fc-4575-8349-1121dea4704d",
    "url": "https://discuss.elastic.co/t/the-index-pattern-index-name-does-not-contain-any-of-the-following-compatible-field-types-geo-point/215689",
    "title": "The index pattern <INDEX_NAME>* does not contain any of the following compatible field types: geo_point",
    "category": [
      "Elasticsearch"
    ],
    "author": "ruranga",
    "date": "January 20, 2020, 9:28am January 20, 2020, 9:07am January 20, 2020, 9:29am January 20, 2020, 9:36am January 20, 2020, 9:52am January 20, 2020, 10:10am January 20, 2020, 10:30am January 20, 2020, 10:51am January 20, 2020, 10:52am January 20, 2020, 11:21am January 20, 2020, 11:42am January 20, 2020, 1:08pm",
    "body": "Hi, When I try to create a visualization -> coordinated map by using geoip i'm getting below error \"The index pattern <INDEX_NAME>* does not contain any of the following compatible field types: geo_point\" geoip error509533 98.3 KB But I'm getting below json out put in Kibana. \"geoip\": { \"continent_code\": \"NA\", \"country_code3\": \"US\", \"country_name\": \"United States\", \"longitude\": -97.822, \"location\": { \"lon\": -97.822, \"lat\": 37.751 }, \"timezone\": \"America/Chicago\", \"latitude\": 37.751, \"country_code2\": \"US\", \"ip\": \"104.75.84.8\" }, Also when check the Kibana -> DevTools -> GET /_template/logstash* I'm getting below \"geoip\" : { \"dynamic\" : true, \"properties\" : { \"ip\" : { \"type\" : \"ip\" }, \"latitude\" : { \"type\" : \"half_float\" }, \"location\" : { \"type\" : \"geo_point\" }, \"longitude\" : { \"type\" : \"half_float\" } } } As per the documentation I should be able to create a Coordinated map. But I'm getting above mentioned error. Can some one help me to sort this? Thanks in advance!",
    "website_area": "discuss"
  },
  {
    "id": "e7f0d72d-0864-4381-be5b-b7a42ce7248b",
    "url": "https://discuss.elastic.co/t/is-there-any-performance-impact-on-elastic-searches-when-you-have-a-lot-of-writes-on-index/215687",
    "title": "Is there any performance impact on Elastic searches when you have a lot of writes on index?",
    "category": [
      "Elasticsearch"
    ],
    "author": "sberthez",
    "date": "January 20, 2020, 8:28am January 20, 2020, 9:30am January 20, 2020, 11:04am January 20, 2020, 11:19am January 20, 2020, 1:06pm",
    "body": "Hi, We are currently working on an application using Elastic search and we try to evaluate benefits and constraints on each differents architecture choices related to spreading data on multiple indexes or using a single shared one. If you use an index with concurrent read/writes is there any penalties if you write too much especially for searches ? Regards",
    "website_area": "discuss"
  },
  {
    "id": "f38d554b-16fa-4026-a0aa-493825077019",
    "url": "https://discuss.elastic.co/t/enabling-ttl-rbac-on-an-existing-cluster-with-indexes/215414",
    "title": "Enabling TTL & RBAC on an existing cluster with Indexes",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 17, 2020, 5:37am January 17, 2020, 5:58am January 20, 2020, 11:22am January 20, 2020, 11:35am January 20, 2020, 11:53am January 20, 2020, 12:02pm January 20, 2020, 12:30pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a8877542-2ec1-4dfe-a371-b8e4a16353e7",
    "url": "https://discuss.elastic.co/t/what-i-specify-a-doc-with-a-field-other-than-id/215736",
    "title": "What I specify a doc with a field other than '_id'?",
    "category": [
      "Elasticsearch"
    ],
    "author": "khalidnajm",
    "date": "January 20, 2020, 12:24pm",
    "body": "In the MLT query, in order to search by document, rather than terms, I know I can specify a document by its _id. I find tedious, because I have to search for the document within the index and then copy other it's _id to my MLT query. I would rather search by path.virtual, or something else that's easily-accessible. Any advice? GET doc_cat/_search { \"query\": { \"more_like_this\" : { \"fields\" : [\"content\",\"file.content_type\"], \"like\" :[ { \"_index\" : \"doc_cat\", \"_id\" : \"59ffedd643299d3fff1f43f2dd43fe3c\" }, { \"_index\" : \"doc_cat\", \"_id\" : \"132c508aa6155c97206db951c4662987\" } ] } } }",
    "website_area": "discuss"
  },
  {
    "id": "2c873c02-f56a-47ab-9b09-7d4645a4bb83",
    "url": "https://discuss.elastic.co/t/how-to-write-rolling-upgrade-it-among-versions-in-elasticsearch-project/215606",
    "title": "How to write rolling upgrade IT among versions in elasticsearch project",
    "category": [
      "Elasticsearch"
    ],
    "author": "kaihong",
    "date": "January 19, 2020, 7:32am January 19, 2020, 9:13am January 20, 2020, 10:38am January 20, 2020, 12:08pm",
    "body": "Recently I checked out a new branch called 6.7.0_rc1 with some minor changes in elasticsearch, and I need to write some BWC test cases (UT or IT) to ensure rolling upgrade scenario from 6.7.0 to 6.7.0-rc1. Currently I didn't figure out how to implement this. Is there anyone can help me with this?",
    "website_area": "discuss"
  },
  {
    "id": "ebb9631b-4fcb-4e54-b387-6fbdab89ffe6",
    "url": "https://discuss.elastic.co/t/indexing-eml-files/215557",
    "title": "Indexing eml files",
    "category": [
      "Elasticsearch"
    ],
    "author": "ataoumi",
    "date": "January 18, 2020, 9:49am January 18, 2020, 12:03pm January 20, 2020, 12:03pm",
    "body": "Hi Everyone, I'm trying to index eml files and I'm not very excited by converting my files to HTML or other formats already supported. Any tips or suggestions Cheers",
    "website_area": "discuss"
  },
  {
    "id": "4952d43a-a5ac-4ec1-9704-7bb84ab57a68",
    "url": "https://discuss.elastic.co/t/working-with-changing-sets-of-synonyms/215334",
    "title": "Working with changing sets of synonyms",
    "category": [
      "Elasticsearch"
    ],
    "author": "ivanibash",
    "date": "January 16, 2020, 2:47pm January 17, 2020, 10:12am January 17, 2020, 11:20am January 17, 2020, 2:01pm January 17, 2020, 2:04pm January 17, 2020, 2:08pm January 17, 2020, 3:25pm January 17, 2020, 3:51pm January 20, 2020, 11:38am January 20, 2020, 11:37am",
    "body": "Hi, in our search case we have several sets of synonyms that we want to apply in different combinations dynamically (at query time). I read this article that talks about search_analyzer, but I believe for out use case we need it to be even more flexible than that. Can I define a synonym filter and include it in a custom analyzer on the fly, at query time? Or maybe define a bunch of synonym filters at indexing and then refer to them at query time? Is something like that possible? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "4bee9d71-ba3c-4ab8-83d9-721b01dead16",
    "url": "https://discuss.elastic.co/t/elasticsearch-has-stopped-and-no-longer-works/215653",
    "title": "Elasticsearch has stopped and no longer works",
    "category": [
      "Elasticsearch"
    ],
    "author": "4y555",
    "date": "January 20, 2020, 9:29am January 20, 2020, 6:17am January 20, 2020, 9:27am January 20, 2020, 11:28pm January 20, 2020, 10:51am January 20, 2020, 11:10am",
    "body": "Hi guys I was using elastic search (with the nextcloud), but the elastic just stopped after a while. Try to install other versions or reinstall the same version, but it doesn't work. Can you help me? Thank you very much! CentOS Linux release 7.7.1908 (Core) Php 7.3.13 Mysql 5.7.29 Nextcloud 17.0.2 elasticsearch-6.8.6.rpm readonlyrest-1.18.10_es6.8.6.zip [root@nextcloud ~]# sudo -u apache /usr/bin/php /var/www/html/occ fulltextsearch:check Full text search 1.3.5 - Search Platform: Elasticsearch 1.4.0 { \"elastic_host\": [ \"http://nextcloud:********@localhost:9200\" ], \"elastic_index\": \"nextcloud\", \"fields_limit\": \"10000\", \"es_ver_below66\": \"0\", \"analyzer_tokenizer\": \"standard\" } - Content Providers: Files 1.3.6 { \"files_local\": \"1\", \"files_external\": \"2\", \"files_group_folders\": \"1\", \"files_encrypted\": \"0\", \"files_federated\": \"0\", \"files_size\": \"20\", \"files_pdf\": \"1\", \"files_office\": \"1\", \"files_image\": \"0\", \"files_audio\": \"0\" } [root@nextcloud ~]# FILE /etc/elasticsearch/readonlyrest.yml readonlyrest: access_controle_rules: name: \"Require HTTP Basic Auth\" type: allow auth_key: nextcloud:nextcloud indices: [\"nextcloud\"] ADD /etc/elasticsearch/elasticsearch.yml xpack.security.enabled: false Elasticsearch service FAILED after 10 seconds. [root@nextcloud ~]# systemctl status elasticsearch  elasticsearch.service - Elasticsearch Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled; vendor preset: disabled) Active: failed (Result: exit-code) since Qui 2020-01-16 09:26:24 -03; 27s ago Docs: http://www.elastic.co Process: 1305 ExecStart=/usr/share/elasticsearch/bin/elasticsearch -p ${PID_DIR}/elasticsearch.pid --quiet (code=exited, status=1/FAILURE) Main PID: 1305 (code=exited, status=1/FAILURE) Jan 16 09:28:01 nextcloud.local systemd[1]: Started Elasticsearch. Jan 16 09:26:24 nextcloud.local systemd[1]: elasticsearch.service: main process exited, code=exited, status=1/FAILURE Jan 16 09:26:24 nextcloud.local systemd[1]: Unit elasticsearch.service entered failed state. Jan 16 09:26:24 nextcloud.local systemd[1]: elasticsearch.service failed. [root@nextcloud ~]# I've tested several versions, updated everything, searched several blogs, but I can't solve the problem. It is really frustrating. Can any kindness help me?",
    "website_area": "discuss"
  },
  {
    "id": "8715d42f-869c-4bb8-88b0-4b8b1e05d7a3",
    "url": "https://discuss.elastic.co/t/how-to-send-same-eventdata-track-1billion-json-documnets-to-multiple-es-instances/215667",
    "title": "How to send same eventdata track 1billion json documnets to multiple ES instances",
    "category": [
      "Elasticsearch",
      "Rally"
    ],
    "author": "suvarna",
    "date": "January 20, 2020, 5:54am January 20, 2020, 6:09am January 20, 2020, 11:04am",
    "body": "Hi , I have configured 4 ES instances independently in single server by providing unique values (for each ES instances) for below commands in Elasticsearch.yml. http.port transport.tcp.port path_data path_logs path_pid node.name cluster.name Successfully 4 ES instances are running in my server with different ports. I have started the Esrally with below commands esrally --track=eventdata --track-repository=eventdata --pipeline=benchmark-only --challenge=elasticlogs-1bn-load --track-params=\"/home/ssuvarna/parameter_custom_evendata.json\" --target-hosts=\"target_host.json\" --report-file=/home/ssuvarna/report.md --report-format=csv & target_host.json: { \"default\": [ {\"host\": \"127.0,0,1\", \"port\": 9200}, {\"host\": \"127.0,0,1\", \"port\": 9202}, {\"host\": \"127.0,0,1\", \"port\": 9203}, {\"host\": \"127.0,0,1\", \"port\": 9204} ] } Currently Esrally sending json documents to each ES instances .. and please find the below issue: 1)We are expecting Esrally to send 1Billion Json documents to each ES instances but we can see its sending \"250,000,000\" json documents to each ES instances (it means 1Billion/4 ES instance=\"250,000,000\" json docs). 2)Each ES instances should have indices name as \"elasticlogs_q-000001\" but only one ES instance will have indices name as \"elasticlogs_q-000001\" and rest of 3 ES instances will have indices name as \"elasticlogs_q_write\" Can you please let me know what changes I have to do on esrally to get the expected behavior ..",
    "website_area": "discuss"
  },
  {
    "id": "d2a37203-c663-4c6e-8758-132fa8a7fa37",
    "url": "https://discuss.elastic.co/t/list-indices-with-shards-on-a-given-node/215486",
    "title": "List indices with shards on a given node",
    "category": [
      "Elasticsearch"
    ],
    "author": "mikewillis",
    "date": "January 17, 2020, 3:31pm January 20, 2020, 10:35am",
    "body": "Elasticsearch 6. Is there a nice way to get a list of indices shards of which are on a given node? The only way I can find is to call _cat/shards and grep the output for the node name. Node information in Kibana Monitoring shows which indices are on that node but I don't know how it gets that list.",
    "website_area": "discuss"
  },
  {
    "id": "d72a06be-2d3d-48da-90d4-3f652f70e51f",
    "url": "https://discuss.elastic.co/t/master-role-does-not-switch-automaticaly/214213",
    "title": "Master role does not switch automaticaly",
    "category": [
      "Elasticsearch"
    ],
    "author": "The-oo",
    "date": "January 8, 2020, 10:59am January 8, 2020, 11:39am January 8, 2020, 12:24pm January 8, 2020, 12:30pm January 8, 2020, 12:42pm January 20, 2020, 8:30am January 8, 2020, 1:01pm January 8, 2020, 3:45pm January 20, 2020, 8:50am January 20, 2020, 9:02am January 20, 2020, 9:10am January 20, 2020, 9:22am January 20, 2020, 9:28am January 20, 2020, 9:30am January 20, 2020, 10:10am January 20, 2020, 10:24am January 20, 2020, 10:35am",
    "body": "Hello everyone, I've recently having issue moving the master role from node to node. I got 3 node in my cluster and there are all 3 master-eligible nodes : I first disable shard allocation on my second node and i let elasticsearch move all the shards across the 2 other node. I then shutdown elasticsearch on the second node to perform a disk change on the VM and add storage (while changing the mounting point names) But whenever i shutdown the master node, the role never got transfer to another node and i'm stuck with a cluster without any entry point available. Logs from the others nodes : [2020-01-08T11:43:38,012][WARN ][r.suppressed ] [hostname] path: /_bulk, params: {} org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/2/no master]; at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:189) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation.handleBlockExceptions(TransportBulkAction.java:534) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation.doRun(TransportBulkAction.java:415) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation$2.onTimeout(TransportBulkAction.java:568) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:325) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:252) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:598) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703) [elasticsearch-7.5.1.jar:7.5.1] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:830) [?:?] [2020-01-08T11:43:38,020][WARN ][r.suppressed ] [hostname] path: /_bulk, params: {} org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/2/no master]; at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:189) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation.handleBlockExceptions(TransportBulkAction.java:534) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation.doRun(TransportBulkAction.java:415) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation$2.onTimeout(TransportBulkAction.java:568) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:325) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:252) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:598) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703) [elasticsearch-7.5.1.jar:7.5.1] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:830) [?:?] [2020-01-08T11:43:38,141][WARN ][r.suppressed ] [hostname] path: /_bulk, params: {} org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/2/no master]; at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:189) ~[elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation.handleBlockExceptions(TransportBulkAction.java:534) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation.doRun(TransportBulkAction.java:415) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation$2.onTimeout(TransportBulkAction.java:568) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:325) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:252) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:598) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703) [elasticsearch-7.5.1.jar:7.5.1] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:830) [?:?] My config file (same on the 3 node except the IP) : cluster.name : MyCluster node.name : ${HOSTNAME} path.data : /data path.logs : /var/log/elasticsearch network.host : my_ip http.port : 9200 discovery.seed_hosts : [\"ip_node1\",\"ip_node2\",\"ip_node3\"] discovery.zen.minimum_master_nodes : 1 cluster.max_shards_per_node: 2000 xpack.monitoring.enabled: false xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: cert/path xpack.security.transport.ssl.truststore.path: cert/path If i restart my node who was previously master, the master role got switch at the moment the node restart It's not the first time i'm having this issue and i never get an explanation on how to properly correct this issue or if there is a misconfiguration from my side ...",
    "website_area": "discuss"
  },
  {
    "id": "22a47daa-3314-48ac-84a7-85894e239e1b",
    "url": "https://discuss.elastic.co/t/elasticsearch-support-both-case-sensitive-insensitive/215686",
    "title": "Elasticsearch support both case sensitive & insensitive",
    "category": [
      "Elasticsearch"
    ],
    "author": "Itay_Bittan",
    "date": "January 20, 2020, 8:23am January 20, 2020, 9:33am January 20, 2020, 9:47am January 20, 2020, 10:16am",
    "body": "Setup: Elasticsearch 6.3 I have an index that represents the products catalog. Every document contains one product's data. One of the fields called categories which is an array of strings - List of relevant categories. 99.9% of the queries are: give me the products that match categories A, B and C. The query about is case insensitive, thus categories mapping looks like: \"categories\": { \"type\": \"keyword\", \"normalizer\": \"lowercase_normalizer\" } For reporting (0.1% of all queries) I need to return a list of all possible categories case sensitive! Consider the following documents: \"_id\": \"product1\", \"_source\": { \"categories\": [ \"WOMEN\", \"Footwear\" ] } \"_id\": \"product2\", \"_source\": { \"categories\": [ \"Men\", \"Footwear\" ] } Running the following query: { \"size\": 0, \"aggs\": { \"categories\": { \"terms\": { \"field\": \"categories\", \"size\": 100 } } } } return: { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 40453, \"max_score\": 0, \"hits\": [ ] }, \"aggregations\": { \"sterms#categories\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 12453, \"buckets\": [ { \"key\": \"men\", \"doc_count\": 27049 }, { \"key\": \"women\", \"doc_count\": 21332 }, ......... ] } } } Is there a way to return the categories with their case sensitivity (as stored in the documents)? I'm interested in [\"WOMEN\", \"Men\"] in this query's result. Thanks, Itay StackOverflow question",
    "website_area": "discuss"
  },
  {
    "id": "fae702a5-13aa-4fe8-91e1-5c53a43199b5",
    "url": "https://discuss.elastic.co/t/index-s3-and-git-files-having-metadata-in-to-elastic/215705",
    "title": "Index S3 and Git Files having metadata in to elastic",
    "category": [
      "Elasticsearch"
    ],
    "author": "Aakash_Sharma",
    "date": "January 20, 2020, 9:53am",
    "body": "we are looking to index meta-info for S3 documents and Github repo files in to elastic. these files will have additional/metadata fields like howtouse, description, licensenumber etc. Ideally indexing should be event based like addition of items in bucket or new code commits etc. Are there any prebuilt tools/scripts that can help us to index existing or new content. Should we write our own script that read S3 buckets for example and then read meta data separately through jsonfiles etc. and then index them one by one . please throw some light.",
    "website_area": "discuss"
  },
  {
    "id": "8d0962d8-f162-454d-a0b6-535c72bf209b",
    "url": "https://discuss.elastic.co/t/synonym-and-synonym-graph-filters/215704",
    "title": "Synonym and synonym graph filters",
    "category": [
      "Elasticsearch"
    ],
    "author": "ivanibash",
    "date": "January 20, 2020, 9:51am",
    "body": "Hi, this page on synonym graphs reads: The synonym_graph token filter allows to easily handle synonyms, including multi-word synonyms correctly during the analysis process. Does that mean that a simple \"synonyms\" filter will necessarily introduce errors when working with multi-word synonyms? Should it be deprecated then? Or are there still cases where I would prefer the simple synonyms filter to synonym graph? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "76625a5a-29f5-44e8-a915-2ad735d86c21",
    "url": "https://discuss.elastic.co/t/search-doesnt-find-the-document-that-it-should-find-when-index-is-big/214386",
    "title": "Search doesn't find the document that it should find when index is big?",
    "category": [
      "Elasticsearch"
    ],
    "author": "aqiank",
    "date": "January 9, 2020, 9:20am January 9, 2020, 12:44pm January 9, 2020, 2:29pm January 9, 2020, 2:48pm January 9, 2020, 3:10pm January 9, 2020, 3:22pm January 9, 2020, 3:41pm January 9, 2020, 3:54pm January 9, 2020, 4:30pm January 10, 2020, 8:06am January 10, 2020, 8:37am January 10, 2020, 9:03am January 10, 2020, 12:27pm January 19, 2020, 4:45am January 19, 2020, 9:28am January 19, 2020, 10:28am January 19, 2020, 10:29am January 19, 2020, 10:35am January 19, 2020, 10:35am January 19, 2020, 2:58pm",
    "body": "Hi, I have a problem with Elasticsearch 6.8 (I also tried with 5.6 and 2.4 since we are upgrading) where it can't find the document that I want when the index has over 800K documents but when I create another index with just the one document in it, it can find it. Does anyone know the reason why? The query, settings, and mappings for both indexes are the same.",
    "website_area": "discuss"
  },
  {
    "id": "be97c4d3-9898-4bcd-82fe-5d66b481fc34",
    "url": "https://discuss.elastic.co/t/elasticsearch-didnt-create-ca-key-how-can-i-create-new-key-crt-for-new-instances/215479",
    "title": "Elasticsearch didn't create ca.key. How can i create new key/crt for new instances?",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 20, 2020, 7:44am January 20, 2020, 9:48am January 20, 2020, 9:50am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5e650e9c-ce8f-4fa6-a5f5-2b8250195b49",
    "url": "https://discuss.elastic.co/t/can-es-not-to-store-original-keyword-content-but-a-mapping-num-value-4-save-space/215664",
    "title": "Can ES not to store original keyword content but a mapping num value (4 save space)?",
    "category": [
      "Elasticsearch"
    ],
    "author": "DeeeFOX",
    "date": "January 20, 2020, 4:38am January 20, 2020, 8:57am January 20, 2020, 9:25am January 20, 2020, 9:29am",
    "body": "We use es as tsdb described in my other question and data look like: { \"metric\": \"metric_name\", \"@timestamp\": \"2020-01-01T01:01:01\", \"t\": { \"key1\": \"val1\", \"key2\": \"val2\" }, \"f\": { \"name1\": 1.2, \"name2\": 2.3 } } But when data grows (to billion datapoint), these file grows to a size so huge as below (already rolling by date): ('Points', '0.07') size: 1,657,438,615 bytes ('DocValues', '0.18') size: 3,876,527,301 bytes ('Term Dictionary', '0.20') size: 4,237,561,161 bytes ('Field Data', '0.44') size: 9,265,590,659 bytes ('Frequencies', '0.08'): size: 1,733,107,687 bytes And the query we use are just term(yes or no) and prefix and some simple aggregation(terms, date_histogram then sum, avg). So, my question is Whether es can translate first then store the the mapping data instead of original keyword content in the storing layer(Transparent to users) to save the space and io? like: { \"metric\": \"0\", \"@timestamp\": \"2020-01-01T01:01:01\", \"t\": { \"key1\": \"1\", \"key2\": \"2\" }, \"f\": { \"name1\": 1.2, \"name2\": 2.3 } } instead of { \"metric\": \"long_metric_name\", \"@timestamp\": \"2020-01-01T01:01:01\", \"t\": { \"key1\": \"long_val1\", \"key2\": \"long_val2\" }, \"f\": { \"name1\": 1.2, \"name2\": 2.3 } }",
    "website_area": "discuss"
  },
  {
    "id": "b7962f04-c319-4921-920c-1936da7fcd0e",
    "url": "https://discuss.elastic.co/t/append-nested-object-to-exisiting-nested-field-in-elasticsearch/215696",
    "title": "Append nested object to exisiting nested field in elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "Ram_29",
    "date": "January 20, 2020, 9:24am",
    "body": "I'm having nested object like below \"appdata\": { \"type\":\"nested\", \"include_in_parent\":true, \"properties\": { \"accessType\": { \"type\": \"text\", \"norms\": false, \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"appname\": { \"type\": \"text\", \"norms\": false, \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"eventtime\": { \"type\": \"text\", \"norms\": false, \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } I'm updating it using below POST dashboard_write/_update/1199CF2F77603304A275DD112D9FA747 { \"script\" : { \"source\": \"if (ctx._source.appdata == null) { ctx._source.appdata = params.appdata } else { ctx._source.appdata=ctx._source.appdata+params.appdata}\", \"lang\": \"painless\", \"params\" : { \"appdata\" : { \"accessType\" : \"second_app\", \"appname\" : \"third\", \"eventtime\" : \"Jan 20 12:18:39\" } } } It's giving runtime error \"type\": \"class_cast_exception\", \"reason\": \"Cannot apply [+] operation to types [java.util.LinkedHashMap] and [java.util.HashMap].\" I'm expecting appdata to be as below \"appdata\" : [{ \"accessType\" : \"test_app\", \"appname\" : \"first\", \"eventtime\" : \"Jan 20 12:18:39\" },{ \"accessType\" : \"test_app\", \"appname\" : \"second\", \"eventtime\" : \"Jan 20 12:18:39\" },]",
    "website_area": "discuss"
  },
  {
    "id": "f9a41d8f-4d29-4d1e-9929-340763c03537",
    "url": "https://discuss.elastic.co/t/accessing-the-snapshot-after-deleting-the-repository/215682",
    "title": "Accessing the snapshot after deleting the repository",
    "category": [
      "Elasticsearch"
    ],
    "author": "Stephy_Jacob",
    "date": "January 20, 2020, 7:51am January 20, 2020, 8:23am January 20, 2020, 8:57am January 20, 2020, 9:03am",
    "body": "Hi Team, We have accidentally deleted a repository that was having a valid snapshot. We understand from the snapshot documentation that, deleting a repository only unregister, that means only removes the reference to the location where the repository is storing the snapshots and snapshots themselves are untouched. Could you please let me know how can I access this snapshot to restore the indices inside that",
    "website_area": "discuss"
  },
  {
    "id": "6cf2097e-7056-4d22-a2ea-9d27dcbd5306",
    "url": "https://discuss.elastic.co/t/how-to-do-multiplication-on-a-field-like-sum-function-do/215690",
    "title": "How to do multiplication on a field like sum function do?",
    "category": [
      "Elasticsearch"
    ],
    "author": "DeeeFOX",
    "date": "January 20, 2020, 8:56am",
    "body": "As title said, how to do multiplication on a field like sum function do like: POST tsdb_alias/_search?routing=metric_name1 { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"metric\": \"metric_name1\" } }, { \"term\": { \"t.key1\": \"val1\" } }, { \"range\": { \"@timestamp\": { \"gte\": \"2020-01-01\", \"lte\": \"2020-01-02\" } } } ] } }, \"aggs\": { \"ts\": { \"date_histogram\": { \"field\": \"@timestamp\", \"interval\": \"1h\" }, \"aggs\": { \"res\": { \"multiplier\": { \"field\": \"f.name1\" } } } } }, \"size\": 0 } In our use case, we use es as tsdb that described in my other topic and data we store are look like: { \"metric\": \"metric_name\", \"@timestamp\": \"2020-01-01T01:01:01\", \"t\": { \"key1\": \"val1\", \"key2\": \"val2\" }, \"f\": { \"name1\": 1.2, \"name2\": 2.3 } }",
    "website_area": "discuss"
  },
  {
    "id": "4b640f7f-74fb-42aa-96ed-74c02317223b",
    "url": "https://discuss.elastic.co/t/xpack-rbac-vs-kibana-instances-spaces-vs-siem/215439",
    "title": "Xpack RBAC vs Kibana Instances/Spaces vs SIEM",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 20, 2020, 7:25am January 20, 2020, 7:30am January 20, 2020, 8:38am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ab0f2b79-e698-40cb-940f-d3d7e4101973",
    "url": "https://discuss.elastic.co/t/rollup-job-with-distinct-unique-count/215684",
    "title": "Rollup job with distinct/unique count",
    "category": [
      "Elasticsearch"
    ],
    "author": "Matthias_Seidl",
    "date": "January 20, 2020, 8:11am",
    "body": "Hi, I have to aggregate my data daily based on a couple of fields. Most of this can be done easily with the rollup functionality. I can get the max value of a field per day, the count of the logs and other simple aggregations (such as Sum, Min, etc.). But what I'm missing is the unique count functionality. I have I field \"ID\" and let's say about 500 000 documents with this field. But there are only 1 000 different IDs per day, and that's the number that I need. So exactly like this unique count functionality, which you can use in any other kibana aggregation. Is there any way to achieve this? Thank you so much!",
    "website_area": "discuss"
  },
  {
    "id": "abaead00-e071-49f7-85e5-4105b2de2625",
    "url": "https://discuss.elastic.co/t/elasticsearch-influx-db-connection/215289",
    "title": "Elasticsearch Influx db connection",
    "category": [
      "Elasticsearch"
    ],
    "author": "Syed.Ubaid",
    "date": "January 16, 2020, 10:08am January 20, 2020, 6:07am January 20, 2020, 6:14am January 20, 2020, 6:16am January 20, 2020, 6:17am January 20, 2020, 6:25am",
    "body": "How to connect elasticsearch with influx db?",
    "website_area": "discuss"
  },
  {
    "id": "66ea3cd1-9f27-468f-8476-91c4b93a0769",
    "url": "https://discuss.elastic.co/t/how-to-write-a-efficient-es-query-for-the-below-sql-query/215599",
    "title": "How to write a efficient ES query for the below SQL query?",
    "category": [
      "Elasticsearch"
    ],
    "author": "DeeeFOX",
    "date": "January 19, 2020, 5:55am January 19, 2020, 6:47am January 19, 2020, 6:50am January 19, 2020, 1:54pm January 19, 2020, 2:08pm January 20, 2020, 4:53am",
    "body": "select tag1, tag2, sum(sum_val) select tag-uid, sum(value) as sum_val from data_table group by tag-uid join tag_table using(tag-uid) group by tag1, tag2",
    "website_area": "discuss"
  },
  {
    "id": "22b9be6c-484c-4d1f-bce9-dcec7caf01b6",
    "url": "https://discuss.elastic.co/t/randomly-performance-issue/214009",
    "title": "Randomly performance issue",
    "category": [
      "Elasticsearch"
    ],
    "author": "nmathur",
    "date": "January 7, 2020, 9:12am January 7, 2020, 11:47am January 10, 2020, 4:22am January 17, 2020, 10:26am January 17, 2020, 11:30am January 20, 2020, 4:53am",
    "body": "Hello All, We are randomly facing performance issue in elasticsearch read/write operations. The elasticsearch in installed on Azure VM (Linux). This is not constant. Generally a common method to insert/update in elasticsearch takes 1 to 5 secs, but when we face the issue the general operation time reach to 2 to 4 mins. We are using hang-fire to update the elasticsearch indexes. Here are the configuration on our VM -> CPU : 4 RAM : 16GB Data Disks - 8 Max IOPS : 6400 Storage : 32 GB Please guide what can be the issue and how to monitor what was the IOPS, Memory usages etc. Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "c12bd3d4-3296-49e5-9edb-c8a46e763e4e",
    "url": "https://discuss.elastic.co/t/master-not-discovered-or-elected-yet-an-election-requires-at-least-2-nodes/213412",
    "title": "Master not discovered or elected yet, an election requires at least 2 nodes",
    "category": [
      "Elasticsearch"
    ],
    "author": "li-peng",
    "date": "December 31, 2019, 7:00am December 31, 2019, 10:00am January 2, 2020, 9:47am January 20, 2020, 3:14am January 20, 2020, 3:16am",
    "body": "[2019-12-31T14:29:23,954][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [5e3LlD8QTHmGaJDhlKY-8w, cJyHBa9ATf-2vg8Tnt7KeQ, JHVEwFfkStOI6qG6QawqkA], have discovered which is not a quorum; discovery will continue using [127.0.0.1:9300, 127.0.0.1:9301, 127.0.0.1:9302, 127.0.0.1:9303, 127.0.0.1:9304] from hosts providers and [{node1}{JHVEwFfkStOI6qG6QawqkA}{tv4ieuTOQn6wW9fUGwNXzg}{172.21.0.1}{172.21.0.1:9300} {ml.machine_memory=3975200768, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 8, last-accepted version 533 in term 8 I built a cluster of 3 nodes with elasticsearch-7.1 two weeks ago. ./elasticsearch -E node.name=node1 -E cluster.name=my-app -E path.data=/datas/es7/node1_data -E path.logs=/logs/node1_log -d ./elasticsearch -E node.name=node2 -E cluster.name=my-app -E path.data=/datas/es7/node2_data -E path.logs=/logs/node2_log -d ./elasticsearch -E node.name=node3 -E cluster.name=my-app -E path.data=/es7/node3_data -E path.logs=/logs/node3_log -d now,I shutdown all nodes, and I want to start the cluster with only one node : node1. Unfortunately , it failed. here is the log: [2019-12-31T14:29:23,954][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [5e3LlD8QTHmGaJDhlKY-8w, cJyHBa9ATf-2vg8Tnt7KeQ, JHVEwFfkStOI6qG6QawqkA], have discovered which is not a quorum; discovery will continue using [127.0.0.1:9300, 127.0.0.1:9301, 127.0.0.1:9302, 127.0.0.1:9303, 127.0.0.1:9304] from hosts providers and [{node1}{JHVEwFfkStOI6qG6QawqkA}{tv4ieuTOQn6wW9fUGwNXzg}{172.21.0.1}{172.21.0.1:9300} {ml.machine_memory=3975200768, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 8, last-accepted version 533 in term 8 here is the elasticsearch.ymal: cluster.name: my-app network.host: 0.0.0.0 node.name: node1 path.data: /datas/es7/node1_data path.logs: /logs/es7/node1_log and I try to set cluster.initial_master_nodes: [\"node1\"],it makes no difference . here is logs when I set logger.org.elasticsearch.discovery: TRACE [2019-12-31T14:55:04,331][INFO ][o.e.n.Node ] [node1] starting ... [2019-12-31T14:55:04,468][INFO ][o.e.t.TransportService ] [node1] publish_address {172.21.0.1:9300}, bound_addresses {0.0.0.0:9300} [2019-12-31T14:55:04,479][INFO ][o.e.b.BootstrapChecks ] [node1] bound or publishing to a non-loopback address, enforcing bootstrap checks [2019-12-31T14:55:04,520][DEBUG][o.e.d.SeedHostsResolver ] [node1] using max_concurrent_resolvers [10], resolver timeout [5s] [2019-12-31T14:55:04,520][INFO ][o.e.c.c.Coordinator ] [node1] cluster UUID [G4ku2BYQRIG-dHdIDQ7tvA] [2019-12-31T14:55:04,524][TRACE][o.e.d.PeerFinder ] [node1] activating with nodes: {node1}{JHVEwFfkStOI6qG6QawqkA}{-FVunPWUSIGpB5ndu00tfw}{172.21.0.1}{172.21.0.1:9300}{ml.machine_memory=3975200768, xpack.installed=true, ml.max_open_jobs=20}, local [2019-12-31T14:55:04,525][TRACE][o.e.d.PeerFinder ] [node1] probing master nodes from cluster state: nodes: {node1}{JHVEwFfkStOI6qG6QawqkA}{-FVunPWUSIGpB5ndu00tfw}{172.21.0.1}{172.21.0.1:9300}{ml.machine_memory=3975200768, xpack.installed=true, ml.max_open_jobs=20}, local [2019-12-31T14:55:04,525][TRACE][o.e.d.PeerFinder ] [node1] startProbe(172.21.0.1:9300) not probing local node [2019-12-31T14:55:04,537][TRACE][o.e.d.SeedHostsResolver ] [node1] resolved host [127.0.0.1] to [127.0.0.1:9300, 127.0.0.1:9301, 127.0.0.1:9302, 127.0.0.1:9303, 127.0.0.1:9304] [2019-12-31T14:55:04,538][TRACE][o.e.d.PeerFinder ] [node1] probing resolved transport addresses [127.0.0.1:9300, 127.0.0.1:9301, 127.0.0.1:9302, 127.0.0.1:9303, 127.0.0.1:9304] [2019-12-31T14:55:04,539][TRACE][o.e.d.PeerFinder ] [node1] Peer{transportAddress=127.0.0.1:9300, discoveryNode=null, peersRequestInFlight=false} attempting connection [2019-12-31T14:55:04,546][TRACE][o.e.d.PeerFinder ] [node1] Peer{transportAddress=127.0.0.1:9301, discoveryNode=null, peersRequestInFlight=false} attempting connection [2019-12-31T14:55:04,546][TRACE][o.e.d.PeerFinder ] [node1] Peer{transportAddress=127.0.0.1:9302, discoveryNode=null, peersRequestInFlight=false} attempting connection [2019-12-31T14:55:04,546][TRACE][o.e.d.PeerFinder ] [node1] Peer{transportAddress=127.0.0.1:9303, discoveryNode=null, peersRequestInFlight=false} attempting connection [2019-12-31T14:55:04,549][TRACE][o.e.d.PeerFinder ] [node1] Peer{transportAddress=127.0.0.1:9304, discoveryNode=null, peersRequestInFlight=false} attempting connection [2019-12-31T14:55:04,551][TRACE][o.e.d.HandshakingTransportAddressConnector] [node1] [connectToRemoteMasterNode[127.0.0.1:9303]] opening probe connection [2019-12-31T14:55:04,553][TRACE][o.e.d.HandshakingTransportAddressConnector] [node1] [connectToRemoteMasterNode[127.0.0.1:9300]] opening probe connection [2019-12-31T14:55:04,553][TRACE][o.e.d.HandshakingTransportAddressConnector] [node1] [connectToRemoteMasterNode[127.0.0.1:9302]] opening probe connection [2019-12-31T14:55:04,554][TRACE][o.e.d.HandshakingTransportAddressConnector] [node1] [connectToRemoteMasterNode[127.0.0.1:9301]] opening probe connection [2019-12-31T14:55:04,559][TRACE][o.e.d.HandshakingTransportAddressConnector] [node1] [connectToRemoteMasterNode[127.0.0.1:9304]] opening probe connection [2019-12-31T14:55:04,600][DEBUG][o.e.d.PeerFinder ] [node1] Peer{transportAddress=127.0.0.1:9304, discoveryNode=null, peersRequestInFlight=false} connection failed org.elasticsearch.transport.ConnectTransportException: [127.0.0.1:9304] connect_exception at org.elasticsearch.transport.TcpTransport$ChannelsConnectedListener.onFailure(TcpTransport.java:1299) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.action.ActionListener.lambda$toBiConsumer$2(ActionListener.java:99) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.common.concurrent.CompletableContext.lambda$addListener$0(CompletableContext.java:42) ~[elasticsearch-core-7.1.0.jar:7.1.0] at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_161] at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_161] at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_161] at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_161] at org.elasticsearch.common.concurrent.CompletableContext.completeExceptionally(CompletableContext.java:57) ~[elasticsearch-core-7.1.0.jar:7.1.0] at org.elasticsearch.transport.netty4.Netty4TcpChannel.lambda$new$1(Netty4TcpChannel.java:72) ~[?:?] at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:511) ~[?:?] at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:504) ~[?:?] at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:483) ~[?:?] at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:424) ~[?:?] at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:121) ~[?:?] at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:327) ~[?:?] at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:343) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:556) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:510) ~[?:?] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:470) ~[?:?] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:909) ~[?:?] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161] Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /127.0.0.1:9304 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:?] at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:?] at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327) ~[?:?] at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) ~[?:?] ... 6 more Caused by: java.net.ConnectException: Connection refused at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:?] at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:?] at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327) ~[?:?] at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) ~[?:?] ... 6 more [2019-12-31T14:55:04,655][TRACE][o.e.d.HandshakingTransportAddressConnector] [node1] [connectToRemoteMasterNode[127.0.0.1:9300]] opened probe connection [2019-12-31T14:55:04,621][DEBUG][o.e.d.PeerFinder ] [node1] Peer{transportAddress=127.0.0.1:9301, discoveryNode=null, peersRequestInFlight=false} connection failed org.elasticsearch.transport.ConnectTransportException: [127.0.0.1:9301] connect_exception ...... so ,waht can I do to start the cluster with only one node ? and why \"an election requires at least 2 nodes\" when i alreay set cluster.initial_master_nodes: [\"node1\"] Thanks",
    "website_area": "discuss"
  },
  {
    "id": "0a1a3c4a-2cf1-41c6-bc75-2927fceb9e07",
    "url": "https://discuss.elastic.co/t/enable-soundex-query/215633",
    "title": "Enable Soundex query",
    "category": [
      "Elasticsearch"
    ],
    "author": "somesh1980",
    "date": "January 19, 2020, 5:07pm",
    "body": "I am using elasticsearch 7.0.1 and trying to enable soundex query First I tried to add following config in elasticsearch.yml index: analysis: analyzer: search_soundex: type: custom tokenizer: soundex_filter filter: soundex_filter: type: phonetic encoder: soundex replace: true But it gave an error as \"Caused by: java.lang.IllegalArgumentException: node settings must not contain any index level settings\" Then I removed above settings and added following while in PUT request of creating the index { \"settings\": { \"analysis\": { \"analyzer\": { \"search_soundex\": { \"type\": \"custom\", \"tokenizer\": \"soundex_filter\", \"filter\": [{ \"soundex_filter\": { \"type\": \"phonetic\", \"encoder\": \"soundex\", \"replace\": true } }] } } } } } But that also gave following error {\"error\":{\"root_cause\":[{\"type\":\"settings_exception\",\"reason\":\"Failed to load settings from [{\"analysis\":{\"analyzer\":{\"search_soundex\":{\"filter\":[{\"soundex_filter\":{\"replace\":true,\"type\":\"phonetic\",\"encoder\":\"soundex\"}}],\"type\":\"custom\",\"tokenizer\":\"soundex_filter\"}}}}]\"}],\"type\":\"settings_exception\",\"reason\":\"Failed to load settings from [{\"analysis\":{\"analyzer\":{\"search_soundex\":{\"filter\":[{\"soundex_filter\":{\"replace\":true,\"type\":\"phonetic\",\"encoder\":\"soundex\"}}],\"type\":\"custom\",\"tokenizer\":\"soundex_filter\"}}}}]\",\"caused_by\":{\"type\":\"illegal_state_exception\",\"reason\":\"only value lists are allowed in serialized settings\"}},\"status\":500} Can somebody please help",
    "website_area": "discuss"
  },
  {
    "id": "96015a9d-ff6a-4f90-bfc0-cef7c0361ad5",
    "url": "https://discuss.elastic.co/t/gc-allocation-failure-causing-shard-to-fail/215628",
    "title": "GC allocation failure causing shard to fail",
    "category": [
      "Elasticsearch"
    ],
    "author": "Alex_Davidovich",
    "date": "January 19, 2020, 3:56pm",
    "body": "In Elastic log I can see master (node-2) removing node-3 and adding multiple times. At around the same time, I see long GC cycles on node-3. [2020-01-17T01:33:49,904][INFO ][o.e.c.s.ClusterApplierService] [node-2] removed {{node-3}{5t-Y05xkTU2TEAWn126Rwg}{8lRQU65YRoK1AkZ1P299dQ}{node-3}{9.151.141.3:9300}{ml.machine_memory=1097878700032, ml.max _open_jobs=20, xpack.installed=true},}, term: 290, version: 3285, reason: ApplyCommitRequest{term=290, version=3285, sourceNode={node-1}{-_UntR0aSFee7J0uGBsXvw}{NgPgPx4nRSGsuxenifYBFg}{node-1}{9.151.141.1 :9300}{ml.machine_memory=1097878695936, ml.max_open_jobs=20, xpack.installed=true}} [2020-01-17T01:33:59.708+0000][4005][gc ] GC(38) Pause Young (Allocation Failure) 216M->79M(494M) 27889.991ms [2020-01-17T01:43:50,918][INFO ][o.e.c.s.ClusterApplierService] [node-2] removed {{node-3}{5t-Y05xkTU2TEAWn126Rwg}{8lRQU65YRoK1AkZ1P299dQ}{node-3}{9.151.141.3:9300}{ml.machine_memory=1097878700032, ml.max _open_jobs=20, xpack.installed=true},}, term: 290, version: 3292, reason: ApplyCommitRequest{term=290, version=3292, sourceNode={node-1}{-_UntR0aSFee7J0uGBsXvw}{NgPgPx4nRSGsuxenifYBFg}{node-1}{9.151.141.1 :9300}{ml.machine_memory=1097878695936, ml.max_open_jobs=20, xpack.installed=true}} [2020-01-17T01:43:51.894+0000][4005][gc ] GC(41) Pause Young (Allocation Failure) 219M->84M(494M) 18547.871ms [2020-01-17T02:09:00,608][INFO ][o.e.c.s.ClusterApplierService] [node-2] removed {{node-3}{5t-Y05xkTU2TEAWn126Rwg}{8lRQU65YRoK1AkZ1P299dQ}{node-3}{9.151.141.3:9300}{ml.machine_memory=1097878700032, ml.max _open_jobs=20, xpack.installed=true},}, term: 290, version: 3315, reason: ApplyCommitRequest{term=290, version=3315, sourceNode={node-1}{-_UntR0aSFee7J0uGBsXvw}{NgPgPx4nRSGsuxenifYBFg}{node-1}{9.151.141.1 :9300}{ml.machine_memory=1097878695936, ml.max_open_jobs=20, xpack.installed=true}} [2020-01-17T02:10:36.413+0000][4005][gc ] GC(47) Pause Young (Allocation Failure) 218M->82M(494M) 112874.834ms [2020-01-17T02:20:34,991][INFO ][o.e.c.s.ClusterApplierService] [node-2] removed {{node-3}{5t-Y05xkTU2TEAWn126Rwg}{8lRQU65YRoK1AkZ1P299dQ}{node-3}{9.151.141.3:9300}{ml.machine_memory=1097878700032, ml.max _open_jobs=20, xpack.installed=true},}, term: 290, version: 3322, reason: ApplyCommitRequest{term=290, version=3322, sourceNode={node-1}{-_UntR0aSFee7J0uGBsXvw}{NgPgPx4nRSGsuxenifYBFg}{node-1}{9.151.141.1 :9300}{ml.machine_memory=1097878695936, ml.max_open_jobs=20, xpack.installed=true}} [2020-01-17T02:22:26.047+0000][4005][gc ] GC(50) Pause Young (Allocation Failure) 219M->88M(494M) 128482.688ms In addition, after these errors, shard marked as failed on node-3 [2020-01-17T01:44:08,502][WARN ][o.e.i.c.IndicesClusterStateService] [node-3] [events_1578091228978][0] marking and sending shard failed due to [failed to create shard] java.io.IOException: failed to obtain in-memory shard lock This problem didn't occur before with the same exact settings. In addition the same GC cycles performed very quickly before and after the errors 2020-01-17T01:29:02.898+0000][4005][gc ] GC(37) Pause Young (Allocation Failure) 215M->79M(494M) 1.181ms [2020-01-17T01:29:02.898+0000][4005][gc,cpu ] GC(37) User=0.01s Sys=0.00s Real=0.00s [2020-01-17T01:39:41.745+0000][4005][gc ] GC(40) Pause Young (Allocation Failure) 218M->82M(494M) 1.560ms [2020-01-17T01:39:41.745+0000][4005][gc,cpu ] GC(40) User=0.01s Sys=0.00s Real=0.00s [2020-01-17T02:03:51.473+0000][4005][gc ] GC(46) Pause Young (Allocation Failure) 218M->82M(494M) 1.380ms [2020-01-17T02:03:51.473+0000][4005][gc,cpu ] GC(46) User=0.01s Sys=0.01s Real=0.00s [2020-01-17T02:16:20.786+0000][4005][gc ] GC(49) Pause Young (Allocation Failure) 218M->82M(494M) 1.377ms [2020-01-17T02:16:20.786+0000][4005][gc,cpu ] GC(49) User=0.01s Sys=0.00s Real=0.00s What can I do? (Assuming, heap size cannot be changed)",
    "website_area": "discuss"
  },
  {
    "id": "47bb6eed-1199-46ab-bc06-5c1d650c2ef8",
    "url": "https://discuss.elastic.co/t/make-elasticsearch-understand-date-time-field-nginx-apache-logs/214968",
    "title": "Make Elasticsearch understand date/time field Nginx/apache logs",
    "category": [
      "Elasticsearch"
    ],
    "author": "spacecabbie",
    "date": "January 14, 2020, 9:39am January 19, 2020, 3:00pm January 19, 2020, 2:57pm",
    "body": "I am sending my logs from apache and nginx to elasticsearch via rsyslog. Parsing to json: version=2 rule=:%remote_addr:word% %ident:word% %auth:word% [%@timestamp:char-to:]%] \"%method:word% %request:word% HTTP/%httpversion:float%\" %status:number% %requesttime:float% \"%referrer:char-to:\"%\" \"%agent:char-to:\"%\" . template(name=\"apache-nginx\" type=\"list\"){ property(name=\"$!all-json\") } The problem is nginx logs like: [14/Jan/2020:08:48:23 +0100] so when send to elastic search it does not recognise it as date/time Any tips tricks to fix it ? (no filebeats.) Has to be done with rsyslog/elasticsearch.",
    "website_area": "discuss"
  },
  {
    "id": "964d731a-d385-4c8b-817e-94005e028b6e",
    "url": "https://discuss.elastic.co/t/refresh-interval/215420",
    "title": "Refresh_interval",
    "category": [
      "Elasticsearch"
    ],
    "author": "111276",
    "date": "January 17, 2020, 7:31am January 17, 2020, 8:56am January 19, 2020, 2:13pm January 19, 2020, 2:39pm",
    "body": "hello I have eight elasticsearch data nodes (ver. 6.4.1) (8 shards, 2 replica) they have following data indexed disk size : 21.5GB Document Count : 67.7m Segment Count : 336 A large amount of data is bached and indexed at once I know there is a way to adjust the segment size please advise how it is good to measure and adjust for quick search, index thanks",
    "website_area": "discuss"
  },
  {
    "id": "1f6df2f9-70f3-4b9e-8960-50eb1360c6de",
    "url": "https://discuss.elastic.co/t/a-performance-issue-about-elasticsearch-on-k8s/214692",
    "title": "A performance issue about elasticsearch on k8s",
    "category": [
      "Elasticsearch"
    ],
    "author": "ftyuuu",
    "date": "January 11, 2020, 5:40am January 11, 2020, 5:52am January 11, 2020, 8:19pm January 16, 2020, 1:13pm January 17, 2020, 12:06pm January 17, 2020, 12:27pm January 17, 2020, 12:33pm January 17, 2020, 12:51pm January 17, 2020, 1:30pm January 18, 2020, 5:37am January 19, 2020, 12:56pm",
    "body": "I'm experiencing an es on k8s performance issue that confuses me, and this is my test environment. First, I have a k8s cluster with 4 nodes, of which 1 master and 3 nodes: NAME STATUS ROLES AGE VERSION 172.24.5.3 Ready master,monitoring 234d v1.13.5 172.24.5.4 Ready monitoring,node 234d v1.13.5 172.24.5.5 Ready node 234d v1.13.5 172.24.5.7 Ready node 234d v1.13.5 You can see my k8s version is 1.13.5, then i use eck (https://github.com/elastic/cloud-on-k8s, then run kubectl apply -f https://download.elastic.co/downloads/eck/1.0.0-beta1/all-in-one.yaml) to get an es cluster with 5 nodes: NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES elasticsearch-sample-es-client-0 1/1 Running 0 18m 10.16.33.84 172.24.5.7 <none> <none> elasticsearch-sample-es-data-0 1/1 Running 0 18m 10.16.33.79 172.24.5.7 <none> <none> elasticsearch-sample-es-data-1 1/1 Running 0 18m 10.16.215.184 172.24.5.5 <none> <none> elasticsearch-sample-es-data-2 1/1 Running 0 18m 10.16.184.199 172.24.5.4 <none> <none> elasticsearch-sample-es-master-0 1/1 Running 0 18m 10.16.215.181 172.24.5.5 <none> <none> And in order to ensure the consistency of the test, I will ensure that the data nodes are distributed on three k8s nodes. The cr file of this es cluster is as follow: apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: elasticsearch-sample namespace: nes-elasticsearch spec: version: 6.8.4 http: tls: selfSignedCertificate: disabled: true nodeSets: - name: master config: node.master: true node.data: false node.ingest: false podTemplate: spec: initContainers: - name: sysctl securityContext: privileged: true command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144'] containers: - name: elasticsearch resources: requests: memory: 16Gi cpu: 8 limits: memory: 16Gi cpu: 8 env: - name: ES_JAVA_OPTS value: \"-Xms4g -Xmx4g\" count: 1 volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 32Gi - name: client config: node.master: false node.data: false node.ingest: false podTemplate: spec: initContainers: - name: sysctl securityContext: privileged: true command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144'] containers: - name: elasticsearch resources: requests: memory: 16Gi cpu: 8 limits: memory: 16Gi cpu: 8 env: - name: ES_JAVA_OPTS value: \"-Xms4g -Xmx4g\" count: 1 volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 32Gi - name: data config: node.master: false node.data: true node.ingest: false podTemplate: spec: initContainers: - name: sysctl securityContext: privileged: true command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144'] containers: - name: elasticsearch resources: requests: memory: 16Gi cpu: 8 limits: memory: 16Gi cpu: 8 env: - name: ES_JAVA_OPTS value: \"-Xms4g -Xmx4g\" count: 3 volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 32Gi You can see that in the above cr, the memory limit set by the memory request is the same, both are 16g. This is a very important parameter. This test is for memory request and memory limit. This is very important. and the pv file is as follow: apiVersion: v1 kind: PersistentVolume metadata: name: es-local-pv3 spec: capacity: storage: 32Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain local: path: /home/data/eck-test1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 172.24.5.4 --- apiVersion: v1 kind: PersistentVolume metadata: name: es-local-pv1 spec: capacity: storage: 32Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain local: path: /home/data/eck-test1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 172.24.5.5 --- apiVersion: v1 kind: PersistentVolume metadata: name: es-local-pv2 spec: capacity: storage: 32Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain local: path: /home/data/eck-test1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 172.24.5.7 --- apiVersion: v1 kind: PersistentVolume metadata: name: es-local-pv7 spec: capacity: storage: 32Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain local: path: /home/data/eck-test2 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 172.24.5.5 --- apiVersion: v1 kind: PersistentVolume metadata: name: es-local-pv6 spec: capacity: storage: 32Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain local: path: /home/data/eck-test2 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 172.24.5.7 With the a bove file, you can quickly create an es cluster like mine. Then i use es-rally (https://github.com/elastic/rally) to test this cluster, i use the http_logs (https://github.com/elastic/rally-tracks/tree/master/http_logs) dataset and only test with the operation of index-append: { \"name\": \"index-append\", \"operation-type\": \"bulk\", \"bulk-size\": {{bulk_size | default(5000)}}, \"ingest-percentage\": {{ingest_percentage | default(100)}}, \"corpora\": \"http_logs\" } The challenges is: \"schedule\": [ { \"operation\": \"delete-index\" }, { \"operation\": { \"operation-type\": \"create-index\", \"settings\": {{index_settings | default({}) | tojson}} } }, { \"name\": \"check-cluster-health\", \"operation\": { \"operation-type\": \"cluster-health\", \"index\": \"logs-*\", \"request-params\": { \"wait_for_status\": \"{{cluster_health | default('green')}}\", \"wait_for_no_relocating_shards\": \"true\" } } }, { \"operation\": \"index-append\", \"warmup-time-period\": 240, \"clients\": {{bulk_indexing_clients | default(30)}} } ] As you can see, I simplified the test by default, only testing the performance of the index. Also, I placed esrally inside the docker container at 172.24.5.3. Get the password of the es cluster first: kubectl get secret elasticsearch-sample-es-elastic-user -n nes-elasticsearch -o=jsonpath='{.data.elastic}' | base64 --decode then run the rally: esrally --pipeline=benchmark-only --target-hosts=192.168.12.3:9200 --track=/rally/.rally/benchmarks/tracks/http_logs --report-format=csv --report-file=result.csv --challenge=append-no-conflicts --client-options=\"use_ssl:false,verify_certs:false,basic_auth_user:'elastic',basic_auth_password:'$PASSWORD'\" 192.168.12.3 is the es http service's cluster ip. I got the test result as follow: Metric,Task,Value,Unit Cumulative indexing time of primary shards,,303.7706,min Min cumulative indexing time across primary shards,,0.9421166666666667,min Median cumulative indexing time across primary shards,,3.1037833333333333,min Max cumulative indexing time across primary shards,,69.81788333333334,min Cumulative indexing throttle time of primary shards,,0,min Min cumulative indexing throttle time across primary shards,,0,min Median cumulative indexing throttle time across primary shards,,0,min Max cumulative indexing throttle time across primary shards,,0,min Cumulative merge time of primary shards,,139.40831666666665,min Cumulative merge count of primary shards,,3138, Min cumulative merge time across primary shards,,0.09126666666666666,min Median cumulative merge time across primary shards,,0.5575166666666667,min Max cumulative merge time across primary shards,,26.99235,min Cumulative merge throttle time of primary shards,,64.86913333333334,min Min cumulative merge throttle time across primary shards,,0,min Median cumulative merge throttle time across primary shards,,0.0576,min Max cumulative merge throttle time across primary shards,,14.664250000000001,min Cumulative refresh time of primary shards,,15.429016666666666,min Cumulative refresh count of primary shards,,6023, Min cumulative refresh time across primary shards,,0.06673333333333333,min Median cumulative refresh time across primary shards,,0.14036666666666667,min Max cumulative refresh time across primary shards,,2.721033333333333,min Cumulative flush time of primary shards,,0.79705,min Cumulative flush count of primary shards,,115, Min cumulative flush time across primary shards,,0.00016666666666666666,min Median cumulative flush time across primary shards,,0.00036666666666666667,min Max cumulative flush time across primary shards,,0.24375,min Total Young Gen GC,,236.31,s Total Old Gen GC,,2.958,s Store size,,22.122190072201192,GB Translog size,,14.817421832121909,GB Heap used for segments,,94.57985973358154,MB Heap used for doc values,,0.1043548583984375,MB Heap used for terms,,81.22084045410156,MB Heap used for norms,,0.036376953125,MB Heap used for points,,5.796648979187012,MB Heap used for stored fields,,7.421638488769531,MB Segment count,,596, Min Throughput,index-append,170934.94,docs/s Median Throughput,index-append,175795.68,docs/s Max Throughput,index-append,182926.54,docs/s 50th percentile latency,index-append,852.5300654582679,ms 90th percentile latency,index-append,1073.6245419830084,ms 99th percentile latency,index-append,1436.844245232641,ms 99.9th percentile latency,index-append,3084.4296338940176,ms 99.99th percentile latency,index-append,3681.6509089201218,ms 100th percentile latency,index-append,4000.8082520216703,ms 50th percentile service time,index-append,852.5300654582679,ms 90th percentile service time,index-append,1073.6245419830084,ms 99th percentile service time,index-append,1436.844245232641,ms 99.9th percentile service time,index-append,3084.4296338940176,ms 99.99th percentile service time,index-append,3681.6509089201218,ms 100th percentile service time,index-append,4000.8082520216703,ms error rate,index-append,0.00,%",
    "website_area": "discuss"
  },
  {
    "id": "90b06599-66e7-4f2a-9a91-f4460c8bde1a",
    "url": "https://discuss.elastic.co/t/how-the-python-2-6-8-to-connect-the-elasticsearch/215561",
    "title": "How the python 2.6.8 to connect the Elasticsearch?",
    "category": [
      "Elasticsearch"
    ],
    "author": "sunnut",
    "date": "January 18, 2020, 1:51pm January 18, 2020, 5:40pm January 19, 2020, 12:26pm",
    "body": "The python version is 2.6.8 that ssl has not create_default_context. how can i skip the ssl or another method to tansport the ssl certificates besides above",
    "website_area": "discuss"
  },
  {
    "id": "0b481671-0232-49e9-9245-ac50a43d089e",
    "url": "https://discuss.elastic.co/t/meeting-a-connectionerror-when-python-connect-the-elasticsearch/215621",
    "title": "Meeting a ConnectionError when python connect the elasticsearch?",
    "category": [
      "Elasticsearch"
    ],
    "author": "sunnut",
    "date": "January 19, 2020, 12:06pm",
    "body": "Code: from elasticsearch import Elasticsearch from ssl import create_default_context context = create_default_context(cafile=\"XXX.pem\") es = Elasticsearch(\"https://IP:9200\", ssl_context=context, http_auth=('username','password')) es.info() ERROR: /opt/anaconda2/envs/py2/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py:148: UserWarning: When using ssl_context, all other SSL related kwargs are ignored \"When using ssl_context, all other SSL related kwargs are ignored\" Traceback (most recent call last): File \"es.py\", line 6, in es.info() File \"/opt/anaconda2/envs/py2/lib/python2.7/site-packages/elasticsearch/client/utils.py\", line 84, in _wrapped return func(*args, params=params, **kwargs) File \"/opt/anaconda2/envs/py2/lib/python2.7/site-packages/elasticsearch/client/init.py\", line 291, in info return self.transport.perform_request(\"GET\", \"/\", params=params) File \"/opt/anaconda2/envs/py2/lib/python2.7/site-packages/elasticsearch/transport.py\", line 358, in perform_request timeout=timeout, File \"/opt/anaconda2/envs/py2/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py\", line 254, in perform_request raise ConnectionError(\"N/A\", str(e), e) elasticsearch.exceptions.ConnectionError: ConnectionError(check_hostname requires server_hostname) caused by: ValueError(check_hostname requires server_hostname)",
    "website_area": "discuss"
  },
  {
    "id": "49915ecf-93f3-4324-8900-cb7cf562ce27",
    "url": "https://discuss.elastic.co/t/slow-ingestion/215601",
    "title": "Slow Ingestion",
    "category": [
      "Elasticsearch"
    ],
    "author": "Kaarthick",
    "date": "January 19, 2020, 6:10am January 19, 2020, 9:11am January 19, 2020, 11:34am",
    "body": "Hi Team, We added a data node in current elasticsearch cluster , currently the shards are getting relocated, i want know because of this whether the ingestion process gets slow thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "b385c69f-f4a4-4516-a7a4-e9298f3662b7",
    "url": "https://discuss.elastic.co/t/javaapi-json-representation-differs-from-http-json-representation/215618",
    "title": "JavaApi Json representation differs from http Json representation",
    "category": [
      "Elasticsearch"
    ],
    "author": "MarcoAbi",
    "date": "January 19, 2020, 11:28am",
    "body": "Hi all, I'm using a little BE proxy in nodeJs to redirect FE search request to ES the results I'm getting are like a direct call to http ES endpoint. The result looks like this: Schermata 2020-01-19 alle 12.21.2811301384 111 KB where aggregations have names like \"2\" or, \"3\", as defined on the query. However, now I need to change the BE proxy using Java API, and the Json representation of SearchResponse looks like this one Schermata 2020-01-19 alle 12.21.521236842 67.2 KB Where the same aggregation results are with # syntax. Is there a way to get the original Json Object? Otherwise all the application clients I have needs huge refactoring that is out of scope. Thanks a lot for you help! Marco",
    "website_area": "discuss"
  },
  {
    "id": "b6053851-bafc-40ad-87e9-47b7bc8842d6",
    "url": "https://discuss.elastic.co/t/elastalert-connection-aborted/215587",
    "title": "Elastalert - Connection aborted",
    "category": [
      "Elasticsearch"
    ],
    "author": "Newtoelastic",
    "date": "January 19, 2020, 1:58am January 19, 2020, 10:42am January 19, 2020, 10:44am",
    "body": "Hey there guys, first time trying to work with Elastalert. I am attempting to use the elastalert-create-index command and I getting an odd response back. I used the same host and port on all my other config files and everything seems to be working fine! yet when I try and run this command I am getting a connection aborted. raise ConnectionError(\"N/A\", str(e), e) elasticsearch.exceptions.ConnectionError: ConnectionError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))) caused by: ConnectionError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))) This is my config file.. What's wrong here? #This is the folder that contains the rule yaml files #Any .yaml file will be loaded as a rule rules_folder: example_rules #How often ElastAlert will query Elasticsearch #The unit can be anything from weeks to seconds run_every: minutes: 1 #ElastAlert will buffer results from the most recent #period of time, in case some log sources are not in real time buffer_time: minutes: 15 #The Elasticsearch hostname for metadata writeback #Note that every rule can have its own Elasticsearch host es_host: \"a54cf51d3649451e9386271e10314481.eu-central-1.aws.cloud.es.io\" #The Elasticsearch port es_port: 9243 #The AWS region to use. Set this when using AWS-managed elasticsearch aws_region: eu-central-1 #The AWS profile to use. Use this if you are using an aws-cli profile. #See http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html #for details profile: test",
    "website_area": "discuss"
  },
  {
    "id": "c55f8907-8ce0-4ad5-bdf8-fd49f05752b2",
    "url": "https://discuss.elastic.co/t/cluster-optiomization/215610",
    "title": "Cluster Optiomization",
    "category": [
      "Elasticsearch"
    ],
    "author": "ragian",
    "date": "January 19, 2020, 9:40am January 19, 2020, 10:01am January 19, 2020, 10:21am",
    "body": "I need to optimize my elasticsearch cluster to have better performance on all kind of queries. The current configuration of the cluster is: Elastic 6.4.0 3 nodes (master, data, ingest) 4 core 16 GB ram (heap 8GB) 1 EBS disk of 1TB ~ 200 indexes ~ 4200 shard (12 shard per index, replica 1 (primary + its copy)) ~ 1.5 billion of documents ~ 1.5 TB of data stored more details about the indexes: 1 index of 100GB 80 indexes of 2GB each-one 80 indexes of 7GB each-one Continues bulk update queries on the index of 100GB (refresh interval 2s). Insert on the other indexes (about 10 inserts/second with refresh interval 60s) Search query on all the indexes with a lot of queries using the index_date_* pattern and that has to scan a lot of indexes (80). The biggest problem that I have with this configuration is an high load of master server when a query that has to scan 80 indexes is launched. The singular query takes about 2 seconds but a lots of this query are launched in parallel and the execution time increase (until 20 seconds and more) with the master server with a very high load. I was thinking to a new clustes configuration like this: 3 nodes (data, ingest) (AWS EC2 i3.xlarge instances) 4 cores 30 GB ram (heap 24GB) Local storage NVMe 3 nodes (master) (AWS EC2 i3.large instances) 2 core 15GB ram (heap 9GB) Local storage NVMe I know that the only way to validate the configuration is a test the configuration on the field but according to your experiance, it could be a good starting point to manage my data?",
    "website_area": "discuss"
  },
  {
    "id": "7ee9d3c3-a3b8-4654-98f2-2e1f2f0d3458",
    "url": "https://discuss.elastic.co/t/wildcard-uses/215613",
    "title": "Wildcard uses",
    "category": [
      "Elasticsearch"
    ],
    "author": "georz",
    "date": "January 19, 2020, 10:15am",
    "body": "I am using ElasticSearch 5.6.3 I am using text fields, and wants to use wildcard query on that field for every words, i am typing also it should be case insensitive. Lets say i have \"Sales & Management\", I am typing as \"s\", \"sa\", \"sal\", \"sale\", \"sales\", \"sale &\", \"sales &\", \"sale & man\", \"sales & man\", \"sale & ger\", \"sales & mangt\", \"sale & mangt\", \"sales and manager\" then in all the cases i want the result for \"Sales & Management\". I have experimented the custom analyzer using [\"lowercase\",\"asciifolding\"] but doesn't solve even half of the problem. Can you please provide me any help in this regard?",
    "website_area": "discuss"
  },
  {
    "id": "3743f53b-2159-4085-a6b3-bf9cf06306f0",
    "url": "https://discuss.elastic.co/t/query-index-in-elasticsearch-cluster/215603",
    "title": "Query index in Elasticsearch cluster",
    "category": [
      "Elasticsearch"
    ],
    "author": "bdn",
    "date": "January 19, 2020, 6:38am January 19, 2020, 11:53am January 19, 2020, 9:50am",
    "body": "I've configured an Elasticsearch Cluster of 3 nodes. Now I'm planning to write a bash script which will query for the index from the curl command without mentioning the ES node name as this cluster setup allows up to 1 node failover. If I mention a specific node name then I may not get the output as that node might be down. Is there any proper way to call the script and get data from outside the ES network?",
    "website_area": "discuss"
  },
  {
    "id": "e0823996-fe75-4be5-bcfd-27871d35588b",
    "url": "https://discuss.elastic.co/t/installing-elasticsearch-on-ubuntu-server-on-aws/215585",
    "title": "Installing Elasticsearch on Ubuntu Server on AWS",
    "category": [
      "Elasticsearch"
    ],
    "author": "droidus",
    "date": "January 19, 2020, 12:56am",
    "body": "I am trying to Install Elasticsearch on Ubuntu Server 16. When I run this command: sudo apt-get install elasticsearch, it fails, saying \"could not find java; set JAVA_HOME\". An echo of that variable returns \"/opt/jdk-13.0.2\". I've read a previous thread, but do not like the idea of running the application as the root user. It also seems that when the elasticsearch user was created, their login directory is set to /bin/false.",
    "website_area": "discuss"
  },
  {
    "id": "b4ab9bd9-6ea0-45e5-b879-3ed8ba019af7",
    "url": "https://discuss.elastic.co/t/field-value-factor-wrong-depending-on-index/215567",
    "title": "Field_value_factor wrong depending on index",
    "category": [
      "Elasticsearch"
    ],
    "author": "nicolas-t",
    "date": "January 18, 2020, 4:38pm January 18, 2020, 9:53pm",
    "body": "Hello, I have two similar documents in two different indices. My score is correct in only one of them. This is my document : { \"id\": 1, \"colors\": { \"colorA\": { \"hue\": 50, \"population\": 0.5 }, \"colorB\": { \"hue\": 90, \"population\": 0.00003 }, } } I want to use population as the score when one color matches my query. I'm using field_value_factor in my query like this : { \"query\": { \"nested\": { \"path\": \"colors\", \"query\": { \"function_score\": { \"query\": { \"match\": { \"colors.colorA.hue\": 50 } }, \"field_value_factor\": { \"field\": \"colors.colorA.population\" }, \"boost_mode\": \"replace\" } } } } } If I use the explain API on a \"new\" index I get the correct score of 0.5 : \"value\": 0.5, \"description\": \"min of:\", \"details\": [ { \"value\": 0.5, \"description\": \"field value function: none(doc['colors.colorA.population'].value * factor=1.0)\", \"details\": [ ] }, ... ] ... But if I do the same on an index containing many other similar documents I get a wrong score of 0 : \"value\": 0, \"description\": \"min of:\", \"details\": [ { \"value\": 0, \"description\": \"field value function: none(doc['colors.colorA.population'].value * factor=1.0)\", \"details\": [ ] }, ... ] ... Strangely, if I change the field to colorB.population \"field_value_factor\": { \"field\": \"colors.colorB.population\" }, I get the expected behaviour (score of 0.00003 ) : \"value\": 0.00003, \"description\": \"min of:\", \"details\": [ { \"value\": 0.00003, \"description\": \"field value function: none(doc['colors.colorA.population'].value * factor=1.0)\", \"details\": [ ] }, ... ] ... I've made a lot of tries but I can't quite understand why is my score wrong when I use colors.colorA.population and not colors.colorB.population, and why is it working on a \"new\" index but not on another one ? This issue seems related to mine: Field value factor ignoring fields with a value less than one But I create my index I use the type nested for my colors array. Thanks for your help",
    "website_area": "discuss"
  },
  {
    "id": "87f0ec0c-d052-4212-b260-643d66ec66ee",
    "url": "https://discuss.elastic.co/t/index-writer-memory-continue-to-rise/213514",
    "title": "Index writer memory Continue to rise",
    "category": [
      "Elasticsearch"
    ],
    "author": "ITzhangqiang",
    "date": "January 2, 2020, 6:57am January 2, 2020, 7:31am January 2, 2020, 8:20am January 2, 2020, 11:22am January 2, 2020, 11:46am January 2, 2020, 11:53am January 2, 2020, 12:12pm January 3, 2020, 9:24am January 3, 2020, 9:41am January 3, 2020, 9:59am January 3, 2020, 10:02am January 3, 2020, 10:08am January 3, 2020, 5:41pm January 9, 2020, 6:36am January 9, 2020, 1:57pm January 10, 2020, 2:35am January 10, 2020, 5:45am January 11, 2020, 4:49pm January 13, 2020, 10:22am January 13, 2020, 1:43pm",
    "body": "hiall I have a question that has puzzled me for a long time One node in the cluster has too much index writer memory (other node is fline), and it keeps going up,and it lead to index throttling . image1806449 145 KB And then I adjusted index.refresh.interval from 30s to 10s , but the situation has not improved much I also found this issued node's refresh queue has a large back-up why index writer cost so much memory?I found nothing in log As index writer memory continues to risein the end will lead to the exccption: org.elasticsearch.common.breaker.CircuitBreakingException: [parent] Data too large, data for [<transport_request>] would be [33137445694/30.8gb], which is larger than the limit of [31621696716/29.4gb] elasticsearch version 7.4.0 jvm heap31G could you help me analyze this problem ?@DavidTurner",
    "website_area": "discuss"
  },
  {
    "id": "39fe8b01-ee61-47e6-89da-984498d31516",
    "url": "https://discuss.elastic.co/t/elasticsearch-keystore-reference-in-kibana-yml/215572",
    "title": "Elasticsearch-keystore reference in kibana.yml",
    "category": [
      "Elasticsearch"
    ],
    "author": "kiyapedia",
    "date": "January 18, 2020, 6:38pm",
    "body": "Hello  I've got Elasticsearch and Kibana running with docker-compose with no issues. In my kibana.yml file, my password is exposed as such: server.name: kibana server.host: \"0\" elasticsearch.hosts: [ \"http://elasticsearch:9200\" ] elasticsearch.username: kibana elasticsearch.password: password When trying to create a kibana-keystore to protect the password value, i get the following error: bash: bin/kibana-keystore: No such file or directory I am following this documentation to create the kibana-keystore: https://www.elastic.co/guide/en/kibana/6.8/secure-settings.html I've also tried storing the password in elasticsearch-keystore but could not reference it and it caused my elasticsearch cluster to fail. I assume this is the reason why: Only some settings are designed to be read from the keystore. However, there is no validation to block unsupported settings from the keystore and they can cause Elasticsearch to fail to start. Any insights on how i can protect the password in kibana.yml would be much appreciated. Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "046cf442-18b6-485d-86bc-2f981e5ca3d6",
    "url": "https://discuss.elastic.co/t/error-geo-distance-sorting/215329",
    "title": "Error- Geo Distance Sorting",
    "category": [
      "Elasticsearch"
    ],
    "author": "Wa_ed_AL-Sawarieh",
    "date": "January 16, 2020, 2:34pm January 18, 2020, 6:04pm",
    "body": "Hello , let body = { \"query\": { \"bool\": { \"should\": [ { \"match\": { } }, { \"geo_distance\": { \"distance\": targetDistance+\"km\", \"location\": { \"lat\": location.lat, \"lon\": location.lon } } } ] } }, \"sort\": [ { \"_geo_distance\": { \"location\":[location.lat,location.lon], \"order\": \"asc\", \"unit\": \"km\", \"mode\": \"min\", \"distance_type\": \"arc\", \"ignore_unmapped\": true } } ] }; Error { error: { root_cause: [ [Object] ], type: 'parsing_exception', reason: 'No text specified for text query', line: 1, col: 39 }, status: 400 } Any Help ? @elastic/elasticsearch : \"version\": \"7.5.0\", Note : I try to use performatted text but it doesn't work",
    "website_area": "discuss"
  },
  {
    "id": "19a0e629-f688-4cc6-a8ce-7f114a474a62",
    "url": "https://discuss.elastic.co/t/jvm-option-file-issue/215553",
    "title": "Jvm.option file issue",
    "category": [
      "Elasticsearch"
    ],
    "author": "Sergey_Mucha",
    "date": "January 18, 2020, 9:29am January 20, 2020, 8:09am January 18, 2020, 1:28pm",
    "body": "Hi Elastic 7.5.1 was installed from tar.gz Config is : export ES_HOME=/var/vcap/jobs/elasticsearch export ES_PATH_CONF=${ES_HOME}/config I have following issue Exception in thread \"main\" java.nio.file.NoSuchFileException: /var/vcap/jobs/elasticsearch/config/jvm.options at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219) at java.base/java.nio.file.Files.newByteChannel(Files.java:374) at java.base/java.nio.file.Files.newByteChannel(Files.java:425) at java.base/java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:420) at java.base/java.nio.file.Files.newInputStream(Files.java:159) at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:62) jvm.options locates in /var/vcap/packages/elasticsearch/config/jvm.options I have config files in /var/vcap/jobs/elasticsearch/config/ /var/vcap/jobs/elasticsearch/config/elasticsearch.yml Also In elasticsearch 5.2.2 I didn't face this issue . It looks like 5.2.2 didn't need jvm.options at all. So question is how can I use jvm.options from another location as elasticsearch.yml?",
    "website_area": "discuss"
  },
  {
    "id": "8664d1da-c303-4542-bf32-ef9a787fa62b",
    "url": "https://discuss.elastic.co/t/calculating-metric-based-on-certain-buckets-in-a-histogram-aggregation/215554",
    "title": "Calculating metric based on certain buckets in a histogram aggregation",
    "category": [
      "Elasticsearch"
    ],
    "author": "leewadhams",
    "date": "January 18, 2020, 9:41am",
    "body": "A simplified version of our price index is as follows: PUT price-example { \"mappings\": { \"properties\": { \"skuId\": { \"type\": \"keyword\" }, \"supplierId\": { \"type\": \"keyword\" }, \"price\": { \"type\": \"float\" }, \"pricedate\": { \"type\": \"date\" } } } } The UI supports the following table definition Supplier Name | Total Items | Mean | Low | High | Median Price Growth The query we wrote initially is the following which I think meets all but the % median price growth` in terms of the aggregate values: GET price-example/_search { \"size\": 0, \"query\": { \"bool\": { \"filter\": { \"range\": { \"pricedate\": { \"gte\": \"2019-11-01\", \"lte\": \"2020-11-30\" } } } } }, \"aggs\": { \"total-buckets\": { \"cardinality\": { \"field\": \"supplierid\" } }, \"supplier-bucket\": { \"terms\": { \"field\": \"supplierid\", \"size\": 10 }, \"aggs\": { \"total-skus\": { \"cardinality\": { \"field\": \"skuId\" } }, \"price-percentiles\": { \"percentiles\": { \"field\": \"price\", \"percents\": [ 0, 25, 50, 75, 100 ] } }, \"dates\": { \"date_histogram\": { \"field\": \"pricedate\", \"calendar_interval\": \"day\" }, \"aggs\": { \"day-median\": { \"percentiles\": { \"field\": \"price\", \"percents\": [ 50 ] } } } } } } } } The first challenge is how to calculate the % median price growth. It is calculated as a percent growth rate, taking the difference between the median prices at the start and end dates and then dividing by the median price for the start date. So in this example we need the median price in the earliest and latest buckets in the dates aggregation and then use these values to calculate the metric. Im not sure how, or even if it is possible to do this. Another thing to bear in mind is that we have to be able to sort on this value too. Any suggestions/ideas would be greatly appreciated",
    "website_area": "discuss"
  },
  {
    "id": "ec6b0b91-d10d-4781-a74e-41b662caaa3f",
    "url": "https://discuss.elastic.co/t/elasticsearh-internals/215367",
    "title": "Elasticsearh internals",
    "category": [
      "Elasticsearch"
    ],
    "author": "adityaPsl",
    "date": "January 16, 2020, 5:46pm January 16, 2020, 6:01pm January 18, 2020, 8:26am",
    "body": "Hello Team, How can i learn elasticsearch internals like Index creation and storage , cross cluster code, storage API, text analysis etc. Thank you, Aditya",
    "website_area": "discuss"
  },
  {
    "id": "3a3774b6-4fe6-4419-9105-5df823ff48a7",
    "url": "https://discuss.elastic.co/t/multi-word-multi-term-querystring-query/215509",
    "title": "Multi-word, multi-term querystring query",
    "category": [
      "Elasticsearch"
    ],
    "author": "David_Von_Lehman",
    "date": "January 17, 2020, 7:33pm January 18, 2020, 7:59am",
    "body": "When running a multi-term querystring query with multiple search fields, I get different results depending on if there is a boolean operator present. For example if I have the following document: { title: \"Car\", description: \"Boat\" } When I run the following query I get back 0 hits. { \"query\": { \"query_string\": { \"default_operator\": \"AND\", \"fields\": [\"title\", \"description\"], \"query\": \"Car Boat\" } } But if I add an \"AND\" in the query I get back the 1 hit as expected: { \"query\": { \"query_string\": { \"default_operator\": \"AND\", \"fields\": [\"title\", \"description\"], \"query\": \"Car AND Boat\" } } I would have thought setting the default_operator to \"AND\" would have resulted in the same hits.",
    "website_area": "discuss"
  },
  {
    "id": "0a4425b1-5f8f-4328-ae5b-12ac82264e21",
    "url": "https://discuss.elastic.co/t/index-is-not-rolling-over/215546",
    "title": "Index is not rolling over",
    "category": [
      "Elasticsearch"
    ],
    "author": "asrinivasan",
    "date": "January 18, 2020, 6:01am",
    "body": "I created a lifec cycle policy to rollover after size reaches 30G, i then changed it to 100MB, however this did not take effect and the index did not rollover image1868104 8.75 KB Running GET xxx-filemgr-000004/_ilm/explain Gives { \"indices\" : { \"xxx-xxxx-000004\" : { \"index\" : \"xxx-xxxx-000004\", \"managed\" : true, \"policy\" : \"xxx_policy\", \"lifecycle_date_millis\" : 1579124902569, \"phase\" : \"hot\", \"phase_time_millis\" : 1579124903572, \"action\" : \"rollover\", \"action_time_millis\" : 1579125444675, \"step\" : \"check-rollover-ready\", \"step_time_millis\" : 1579125444675, \"phase_execution\" : { \"policy\" : \"dlp_policy\", \"phase_definition\" : { \"min_age\" : \"0ms\", \"actions\" : { \"rollover\" : { \"max_size\" : \"30gb\" } } }, \"version\" : 1, \"modified_date_in_millis\" : 1574287086021 } } } }",
    "website_area": "discuss"
  },
  {
    "id": "fd10ce32-0668-45b0-8de3-aeb8d8ed9a1f",
    "url": "https://discuss.elastic.co/t/object-mapping-channels-channel-people-cant-be-changed-from-nested-to-non-nested/215543",
    "title": "Object mapping [channels.channel_people] can't be changed from nested to non-nested",
    "category": [
      "Elasticsearch"
    ],
    "author": "Abhilash_Sathe1",
    "date": "January 18, 2020, 4:47am",
    "body": "I first created a mapping. PUT /my_index/ { \"mappings\": { \"properties\": { \"channels\": { \"type\": \"object\", \"properties\": { \"channel_people\": { \"type\": \"nested\", \"properties\": { \"email\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"id\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } }, \"subtype\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"channel_type\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"source_type\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } } } } POST my_index/my_type/12 { \"channels\": [ { \"channel_label\": \"some_value\", \"channel_subtype\": \"some_value\", \"source_type\": \"some_value\", \"channel_people\": [ { \"id\": \"some_value\", \"email\": \"some_value\" }, { \"id\": \"some_value\", \"email\": \"some_value\" } ], \"channel_type\": \"some_value\", \"channel_id\": \"some_value\" } ] } After executing POST request, I get object mapping [channels.channel_people] can't be changed from nested to non-nested",
    "website_area": "discuss"
  },
  {
    "id": "de615b49-f1ce-442c-8b54-3dd8aec59546",
    "url": "https://discuss.elastic.co/t/query-not-working/215538",
    "title": "QUERY - not working",
    "category": [
      "Elasticsearch"
    ],
    "author": "GuessMyName",
    "date": "January 18, 2020, 1:10am",
    "body": "Hey @ll I don't understand why this query is not bringing any results data.win.eventdata.parentImage: \"C:\\Windows\\explorer.exe\" or data.win.eventdata.parentImage: \"explorer.exe\" data is there... image75369 6.22 KB Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "7f2d16f6-4b6b-461f-a7a2-f78f5be33653",
    "url": "https://discuss.elastic.co/t/security-exception-when-running-sample-security-plugin/214671",
    "title": "Security Exception when Running Sample Security Plugin",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 10, 2020, 9:15pm January 11, 2020, 12:36pm January 13, 2020, 8:51pm January 14, 2020, 10:45am January 17, 2020, 11:23pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9e79f642-260f-48e4-a32a-5d9f28b00a31",
    "url": "https://discuss.elastic.co/t/sorting-lists-in-painless/215530",
    "title": "Sorting lists in painless",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 17, 2020, 10:21pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6b463831-0582-4152-bcb4-e18c9231c757",
    "url": "https://discuss.elastic.co/t/java-lang-nosuchmethoderror-org-elasticsearch-search-searchhits-gettotalhits/215484",
    "title": "java.lang.NoSuchMethodError: org.elasticsearch.search.SearchHits.getTotalHits()",
    "category": [
      "Elasticsearch"
    ],
    "author": "SachaMaiornikoff",
    "date": "January 17, 2020, 3:11pm January 17, 2020, 3:20pm January 17, 2020, 3:27pm January 17, 2020, 6:41pm",
    "body": "Hello guys, I know there is a topic with the same issue but the answer doesn't solve my issue. I got a really basic spring boot 2.2.0 app which is supposed to connect to a ES 7.2.0. but when i try to search anything i got the error : java.lang.NoSuchMethodError: org.elasticsearch.search.SearchHits.getTotalHits()J at org.springframework.data.elasticsearch.core.ElasticsearchRestTemplate.doCount(ElasticsearchRestTemplate.java:605) ~[spring-data-elasticsearch-3.2.4.RELEASE.jar:3.2.4.RELEASE] at org.springframework.data.elasticsearch.core.ElasticsearchRestTemplate.count(ElasticsearchRestTemplate.java:566) ~[spring-data-elasticsearch-3.2.4.RELEASE.jar:3.2.4.RELEASE] I can provide the full stack trace but there is a char limit and i think the dependency is more useful I got my code here : https://github.com/SachaMaiornikoff/digger-back/tree/RestHighLevelClient_and_bonsai_impl I already forced my ES version on my pom.xml with <elasticsearch.version>7.2.0</elasticsearch.version> and it's basically the only solution that i found on google. here is the result of my mvn dependency:tree : [INFO] com.smaiornikoff:back:jar:0.0.1-SNAPSHOT [INFO] +- org.springframework.boot:spring-boot-starter-data-elasticsearch:jar:2.2.0.RELEASE:compile [INFO] +- org.elasticsearch:elasticsearch:jar:7.2.0:compile [INFO] | +- org.elasticsearch:elasticsearch-core:jar:7.2.0:compile [INFO] | +- org.elasticsearch:elasticsearch-secure-sm:jar:7.2.0:compile [INFO] | +- org.elasticsearch:elasticsearch-x-content:jar:7.2.0:compile [INFO] | | +- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.10.0:compile [INFO] | | +- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.10.0:compile [INFO] | | \\- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.10.0:compile [INFO] | +- org.elasticsearch:elasticsearch-geo:jar:7.2.0:compile [INFO] | +- org.apache.lucene:lucene-core:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-analyzers-common:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-backward-codecs:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-grouping:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-highlighter:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-join:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-memory:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-misc:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-queries:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-queryparser:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-sandbox:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-spatial:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-spatial-extras:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-spatial3d:jar:8.0.0:compile [INFO] | +- org.apache.lucene:lucene-suggest:jar:8.0.0:compile [INFO] | +- org.elasticsearch:elasticsearch-cli:jar:7.2.0:compile [INFO] | | \\- net.sf.jopt-simple:jopt-simple:jar:5.0.2:compile [INFO] | +- com.carrotsearch:hppc:jar:0.7.1:compile [INFO] | +- joda-time:joda-time:jar:2.10.5:compile [INFO] | +- com.tdunning:t-digest:jar:3.2:compile [INFO] | +- org.hdrhistogram:HdrHistogram:jar:2.1.9:compile [INFO] | +- org.apache.logging.log4j:log4j-api:jar:2.12.1:compile [INFO] | \\- org.elasticsearch:jna:jar:4.5.1:compile [INFO] +- org.elasticsearch.client:elasticsearch-rest-client:jar:7.2.0:compile [INFO] | +- org.apache.httpcomponents:httpclient:jar:4.5.10:compile [INFO] | +- org.apache.httpcomponents:httpcore:jar:4.4.12:compile [INFO] | +- org.apache.httpcomponents:httpasyncclient:jar:4.1.4:compile [INFO] | +- org.apache.httpcomponents:httpcore-nio:jar:4.4.12:compile [INFO] | \\- commons-codec:commons-codec:jar:1.13:compile [INFO] +- org.elasticsearch.client:elasticsearch-rest-high-level-client:jar:7.2.0:compile [INFO] | +- org.elasticsearch.plugin:parent-join-client:jar:7.2.0:compile [INFO] | +- org.elasticsearch.plugin:aggs-matrix-stats-client:jar:7.2.0:compile [INFO] | +- org.elasticsearch.plugin:rank-eval-client:jar:7.2.0:compile [INFO] | \\- org.elasticsearch.plugin:lang-mustache-client:jar:7.2.0:compile [INFO] | \\- com.github.spullara.mustache.java:compiler:jar:0.9.3:compile [INFO] +- org.springframework.data:spring-data-elasticsearch:jar:3.2.4.RELEASE:compile [INFO] | +- org.springframework:spring-context:jar:5.2.1.RELEASE:compile [INFO] | | +- org.springframework:spring-aop:jar:5.2.1.RELEASE:compile [INFO] | | +- org.springframework:spring-beans:jar:5.2.1.RELEASE:compile [INFO] | | \\- org.springframework:spring-expression:jar:5.2.1.RELEASE:compile [INFO] | +- org.springframework:spring-tx:jar:5.2.1.RELEASE:compile [INFO] | +- org.springframework.data:spring-data-commons:jar:2.2.1.RELEASE:compile [INFO] | +- org.elasticsearch.client:transport:jar:7.2.0:compile [INFO] | | +- org.elasticsearch.plugin:reindex-client:jar:7.2.0:compile [INFO] | | | \\- org.elasticsearch:elasticsearch-ssl-config:jar:7.2.0:compile [INFO] | | \\- org.elasticsearch.plugin:percolator-client:jar:7.2.0:compile [INFO] | +- org.elasticsearch.plugin:transport-netty4-client:jar:7.2.0:compile [INFO] | | +- io.netty:netty-buffer:jar:4.1.43.Final:compile [INFO] | | +- io.netty:netty-codec:jar:4.1.43.Final:compile [INFO] | | +- io.netty:netty-codec-http:jar:4.1.43.Final:compile [INFO] | | +- io.netty:netty-common:jar:4.1.43.Final:compile [INFO] | | +- io.netty:netty-handler:jar:4.1.43.Final:compile [INFO] | | +- io.netty:netty-resolver:jar:4.1.43.Final:compile [INFO] | | \\- io.netty:netty-transport:jar:4.1.43.Final:compile [INFO] | +- com.fasterxml.jackson.core:jackson-core:jar:2.10.0:compile [INFO] | +- com.fasterxml.jackson.core:jackson-databind:jar:2.10.0:compile [INFO] | | \\- com.fasterxml.jackson.core:jackson-annotations:jar:2.10.0:compile [INFO] | \\- org.slf4j:slf4j-api:jar:1.7.29:compile [INFO] +- org.springframework.boot:spring-boot-starter:jar:2.2.0.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot:jar:2.2.1.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-autoconfigure:jar:2.2.1.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-starter-logging:jar:2.2.1.RELEASE:compile [INFO] | | +- ch.qos.logback:logback-classic:jar:1.2.3:compile [INFO] | | | \\- ch.qos.logback:logback-core:jar:1.2.3:compile [INFO] | | +- org.apache.logging.log4j:log4j-to-slf4j:jar:2.12.1:compile [INFO] | | \\- org.slf4j:jul-to-slf4j:jar:1.7.29:compile [INFO] | +- jakarta.annotation:jakarta.annotation-api:jar:1.3.5:compile [INFO] | +- org.springframework:spring-core:jar:5.2.1.RELEASE:compile [INFO] | | \\- org.springframework:spring-jcl:jar:5.2.1.RELEASE:compile [INFO] | \\- org.yaml:snakeyaml:jar:1.25:compile [INFO] +- org.springframework.boot:spring-boot-starter-test:jar:2.2.0.RELEASE:test [INFO] | +- org.springframework.boot:spring-boot-test:jar:2.2.1.RELEASE:test [INFO] | +- org.springframework.boot:spring-boot-test-autoconfigure:jar:2.2.1.RELEASE:test [INFO] | +- com.jayway.jsonpath:json-path:jar:2.4.0:test [INFO] | | \\- net.minidev:json-smart:jar:2.3:test [INFO] | | \\- net.minidev:accessors-smart:jar:1.2:test [INFO] | | \\- org.ow2.asm:asm:jar:5.0.4:test [INFO] | +- jakarta.xml.bind:jakarta.xml.bind-api:jar:2.3.2:compile [INFO] | +- org.junit.jupiter:junit-jupiter:jar:5.5.2:test [INFO] | | +- org.junit.jupiter:junit-jupiter-api:jar:5.5.2:test [INFO] | | | +- org.apiguardian:apiguardian-api:jar:1.1.0:test [INFO] | | | +- org.opentest4j:opentest4j:jar:1.2.0:test [INFO] | | | \\- org.junit.platform:junit-platform-commons:jar:1.5.2:test [INFO] | | +- org.junit.jupiter:junit-jupiter-params:jar:5.5.2:test [INFO] | | \\- org.junit.jupiter:junit-jupiter-engine:jar:5.5.2:test [INFO] | | \\- org.junit.platform:junit-platform-engine:jar:1.5.2:test [INFO] | +- org.mockito:mockito-junit-jupiter:jar:3.1.0:test [INFO] | +- org.assertj:assertj-core:jar:3.13.2:test [INFO] | +- org.hamcrest:hamcrest:jar:2.1:test [INFO] | +- org.mockito:mockito-core:jar:3.1.0:test [INFO] | | +- net.bytebuddy:byte-buddy:jar:1.10.2:compile [INFO] | | +- net.bytebuddy:byte-buddy-agent:jar:1.10.2:test [INFO] | | \\- org.objenesis:objenesis:jar:2.6:test [INFO] | +- org.skyscreamer:jsonassert:jar:1.5.0:test [INFO] | | \\- com.vaadin.external.google:android-json:jar:0.0.20131108.vaadin1:test [INFO] | +- org.springframework:spring-test:jar:5.2.1.RELEASE:test [INFO] | \\- org.xmlunit:xmlunit-core:jar:2.6.3:test [INFO] +- org.projectlombok:lombok:jar:1.16.10:compile [INFO] +- org.springframework.boot:spring-boot-starter-web:jar:2.2.0.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-starter-json:jar:2.2.1.RELEASE:compile [INFO] | | +- com.fasterxml.jackson.datatype:jackson-datatype-jdk8:jar:2.10.0:compile [INFO] | | +- com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.10.0:compile [INFO] | | \\- com.fasterxml.jackson.module:jackson-module-parameter-names:jar:2.10.0:compile [INFO] | +- org.springframework.boot:spring-boot-starter-tomcat:jar:2.2.1.RELEASE:compile [INFO] | | +- org.apache.tomcat.embed:tomcat-embed-core:jar:9.0.27:compile [INFO] | | +- org.apache.tomcat.embed:tomcat-embed-el:jar:9.0.27:compile [INFO] | | \\- org.apache.tomcat.embed:tomcat-embed-websocket:jar:9.0.27:compile [INFO] | +- org.springframework.boot:spring-boot-starter-validation:jar:2.2.1.RELEASE:compile [INFO] | | +- jakarta.validation:jakarta.validation-api:jar:2.0.1:compile [INFO] | | \\- org.hibernate.validator:hibernate-validator:jar:6.0.18.Final:compile [INFO] | +- org.springframework:spring-web:jar:5.2.1.RELEASE:compile [INFO] | \\- org.springframework:spring-webmvc:jar:5.2.1.RELEASE:compile [INFO] +- mysql:mysql-connector-java:jar:8.0.18:compile [INFO] \\- org.springframework.boot:spring-boot-starter-data-jpa:jar:2.2.0.RELEASE:compile [INFO] +- org.springframework.boot:spring-boot-starter-aop:jar:2.2.1.RELEASE:compile [INFO] | \\- org.aspectj:aspectjweaver:jar:1.9.4:compile [INFO] +- org.springframework.boot:spring-boot-starter-jdbc:jar:2.2.1.RELEASE:compile [INFO] | +- com.zaxxer:HikariCP:jar:3.4.1:compile [INFO] | \\- org.springframework:spring-jdbc:jar:5.2.1.RELEASE:compile [INFO] +- jakarta.activation:jakarta.activation-api:jar:1.2.1:compile [INFO] +- jakarta.persistence:jakarta.persistence-api:jar:2.2.3:compile [INFO] +- jakarta.transaction:jakarta.transaction-api:jar:1.3.3:compile [INFO] +- org.hibernate:hibernate-core:jar:5.4.8.Final:compile [INFO] | +- org.jboss.logging:jboss-logging:jar:3.4.1.Final:compile [INFO] | +- org.javassist:javassist:jar:3.24.0-GA:compile [INFO] | +- antlr:antlr:jar:2.7.7:compile [INFO] | +- org.jboss:jandex:jar:2.0.5.Final:compile [INFO] | +- com.fasterxml:classmate:jar:1.5.1:compile [INFO] | +- org.dom4j:dom4j:jar:2.1.1:compile [INFO] | +- org.hibernate.common:hibernate-commons-annotations:jar:5.1.0.Final:compile [INFO] | \\- org.glassfish.jaxb:jaxb-runtime:jar:2.3.2:compile [INFO] | +- org.glassfish.jaxb:txw2:jar:2.3.2:compile [INFO] | +- com.sun.istack:istack-commons-runtime:jar:3.0.8:compile [INFO] | +- org.jvnet.staxex:stax-ex:jar:1.8.1:compile [INFO] | \\- com.sun.xml.fastinfoset:FastInfoset:jar:1.2.16:compile [INFO] +- org.springframework.data:spring-data-jpa:jar:2.2.1.RELEASE:compile [INFO] | \\- org.springframework:spring-orm:jar:5.2.1.RELEASE:compile [INFO] \\- org.springframework:spring-aspects:jar:5.2.1.RELEASE:compile So i believe the dependencies are set just right. I'm sure it's something really simple that i'm missing out but i'd be thankful if someone got any clue in which direction i should look. Thanks, Sacha",
    "website_area": "discuss"
  },
  {
    "id": "11b8df09-bf00-45bf-8672-c13ed9475aca",
    "url": "https://discuss.elastic.co/t/port-forwarding-to-elk-application-in-ubuntu/215226",
    "title": "Port forwarding to ELK Application in Ubuntu",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mehak_Bhargava",
    "date": "January 15, 2020, 11:34pm January 17, 2020, 6:24pm",
    "body": "I am trying to make my kibana webpage application accessible to other computer from outside network. In Port forwarding, what should be the Host Ip, Host Port, Guest Ip, and Guest port?",
    "website_area": "discuss"
  },
  {
    "id": "5af6f454-e2d3-4e12-8158-0983accb04ba",
    "url": "https://discuss.elastic.co/t/bulk-export-more-than-10k-events/215497",
    "title": "Bulk Export More Than 10k Events",
    "category": [
      "Elasticsearch"
    ],
    "author": "wwalker",
    "date": "January 17, 2020, 5:08pm",
    "body": "I occasionally have the need to pull events out of Elasticsearch. I built a script using PowerShell that utilizes scroll to pull all the records. The problem I have though is that I can only pull 10k events per request. I typically get approx. 5-8 million events per day. As you can imagine, this results in a large number of requests hitting the server and it takes a looong time to pull all the records, usually takes about 60-90 seconds per request. Is it possible to get more than 10k at a time? Below is the script I'm using #Force PowerShell to use TLS 1.2 [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::TLS12 #Get username/password for kibana user $cred = Get-Credential #Specify index to query - wildcard ending implied $is = \"myindex-*\" #Set base elasticsearch URL $esbase = \"https://ELASTICSEARCHFQDN:9200\" #Set folder to save files to $path = \"PATH_TO_SAVE_TO\" #Set headers $Options = @{ headers = @{ 'kbn-version' = \"7.5.0\" } ContentType = \"application/json\" Method = \"POST\" } #This can be used as an example, it is performing a range query on field edge.timestamp_start whose values are in epoch seconds. $query = '{ \"sort\": [\"_doc\"], \"query\": { \"range\": { \"edge.timestamp_start\": { \"gte\": \"1577772000\", \"lte\": \"1577858399\" } } } }' #Initial query specifying parameters and index to search $isr = Invoke-RestMethod @Options -Uri \"$esbase/$is*/_search?scroll=10m&size=10000\" -Body $query -Credential $cred -Verbose #Verify destination folder exists, create if it does not. if ((Test-Path $path) -eq $false) { New-Item -Type Directory $path | Out-Null } #Start loop to process the initial 10k results and subsequent results. do { #Grab the current time to be used in the filename $date = Get-Date -Format MMddyyyy_hhmmss #Start loop to dump results into a text file foreach ($hit in $isr.hits.hits) { #Narrow file output data to only the raw message field. $out = $hit._source.message #Output message field to a text field, appending the existing file so as not to create 10k tiny files. $out | Out-File $path\\June$Date.txt -Encoding utf8 -Append } #Compress the resulting txt file. In my use case, each txt file was 16MB, compressed was 1.5MB #Compress-Archive -Path $path\\June$Date.txt -Update -DestinationPath $path\\June$Date.zip -CompressionLevel Fastest #Remove original txt file after compression is complete. #Remove-Item $path\\June$Date.txt -Confirm:$false #Grab the scroll id from the previous successful query to feed into the next query. $sid = $isr._scroll_id #Reset query timeout to 5 minutes and insert scroll id $query = '{ \"scroll\": \"5m\", \"scroll_id\": \"'+$sid+'\" }' #Gather the next 10k results $isr = Invoke-RestMethod @Options -Uri \"$esbase/_search/scroll\" -Body $query -Credential $cred -Verbose #Continue loop until the last query returns 0 results. } until ($isr.hits.hits.count -eq 0)",
    "website_area": "discuss"
  },
  {
    "id": "b86891ac-faea-4b96-bfce-578f3bc45d01",
    "url": "https://discuss.elastic.co/t/index-parent-child-with-id/215491",
    "title": "Index parent-child with _id",
    "category": [
      "Elasticsearch"
    ],
    "author": "johnkary",
    "date": "January 17, 2020, 5:04pm",
    "body": "Hi all! I use parent-child model in my index \"npk\" and here are some questions basically on the ways i can search. I studied has-child and has-parent queries but i don't get any results for the docs i inserted to this index. My index creation: PUT /npk { \"settings\" : { \"index\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 2 } }, \"dynamic\": true, \"mappings\": { \"doc\": { \"properties\": { \"my_join_field\" : { \"type\" : \"join\", \"relations\": { \"header\": \"body\" }}}}}} I indexed 2 parents with _ids 1 and 4: PUT /npk/doc/1 { \"header_column_1\": \"value1\", \"header_column_2\": \"value2\", \"my_join_field\": \"header\" } I indexed 2 children for each parent with ids 2, 3 and 5, 6: PUT /npk/doc/2?routing=1&refresh { \"body_column_1\": 220, \"body_column_2\": 220, \"my_join_field\": { \"name\": \"body\", \"parent\": \"1\" } } My question is: Both parent and child have to be in the field '_id' ? Do i implement it in the right way here? I have read somewhere that '_id' field has a limit of 512 bytes, so we cannot increase it too much. I have made a script that puts parents-children and in the '_id' i have an auto-increment routine to increase it as a number. Is it right to be a number? Because by default it takes alphanumeric values. The weird here is that when i run the below query for both parents (_ids 1 and 4) to get their children i receive correct results: GET /npk/_search { \"query\": { \"parent_id\": { \"type\": \"body\", \"id\": \"4\" } } } response: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 3, \"successful\": 3, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 2, \"max_score\": 0.87546873, \"hits\": [ { \"_index\": \"npk\", \"_type\": \"doc\", \"_id\": \"5\", \"_score\": 0.87546873, \"_routing\": \"1\", \"_source\": { \"body_column_1\": 200, \"body_column_2\": 200, \"my_join_field\": { \"name\": \"body\", \"parent\": \"4\" } } }, { \"_index\": \"npk\", \"_type\": \"doc\", \"_id\": \"6\", \"_score\": 0.87546873, \"_routing\": \"1\", \"_source\": { \"body_column_1\": 220, \"body_column_2\": 220, \"my_join_field\": { \"name\": \"body\", \"parent\": \"4\" } } } ] } } ... BUT, when i run the has_child query GET /npk/_search { \"query\": { \"has_child\" : { \"type\" : \"body\", \"query\" : { \"match_all\" : {} }, \"max_children\": 10, \"min_children\": 1, \"score_mode\" : \"min\" } } } i get results only for the parent with _id = 1. The other with _id=4 is missing! { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 3, \"successful\": 3, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 1, \"max_score\": 1, \"hits\": [ { \"_index\": \"npk\", \"_type\": \"doc\", \"_id\": \"1\", \"_score\": 1, \"_source\": { \"header_column_1\": \"value1\", \"header_column_2\": \"value1\", \"my_join_field\": \"header\" } } ] } }",
    "website_area": "discuss"
  },
  {
    "id": "74c94736-0f72-422b-bb11-d41ebed30a77",
    "url": "https://discuss.elastic.co/t/how-to-get-all-unique-values-of-a-field-for-a-single-index/215492",
    "title": "How to get all unique values of a field for a single index?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Tom_Somerville",
    "date": "January 17, 2020, 4:12pm",
    "body": "Hey All, Running into an issue that I cannot seem to find a solution. I currently have a number of indexes, all of them billions of docs large, and I need a way to extract all unique values for any given field in the index. I would say the majority of the data is duplicated data in the field, I would expect about 50m-100m unique results for the 1bil+ docs. When I do a terms aggregation Im limited to the 10000 buckets, and cannot get more than 10k unique values and I cannot seem to figure out how to paginate the aggs buckets (I can scroll the hits, but this isnt of value to me as it is just giving me every doc, 10k at a time, until I scroll through all 1bil+ docs) Also, I cant seem to get a composit search to work. Again, the ['hits']['hits'] are not unique values, and when i look at the buckets, i am being returned values that do not exist that start with hypens, and illegal characters for the field type ( like * and : ) Can anyone explain to me how I can grab all 50m unique values of an index. Currently the only way I can actually get this to work is to scroll through every doc, extract the value, and dedupe. But with 2-3 second queries at 10k at a time, this will take over 3 days! I put my queries below, also have tried playing around with doc and bucket size parameters with no luck. what I have tried: Attempt 1: Scrolling terms agg GET /dns-2020.01.13/_search?scroll=1m { \"aggs\": { \"unique_quieries\": { \"terms\": { \"field\": \"query.keyword\", \"size\": 10000 } } } } followed up with GET /_search/scroll { \"scroll\" : \"5m\", \"scroll_id\" : \"DnF1ZXJ5VGhlbkZldGNoBAAAAAAAEVZiFkx1REFCM0FXU3dpNGpFOEZXNnZaRGcAAAAAABB4zRY1NHdiS2dzc1JaZXhoSjNTaFZvbmVRAAAAAAAP690WOEZWeFd5cU1Td3VIVWFvbEo0MXh0ZwAAAAAAPubpFkl5RDl6b1d2U1JxMmMzejQ3V05odlE=\" } Attempt 2: Composit search, returns odd values and non unique hits. GET /dns-2020.01.13/_search { \"track_total_hits\": false, \"aggs\" : { \"my_buckets\": { \"composite\" : { \"sources\" : [ { \"query\": { \"terms\" : { \"field\": \"query.keyword\" } } } ] } } } }",
    "website_area": "discuss"
  },
  {
    "id": "7dfa7275-e87b-45e4-9ff4-219b22fa49e5",
    "url": "https://discuss.elastic.co/t/license-api-for-put/215490",
    "title": "License API for Put",
    "category": [
      "Elasticsearch"
    ],
    "author": "balogan",
    "date": "January 17, 2020, 3:53pm",
    "body": "I'm working on writing a play to update the license key. Get and delete works well, but when I put the license I got from the get it throws this error. { \"errors\": [{ \"code\": \"license.invalid_license\", \"message\": \"Invalid license: null\" }] } I added this to the get return to match the docs. { \"license\": { \"cluster_licenses\": [ { \"license\" : {} } ], \"expiry_date_in_millis\": 1581185762350, ... Looks as though I need a value for what I added to the JSON. Am I correct?",
    "website_area": "discuss"
  },
  {
    "id": "826ef8c6-de90-4151-8ff6-5e9a42878e33",
    "url": "https://discuss.elastic.co/t/ingest-pipeline-rename-a-field-that-has-a-dynamic-part/215488",
    "title": "Ingest pipeline, rename a field that has a dynamic part",
    "category": [
      "Elasticsearch"
    ],
    "author": "wedkarz014",
    "date": "January 17, 2020, 3:48pm",
    "body": "hi, I have a metrics with fields which look like that: prometheus.xyz-dev.label.name prometheus.xyz-dev.tomcat_global_error_total.value prometheus.xyz-dev.tomcat_global_received_bytes_total.value prometheus.xyz-dev.tomcat_global_request_max_seconds.value prometheus.xyz-dev.tomcat_global_request_seconds.count I would like to delete first two parts, but the second one is a kubernetes.namespace value so this is a dynamic value. This is what i need: prometheus.xyz-dev.label.name => label.name prometheus.abc-prod.label.name => label.name or eventually prometheus.xyz-dev.label.name => prometheus.label.name I need to create an ingest pipeline which can to that, but i have no idea how to do that. Can you help?",
    "website_area": "discuss"
  },
  {
    "id": "24b161d1-2687-4fbd-a018-09a4ca8743c2",
    "url": "https://discuss.elastic.co/t/query-string-using-wildcard-vs-prefix-query/215481",
    "title": "Query_string using '*' wildcard VS prefix query",
    "category": [
      "Elasticsearch"
    ],
    "author": "CesarPozo",
    "date": "January 17, 2020, 2:59pm January 17, 2020, 3:28pm",
    "body": "I wrote a query for get all clients whose name start with c. I am getting the same results with the two following queries: #Q1 Query String and * wildcard GET /client/_search { \"query\": { \"query_string\": { \"default_field\": \"Name.keyword\", \"query\": \"c*\" } } } #Q2 prefix query GET /client/_search { \"query\": { \"prefix\": { \"Name.keyword\": { \"value\": \"c\" } } } } I guess the string query is less efficient as it is explained here: Be aware that wildcard queries can use an enormous amount of memory and perform very badly  just think how many terms need to be queried to match the query string \" a* b* c* \". But could somebody tell me which one would be the best and more efficient for my approach? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "b9796775-3721-4efd-9512-67d653ad3de9",
    "url": "https://discuss.elastic.co/t/elastic-does-not-respond-with-many-security-index-patterns/215376",
    "title": "Elastic does not respond with many security index patterns",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 16, 2020, 7:01pm January 17, 2020, 6:06am January 17, 2020, 3:14pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a87c4a77-28ff-4441-b200-4296adc316d2",
    "url": "https://discuss.elastic.co/t/migrate-billing-to-google-cloud/215485",
    "title": "Migrate Billing to Google Cloud",
    "category": [
      "Elasticsearch"
    ],
    "author": "Edgar_Peixoto",
    "date": "January 17, 2020, 3:11pm",
    "body": "My team already have a Cluster in Elastic Cloud. But now we want to migrate the billing to our Google Cloud. Is it possible or do we have to recreate the cluster using Google Cloud Marketplace and delete the cluster we already have?",
    "website_area": "discuss"
  },
  {
    "id": "23cee514-6185-46f3-a4cf-e070dc584aed",
    "url": "https://discuss.elastic.co/t/stack-monitoring-cannot-see-indices/214478",
    "title": "Stack monitoring cannot see Indices",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 9, 2020, 6:41pm January 10, 2020, 6:31am January 10, 2020, 9:41am January 10, 2020, 2:52pm January 17, 2020, 2:43pm January 17, 2020, 2:55pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "31638065-de1b-4d5e-a225-cc8503f0c151",
    "url": "https://discuss.elastic.co/t/wildcard-searching-on-mult-word-phrase/215480",
    "title": "Wildcard searching on mult-word phrase",
    "category": [
      "Elasticsearch"
    ],
    "author": "cas4",
    "date": "January 17, 2020, 2:51pm",
    "body": "I have a particular document that contains North American and North America's but not North America. I'm trying to come up with a query string syntax that will return all North America records trying a query similar to the following: POST movies/_search { \"query\": { \"query_string\": { \"fields\": [\"title\"], \"query\": \"\\\"north america*\\\"\" } } } After some reading, it appears that wildcards are not allowed inside quotes. If that is truly the case, is there some alternative to searching for a phrase where one of the words is not an exact match? Reading this article Learn Elasticsearch Stemming with Example, it appears using the English analyzer in parallel to the Standard analyzer might be an approach. This seems awfully expensive as each string field would be indexed twice. Any advice would be greatly appreciated.",
    "website_area": "discuss"
  },
  {
    "id": "83f7d500-96bf-4a11-8687-72ddbba48c9c",
    "url": "https://discuss.elastic.co/t/clarity-on-ecs-1-4-service-field-group/214657",
    "title": "Clarity on ECS 1.4 service field group",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 10, 2020, 8:19pm January 17, 2020, 2:22pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f8f18b50-8c53-467e-8297-23574774b055",
    "url": "https://discuss.elastic.co/t/is-elasticsearch-the-right-tool-for-a-spellchecker/215469",
    "title": "Is Elasticsearch the right tool for a spellchecker?",
    "category": [
      "Elasticsearch"
    ],
    "author": "illyThe",
    "date": "January 17, 2020, 1:51pm",
    "body": "Hello. I'm not sure if this is the right place to ask. I'm going to build a spellchecker as a project for a Java application and I had an idea on how I could implement it with Elasticsearch. I would index the dictionaries in to Elasticsearch. Separate index for each language. Then run a query for each word in the index that user picks as main language. If word is found without fuzziness then it is correctly spelled else I would run the query with the word again but with some fuzziness values and display the results as suggested spelling. Do you think Elasticsearch is a good tool for this? Or would the overhead be too much for a task like this? I have limited knowledge on Elasticsearch, I would appreciate any feedback.",
    "website_area": "discuss"
  },
  {
    "id": "e609992f-070a-483a-959e-21034522758e",
    "url": "https://discuss.elastic.co/t/querying-for-document-a-then-document-b-within-x-time/215462",
    "title": "Querying for document a then document b within x time",
    "category": [
      "Elasticsearch"
    ],
    "author": "Luke_Nicholson",
    "date": "January 17, 2020, 1:07pm",
    "body": "Hi, Is there a way to query for documents that do X and then Y within say, 1 hour? For example, say i worked for amazon, id like to figure out who viewed an item for sale (document X), and then purchased the item within one hour (document Y). Thanks in advanced.",
    "website_area": "discuss"
  },
  {
    "id": "a167a91f-d1b4-499c-a485-a99599e46c02",
    "url": "https://discuss.elastic.co/t/setting-up-environment-variables-in-windows10/215456",
    "title": "Setting up environment variables in Windows10",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 17, 2020, 12:33pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9b7f3a7d-7eaa-4481-aa36-1ee98f118d98",
    "url": "https://discuss.elastic.co/t/retrieve-data-from-elasticsearch-5-6-0-using-java-api/214729",
    "title": "Retrieve data from elasticsearch 5.6.0 using java API",
    "category": [
      "Elasticsearch"
    ],
    "author": "Venkatesh_Ponnusami",
    "date": "January 12, 2020, 12:44pm January 13, 2020, 1:39pm January 17, 2020, 12:17pm",
    "body": "Hi, I am trying to write a java program for storing a string in elasticsearch and then retrieve from it My code is GetResponse response = client.prepareGet(\"correct\", \"Doe\",\"11\").setStoredFields(\"message\") .execute() .actionGet(); String message = (String)response.getField(\"message\").getValue(); For that ,I am always getting null value But if I try \"response.exists()\", it gives me \"true\".when I see the index in elasticsearch head chrome extension it exists and also has the data in message field Note: I am using elasticsearch 5.6.0 with java API Can anyone please help me to get the field from the index",
    "website_area": "discuss"
  },
  {
    "id": "63a6a90b-6309-450f-9fb9-677750235aa7",
    "url": "https://discuss.elastic.co/t/elasticsearch-integtest-v6-8-2andv7-2-1-buildfailure-error/215448",
    "title": "Elasticsearch_integTest_v6.8.2andv7.2.1_BuildFailure/Error",
    "category": [
      "Elasticsearch"
    ],
    "author": "soniya20",
    "date": "January 17, 2020, 11:29am",
    "body": "Hi, I'm facing issue while performing testing of elasticsearch versions 6.8.2 and 7.2.1. I've performed the testing on both version of elasticsearch and getting same error. Following steps have been performed: For elasticsearch v6.8.2: Gradle Version : 5.2.1 OS Info : Linux 3.10.0-957.10.1.el7.x86_64 (amd64) JDK Version : Oracle Corporation 12.0 [Java HotSpot(TM) 64-Bit Server VM 11.0.1+13-LTS] JAVA_HOME : /usr/lib/jvm/jdk-12/ Run ./gradlew integtest Error encountered: For elasticsearch v7.2.1: Gradle Version : 5.2.1 OS Info : Linux 3.10.0-957.10.1.el7.x86_64 (amd64) JDK Version : Oracle Corporation 12.0 [Java HotSpot(TM) 64-Bit Server VM 11.0.1+13-LTS] JAVA_HOME : /usr/lib/jvm/jdk-12/ Run ./gradlew integtest Error encountered: Could anyone please help me to get out of this? Regards, Soniya",
    "website_area": "discuss"
  },
  {
    "id": "491b9684-043a-4669-88a2-dcbd18489933",
    "url": "https://discuss.elastic.co/t/request-of-field-value-samefield-anothervalue-combinations-using-bool-must-or-query-string-ask-for-help/215395",
    "title": "Request of field + value & samefield + anothervalue combinations using bool must or query_string - ask for help",
    "category": [
      "Elasticsearch"
    ],
    "author": "Datakids",
    "date": "January 16, 2020, 11:18pm January 17, 2020, 6:08am January 17, 2020, 9:02am January 17, 2020, 11:18am",
    "body": "Hi, i just tried to build following request to count documents containing player active status but no match found: GET /players/_search { \"size\": 0, \"query\": { \"bool\": { \"must\" : [ {\"terms\" : { \"player\" : [\"576\",\"234\"]}}, {\"terms\": { \"active\": [\"y\", \"z\"]}} ] } } } Ends up in zero hits! Ok then I tried this to test wheter one of each serves a result: GET /players/_search { \"size\": 10, \"query\": { \"query_string\" : { \"fields\" : [\"player\", \"active\"], \"query\" : \"576 AND y\" } } } \"hits\" : { \"total\" : { \"value\" : 56, \"relation\" : \"eq\" }, then GET /players/_search { \"size\": 10, \"query\": { \"query_string\" : { \"fields\" : [\"player\", \"active\"], \"query\" : \"234 AND z\" } } } \"hits\" : { \"total\" : { \"value\" : 13, \"relation\" : \"eq\" }, But I need booth in combination to the other which should end in 2 hits means this 2 different players had an active status of y and z at the same time and I need to find them to build testcases. How can I ? Thanks and Brdgs",
    "website_area": "discuss"
  },
  {
    "id": "c31568b4-a59d-4f1b-a367-e7379de0f361",
    "url": "https://discuss.elastic.co/t/path-repo/215444",
    "title": "Path.repo",
    "category": [
      "Elasticsearch"
    ],
    "author": "Andrea_Barocco",
    "date": "January 17, 2020, 10:57am",
    "body": "When i uncomment the path.repo in the elasticsearch.yml, elk doesn't work",
    "website_area": "discuss"
  },
  {
    "id": "9150abe7-fed4-4e51-82c8-7b40e5968d09",
    "url": "https://discuss.elastic.co/t/master-not-discovered-yet-this-node-has-not-previously-joined-a-bootstrapped-v7-cluster-and-this-node-must-discover-master-eligible-nodes-elasticsearch/214185",
    "title": "Master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and this node must discover master-eligible nodes elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 8, 2020, 8:32am January 17, 2020, 10:33am January 17, 2020, 10:33am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7503ed52-69b3-4698-9488-38a94c9bce68",
    "url": "https://discuss.elastic.co/t/shards-needed-in-parent-child-indexing/214867",
    "title": "Shards needed in parent-child indexing",
    "category": [
      "Elasticsearch"
    ],
    "author": "johnkary",
    "date": "January 13, 2020, 4:27pm January 13, 2020, 4:49pm January 14, 2020, 9:26am January 14, 2020, 11:18am January 14, 2020, 12:24pm January 14, 2020, 12:39pm January 14, 2020, 1:26pm January 14, 2020, 2:18pm January 14, 2020, 3:22pm January 15, 2020, 6:03am January 15, 2020, 2:34pm January 15, 2020, 2:52pm January 16, 2020, 10:47am January 17, 2020, 9:56am January 17, 2020, 10:19am",
    "body": "Hi all, I use Elasticsearch 6.6 and i have a question about the number of shards that i need. I plan to index csv log files which have thousands of rows each. These csvs have also a header (first 8 lines with '#' in front of), see below image. So, i thought to use the parent-child approach as below Create the index: PUT /npk_log { \"settings\" : { \"index\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 2 } } } Create the mapping: PUT npk_log/_mapping/doc { \"dynamic\": true, \"properties\": { \"join_field\" : { \"type\" : \"join\", \"relations\": { \"header\": \"body\" } }, \"latitude\": { \"type\": \"float\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"longitude\": { \"type\": \"float\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } My question is: How can i handle the shards that both parent-children may store in? Is it right the way i created my index? I use the _routing field as below. Is it right in this way? I read here, but i am confused: https://www.elastic.co/guide/en/elasticsearch/guide/current/indexing-parent-child.html index a parent PUT /npk/doc/1 { \"SystemId\": \"00:04:4b:df:35:96\", \"Mode\": \"NORMAL\", \"Operation\": \"Manual Fertilizer Application\", \"join_field\": \"header\" } index 3 children POST /npk/_bulk { \"_type\" : \"doc\", \"_id\" : \"1\", \"_routing\": 1, \"latitude\": 39.12366, \"langitude\": 23.4566, \"join_field\": {\"name\": \"body\",\"parent\": \"1\"}} { \"_type\" : \"doc\", \"_id\" : \"2\", \"_routing\": 1, \"latitude\": 36.29867, \"langitude\": 23.213, \"join_field\": {\"name\": \"body\",\"parent\": \"1\"}} { \"_type\" : \"doc\", \"_id\" : \"3\", \"_routing\": 1, \"latitude\": 39.298671,\"langitude\": 23.223, \"join_field\": {\"name\": \"body\",\"parent\": \"1\"}}",
    "website_area": "discuss"
  },
  {
    "id": "7e2438b6-4d31-44f9-9582-db3205f2fa88",
    "url": "https://discuss.elastic.co/t/how-to-add-refresh-option-in-python-elasticsearch-parallel-bulk-helper/215433",
    "title": "How to add refresh option in python elasticsearch parallel_bulk helper",
    "category": [
      "Elasticsearch"
    ],
    "author": "akhilmathew001",
    "date": "January 17, 2020, 9:25am",
    "body": "I am using python's elasticsearch client for doing elasticsearch 7 related activities. For bulk indexing documents to ES elasticsearch's parallel_bulk helper function is being used. Following is the sample code... helpers.parallel_bulk(self.esclient, gen_es_data, thread_count = 4, chunk_size = 1000, max_chunk_bytes = 104857600, queue_size = 4, raise_on_exception = True) In this way how can add refresh option, so that I can trigger a refresh and the documents that I indexed will be readily available",
    "website_area": "discuss"
  },
  {
    "id": "cf1b218b-3462-4acd-b99f-969b710d7bfa",
    "url": "https://discuss.elastic.co/t/elasticsearch-community-edition-and-enterprise-edition-documentation/215428",
    "title": "Elasticsearch community edition and enterprise edition documentation",
    "category": [
      "Elasticsearch"
    ],
    "author": "sreenivasulu",
    "date": "January 17, 2020, 8:50am January 17, 2020, 9:02am",
    "body": "Hi Team, Could you please provide me URLs are any documentation for \"Elasticsearch community edition and enterprise edition documentation\" to understand complete environment. Thanks Srinivas",
    "website_area": "discuss"
  },
  {
    "id": "9dbe72d7-528a-4d76-86f4-d34a7ae21fa1",
    "url": "https://discuss.elastic.co/t/new-index-is-not-created/215091",
    "title": "New index is not created",
    "category": [
      "Elasticsearch"
    ],
    "author": "Wojciech_Gomola",
    "date": "January 15, 2020, 8:22am January 15, 2020, 10:12am January 15, 2020, 12:29pm January 15, 2020, 12:33pm January 16, 2020, 2:12pm January 16, 2020, 3:39pm January 17, 2020, 8:56am",
    "body": "Hi. I have Filebeat + ELK stack. My Logstash has a pretty standard configuration: input { beats { port => 5044 } } filter { grok { match => { \"message\" => \"^(?<log_date>\\d{4}-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d.\\d\\d\\d)\\d?\\|(?<level>WARN|INFO|DEBUG|ERROR|FATAL)\\|%{GREEDYDATA:message}\" } } date { match => [\"log_date\", \"yyyy-MM-dd HH:mm:ss.SSS\"] timezone => \"+01:00\" remove_field => [\"log_date\"] } } output { elasticsearch { hosts => [\"elastic:9201\", \"elastic:9202\"] } } Yes, I have 2 instances of ES on a single machine, but they use different directories. For some reason indices are not created daily I have 2 indices. Both created the 28th of December. New docs are added to the second one. Does someone know what I did wrong?",
    "website_area": "discuss"
  },
  {
    "id": "7855b3e3-3bb6-45c9-99d8-63114b407814",
    "url": "https://discuss.elastic.co/t/changing-kibana-elasticsearch-end-point-url/215427",
    "title": "Changing Kibana/Elasticsearch end point url",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mohamad-Sobhie",
    "date": "January 17, 2020, 8:43am January 17, 2020, 8:55am",
    "body": "my cluster is deployed on the Elastic cloud, i want to change my end point URLs, where do i do that?",
    "website_area": "discuss"
  },
  {
    "id": "36cff070-8170-4cd0-a8c4-f76c389c7000",
    "url": "https://discuss.elastic.co/t/shard-allocation-awareness-stopped-working/215330",
    "title": "Shard Allocation awareness stopped working",
    "category": [
      "Elasticsearch"
    ],
    "author": "74db36a597f21b891b3f",
    "date": "January 17, 2020, 8:39am January 16, 2020, 2:45pm January 17, 2020, 8:39am",
    "body": "Hello. I configured forced shard allocation awareness on my cluster as described here some time earlier. And it was working well. But now I find out that some indices with one shard and one replica allocated on the nodes with same label. Nodes has approximately equal free space below 80% watermark. So I can't find another explanation of such cluster allocation behaviour. Need help please.",
    "website_area": "discuss"
  },
  {
    "id": "372e0b63-78da-4e8a-87f5-e92092559591",
    "url": "https://discuss.elastic.co/t/ingest-pipeline-data-processor-adds-hours/215154",
    "title": "Ingest Pipeline data processor adds hours",
    "category": [
      "Elasticsearch"
    ],
    "author": "fabian",
    "date": "January 15, 2020, 2:32pm January 15, 2020, 8:46pm January 17, 2020, 7:05am",
    "body": "Hey there, I have a problem with the data ingest processor for one logfile. It adds the timezone hours to my @timestamp. In my case +1h. I use the pipelines with data processor a lot, so I cannot figure out why this is happening with just this logfile. I use filebeat to collect my logs then ship it to logstash and from there to elastic. I try to ingest the following logline: I2020-01-15T14:28:09,453 INFO [main] de.my.super.class: Started server As you can see there is no timezone information. I add the timezone information with filebeat processors and use it in the pipeline. Here you see the filebeat log event from debug log filebeat event { \"@timestamp\": \"2020-01-15T13:28:17.945Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"_doc\", \"version\": \"7.4.2\" }, \"input\": { \"type\": \"log\" }, \"host\": { \"name\": \"mysuperserver.domain.local\" }, \"agent\": { \"ephemeral_id\": \"93ab7bfd-7433-4873-8890-4b17c3b8221f\", \"hostname\": \"mysuperserver.domain.local\", \"id\": \"91e386db-0acf-43c1-a618-1338b922be63\", \"version\": \"7.4.2\", \"type\": \"filebeat\" }, \"message\": \"I2020-01-15T14:28:09,452 INFO [main] de.my.super.class: Started server\", \"fields\": { \"log_type\": \"log\", \"beats_output\": \"application\" }, \"ecs\": { \"version\": \"1.1.0\" }, \"event\": { \"timezone\": \"+01:00\" }, \"log\": { \"offset\": 5696, \"file\": { \"path\": \"/opt/application/logs/application.log\" } } } In this example the @timestamp field is \"2020-01-15T15:28:09.452+01:00\". But it have to be \"2020-01-15T14:28:09.452+01:00\" The event.created timestamp shows UTC with timezone +0100. So the timestamp from filebeat seems to be sent correctly. If I test the pipeline with _simulate it is the exact same problem. What can I do to get the correct timestamp? My Configuration: Filebeat Config filebeat.config.inputs: enabled: true path: /etc/filebeat/conf.d/*.yml filebeat.config.modules: enabled: true path: /etc/filebeat/modules.d/*.yml setup.kibana.host: \"http://kibana.local\" processors: - add_locale: ~ output.logstash: hosts: [\"logstash.local:5044\"] ssl.certificate_authorities: [\"/etc/ssl/logstash.crt\"] loadbalance: true name: mysuperserver.domain.local Filebeat ConfigII - type: log paths: - /opt/application/logs/application.log fields: log_type: log beats_output: application multiline: pattern: '^I[[:digit:]][[:digit:]][[:digit:]][[:digit:]]-[[:digit:]][[:digit:]]-[[:digit:]][[:digit:]]' negate: true match: after Logstash Config output { elasticsearch { hosts => [\"http://elastic.local:9200\"] manage_template => false index => \"filebeat-7.4.2-super-index\" pipeline => \"application-superlog\" ilm_enabled => false } } My Pipeline { \"description\": \"Pipeline for parsing application log.\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [ \"(?m)I%{TIMESTAMP_ISO8601:application.timestamp} +%{LOGLEVEL:log.level} +\\\\[%{DATA:process.thread.name}\\\\] +%{DATA:log.logger}: %{GREEDYDATA:application.message}\" ], \"ignore_missing\": true } }, { \"remove\": { \"field\": \"message\" } }, { \"rename\": { \"field\": \"@timestamp\", \"target_field\": \"event.created\" } }, { \"rename\": { \"field\": \"application.message\", \"target_field\": \"message\" } }, { \"date\": { \"field\": \"application.timestamp\", \"target_field\": \"@timestamp\", \"formats\": [ \"ISO8601\" ], \"timezone\": \"{{event.timezone}}\", \"ignore_failure\": true } }, { \"remove\": { \"field\": \"application.timestamp\", \"ignore_failure\": true } } ], \"on_failure\": [ { \"set\": { \"field\": \"error.message\", \"value\": \"{{ _ingest.on_failure_message }}\" } } ] } The Elastic output: Elastic Document { \"_index\" : \"filebeat-7.4.2-super-index-000001\", \"_type\" : \"_doc\", \"_id\" : \"q4hjqW8B-cbAsJcK-aJp\", \"_score\" : 1.0, \"_source\" : { \"agent\" : { \"hostname\" : \"mysuperserver.domain.local\", \"id\" : \"91e386db-0acf-43c1-a618-1338b922be63\", \"type\" : \"filebeat\", \"ephemeral_id\" : \"93ab7bfd-7433-4873-8890-4b17c3b8221f\", \"version\" : \"7.4.2\" }, \"process\" : { \"thread\" : { \"name\" : \"main\" } }, \"log\" : { \"file\" : { \"path\" : \"/opt/application/logs/application.log\" }, \"offset\" : 4334, \"level\" : \"INFO\", \"logger\" : \"de.my.super.class\" }, \"microapps\" : { }, \"message\" : \"Started server\", \"tags\" : [ \"beats_input_codec_plain_applied\" ], \"input\" : { \"type\" : \"log\" }, \"environment\" : \"cloud\", \"@timestamp\" : \"2020-01-15T15:28:09.452+01:00\", \"stage\" : \"dev\", \"ecs\" : { \"version\" : \"1.1.0\" }, \"@version\" : \"1\", \"host\" : { \"name\" : \"mysuperserver.domain.local\" }, \"event\" : { \"timezone\" : \"+01:00\", \"created\" : \"2020-01-15T13:28:17.945Z\" }, \"fields\" : { \"log_type\" : \"log\", \"beats_output\" : \"application\" } } } //Edit: I use 7.4.2 for filebeat, logstash and elasticsearch.",
    "website_area": "discuss"
  },
  {
    "id": "ed24834e-c3f7-4e05-9e2d-967eb4bbbef9",
    "url": "https://discuss.elastic.co/t/python-curator-possible-to-filter-indexlist-on-the-first-call/215403",
    "title": "Python Curator - possible to filter IndexList on the first call",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 17, 2020, 3:16am January 17, 2020, 4:28am January 17, 2020, 6:02am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0e2ca7f1-1261-49ab-9e01-37ef18fd484d",
    "url": "https://discuss.elastic.co/t/confidence-scores-change-once-on-updating-the-object-then-change-again-on-another-search-query-with-no-conditions-changed/215236",
    "title": "Confidence Scores change once on updating the object, then change again on another search query with no conditions changed",
    "category": [
      "Elasticsearch"
    ],
    "author": "Pushkal_Bhatia",
    "date": "January 17, 2020, 3:36am",
    "body": "Hello, We are using Elasticsearch for our business solution. The problem we are facing is that, once an object of the index is updated, the search gives some score (Pic 2), and on searching again - keeping every single thing the same, scores change. From thereon, i.e after that one change, the scores remain static (We're using 2 shards). This has been causing us major troubles. Any idea why does that happen?! Pic 1 - Original Scores (The Upvote button updates the Index Object) image2746396 20.3 KB Pic 2 - Scores Generated image2756408 19.2 KB Pic 3 - On second search, the scores change - after this search the scores remain static. image2738406 18.2 KB What can be the solution to this problem? If this is because round robin shard process, then why does it happen only once? Even if I use preference with search, how to make sure that updates are done on the same shards from which are being accessed every time with the help of preference string? Here's the mapping for your reference: mapping = { \"mappings\": { \"properties\": { \"ArticleNumber\": { \"type\": \"text\" }, \"timestamp\": { \"type\": \"date\" }, \"introduction\": { \"type\": \"text\" }, \"file_name\": { \"type\": \"text\" }, \"RequirementsAndPreRequisites\": { \"type\": \"text\" }, \"SampleOutput\": { \"type\": \"text\" }, \"ScriptLogging\": { \"type\": \"text\" }, \"title\": { \"type\": \"text\" }, \"HowToRun\": { \"type\": \"text\" }, \"Title\": { \"type\": \"text\" }, \"UrlName\": { \"type\": \"text\" }, \"mop_url\": { \"type\": \"text\" }, \"Id\": { \"type\": \"text\" }, \"votes\": { \"type\": \"text\" }, \"feedback\": { \"type\": \"nested\", \"properties\": { \"problem_description\": { \"type\": \"text\" }, \"upvote_users\": { ---> This is Upvoted and changed \"type\": \"text\", \"boost\": 2 }, \"downvote_users\": { \"type\": \"text\" } } } } }, \"settings\" : { \"number_of_shards\": 2, \"number_of_replicas\": 2 } } Any help is greatly appreciated. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "53a9a380-4519-46f7-adc7-35f6f48723dc",
    "url": "https://discuss.elastic.co/t/issue-with-index-rollover-via-ilm/215398",
    "title": "Issue with index rollover via ILM",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 17, 2020, 12:49am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "29032e8c-30b2-4979-8408-d37a2be88676",
    "url": "https://discuss.elastic.co/t/elasticsearch-public-access-failure-with-self-signed-certificate-xpack-security-is-it-really-needed/215392",
    "title": "Elasticsearch public access failure with self-signed certificate. xpack.security is it really needed",
    "category": [
      "Elasticsearch"
    ],
    "author": "O_K",
    "date": "January 16, 2020, 10:10pm",
    "body": "Hi Everyone, Based on my previous post Elasticsearch with xpack.security.enabled throws Cluster is not yet ready I configured xpack.security and selfsigned certificate for elasticsearch. I wonder whether xpack.security is really needed as previously I was getting error after elasticsearch kubernetes pods restart: ERROR: [1] bootstrap checks failed [1]: Transport SSL must be enabled if security is enabled on a [basic] license. Please set [xpack.security.transport.ssl.enabled] to [true] or disable security by setting [xpack.security.enabled] to [false] SO, currently xpack security is enabled and I also configured certificates, but when I try to access elastic cluster from outside I'm getting below error. Everything works fine when I access elasticsearch inside the cluster curl -vk -u elastic:pass -GET \"https://elasticsearch-master.logging.svc.cluster.local:9200/_cat/indices\" --cacert ca.pem --cert elasticsearch-master.crt --key elasticsearch-master.key But access outside of cluster throws me error: {\"type\": \"server\", \"timestamp\": \"2020-01-16T21:07:29,024+0000\", \"level\": \"WARN\", \"component\": \"o.e.h.AbstractHttpServerTransport\", \"cluster.name\": \"elasticsearch\", \"node.name\": \"elasticsearch-master-0\", \"cluster.uuid\": \"s0W6l0pwRxaRRg0dxo2XXX\", \"node.id\": \"vExaDAzAT3yclCSAL47XXX\", \"message\": \"caught exception while handling client http traffic, closing connection Netty4HttpChannel{localAddress=0.0.0.0/0.0.0.0:9200, remoteAddress=/172.30.182.111:38686}\" , \"stacktrace\": [\"io.netty.handler.codec.DecoderException: io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record: 47455420\", \"at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:472) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1408) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:682) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:582) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:536) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:906) [netty-common-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.35.Final.jar:4.1.35.Final]\", \"at java.lang.Thread.run(Thread.java:835) [?:?]\", \"Caused by: io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record: 474554202f66617669636f6e2e69636f204850d0a\", \"at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1206) ~[netty-handler-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1274) ~[netty-handler-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final]\", \"at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:441) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final]\", \"... 16 more\"] } Please let me know how to deal with this issue. Should I fix certificates access or probably there is an option to disable xpack security? But will elasticsearch be stable after restarts (see error at the top) Thank you, OK",
    "website_area": "discuss"
  },
  {
    "id": "2e6b0316-4da6-49f0-b6eb-52b3cf10587f",
    "url": "https://discuss.elastic.co/t/is-there-any-way-to-enable-versioning-or-concurrency-control-of-documents-based-on-a-particular-field-timestamp-field/215388",
    "title": "Is there any way to enable versioning or concurrency control of documents based on a particular field? (Timestamp field)",
    "category": [
      "Elasticsearch"
    ],
    "author": "perahimi",
    "date": "January 16, 2020, 8:33pm",
    "body": "Hello, I am currently working with catalog data that is coming in hourly in our pipeline. This is an example of how the written source document might look like. { \"name\": \"Hoodie\", \"sku\": \"123\", \"timeStamp\": \"2020-01-16T17:39:23Z\", \"price\": 20 } Lets say there is an issue and old data happens to come in later than the new data for whatever reason. Is there any way to tell ElasticSearch to look at the timeStamp field first before writing the document? For example, lets say the document above was written, and a few mins later this document comes in... { \"name\": \"Hoodie\", \"sku\": \"123\", \"timeStamp\": \"2019-12-31T17:39:23Z\", \"price\": 15 } I obviously wouldn't want this to overwrite the more recent one. Is there a way to let ES know to version the documents by a particular field? I looked into using a script, but i can't figure out a way to have access to both the _source context and the incoming document context.",
    "website_area": "discuss"
  },
  {
    "id": "e76261c7-1a65-4aac-9439-b6baa098340a",
    "url": "https://discuss.elastic.co/t/ingesting-appdynamics-metric-information-into-elasticsearch/215381",
    "title": "Ingesting AppDynamics metric information into Elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "byoungman",
    "date": "January 16, 2020, 7:46pm",
    "body": "We are using AppDynamics APM for our system monitoring and alerting and I need to get that information into Elasticsearch before AppDynamics starts rolling up it's data and we start losing information granularity. The issue that I am currently running into is that our AppDynamics Controller API is configured to use OAUTH 2.0 and my research into using the http_poller plugin for Logstash is telling me that to get that to work is that I will have to write something such as a PowerShell script that automates generating the OAUTH key and then inject that into my logstash http_poller configuration. I can do this but is there any other approach that is more efficient? TIA, Bill Youngman",
    "website_area": "discuss"
  },
  {
    "id": "22d378f9-d104-4a16-a514-4981ce8ba745",
    "url": "https://discuss.elastic.co/t/new-indices-remain-yellow-when-there-are-relocations-in-the-cluster/215345",
    "title": "New indices remain yellow when there are relocations in the cluster",
    "category": [
      "Elasticsearch"
    ],
    "author": "itiyamas",
    "date": "January 16, 2020, 3:30pm January 16, 2020, 6:12pm January 16, 2020, 6:49pm January 16, 2020, 6:54pm",
    "body": "Hi, When we reduce the number of nodes in our cluster to half, a lot of recoveries start in the cluster as a result of excluding the old ips. We are running a 120 node cluster with a total of 4M docs/second. There are 60 primaries and 60 replicas in the latest index. During recovery, if rollover happens- new indices start as yellow and remain yellow for a long time until rolled-over. The new rolled over index comes up as yellow again till recoveries complete. I also noticed that the shards remain unassigned for 5 minutes. Allocation explain API shows that recoveries limit on the node is breached. I was wondering if we can have a different throttling limit for newly created indices as the primaries are empty and replicas can be started quite fast with minimal network bandwidth as opposed to other shards that are being moved. The primaries come up quite fast as they rely on a different throttling limit- node_initial_primaries_recoveries. This is impacting the availability of our new indices and any node going down will make the cluster red during scale-down. We even evaluated waiting for index to become green before alias switch, but it takes close to a few minutes before the existing recoveries finish and replicas are assigned and will also depend on the network bandwidth and existing concurrent recoveries. It makes client side handling of the rollover difficult. How do you suggest solving this problem?",
    "website_area": "discuss"
  },
  {
    "id": "acf7fa23-52c2-410c-b5f5-3bbf8e916e63",
    "url": "https://discuss.elastic.co/t/blocked-by-service-unavailable-1-state-not-recovered-initialized/215372",
    "title": "Blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];\"}]",
    "category": [
      "Elasticsearch"
    ],
    "author": "symsymmas",
    "date": "January 16, 2020, 6:19pm",
    "body": "I have a cluster with 4 nodes. When the 3 slaves are stoped and I start the master, Ive got this error. Could you help me please. {\"error\":{\"root_cause\":[{\"type\":\"cluster_block_exception\",\"reason\":\"blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];\"}],\"type\":\"cluster_block_exception\",\"reason\":\"blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];\"},\"status\":503}",
    "website_area": "discuss"
  },
  {
    "id": "f2d685a9-99c9-4b75-beaa-32a88f10669d",
    "url": "https://discuss.elastic.co/t/synonyms-not-getting-applied/215366",
    "title": "Synonyms not getting applied",
    "category": [
      "Elasticsearch"
    ],
    "author": "adityaPsl",
    "date": "January 16, 2020, 5:42pm January 16, 2020, 6:03pm",
    "body": "Hello Team, I have below document in the index { \"name\" :\"test\", \"products\": [ { \"pname\" : \"p1\", \"qty\" : \"10 pcs\" }, { \"pname\" : \" p3 \", \"qty\" : \"10 PCS\" } ] } and i have below index settings PUT /tst { \"settings\": { \"index\" : { \"analysis\" : { \"analyzer\" : { \"synonym\" : { \"tokenizer\" : \"standard\", \"filter\" : [\"synonym\"] } }, \"filter\" : { \"synonym\" : { \"type\" : \"synonym\", \"lenient\": true, \"synonyms\" : [\"pcs,PCS => piece\"] } } } } } } But when i try to execute search it re turns 0 results POST tst/_search { \"query\": { \"query_string\": { \"default_field\": \"products.qty\", \"query\": \"piece\" } } } But below query returns 1 result (as expected) POST tst/_search { \"query\": { \"query_string\": { \"default_field\": \"products.qty\", \"query\": \"pcs\" } } } could you please help to understand how synonyms work and if anything i am missing for configuring the synonyms. Thank you for your help. Thank you, Aditya",
    "website_area": "discuss"
  },
  {
    "id": "e38ee023-c7c1-4c61-a0f2-69f6b0ab1341",
    "url": "https://discuss.elastic.co/t/cluster-outage-with-old-gen-size-increasing/215229",
    "title": "Cluster outage with old-gen size increasing",
    "category": [
      "Elasticsearch"
    ],
    "author": "musfiqur",
    "date": "January 16, 2020, 6:02pm",
    "body": "Hi, We are seeing logs like this in our ElasticSearch 6.7: [gc][young][678215][290288] duration [718ms], collections [1]/[1s], total [718ms]/[4.5h], memory [9.6gb]->[10.3gb]/[30.1gb], all_pools {[young] [108.4mb]->[332.8mb]/[998.5mb]}{[survivor] [124.7mb]->[124.7mb]/[124.7mb]}{[old] [9.3gb]->[9.8gb]/[29gb]} The timing coincides with a cluster outage. What does the above log mean/indicate? Why the garbage collection is taking long? -Musfiqur",
    "website_area": "discuss"
  },
  {
    "id": "be46fae7-8fb2-4e58-9223-7c4a58001cbb",
    "url": "https://discuss.elastic.co/t/why-nfs-is-to-be-avoided-for-data-directories/215240",
    "title": "Why NFS is to be avoided for data directories",
    "category": [
      "Elasticsearch"
    ],
    "author": "malshan",
    "date": "January 16, 2020, 5:30am January 16, 2020, 5:42am January 16, 2020, 6:37am January 16, 2020, 6:25am January 16, 2020, 6:39am January 16, 2020, 7:28am January 16, 2020, 10:42am January 16, 2020, 5:30pm",
    "body": "ES: 7.5 I have seen the recommendations in official documentation to 'avoid NFS' when using them as data directories for elasticsearch. I have also seen almost everywhere in the forums to 'avoid NFS' but couldn't find a proper explanation. Is it due to the fact that NFS is exported as a file system, and not a block device? Since there would be higher number of concurrent writes and reads, NFS would have protocol level bottlenecks. Is it due to network latency related issues of any network related storage (against local storage). Is it only the (1) reason, if that so, can we use a network bloack storage like ceph or SAN. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "9b8d22f9-d5f8-4971-90e4-b036c77571ed",
    "url": "https://discuss.elastic.co/t/sort-by-nested-values-across-all-documents-duplicate-parent-documents/213257",
    "title": "Sort by nested values \"across\" all documents (duplicate parent documents)",
    "category": [
      "Elasticsearch"
    ],
    "author": "awagner",
    "date": "December 28, 2019, 3:28pm January 3, 2020, 9:29pm January 16, 2020, 4:06pm",
    "body": "Hi all, First question, please forgive me if I'm ignorant of some essentials. Here is the issue: Among other options, I want to offer an ordering of query results based on the values of nested fields. That is, if a document has the first and the third value of all possible values, it should be listed in the first place, together with the corresponding nested field value, and then again in the third place, with the other nested field value. An examplary list is at the end of this post. I have some documents like these: PUT /test: { \"mappings\": { \"doc\": { \"dynamic\": false, \"properties\": { \"elternid\": { \"type\": \"keyword\" }, \"datum\": { \"type\" : \"keyword\" }, \"form\": { \"type\": \"text\", \"fields\": { \"raw\": { \"type\": \"keyword\" } } }, \"titel\": { \"type\": \"text\" }, \"betreff\": { \"type\": \"nested\", \"dynamic\": false, \"properties\": { \"grp_id\": { \"type\" : \"keyword\" }, \"grp_string\" : { \"type\" : \"keyword\" }, \"gruppe\": { \"type\" : \"text\" } } } } } } } And documents: PUT /test/_doc/1 { \"elternid\": \"Dokument 1\", \"datum\": \"1950-12-30\", \"form\": \"irgendwas langes\", \"titel\": \"noch ein lngerer Text\", \"betreff\": [ { \"grp_string\" : \"a.1\", \"grp_id\": \"blabla\", \"gruppe\": \"schon wieder langer Text\" }, { \"grp_string\" : \"b.2\", \"grp_id\": \"blabla\", \"gruppe\": \"schon wieder langer Text\" }, { \"grp_string\" : \"c.1\", \"grp_id\": \"bloblo\", \"gruppe\": \"Kurztext\" } ] } PUT /test/_doc/2: { \"elternid\": \"Dokument 2\", \"datum\": \"1970-01-01\", \"form\": \"irgendwas langes\", \"titel\": \"noch ein lngerer Text\", \"betreff\": [ { \"grp_string\" : \"a.1\", \"grp_id\": \"blabla\", \"gruppe\": \"schon wieder langer Text\" }, { \"grp_string\" : \"a.3\", \"grp_id\": \"foo\", \"gruppe\": \"schon wieder langer Text\" } ] } PUT /test/_doc/3: { \"elternid\": \"Dokument 3\", \"datum\": \"1940-01-01\", \"form\": \"irgendwas langes\", \"titel\": \"noch ein lngerer Text\", \"betreff\": [ { \"grp_string\" : \"a.2\", \"grp_id\": \"bar\", \"gruppe\": \"schon wieder langer Text\" }, { \"grp_string\" : \"b.1\", \"grp_id\": \"baz\", \"gruppe\": \"nul\" } ] } For some reason, I was under the impression that I would have to do it with an aggregation and extract the data/ignore the hits client-side. This is what I have: POST /test/_search: { \"from\": 0, \"size\": 10, \"query\": { \"bool\": { \"filter\": [ { \"match_all\": {} } ] } }, \"aggs\": { \"my_buckets\": { \"composite\" : { \"sources\" : [ { \"betreff\": { \"terms\": { \"script\": \"params._source.betreff.grp_string\" } } }, { \"datum\": { \"terms\": {\"field\": \"datum\" } } }, { \"form\": { \"terms\": {\"field\": \"form.raw\" } } }, { \"ter\": { \"terms\": {\"field\": \"elternid\" } } } ] } } } } But it gives an error (\"Illegal list shortcut value [grp_string].\"). In the end, I want to have a sorted list like this: 1. \"a.1\" \"1950-12-30\" \"Dokument 1\" 2. \"a.1\" \"1970-01-01\" \"Dokument 2\" 3. \"a.2\" \"1940-01-01\" \"Dokument 3\" 4. \"a.3\" \"1970-01-01\" \"Dokument 2\" 5. \"b.1\" \"1940-01-01\" \"Dokument 3\" 6. \"b.2\" \"1950-12-30\" \"Dokument 1\" 7. \"c.1\" \"1950-12-30\" \"Dokument 1\" How should I be approaching this? Thanks for any insight, Andreas PS. I'm on ES 6.8.6.",
    "website_area": "discuss"
  },
  {
    "id": "997e094a-14de-4956-8e1f-861a1645fe48",
    "url": "https://discuss.elastic.co/t/forbidden-12-index-read-only/215301",
    "title": "FORBIDDEN/12/index read-only",
    "category": [
      "Elasticsearch"
    ],
    "author": "johnkary",
    "date": "January 16, 2020, 11:59am January 16, 2020, 12:23pm January 16, 2020, 12:53pm January 16, 2020, 12:55pm January 16, 2020, 1:02pm January 16, 2020, 3:57pm January 16, 2020, 3:51pm",
    "body": "Hi team! I often encounter the following error when i insert documents to my index. elasticsearch.exceptions.AuthorizationException: AuthorizationException(403, 'cluster_block_exception', 'blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];') As a temporary workaround i use one of the following, i index successfully some docs, but again after a while i see the same error. PUT _settings {\"index\": {\"blocks\": {\"read_only_allow_delete\": \"false\"}}} PUT /_settings { \"index.blocks.read_only_allow_delete\": null} How can i permanently fix it? Searching on the web didn't found a permanent solution to this issue, apart from the workarounds i mention above. I have enough disk space in the Elasticsearch instance and this error occurs even when my index is empty! What is wrong here? Cluster info: { \"name\" : \"rLNxkrB\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"7GjE59rERPeuOyk1PAzDyQ\", \"version\" : { \"number\" : \"6.6.1\", \"build_flavor\" : \"default\", \"build_type\" : \"zip\", \"build_hash\" : \"1fd8f69\", \"build_date\" : \"2019-02-13T17:10:04.160291Z\", \"build_snapshot\" : false, \"lucene_version\" : \"7.6.0\", \"minimum_wire_compatibility_version\" : \"5.6.0\", \"minimum_index_compatibility_version\" : \"5.0.0\" }, \"tagline\" : \"You Know, for Search\" } ...and my index settings: { \"settings\" : { \"index\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 2 } } ... and disc usage info (my indices that i created are the yellow ones): health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open fertilization O_yvMFedS0SA3qsOd12GwA 3 2 0 0 783b 783b yellow open monitoring mausy_dxTTW1JiB88g7lWA 3 2 0 0 783b 783b green open .reporting-2019.12.01 CMSHgEkDQ_S7kuz4fRI2Dg 1 0 1 0 10.8kb 10.8kb yellow open augmenta_snapshots hqU4hp14SCmR6BtkJCHxyg 3 2 1 0 11.5kb 11.5kb green open .reporting-2019.11.17 T5Q8qUJTTXeECng9bHWczw 1 0 2 0 20.5kb 20.5kb green open .kibana viyg013TT0yanW3qmvTaNA 1 0 32 2 151kb 151kb green open .reporting-2019.12.08 HZ0Fsxd5RBqimiJ_5dwjOw 1 0 6 2 388kb 388kb green open .monitoring-kibana-6-2020.01.12 UqZsn-A9QJyfKUZfRkWfEw 1 0 3226 0 884.6kb 884.6kb green open .monitoring-kibana-6-2020.01.11 aSBTioOrR7e6ejZwruZfaw 1 0 3111 0 886.2kb 886.2kb green open .monitoring-kibana-6-2020.01.13 o0ixyC_nQt2TwsdT0v51CQ 1 0 3602 0 956.9kb 956.9kb green open .monitoring-kibana-6-2020.01.10 VkVVfVwmREGXZcQpKBeHFA 1 0 3408 0 978.6kb 978.6kb green open .monitoring-kibana-6-2020.01.16 F9KLSx4UQAuUTIzXybVG-A 1 0 3825 0 1mb 1mb green open .monitoring-kibana-6-2020.01.14 9t1nOBx_QlWZPpJjRqhgyg 1 0 6304 0 1.5mb 1.5mb green open .monitoring-kibana-6-2020.01.15 RHbxXJJhQE2_Z_3f4VyTxg 1 0 8639 0 2.1mb 2.1mb green open .monitoring-es-6-2020.01.12 GToNe2rpT0-p5wdFyWnzIg 1 0 147730 306 55.9mb 55.9mb green open .monitoring-es-6-2020.01.11 ikJa_VhZS9K419yIq4QIjA 1 0 142595 243 58.5mb 58.5mb green open .monitoring-es-6-2020.01.10 jWRUwxBRRgOjrP8ivNJugg 1 0 161332 672 60.6mb 60.6mb green open .monitoring-es-6-2020.01.16 bdLeDXy9SDmsNCIGfmoemw 1 0 139641 424 63.8mb 63.8mb green open .monitoring-es-6-2020.01.13 TqjMNZcET1CwQRIdLzQuww 1 0 166226 651 67mb 67mb green open .monitoring-es-6-2020.01.14 9_uLKbBuQ5-dvEnsRxoxIg 1 0 243080 812 101.7mb 101.7mb green open .monitoring-es-6-2020.01.15 IXCPlrtgRPCoKBic-yt60Q 1 0 293130 536 130.2mb 130.2mb yellow open npk S1exfiWiThmtc7iCwdD1_Q 3 2 86832 0 182.1mb 182.1mb Conciselly my disk space info: \"fs\": { \"timestamp\": 1579175865660, \"total\": { \"total_in_bytes\": 10222829568, \"free_in_bytes\": 569589760, \"available_in_bytes\": 552812544 Thank you in advance!",
    "website_area": "discuss"
  },
  {
    "id": "83026d08-9e05-4c8e-8a7f-9dce5f2c1116",
    "url": "https://discuss.elastic.co/t/suggester-doesnt-suggest-as-expected-based-on-beginning-of-word/214715",
    "title": "Suggester doesn't suggest as expected, based on beginning of word",
    "category": [
      "Elasticsearch"
    ],
    "author": "Casper_Thrane",
    "date": "January 12, 2020, 9:09pm January 13, 2020, 9:39am January 13, 2020, 10:11am January 13, 2020, 11:45am January 16, 2020, 3:06pm",
    "body": "I want ElasticSearch to suggest more words, based on the beginning of a word. My example is the following. I have these titles: affaldsregister affaldsdatasystem - Import af indberetninger anmod om ndring af affaldsordning ansgning om dispensation for affaldsregulativ I want the suggester to find, the words in bold, when I use the word affald or afald . I have tried the following: POST virk_search_pub_dan/_search { \"suggest\": { \"simple_terms\": { \"text\": \"affald\", \"term\": { \"max_edits\": 1, \"max_term_freq\": 1, \"max_inspections\": 5, \"field\": \"title\", \"size\": 5, \"suggest_mode\": \"always\", \"prefix_length\": 3 } } } } But I get no suggestions. My title field is analyzed with the standard tokenizer. What am I missing? Br Casper",
    "website_area": "discuss"
  },
  {
    "id": "5e7c989a-92c6-4d02-b825-95318674cb73",
    "url": "https://discuss.elastic.co/t/org-apache-lucene-store-alreadyclosedexception-underlying-file-changed-by-an-external-force/215311",
    "title": "org.apache.lucene.store.AlreadyClosedException: Underlying file changed by an external force",
    "category": [
      "Elasticsearch"
    ],
    "author": "rdl",
    "date": "January 16, 2020, 12:21pm January 16, 2020, 12:39pm January 16, 2020, 2:45pm January 16, 2020, 2:52pm",
    "body": "Environment: GlusterFS 6.0 OpenShift 3.11 ES 6 Hi, while deploying sonarqube which in turn uses ES6 in a POD I encounter the following error: 2020.01.16 12:16:19 WARN es[o.e.c.r.a.AllocationService] failing shard [failed shard, shard [projectmeasures][4], node[yj7F4GFeTLWJIBBGdlG6OA], [P], s[STARTED], a[id=PYKYQXW4S26dAErK2wBVCQ], message [shard failure, reason [lucene commit failed]], failure [AlreadyClosedException[Underlying file changed by an external force at 2020-01-16T09:50:02.846963Z, (lock=NativeFSLock(path=/opt/sonarqube/data/es6/nodes/0/indices/r1Iw474jQPWfDDwVoJoAwg/4/index/write.lock,impl=sun.nio.ch.FileLockImpl[0:9223372036854775807 exclusive valid],creationTime=2020-01-16T09:50:02.846628Z))]], markAsStale [true]] org.apache.lucene.store.AlreadyClosedException: Underlying file changed by an external force at 2020-01-16T09:50:02.846963Z, (lock=NativeFSLock(path=/opt/sonarqube/data/es6/nodes/0/indices/r1Iw474jQPWfDDwVoJoAwg/4/index/write.lock,impl=sun.nio.ch.FileLockImpl[0:9223372036854775807 exclusive valid],creationTime=2020-01-16T09:50:02.846628Z)) Do you have any suggestions? Thanks in advance Bests",
    "website_area": "discuss"
  },
  {
    "id": "cdc1ff32-bb85-4bd4-b6df-25b3883027a5",
    "url": "https://discuss.elastic.co/t/elasticsearch-cluster-architecture/215320",
    "title": "Elasticsearch cluster architecture",
    "category": [
      "Elasticsearch"
    ],
    "author": "Ged",
    "date": "January 16, 2020, 2:37pm",
    "body": "Hi guys, As per attached diagram I've 8 application servers in 2 data centers (4 per each DC) and 4 monitoring servers przezn for ELK Stack (2 per DC). 4 app servers (1-4) push their log data to Logstash placed on Monitoring server 1 (DC1) 4 app servers (5-6) push their log data to Logstash placed on Monitoring server 3 (DC2). Connectivity between Data Centers is not a problem. Additionally i want to have access to data from one DC when the second one is not available. ELK Stack Cert522522 4.65 KB Question is: What would be best solution to create Elasticsearch Cluster having 4 monitoring servers available ? I need to have data available for analysis in one Kibana. Could you please advise and share best practices for such a configuration ? Thanks a lot in advance ! Ged",
    "website_area": "discuss"
  },
  {
    "id": "9cb6f70b-b005-4a36-9056-3042a7341490",
    "url": "https://discuss.elastic.co/t/specify-timezone-when-trigger-watchers/215222",
    "title": "Specify timezone when trigger watchers",
    "category": [
      "Elasticsearch"
    ],
    "author": "robinmhj",
    "date": "January 15, 2020, 10:33pm January 15, 2020, 10:45pm January 16, 2020, 2:32pm",
    "body": "Hi there, I am using watcher to aggregate data, our elasticsearch instance is using utc time zone. Is it possible to specify the time based on timezone, the reason I am asking it because that there are day light saving and the time it trigger really matters. For example, I want to a watcher to run at 12PM US/EAST time, for now, it is 17 UTC, but when the day light saving is done, 17 UTC becomes 13 US/EAST, and I want the watcher to run at 12 US/EAST time all the time. Is there a feature to specify the timezone when schedule a watcher job without changing the timezone of the elasticsearch instance? If not, is there a plan to add such feature? Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "1ade114d-99e2-4372-b732-28c2b09571da",
    "url": "https://discuss.elastic.co/t/foreach-index-action-error/215107",
    "title": "Foreach index action error",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 15, 2020, 10:26am January 15, 2020, 11:18am January 15, 2020, 12:31pm January 15, 2020, 4:27pm January 15, 2020, 4:31pm January 15, 2020, 4:37pm January 15, 2020, 4:50pm January 15, 2020, 5:13pm January 16, 2020, 2:05pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "52e4dd1d-9dcc-452c-9632-63ae7793f55a",
    "url": "https://discuss.elastic.co/t/unable-to-comunicate-elastic-search-server-6-2-3-usingrest-high-level-client/215316",
    "title": "Unable to comunicate elastic search server 6.2.3 usingrest high level client",
    "category": [
      "Elasticsearch"
    ],
    "author": "chubb123",
    "date": "January 16, 2020, 1:19pm",
    "body": "Hi Team, We have elastic search server 6.2.3 it is installed in MDM 10.3 .we have certificates for elastic search server it is working fine in browser. when I try to call the rest high level client it is not working.always getting below errors MDM_ESKEYSTORE_FILE_JKS.keystore Error : Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target .P12 certificate also same issue. Please do the needfull",
    "website_area": "discuss"
  },
  {
    "id": "e83f65e4-6632-424a-82f0-ebd48d2c9b1d",
    "url": "https://discuss.elastic.co/t/cpu-utilization-always-high/215274",
    "title": "CPU utilization always high",
    "category": [
      "Elasticsearch"
    ],
    "author": "Ankit_Kumar",
    "date": "January 16, 2020, 9:13am January 16, 2020, 12:34pm",
    "body": "HI guys, I have around 20 index and 5 shards for each index, each shard takes bout 80-100 mb, data will grow exponentially in future, how many nodes should i have? Currently i have only 1 node and i am doing many search query and bulk updates parallely. Please suggest.",
    "website_area": "discuss"
  },
  {
    "id": "3fcce029-a2be-4379-98d1-fa98e737e3d5",
    "url": "https://discuss.elastic.co/t/does-esrally-cheating-big-difference-between-the-two-measurements/215070",
    "title": "Does esrally cheating?Big difference between the two measurements",
    "category": [
      "Elasticsearch",
      "Rally"
    ],
    "author": "Liujinan",
    "date": "January 15, 2020, 10:23am January 15, 2020, 8:27pm January 15, 2020, 2:57pm January 16, 2020, 1:15am January 16, 2020, 12:27pm",
    "body": "Problems: I run twice esrally testings(each testing only perform index-append), but the result are very different. Step: 1.create the index geonames manually 2.First test: esrally race --pipeline=benchmark-only --target-hosts=28.28.0.4:9200 --track=geonames The result: |Min Throughput | index-append | 144447 | docs/s | |Median Throughput | index-append | 209693 | docs/s | |Max Throughput | index-append | 225118 | docs/s | 3.delete the index geonames manually 4.create the index geonames manually 5.Second test: esrally race --pipeline=benchmark-only --target-hosts=28.28.0.4:9200 --track=geonames The result: |Min Throughput | index-append | 228746 | docs/s | |Median Throughput | index-append | 256881 | docs/s | |Max Throughput | index-append | 263886 | docs/s | The throughput of the two measurements is very different,about 50000 docs/s.I repeated the testing many times, and this result will appear. Why?",
    "website_area": "discuss"
  },
  {
    "id": "5d08ec04-632b-4522-b5ca-cec29023a2ff",
    "url": "https://discuss.elastic.co/t/not-able-to-install-elastic-search-as-windows/215309",
    "title": "Not able to Install Elastic Search as windows",
    "category": [
      "Elasticsearch"
    ],
    "author": "Saravana37",
    "date": "January 16, 2020, 12:19pm",
    "body": "Hello , I was trying to install Elastic search as windows service with the below steps. When i try to start the service I am receiving the attached error ? I tried with nssm.exe utility as well , Uninstall and install again as well but no luck . Any idea why iam receving the attached error ? , Please help ... PS E:\\ELK\\Elasticsearch\\bin> .\\elasticsearch-service.bat install Installing service : \"elasticsearch-service-x64\" Using JAVA_HOME (64-bit): \"C:\\Program Files\\Java\\jre1.8.0_201\" -Xms1g;-Xmx1g;-XX:+UseConcMarkSweepGC;-XX:CMSInitiatingOccupancyFraction=75;-XX:+UseCMSInitiatingOccupancyOnly;-Des.netw orkaddress.cache.ttl=60;-Des.networkaddress.cache.negative.ttl=10;-XX:+AlwaysPreTouch;-Xss1m;-Djava.awt.headless=true;-D file.encoding=UTF-8;-Djna.nosys=true;-XX:-OmitStackTraceInFastThrow;-Dio.netty.noUnsafe=true;-Dio.netty.noKeySetOptimiza tion=true;-Dio.netty.recycler.maxCapacityPerThread=0;-Dlog4j.shutdownHookEnabled=false;-Dlog4j2.disable.jmx=true;-Djava. io.tmpdir=C:\\Users\\E122_A_SELVASA\\AppData\\Local\\Temp\\3\\elasticsearch;-XX:+HeapDumpOnOutOfMemoryError;-XX:HeapDumpPath=da ta;-XX:ErrorFile=logs/hs_err_pid%p.log;-XX:+PrintGCDetails;-XX:+PrintGCDateStamps;-XX:+PrintTenuringDistribution;-XX:+Pr intGCApplicationStoppedTime;-Xloggc:logs/gc.log;-XX:+UseGCLogFileRotation;-XX:NumberOfGCLogFiles=32;-XX:GCLogFileSize=64 m The service 'elasticsearch-service-x64' has been installed.",
    "website_area": "discuss"
  },
  {
    "id": "afcab778-6262-4003-aac3-87386fc079bb",
    "url": "https://discuss.elastic.co/t/configurable-retention-lease-period-when-it-comes-to-ccr/215276",
    "title": "Configurable retention lease period when it comes to CCR",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 16, 2020, 9:59am January 16, 2020, 10:50am January 16, 2020, 10:42am January 16, 2020, 10:48am January 16, 2020, 10:56am January 16, 2020, 11:15am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "80bb5f52-5d60-4249-985c-bd45f45a8f36",
    "url": "https://discuss.elastic.co/t/unbalanced-primary-shards-affects-search-queries/215293",
    "title": "Unbalanced primary shards affects search queries?",
    "category": [
      "Elasticsearch"
    ],
    "author": "ragian",
    "date": "January 16, 2020, 10:40am January 16, 2020, 10:54am",
    "body": "In a lot of topic is said that we shouldn't worry about balancing primary shards across all nodes. Currently my primaries are highly unbalanced in my elasticsearch cluster (3 nodes with elastic 6.4.0). cerebro949402 78.5 KB On the cluster are performed mainly read/search queries and according to the documentation they should balanced between the node of the shard, but it seems that they are performed only on the server 3 (that one with high load). curl -XGET 's01.node.consul:9200/_cat/thread_pool' | grep \"search\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1845 100 1845 0 0 75275 0 --:--:-- --:--:-- --:--:-- 76875 \"s01\" search 0 0 2 \"s02\" search 0 0 0 \"s03\" search 7 8 164 This is one of the example of the queries: GET host.elastic:9200/index_*/_search/?scroll=120s { \"query\": { \"bool\": { \"must\": [{ \"match\": { \"param1\": \"1111\" } }, { \"match\": { \"param2\": \"XXXXXX\" } }] } }, \"sort\": [{ \"entered_date\": { \"order\": \"asc\" } }], \"size\": 1000 }",
    "website_area": "discuss"
  },
  {
    "id": "e35e6d37-cde5-4c92-9b1a-0bcd53a940c7",
    "url": "https://discuss.elastic.co/t/apm-agent-issue/215271",
    "title": "APM AGENT Issue",
    "category": [
      "Elasticsearch"
    ],
    "author": "Peter_Versan_Setier",
    "date": "January 16, 2020, 8:59am January 16, 2020, 10:36am",
    "body": "I have tried the version elastic-apm-agent-1.11 and 1.12 [apm-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Error sending data to APM server: Server returned HTTP response code: 400 for URL: http://131.101.24.12:9200/intake/v2/events, response code is 400 [1/16/20 2:15:47:546 EST] 00000014 SystemOut O 2020-01-16 02:15:47.546 [apm-reporter] WARN co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - {\"error\":{\"root_cause\":[{\"type\":\"mapper_parsing_exception\",\"reason\":\"failed to parse\"}],\"type\":\"mapper_parsing_exception\",\"reason\":\"failed to parse\",\"caused_by\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Malformed content, found extra data after parsing: START_OBJECT\"}},\"status\":400}",
    "website_area": "discuss"
  },
  {
    "id": "23c0a205-9bc7-4053-a937-2a99f7711ec3",
    "url": "https://discuss.elastic.co/t/cant-parse-boolean-value/215273",
    "title": "Can't parse boolean value",
    "category": [
      "Elasticsearch"
    ],
    "author": "Valentin_Hirson",
    "date": "January 16, 2020, 9:01am January 16, 2020, 1:27pm January 16, 2020, 10:14am",
    "body": "Hello, I have got the following issue on my production environnement : 2020-01-15 16:26:54] report.ERROR: {\"error\":{\"root_cause\":[{\"type\":\"query_shard_exception\",\"reason\":\"failed to create query: { \"terms\" : { \"spt_tea_organic\" : [ \"1\" ], \"boost\" : 1.0 } }\",\"index_uuid\":\"M5lx_hYUQvecb-W4TT_87A\",\"index\":\"magento2_fr_fr_catalog_product_20200115_145333\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"magento2_fr_fr_catalog_product_20200115_145333\",\"node\":\"2oLy_vV2Q7CFfKJWHV1NYA\",\"reason\":{\"type\":\"query_shard_exception\",\"reason\":\"failed to create query: { \"terms\" : { \"spt_tea_organic\" : [ \"1\" ], \"boost\" : 1.0 } }\",\"index_uuid\":\"M5lx_hYUQvecb-W4TT_87A\",\"index\":\"magento2_fr_fr_catalog_product_20200115_145333\",\"caused_by\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Can't parse boolean value [1], expected [true] or [false]\"}}}]},\"status\":400} I understand the error, but I don't know wky I haven't got the same issue on my dev environnement. Elasticsearch versions are not the same, I looked for an \"autoformat\" setting which will be not set on production but I did'nt find. Elasticsearch version on dev environnement : $ curl -X GET \"localhost:9200\" { \"name\" : \"ECxs_A7\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"V6_C1U1kTyuC_MlV4wS9Pg\", \"version\" : { \"number\" : \"5.3.2\", \"build_hash\" : \"3068195\", \"build_date\" : \"2017-04-24T16:15:59.481Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.4.2\" } , \"tagline\" : \"You Know, for Search\" } Version production : curl -X GET \"elasticsearch-persistent:9200\" { \"name\" : \"2oLy_vV\", \"cluster_name\" : \"docker-cluster\", \"cluster_uuid\" : \"iSGq_Z8FSguXlle9WtX3Dg\", \"version\" : { \"number\" : \"6.8.5\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"78990e9\", \"build_date\" : \"2019-11-13T20:04:24.100411Z\", \"build_snapshot\" : false, \"lucene_version\" : \"7.7.2\", \"minimum_wire_compatibility_version\" : \"5.6.0\", \"minimum_index_compatibility_version\" : \"5.0.0\" } , \"tagline\" : \"You Know, for Search\" } Have you got an idea ? Thanks you",
    "website_area": "discuss"
  },
  {
    "id": "7cc40d09-d258-4de9-a5af-db411130282d",
    "url": "https://discuss.elastic.co/t/date-field-type-problem-in-reindexing/212865",
    "title": "Date field type problem in reindexing",
    "category": [
      "Elasticsearch"
    ],
    "author": "Sina_Gholami",
    "date": "December 23, 2019, 3:56pm December 31, 2019, 10:13am January 2, 2020, 11:46am January 3, 2020, 7:47am January 5, 2020, 10:33am January 6, 2020, 11:23am January 16, 2020, 8:08am",
    "body": "I am trying to merge multiple indices with same fields in Elasticsearch by using _reindex API as follow: POST _reindex { \"source\": { \"index\": [\"a\",\"b\"] }, \"dest\": { \"index\": \"ab\" } } my problem is the date field. In \"ab\" index the date field becomes string and I cannot understand why. I used the following command in _reindex and tried to create and set mapping for index \"ab\" before _reindex, but the problem is not resolved. \"script\": { \"source\": \"ctx._source.date = new SimpleDateFormat('yyyy-MM-dd HH:mm:ss').parse(ctx._source.date);\" } PUT /data { \"settings\": { \"number_of_shards\": 1, \"number_of_replicas\": 1 }, \"mappings\": { \"properties\" : { \"cashtags\" : { \"type\" : \"keyword\" }, \"conversation_id\" : { \"type\" : \"long\" }, \"created_at\" : { \"type\" : \"long\" }, \"date\" : { \"type\": \"date\", \"format\" : \"yyyy-MM-dd HH:mm:ss\" } } }",
    "website_area": "discuss"
  },
  {
    "id": "546187a0-25ae-4cab-a154-3b206d16b421",
    "url": "https://discuss.elastic.co/t/is-open-java-with-elasticsearch-is-suggestable/215260",
    "title": "Is Open Java with Elasticsearch is suggestable?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Sanjeev_Kumar",
    "date": "January 16, 2020, 7:39am January 16, 2020, 7:51am",
    "body": "Is Open Java with Elasticsearch is suggestable? I mean, is there any limitations in terms of features and usage? Also is the latest OravleJava version is included in OpenJava that impacts Elasticsearch operations?",
    "website_area": "discuss"
  },
  {
    "id": "bd14a7e6-57c2-4746-b880-5191f29c2345",
    "url": "https://discuss.elastic.co/t/heap-memory-leak-in-elasticsearch-6-2-4/214419",
    "title": "Heap memory leak in Elasticsearch 6.2.4",
    "category": [
      "Elasticsearch"
    ],
    "author": "bsarkar",
    "date": "January 9, 2020, 11:13am January 16, 2020, 7:11am January 16, 2020, 7:36am",
    "body": "We have a cluster with 60 data nodes (8 CPU cores, 28 GB memory, 14 GB reserved for JVM heap) in addition to 3 master and 3 client nodes. There are around 35K shards (including replicas) in the cluster. In the past few weeks, we have had a couple of outages when heap usage of all data nodes rose quickly at around the same time and they went OOM. Configuration details: Output of java -version: openjdk version \"1.8.0_212\" OpenJDK Runtime Environment (Zulu 8.38.0.13-win64)-xxx OpenJDK 64-Bit Server VM (Zulu 8.38.0.13-win64)-xxx We're using G1GC instead of CMS GC because we found that it handles large heaps better. Below is the content of the jvm.options file on all data nodes. 8:-XX:+PrintGCApplicationStoppedTime 8:-XX:+PrintGCDateStamps 8:-XX:+PrintGCDetails 8:-XX:+PrintTenuringDistribution 9-:-Djava.locale.providers=COMPAT 9-:-Xlog:gc*,gc+age=trace,safepoint:file=gc.log:utctime,pid,tags:filecount=10,filesize=100m -Dfile.encoding=UTF-8 -Dio.netty.noKeySetOptimization=true -Dio.netty.noUnsafe=true -Dio.netty.recycler.maxCapacityPerThread=0 -Djava.awt.headless=true -Djava.io.tmpdir=tmp -Djna.nosys=true -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Xloggc:gc.log -Xms14336m -Xmx14336m -Xss1m -XX:+AlwaysPreTouch -XX:+HeapDumpOnOutOfMemoryError -XX:+PrintGCTimeStamps -XX:+UseG1GC -XX:+UseGCLogFileRotation -XX:GCLogFileSize=100M -XX:NumberOfGCLogFiles=10 -XX:-OmitStackTraceInFastThrow Issue: Coming back to the incident, the graph below shows % heap usage of all data nodes on 19th Dec when we had an outage. It's interesting how all data nodes followed the same pattern. They all leaked memory at the same time. We collected heap dump from one of the data nodes and analyzed it. I am pasting some screenshots of the analyzed report below: image26131569 328 KB This shows that most of the heap memory was consumed by org.elasticsearch.transport.TransportService. From the \"Current object set\" view of the analyzed data, I found that there is only one object of this class in memory. If we drill down further, we see the following: image820602 58 KB The array of type java.util.concurrent.ConcurrentHashMap$Node[] has around 35K objects so I guess each element represents a shard. Each of this object consumes more than 100 MB of memory! Is this expected? Drilling down inside the 237 MB object, we see the following: image978788 83.6 KB The highlighted objects represent an array of org.elasticsearch.action.admin.cluster.node.stats.NodeStats objects. The number of elements in this array equals the number of data nodes. I am curious as to why does this shard object have 3 copies of node stats data. Not all shard objects have this pattern though. Note that we have installed monitoring agents on each data node that execute GET _nodes/_local/stats/http,indices,jvm,transport every minute and push the results to a monitoring channel. But this is not a new setup. We've been running with this setup for a long time now. I understand that Elasticsearch 6.2.4 is past its EOL. We are soon going to upgrade to 6.8.5. Until then, I want to know if we're hitting any known issue here. If so, which version of Elasticsearch has a fix for it, and if not, does this bug needs fixing. Let me know if you need more details.",
    "website_area": "discuss"
  },
  {
    "id": "613a758f-a549-465d-b304-06e97db324a1",
    "url": "https://discuss.elastic.co/t/accessing-elastic-search-from-a-network/215116",
    "title": "Accessing elastic search from a network",
    "category": [
      "Elasticsearch"
    ],
    "author": "fekade",
    "date": "January 15, 2020, 10:50am January 15, 2020, 11:19am January 15, 2020, 1:11pm January 15, 2020, 1:43pm January 15, 2020, 1:45pm January 15, 2020, 2:00pm January 15, 2020, 2:31pm January 15, 2020, 3:42pm January 15, 2020, 3:42pm January 15, 2020, 4:19pm January 15, 2020, 4:36pm January 15, 2020, 4:40pm January 16, 2020, 7:35am",
    "body": "I configured Elasticsearch and kibana 7.5.1 in windows server 2016 and working successfully. I need to access elasticsearch from the network but I couldn't please any one have some idea. kibana is accessible from the network Elasticsearch.yml bootstrap.memory_lock: false cluster.name: elasticsearch http.port: 9200 node.data: true node.ingest: true node.master: true node.max_local_storage_nodes: 1 node.name: Test-Elastic path.data: C:\\ProgramData\\Elastic\\Elasticsearch\\data path.logs: C:\\ProgramData\\Elastic\\Elasticsearch\\logs transport.tcp.port: 9300 xpack.license.self_generated.type: trial xpack.security.enabled: true",
    "website_area": "discuss"
  },
  {
    "id": "8d84f4f9-3fb4-4997-86f8-15d2ad4a5f4f",
    "url": "https://discuss.elastic.co/t/passing-the-security-credentials-in-rest-search-api/214944",
    "title": "Passing the security credentials in REST Search API",
    "category": [
      "Elasticsearch"
    ],
    "author": "mruthyu",
    "date": "January 14, 2020, 8:14am January 14, 2020, 8:38am January 16, 2020, 6:29am",
    "body": "I am looking for the way to transfer the credential details for the REST Search API request. Checked the API specification and haven't found any fields. github.com elastic/elasticsearch/blob/master/rest-api-spec/src/main/resources/rest-api-spec/api/search.json { \"search\":{ \"documentation\":{ \"url\":\"https://www.elastic.co/guide/en/elasticsearch/reference/master/search-search.html\", \"description\":\"Returns results matching a query.\" }, \"stability\":\"stable\", \"url\":{ \"paths\":[ { \"path\":\"/_search\", \"methods\":[ \"GET\", \"POST\" ] }, { \"path\":\"/{index}/_search\", \"methods\":[ \"GET\", This file has been truncated. show original Would like to know how the authentication and authorization is being handled when we directly interact with Elasticsearch through API.",
    "website_area": "discuss"
  },
  {
    "id": "b12d2084-7d34-4e34-b6d8-f4d9683d349d",
    "url": "https://discuss.elastic.co/t/is-there-a-way-to-set-security-based-on-value-level-of-field/215246",
    "title": "Is there a way to set security based on value-level of field",
    "category": [
      "Elasticsearch"
    ],
    "author": "Tennis",
    "date": "January 16, 2020, 6:14am",
    "body": "Data Framework: Category: A, number: 1 Category: B, number: 2 Category: A, number: 3 Category: C, number: 4 Category: B, number: 5 As we can set the filed level security as official mentioned. (https://www.elastic.co/guide/en/elasticsearch/reference/current/field-level-security.html) Can we set security based on value level, for example, role-1: privilege on Category value: A, B role-2: privilege on Category value: C Currently, support this?",
    "website_area": "discuss"
  },
  {
    "id": "cfe2d593-6887-45eb-91da-c81fff0b42aa",
    "url": "https://discuss.elastic.co/t/how-to-use-dm-crypt-with-elasticsearch/214781",
    "title": "How to use dm_crypt with elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "Yungyoung_Ok",
    "date": "January 13, 2020, 9:07am January 13, 2020, 9:23am January 16, 2020, 1:54am January 16, 2020, 5:41am",
    "body": "In my case, the data stored in elasticsearch must be encrypted. So i will use dm_crypt + LUKS. Here is my file system. image694697 65.2 KB elasticsearch.yml image929589 36.4 KB In the current state, inserting and searching data using kibana dev tools works fine. like this... image1340774 52.8 KB But, I have two questions. First, Can dm_crypt be used with a elasticsearch basic license? currently my elastic cluster is a basic license. The official documentation says that platinum licenses are only supported. Is there a task to run separately in elasticsearch for dm crypt? Second, Can server users see data sources(plain text) even using dm_crpyt? I can connect to the server(login elasticsearch user) and see the text(plain text) of the translog in /var/elasticsearch/data. Is dm_crypt currently applied? Or is it not applied because of the license?",
    "website_area": "discuss"
  },
  {
    "id": "ed3ab201-7d98-4d46-8027-762d8dd4b0bb",
    "url": "https://discuss.elastic.co/t/newly-added-keyword-field-is-not-searchable/215239",
    "title": "Newly added keyword field is not searchable",
    "category": [
      "Elasticsearch"
    ],
    "author": "tamjid",
    "date": "January 16, 2020, 5:25am",
    "body": "ES version: 6.8.x Hi, I added 2 new keyword fields to some of my documents (10%) in an index. But those 2 new fields are not searchable. I refreshed the field list from kibana (management -> index pattern -> Refresh button). And in kibana I have green dots in the Searchable and Aggregatable columns for those 2 fields. But whey I try to search them from kibana, or from the dev tool I see no result. The mapping for those 2 fields is keyword. I double-checked it. What could be the problem? How can I solve it? Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "bbd7e5c2-783c-4873-b5c8-b1dd40b99ea1",
    "url": "https://discuss.elastic.co/t/elasticsearch-with-ldap-authentication-xpack-security/215072",
    "title": "Elasticsearch with LDAP Authentication XPack Security",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 15, 2020, 4:30am January 15, 2020, 7:29am January 15, 2020, 7:46am January 15, 2020, 11:13am January 16, 2020, 3:34am January 16, 2020, 5:14am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "93a4d9ad-d0a7-4c51-8965-3843a6828849",
    "url": "https://discuss.elastic.co/t/ingest-pipeline-for-nested-type/215235",
    "title": "Ingest pipeline for nested type",
    "category": [
      "Elasticsearch"
    ],
    "author": "shaileshgaikwad",
    "date": "January 16, 2020, 4:43am",
    "body": "I am trying to use Ingest pipeline for nested object and I am getting error. \"type\" : \"ingest_processor_exception\", \"reason\" : \"java.lang.IllegalArgumentException: field [content] not present as part of path [content]\", This is my Index which has nested type.. { \"settings\": { \"index\": { \"analysis\": { \"analyzer\": { \"windows_path_hierarchy_analyzer\": { \"type\": \"custom\", \"tokenizer\": \"windows_path_hierarchy_tokenizer\" } }, \"tokenizer\": { \"windows_path_hierarchy_tokenizer\": { \"type\": \"path_hierarchy\", \"delimiter\": \"\\\" } } }, \"number_of_shards\": \"1\", \"number_of_replicas\": \"1\" } }, \"mappings\": { \"_doc\": { \"properties\": { \"casePartList\": { \"type\": \"nested\" }, \"isRead\": { \"type\": \"boolean\" }, \"CreationDate\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"userId\": { \"type\": \"keyword\" }, \"titleKey\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"regimeNumber\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"documentList\": { \"type\": \"nested\", \"properties\": { \"path\": { \"analyzer\": \"windows_path_hierarchy_analyzer\", \"type\": \"text\" }, \"attachment\": { \"properties\": { \"date\": { \"type\": \"date\" }, \"indexed_chars\": { \"type\": \"long\" }, \"content_type\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"keywords\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"author\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"containsMetadata\": { \"type\": \"boolean\" }, \"name\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"language\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"title\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"content\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"content_length\": { \"type\": \"long\" }, \"detect_language\": { \"type\": \"boolean\" } } }, \"id\": { \"type\": \"integer\" }, \"documentTitle\": { \"type\": \"keyword\" }, \"incidentId\": { \"type\": \"integer\" }, \"content\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } } } }, \"caseId\": { \"type\": \"integer\" }, \"regimeName\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"caseName\": { \"type\": \"keyword\" }, \"folderName\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256.0, \"type\": \"keyword\" } } }, \"iD\": { \"type\": \"integer\" }, \"incidentId\": { \"type\": \"integer\" } } } } } my ingest pipeline is {\"description\":\"Document attachment pipeline\",\"processors\":[{\"attachment\":{\"field\":\"content\",\"target_field\":\"attachment\"}},{\"remove\":{\"field\":[\"content\"]}}]} Actual C# Code using NEST: var IngestPipeline = _esProvider.Client.Ingest.PutPipeline(\"attachments\", p => p .Description(\"Document attachment pipeline\") .Processors(pr => pr .Attachment<Document>(a => a .Field(f => f.Content) .TargetField(f => f.Attachment) ) .Remove<Document>(r => r .Field(f => f.Field(cx => cx.Content)) ) ) ); } Index is getting created but when i try to create document it gives below error \"type\" : \"ingest_processor_exception\", \"reason\" : \"java.lang.IllegalArgumentException: field [content] not present as part of path [content]\", I think my ingest pipeline is not properly mapped with Document type which is nested type of Incident. I am new to elastic search. Can you please guide me for this issue?",
    "website_area": "discuss"
  },
  {
    "id": "44f7a34d-98d6-4e2e-b675-16c5a3846563",
    "url": "https://discuss.elastic.co/t/elasticsearch-version-6-8-6-oss-unable-to-start/214918",
    "title": "Elasticsearch version 6.8.6 oss unable to start",
    "category": [
      "Elasticsearch"
    ],
    "author": "mrnguuyen",
    "date": "January 14, 2020, 12:53am January 14, 2020, 6:25am January 14, 2020, 7:16pm January 15, 2020, 5:19pm January 15, 2020, 11:12pm",
    "body": "Hi all, I am trying to do a fresh install of elasticsearch-oss-6.8.6 with OpenJDK version 1.8.0_181. An elasticsearch-oss-6.5 was able to successfully install before but I am receiving errors related to the keystore and trust manager. There were no issues related to the trust manager and keystore before. Here is the exception from elasticsearch.log: [2020-01-13T19:11:26,760][INFO ][o.e.p.PluginsService ] [xSBU6-2] no plugins loaded [2020-01-13T19:11:29,780][ERROR][o.e.b.Bootstrap ] [xSBU6-2] Exception org.elasticsearch.common.ssl.SslConfigException: failed to initialize a TrustManager for the system keystore at org.elasticsearch.common.ssl.DefaultJdkTrustConfig.createTrustManager(DefaultJdkTrustConfig.java:70) ~[?:?] at org.elasticsearch.common.ssl.SslConfiguration.createSslContext(SslConfiguration.java:109) ~[?:?] at org.elasticsearch.index.reindex.ReindexSslConfig.reload(ReindexSslConfig.java:145) ~[?:?] at org.elasticsearch.index.reindex.ReindexSslConfig.<init>(ReindexSslConfig.java:115) ~[?:?] at org.elasticsearch.index.reindex.ReindexPlugin.createComponents(ReindexPlugin.java:88) ~[?:?] at org.elasticsearch.node.Node.lambda$new$11(Node.java:472) ~[elasticsearch-6.8.6.jar:6.8.6] at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267) ~[?:1.8.0_181] at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ~[?:1.8.0_181] at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_181] at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_181] at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[?:1.8.0_181] at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_181] at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_181] at org.elasticsearch.node.Node.<init>(Node.java:475) ~[elasticsearch-6.8.6.jar:6.8.6] at org.elasticsearch.node.Node.<init>(Node.java:266) ~[elasticsearch-6.8.6.jar:6.8.6] at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:212) ~[elasticsearch-6.8.6.jar:6.8.6] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:212) ~[elasticsearch-6.8.6.jar:6.8.6] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:333) [elasticsearch-6.8.6.jar:6.8.6] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) [elasticsearch-6.8.6.jar:6.8.6] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) [elasticsearch-6.8.6.jar:6.8.6] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) [elasticsearch-6.8.6.jar:6.8.6] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) [elasticsearch-cli-6.8.6.jar:6.8.6] at org.elasticsearch.cli.Command.main(Command.java:90) [elasticsearch-cli-6.8.6.jar:6.8.6] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:116) [elasticsearch-6.8.6.jar:6.8.6] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) [elasticsearch-6.8.6.jar:6.8.6] Caused by: java.security.KeyStoreException: problem accessing trust storejava.io.IOException: Invalid keystore format at sun.security.ssl.TrustManagerFactoryImpl.engineInit(TrustManagerFactoryImpl.java:74) ~[?:?] at javax.net.ssl.TrustManagerFactory.init(TrustManagerFactory.java:250) ~[?:1.8.0_181] at org.elasticsearch.common.ssl.KeyStoreUtil.createTrustManager(KeyStoreUtil.java:151) ~[?:?] at org.elasticsearch.common.ssl.DefaultJdkTrustConfig.createTrustManager(DefaultJdkTrustConfig.java:68) ~[?:?] ... 24 more Not sure if this will help but this is from \"systemctl status elasticsearch\": [red dot] elasticsearch.service - Elasticsearch Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; disabled; vendor preset: disabled) Active: failed (Result: exit-code) since Tue 2020-01-14 08:34:19 UTC; 8s ago Docs: http://www.elastic.co Process: 5553 ExecStart=/usr/share/elasticsearch/bin/elasticsearch -p ${PID_DIR}/elasticsearch.pid --quiet (code=exited, status=1/FAILURE) Main PID: 5553 (code=exited, status=1/FAILURE) Jan 14 08:34:13 thetemplate systemd[1]: Started Elasticsearch. Jan 14 08:34:13 thetemplate systemd[1]: Starting Elasticsearch... Jan 14 08:34:19 thetemplate systemd[1]: elasticsearch.service: main process exited, code=exited, status=1/FAILURE Jan 14 08:34:19 thetemplate systemd[1]: Unit elasticsearch.service entered failed state. Jan 14 08:34:19 thetemplate systemd[1]: elasticsearch.service failed.",
    "website_area": "discuss"
  },
  {
    "id": "3e923365-6cee-437e-aa29-002f33837afe",
    "url": "https://discuss.elastic.co/t/scripted-field-for-elements-inside-json-objects/215224",
    "title": "Scripted Field for elements inside JSON objects",
    "category": [
      "Elasticsearch"
    ],
    "author": "yishaihl",
    "date": "January 15, 2020, 10:54pm",
    "body": "Hey Guys, I'm using FluentD (deployed as DaemonSet) to stream k8s app (containers) logs to elasticsearch. i want to extract specific keys from the 'log' key string (see attached) in elastic such as logKey: ... or statusCode:.. i want to use scripted field ~ painless script. i struggled a bit with it, can someone please advise? Screen Shot 2020-01-16 at 0.50.4418601228 414 KB",
    "website_area": "discuss"
  },
  {
    "id": "e8ee704f-ddad-4bb8-ba62-9d5679ba6906",
    "url": "https://discuss.elastic.co/t/cannot-join-nodes-to-master/215166",
    "title": "Cannot join nodes to master",
    "category": [
      "Elasticsearch"
    ],
    "author": "Julio_Edel_Salas_Dia",
    "date": "January 15, 2020, 4:48pm January 15, 2020, 4:48pm January 15, 2020, 9:17pm January 15, 2020, 9:53pm",
    "body": "I am trying to create an elasticsearch cluster with several nodes, I have 3 master nodes, but every time they try to join it gives this error. [2020-01-15T15:37:21,421][WARN ][o.e.c.c.Coordinator ] [emc-master-03] failed to validate incoming join request from node [{emc-cliente-01}{iqUQMm0fQJCJTpPxZH8N4g}{iTqfES1CRM6BKSSOQb30LQ}{10.28.11.1}{10.28.11.1:9300}{il}{ml.machine_memory=2684354560, ml.max_open_jobs=20, xpack.installed=true}] org.elasticsearch.transport.ReceiveTimeoutTransportException: [emc-cliente-01][10.28.11.1:9300][internal:cluster/coordination/join/validate] request_id [21] timed out after [60034ms] at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:1022) [elasticsearch-7.5.1.jar:7.5.1] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703) [elasticsearch-7.5.1.jar:7.5.1] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:830) [?:?] HELP!!!!",
    "website_area": "discuss"
  },
  {
    "id": "eeb984da-eb47-40c6-8a6d-6a7b0f5b68f0",
    "url": "https://discuss.elastic.co/t/how-to-get-the-recent-snapshot-of-my-dashboard-as-a-picture-pdf-to-send-to-slack/210585",
    "title": "How to get the recent snapshot of my dashboard as a picture/pdf to send to Slack?",
    "category": [
      "Elasticsearch"
    ],
    "author": "EZprogramming",
    "date": "December 4, 2019, 7:59pm December 4, 2019, 7:47pm December 4, 2019, 7:58pm December 4, 2019, 8:28pm December 28, 2019, 1:46am January 10, 2020, 12:26am January 10, 2020, 6:07am January 15, 2020, 8:58pm",
    "body": "I would like to receive the recent picture of my graphs (not manually; automated) and post the image URL to my slack for my team to view. Is that possible to do using Elasticsearch Watcher? I am also working with Slack APIs (attachments, text, etc).",
    "website_area": "discuss"
  },
  {
    "id": "0bcb8417-f8ad-43d3-9aca-7d357a1deb3b",
    "url": "https://discuss.elastic.co/t/how-to-aggregate-and-sum-the-data-grouped-by-some-fields/215207",
    "title": "How to aggregate and sum the data grouped by some fields",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mihir_Kothari",
    "date": "January 15, 2020, 8:54pm",
    "body": "Based on below 2 messages, I need to aggregate the data based on: granularity (MyHeader.granularity) myType (MyHeader.myType) Currency. { \"_source\": { \"MyData\": { \"MyHeader\": { \"granularity\": \"Level1\", \"myType\": \"YTD\", \"businessDate\": \"2018-12-27\" }, \"MyBreakDown\": [ { \"category\": null, \"Currency\": \"eur\", \"MyDetails\": [ { \"Id\": 123, \"myLocalAmount\": 100 } ] }, { \"category\": null, \"Currency\": \"clf\", \"MyDetails\": [ { \"Id\": 234, \"myLocalAmount\": 130 } ] }, { \"category\": null, \"Currency\": \"usd\", \"MyDetails\": [ { \"Id\": 120, \"myLocalAmount\": 250 } ] } ] } } }, { \"_source\": { \"MyData\": { \"MyHeader\": { \"granularity\": \"Level1\", \"myType\": \"MTD\", \"businessDate\": \"2018-12-27\" }, \"MyBreakDown\": [ { \"category\": null, \"Currency\": \"eur\", \"MyDetails\": [ { \"Id\": 123, \"myLocalAmount\": 110 } ] }, { \"category\": null, \"Currency\": \"clf\", \"MyDetails\": [ { \"Id\": 234, \"myLocalAmount\": 120 } ] }, { \"category\": null, \"Currency\": \"usd\", \"MyDetails\": [ { \"Id\": 120, \"myLocalAmount\": 253 } ] } ] } } } Here I can do basic aggregation: \"aggs\": { \"by_granularity\": { \"terms\" : { \"field\": \"MyHeader.granularity\" }, \"aggs\": { \"by_myType \": { \"terms\": { \"field\": \"MyHeader.myType\" } } } } }, \"size\":0 However, Currency is under the array field and unable to use it. I need help how to also use currency in aggregations, so it will give me correct data in buckets. Data is expected something like: Level1->YTD->eur = 100 Level1->YTD->clf= 130 Level1->YTD->usd = 250 Level1->MTD->eur = 110 Level1->MTD->clf = 120 Level1->MTD->usd = 253",
    "website_area": "discuss"
  },
  {
    "id": "c2e7ab0e-6bf4-4ccd-8539-0b2dd572bd19",
    "url": "https://discuss.elastic.co/t/how-to-ignore-missing-fields-while-reindexing-in-elasticsearch/215200",
    "title": "How to ignore missing fields while reindexing in Elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "OliverWu",
    "date": "January 15, 2020, 8:14pm",
    "body": "If I have a destination index with only portion of source index, and I only want to reindex these fields from source index to destination index, Can I still using the reindex API or what should I do with this situation? Any solutions or suggestions thanks! If I direct do reindex, the error is like Could not convert [fieldName] to boolean. fieldName is the missing field.",
    "website_area": "discuss"
  },
  {
    "id": "505f0320-eec8-496a-b9e7-3f92c293b139",
    "url": "https://discuss.elastic.co/t/autosuggest-in-7-5-1/215190",
    "title": "Autosuggest in 7.5.1",
    "category": [
      "Elasticsearch"
    ],
    "author": "PatOPaiar",
    "date": "January 15, 2020, 7:05pm",
    "body": "We have a working integration running a 5.6 ElasticSearch deployment on elastic.co connected to a site using the elasticsearch-php Branch 5.0 as suggested in the Version Matrix Everything works fine here, including our autosuggest endpoint which returns results correctly as shown in this example response. We are attempting to upgrade to ElasticSearch 7 so created a new deployment in elastic.co with version 7.5.1 and upgraded elasticsearch-php to Branch 7.4. The problem we are having is that the autosuggest endpoint stopped working. For the same request that before returned the results above, it now always returns no results as shown here We see nothing that points to what the problem can be in elastic.co logs, nor in our application logs. Something strange is that sometimes the autosuggest endpoint returns 504 gateway timeout, but with no further indication or logs to help us point where the problem can be. Any ideas, recommendations on how to troubleshoot this issue would be super appreciated. Can share example requests and further details.",
    "website_area": "discuss"
  },
  {
    "id": "64885d94-fb72-429e-993c-294c1ec2b9c6",
    "url": "https://discuss.elastic.co/t/es-7-5-1-exception-while-generating-certificates/215186",
    "title": "ES 7.5.1 exception while generating certificates",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 15, 2020, 6:16pm January 15, 2020, 6:14pm January 15, 2020, 6:17pm January 15, 2020, 6:19pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8e36ec70-f845-44a3-bc84-31d681d643a6",
    "url": "https://discuss.elastic.co/t/configure-es-node-to-only-host-frozen-indexes/215138",
    "title": "Configure ES node to only host frozen indexes",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 15, 2020, 4:30pm January 15, 2020, 5:10pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "402483a1-cf4f-47d2-80be-7bfa6b77eac7",
    "url": "https://discuss.elastic.co/t/elasticsearch-7-5-1/215178",
    "title": "Elasticsearch 7.5.1",
    "category": [
      "Elasticsearch"
    ],
    "author": "akash.gund",
    "date": "January 15, 2020, 5:00pm",
    "body": "while moving from ES7.2.0 to ES 7.5.1 have observed that the following JVM options thatwere present in ES 7.2 are not present in 7.5. pre-touch memory pages used by the JVM during initialization -XX:+AlwaysPreTouch basic explicitly set the stack size -Xss1m set to headless, just in case -Djava.awt.headless=true ensure UTF-8 encoding by default (e.g. filenames) -Dfile.encoding=UTF-8 use our provided JNA always versus the system one -Djna.nosys=true turn off a JDK optimization that throws away stack traces for common exceptions because stack traces are important for debugging -XX:-OmitStackTraceInFastThrow flags to configure Netty -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 log4j 2 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true WIll the options still be respected if they were to be added to the jvm options file??",
    "website_area": "discuss"
  },
  {
    "id": "422a3362-462c-4bae-9c16-e1361152d676",
    "url": "https://discuss.elastic.co/t/a-problem-with-numeric-range-query/215174",
    "title": "A problem with numeric range query",
    "category": [
      "Elasticsearch"
    ],
    "author": "D3ivid",
    "date": "January 15, 2020, 4:30pm",
    "body": "Hi, My name is David and I have a problem with a query. I tried to search the forum for similar topics but I haven't found anything, I appreciate any help in advance. This is the query that is giving me problems: { \"query\":{ \"bool\":{ \"must\": [ {\"nested\": { \"path\": \"valores\", \"query\": { \"range\": { \"valores.dESDE_TRANSF\": { \"gte\": 962031395626955 } } } }}, {\"nested\": { \"path\": \"valores\", \"query\": { \"range\": { \"valores.hASTA_TRANSF\": { \"lte\": 962031395626955 } } } }} ] } } } The results that the query returns do not match this one Result 1 => \"valores\" : { \"hASTA_TRANSF\" : 962031425341212, \"dESDE_TRANSF\" : 962031425341212 } Result 2 => \"valores\" : { \"hASTA_TRANSF\" : 962031401341185, \"dESDE_TRANSF\" : 962031401341185 } Why? This is my mapping configuration for these fields: \"hASTA_TRANSF\" : { \"type\" : \"float\" }, \"dESDE_TRANSF\" : { \"type\" : \"float\" }, Thanks in advance. Best regards.",
    "website_area": "discuss"
  },
  {
    "id": "a32b8334-f6a7-4c1d-84a9-ce37a5dc97e2",
    "url": "https://discuss.elastic.co/t/not-able-to-start-machine-learning-job/214467",
    "title": "Not able to start machine learning job",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 9, 2020, 4:36pm January 10, 2020, 10:08am January 15, 2020, 3:01pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8a7da16c-f1c8-44d1-b8b5-76fbda2b32cb",
    "url": "https://discuss.elastic.co/t/can-i-use-elasticsearch-for-data-warehouse/215158",
    "title": "Can I use elasticsearch for data warehouse?",
    "category": [
      "Elasticsearch"
    ],
    "author": "yogender20",
    "date": "January 15, 2020, 2:43pm",
    "body": "Can I use elasticsearch for data warehouse or HDFS is better option?",
    "website_area": "discuss"
  },
  {
    "id": "d046dbfb-54b5-41b5-a1e8-06c02c8a1564",
    "url": "https://discuss.elastic.co/t/sorting-result-for-text-is-not-proper/215153",
    "title": "Sorting Result For Text is not proper",
    "category": [
      "Elasticsearch"
    ],
    "author": "Shadab_Imam",
    "date": "January 15, 2020, 2:28pm",
    "body": "Hi All, I am using Nest Library for elastic search in my .net core project. I am facing problem in sorting of result. the sorted result returned from elasticSearch is not properly sorted. I am also using paging. below is the search query var searchRequest = new SearchRequest(indexName) { Size = pageSize, From = ((pageIndex - 1) * pageSize), Query = new MatchAllQuery(), Sort = new List { new FieldSort { Field = \"93.keyword\", Order = sortOrder }, } }; var searchResponse = await _client.SearchAsync<dynamic>(searchRequest); return searchResponse; Note : Field = \"93.keyword\" is text field And the mapping are as follows : \"93\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }",
    "website_area": "discuss"
  },
  {
    "id": "02ba6ea5-c5e9-4ff2-b8e0-4cce0068e132",
    "url": "https://discuss.elastic.co/t/sorting-aggregation-by-specified-value/215149",
    "title": "Sorting aggregation by specified value",
    "category": [
      "Elasticsearch"
    ],
    "author": "Sodium",
    "date": "January 15, 2020, 2:25pm",
    "body": "Hello, I'm building an ecommerce website using elastic search engine for my products. Product have tags that can be used for filtering using checkbox. I display the tags ordered by descending docs count with a limit of 5 lines, 25 if the user clicks a \"show more\" button. The issue i'm having having is that i need to display active tags first in the list. I could do it on the client side but that would require getting all possible tag values because at some point, the selected tag may not be one of the 25 first matching. Is there a way to boost an aggregated term using a specific value? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "0272fe37-f2f1-4787-a4cd-01be512591a5",
    "url": "https://discuss.elastic.co/t/nodes-are-not-getting-back-into-cluster/215147",
    "title": "Nodes are not getting back into cluster",
    "category": [
      "Elasticsearch"
    ],
    "author": "Vikas_Goswami",
    "date": "January 15, 2020, 2:23pm",
    "body": "I upgraded my cluster having 2 nodes from version 6.3 to 6.8 but after upgrade when I enable x-pack security nodes are not coming in cluster. I also generate the ssl for that but nothing happened. Kindly help me out.",
    "website_area": "discuss"
  },
  {
    "id": "031d488b-23ab-436c-b60e-4d3f780b543c",
    "url": "https://discuss.elastic.co/t/field-timezone-conversion/215007",
    "title": "Field Timezone conversion",
    "category": [
      "Elasticsearch"
    ],
    "author": "elasticforme",
    "date": "January 14, 2020, 3:30pm January 14, 2020, 4:48pm January 14, 2020, 5:03pm January 15, 2020, 2:08pm",
    "body": "I have data in Singapore and I want to save data as is and display as is. I understand elk everything gets to UTC. hence when I save data to elasticsearch by default it moves to UTC and gets messed. reason I said messed up because ( when someone runs rest query it does not shows up where it occur) if I use timezone=>\"something\" it saves data as is. i.e 10:10:10 will saves as 10:10:10 (when I look actual data in discovery or run sql found data where it occur) problem now I have is that \"NOW\" time in kibana is eight hour behind. ( because I use timezone=>\"Etc/GMT\" on that data.) I have been trying to figure out this from more then month now.",
    "website_area": "discuss"
  },
  {
    "id": "fff19a3a-5a99-4cc6-8ffa-475835b72899",
    "url": "https://discuss.elastic.co/t/can-fscrawler-index-files-from-different-servers/214912",
    "title": "Can fscrawler index files from different servers?",
    "category": [
      "Elasticsearch"
    ],
    "author": "SwatiVavilapalli",
    "date": "January 13, 2020, 10:25pm January 16, 2020, 5:40am January 14, 2020, 4:04am January 14, 2020, 4:07am January 14, 2020, 6:23am January 14, 2020, 10:51am January 14, 2020, 11:12am January 14, 2020, 4:08pm January 15, 2020, 7:56am January 15, 2020, 2:08pm",
    "body": "Hi, I am new to Elasticsearch and fscrawler, could you please let me know what are the settings I need to mention in the _settings.yaml (in fscrawler index) file so that I can index files from multiple servers. I tried with single server, it is working! but I have files in multiple servers. here is the single server settings file. CODE name: \"books\" fs: url: \"/var/www/html/file-scanner/ESFiles\" update_rate: \"15m\" excludes: - \"*/~*\" json_support: false filename_as_id: false add_filesize: true remove_deleted: true add_as_inner_object: false store_source: false index_content: true attributes_support: false raw_metadata: false xml_support: false index_folders: true lang_detect: false continue_on_error: false ocr: language: \"eng\" enabled: true pdf_strategy: \"ocr_and_text\" follow_symlinks: false server: hostname: \"dev2.com\" port: 22 username: \"swati\" password: \"password123\" elasticsearch: nodes: - url: \"http://127.0.0.1:9200\" bulk_size: 100 flush_interval: \"5s\" byte_size: \"10mb\" The versions of Elastics search version - 7.5.1, fsCrawler - 7.2.7 and they are running in one server and my documents are in 3 different servers. Thanks, Swati",
    "website_area": "discuss"
  },
  {
    "id": "5db9b94b-e6e9-4346-a392-efc80575fb01",
    "url": "https://discuss.elastic.co/t/json-string-to-nested-type/214074",
    "title": "JSON string to Nested type",
    "category": [
      "Elasticsearch"
    ],
    "author": "arham",
    "date": "January 7, 2020, 2:31pm January 7, 2020, 2:42pm January 8, 2020, 3:51pm January 8, 2020, 1:30pm January 8, 2020, 1:37pm January 8, 2020, 3:53pm January 8, 2020, 3:57pm January 8, 2020, 4:08pm January 9, 2020, 5:40am January 10, 2020, 7:03am January 10, 2020, 7:38am January 15, 2020, 12:08pm January 15, 2020, 12:28pm January 15, 2020, 1:23pm January 15, 2020, 1:44pm",
    "body": "Hi, I am using ingest pipeline to map FScrawler fields to elasticsearch index fields. I have json string like \"Tags\": [{\"Tag\":\"Ford\"}, {\"Tag\":\"BMW\"}, {\"Tag\":\"Fiat\"}] . How i can map this data to my index nested type field using ingest pipeline? Following is my index setting PUT my_index { \"settings\": { \"number_of_shards\": 1 }, \"mappings\": { \"dynamic\": false, \"properties\": { \"Tags\": { \"type\": \"nested\", \"properties\": { \"Tag\": { \"type\": \"keyword\" }} } } } }",
    "website_area": "discuss"
  },
  {
    "id": "6d4f27e2-6110-4ebb-bf2a-6ac9294f4254",
    "url": "https://discuss.elastic.co/t/ingest-node-docker-env-variables/215139",
    "title": "Ingest Node Docker Env Variables",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 15, 2020, 1:26pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "70b32798-c967-41cb-8770-75dd01fa4b55",
    "url": "https://discuss.elastic.co/t/highlight-problem-on-fuzzy-search/214989",
    "title": "Highlight problem on fuzzy search",
    "category": [
      "Elasticsearch"
    ],
    "author": "Casper_Thrane",
    "date": "January 14, 2020, 8:58pm January 15, 2020, 12:41pm",
    "body": "I have a problem with highlighting, on a fuzzy search. The problem is the highlighting is a char to long. See example below. PUT test { \"settings\": { \"index\": { \"analysis\": { \"analyzer\": { \"my_suggest\": { \"type\": \"custom\", \"tokenizer\": \"my_tokenizer\", \"filter\": [ \"danish_stop\", \"danish_stemmer\", \"lowercase\", \"remove_duplicates\" ] } }, \"filter\": { \"danish_stemmer\": { \"type\": \"stemmer\", \"language\": \"danish\" }, \"danish_stop\": { \"type\": \"stop\", \"stopwords\": \"_danish_\" } }, \"tokenizer\": { \"my_tokenizer\": { \"type\": \"edge_ngram\", \"min_gram\": 3, \"max_gram\": 15, \"token_chars\": [ \"letter\", \"digit\" ] } } } } }, \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\", \"fields\": { \"suggest\": { \"type\": \"text\", \"analyzer\": \"my_suggest\", \"fielddata\": true } } } } } } PUT test/_doc/1 { \"title\": \"affaldsregister\", } PUT test/_doc/2 { \"title\": \"affaldsregulativ\", } GET test/_search { \"_source\": \"title\", \"query\": { \"match\": { \"title.suggest\": { \"query\": \"affaldsreg\", \"fuzziness\": 1 } } }, \"highlight\": { \"fields\": { \"title.suggest\": { } } } } It will highlight the char after the 'g'. But if I remove the fuzziness, it highlights until 'g'.",
    "website_area": "discuss"
  },
  {
    "id": "4108c809-363e-42be-9ec9-6a3ae059fd71",
    "url": "https://discuss.elastic.co/t/change-the-size-of-an-index-that-has-already-been-created/215105",
    "title": "Change the size of an index that has already been created",
    "category": [
      "Elasticsearch"
    ],
    "author": "gabbar",
    "date": "January 15, 2020, 9:47am January 15, 2020, 1:35pm January 15, 2020, 10:58am",
    "body": "Hello, I'm pretty a newbie with ES and I've created an index that's too big for my purposes, therefore I'd like to limit it's growth to a more reasonable value (which also doesn't make my storage collapse). I've already fixed the template but I'm not able to find a way to change the max size of the index itself. Do you have any suggestion? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "65127b40-acfe-444c-aa0d-098af94ac8a0",
    "url": "https://discuss.elastic.co/t/throws-an-error-method-not-found-while-creating-index-and-document/215081",
    "title": "Throws an Error :Method not found .While Creating Index and Document",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 15, 2020, 10:23am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "644784e1-1f72-426d-89b8-c53c77545046",
    "url": "https://discuss.elastic.co/t/create-a-watch-with-multiple-actions-each-with-a-different-query/215067",
    "title": "Create a watch with multiple actions each with a different query",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 15, 2020, 10:19am January 15, 2020, 10:22am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9cee470d-4ba2-4d7f-8fbb-240bcf7f594f",
    "url": "https://discuss.elastic.co/t/elastic-cloud-cross-cluster-replication/213778",
    "title": "Elastic Cloud Cross Cluster Replication",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 4, 2020, 4:42pm January 6, 2020, 5:42pm January 15, 2020, 10:20am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "21c105b3-12e8-4ca4-b44e-b71b4ceafe3a",
    "url": "https://discuss.elastic.co/t/elasticsearch-service-hosted-on-elastic-cloud-with-kibana-running-on-local-machine/215099",
    "title": "Elasticsearch service Hosted on elastic cloud with Kibana running on local machine",
    "category": [
      "Elasticsearch"
    ],
    "author": "jaydengre",
    "date": "January 15, 2020, 9:07am January 22, 2020, 4:37am",
    "body": "We are using hosted elasticsearch service with platinum license. And we are trying to connect this hosted ES service by KIbana standard distribution server on local machine. Which is working great. We had to use kibana on local as we need to customize few features in it. When I checked the license it is using, it was same as ES hosted on elastic cloud. So my question is, is it a recommended way? Is it a right that our local Kibana is using hosted elasticsearch service? Is there going to be a problem with license?",
    "website_area": "discuss"
  },
  {
    "id": "16744b05-b620-4524-8200-3524b06a8d2a",
    "url": "https://discuss.elastic.co/t/prioritized-search-in-nested-object-nest-c/215096",
    "title": "Prioritized search in nested object (NEST, C#)",
    "category": [
      "Elasticsearch"
    ],
    "author": "wsteinert",
    "date": "January 15, 2020, 8:59am",
    "body": "Hello all! I am just getting started with Elasticsearch in .NET (C# + NEST), and I have a question where I don't find a solution. Maybe you can help me or point me to some kind of tutorial where I find the answer? My problem is this: I have an index containing an object with some properties and one of these properties is a list of another object, which I created as a nested property (at least I think I did :)). The objects look like this: public class MainObject { public int Id { get; set; } public string PropertyA { get; set; } public string PropertyB { get; set; } [Nested] public List<NestedObject> Values { get; set; } } public class NestedObject { public string Key { get; set; } public string Value { get; set; } } Now I want to set a search query which contains one or more words and each word should appear in either one of the properties of the main object or one of the properties of any of the nested values. In case I search for 3 words, I want each result containing all 3 words (regardless of which property they appear in) prioritized very high, results containing only 2 of the words prioritized medium, those containing only 1 of the words low, and those containing none not showing up. (The same of course with 2 or 4 or 5 words) The properties may contain whole sentences and they just have to contain the word, not be equal to the word. So that's how I created the index: var indexName = \"mytestindex\"; var settings = new ConnectionSettings(new Uri(\"http://localhost:9200\")).DefaultIndex(indexName).DisableDirectStreaming(); var client = new ElasticClient(settings); var createIndexResponse = client.Indices.Create(indexName, c => { return c.Map<MainObject>(m => { return m.Properties(ps => { return ps.Nested<NestedObject>(n => { return n.Name(nn => nn.Values); }); }); }); }); This is how I index each object: var response = client.IndexDocument(obj); And this is how my search currently looks like (where I don't know how to change it to produce the result outlined above): var searchPhrase = \"these are the words to be searched\"; var searchResponse = client.Search<MainObject>(s => { return s.Query(q => { return q.Nested(c => { return c.Name(\"named_query\") .Boost(1.1) .InnerHits(i => i.Explain()) .Path(p => p.Values) .Query(nq => { return nq.QueryString(qs => { return qs.DefaultField(f => f.Values.First().Value) .Query(\"*\" + searchPhrase + \"*\"); }); }) .IgnoreUnmapped(); }); }); }); What would I have to change to produce the result I am looking for? Anyone got any idea? Thank you so much for your time! Kind regards, Wolfgang (PS: The application and data is in German, so if I have to change anything that Elasticsearch knows how to work with german words instead of english words, that might be relevant as well :))",
    "website_area": "discuss"
  },
  {
    "id": "ebfffa1c-167a-4f40-85f8-f077051a8e87",
    "url": "https://discuss.elastic.co/t/watcher-returns-null-values/215092",
    "title": "Watcher returns null values",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 15, 2020, 8:25am January 15, 2020, 8:25am January 15, 2020, 8:45am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "348f5c16-90d3-4968-a831-4b45546e141c",
    "url": "https://discuss.elastic.co/t/more-like-this-and-fields-error/215095",
    "title": "More_like_this and fields error",
    "category": [
      "Elasticsearch"
    ],
    "author": "LMaz",
    "date": "January 15, 2020, 8:32am",
    "body": "I am using the query more_like_this to check the similarity between 2 texts. However the results are wrong (i put 2 identical documents and the similarity was not high) I think it can be related with the fields, because we are using fields with spaces and as i can see this is not really good. Could this be the reason? how should i improve the results?",
    "website_area": "discuss"
  },
  {
    "id": "327a4c39-a612-47eb-b7f7-c0b8cace1123",
    "url": "https://discuss.elastic.co/t/can-set-the-watcher-with-time-period-in-2-codition/212825",
    "title": "Can set the Watcher with Time period in 2 Codition",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "December 23, 2019, 9:09am January 7, 2020, 4:47pm January 8, 2020, 10:15am January 15, 2020, 7:29am January 15, 2020, 8:27am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d5d247bf-0674-426f-97af-42e959bd7da7",
    "url": "https://discuss.elastic.co/t/esrally-how-to-add-assertion-logic/215074",
    "title": "ESRally - How to add assertion logic",
    "category": [
      "Elasticsearch",
      "Rally"
    ],
    "author": "dbakti7",
    "date": "January 15, 2020, 4:51am January 15, 2020, 6:57am",
    "body": "Is there any way to add assertion logic into ESRally? For example, after bulk operation is completed, I want to assert that number of documents indexed are as expected. I found that ESRally doesn't throw any error/stop when there is something wrong with corpus. For example, having 4 documents with one malformed document will still result in successful run, but there are only 3 documents in ES.",
    "website_area": "discuss"
  },
  {
    "id": "b1fccab7-85c2-4b50-8ba1-dd863c25e915",
    "url": "https://discuss.elastic.co/t/getting-es-6-2-3-error-java-lang-assertionerror-java-text-parseexception-failed-to-parse-major-version-from-got/214647",
    "title": "Getting ES 6.2.3 error : java.lang.AssertionError: java.text.ParseException: Failed to parse major version from \"\" (got: )",
    "category": [
      "Elasticsearch"
    ],
    "author": "lastbulletbender",
    "date": "January 10, 2020, 6:17pm January 13, 2020, 3:05pm January 15, 2020, 6:35am",
    "body": "Hello everyone! We have a 3 node (1 master only and 2 data nodes) cluster and we are getting the below error intermittently. The elastic search service fails on the node and after restarting the service, I got the same stack trace again but after a couple of minutes the node gets stable and reallocation starts. Elastic search version is 6.2.3. Please let me know if you need any more details. 2020-01-03T23:31:43,334][ERROR][o.e.t.n.Netty4Utils ] fatal error on the network layer at [org.elasticsearch.transport.netty4.Netty4Utils.maybeDie(Netty4Utils.java:184](http://org.elasticsearch.transport.netty4.Netty4Utils.maybeDie(Netty4Utils.java:184)) at [org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.exceptionCaught(Netty4MessageChannelHandler.java:73](http://org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.exceptionCaught(Netty4MessageChannelHandler.java:73)) .. .. .. 1. [2020-01-03T23:31:43,343][WARN ][o.e.t.n.Netty4Transport ] [zdKm3J4] exception caught on transport layer [NettyTcpChannel{localAddress=/172.20.8.20:9300, remoteAddress=/172.20.8.44:45354}], closing connection org.elasticsearch.ElasticsearchException: java.lang.AssertionError: java.text.ParseException: Failed to parse major version from \"\" (got: ) There doesn't seem to be a network issue during this time. Complete logs here. https://pastebin.com/hSw9eNfz",
    "website_area": "discuss"
  },
  {
    "id": "d95c99f1-b2d6-44d2-842e-cd22f5426492",
    "url": "https://discuss.elastic.co/t/elasticsearch-status-is-in-red/214933",
    "title": "Elasticsearch status is in red",
    "category": [
      "Elasticsearch"
    ],
    "author": "gsumilhig",
    "date": "January 14, 2020, 6:30am January 14, 2020, 6:32am January 14, 2020, 7:59am January 15, 2020, 2:58am January 15, 2020, 6:14am",
    "body": "elasticsearch is in red status and found this log in master node. [2020-01-14T14:25:39,232][INFO ][o.e.c.r.a.DiskThresholdMonitor] [elastic02] low disk watermark [85%] exceeded on [h_w0x3riSpS1Dh-WQANUhQ][elastic02][/var/lib/elasticsearch/nodes/0] free: 9.2gb[10.8%], replicas will not be assigned to this node space in /var is 90% Thanks,",
    "website_area": "discuss"
  },
  {
    "id": "a4eb91a2-c932-4dd2-95b9-48207d0752f9",
    "url": "https://discuss.elastic.co/t/failed-to-authenticate-user-with-openid-connect/212600",
    "title": "Failed to authenticate user with OpenID Connect",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "December 20, 2019, 6:14am January 15, 2020, 5:55am January 15, 2020, 6:01am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c413ddb4-ed81-49ab-9c1d-73e67836f4a7",
    "url": "https://discuss.elastic.co/t/how-do-i-set-total-fields-limit-to-be-2000-by-default/215019",
    "title": "How do i set total fields limit to be 2000 by default",
    "category": [
      "Elasticsearch"
    ],
    "author": "Avinash_D_Silva",
    "date": "January 14, 2020, 5:04pm January 15, 2020, 5:53am",
    "body": "Hi. Got a Q on this.. I would like to have a template so whenever a new Twitter index gets created the ES system makes total fields limit to be 2000 by default. Bit like below... Anybody knows how to modify the below to have this setting included? curl -XPUT localhost:9200/_template/twothousandfieldslimit -d ' { \"order\" : 1, \"template\" : \"twitter*\", \"settings\" : { \"index\" : { \"mapping.total_fields.limit\" : \"2000\" } }",
    "website_area": "discuss"
  },
  {
    "id": "6be8094c-8c7c-437b-90d4-89d8d6bd148a",
    "url": "https://discuss.elastic.co/t/elasticsearch-json-representation-for-metadata-and-content/215073",
    "title": "Elasticsearch json representation for metadata and content",
    "category": [
      "Elasticsearch"
    ],
    "author": "Abhinav_Gandhi",
    "date": "January 15, 2020, 4:42am",
    "body": "Each document json information has metadata (status, type, name, description, size ) and the article content itself. Metadata is of smaller size around 20KB. And article content is of larger size, it may range anywhere from 100KB to 100Mb. Content will not be updated however metadata information is updated (max 5-6 times in its lifetime) We need to to search the content and metadata together and since elastic search mentions keeping - A single document should contain all of the information that is required to decide whether it matches a search request. So I want to know how to document this piece. But there are several approaches - Should we keep the metadata and content together, considering the extract from how updates work in elastic search Consequently, updating a previously indexed document is a delete followed by a re-insertion of the document. Note that this means that updating a document is even more expensive than adding it in the first place. Thus, storing things like rapidly changing counters in a Lucene index is usually not a good idea  there is no in-place update of values. Also since the content size can range from kbs to mbs, should we keep the entire content or should we split it into smaller segments and keep. Are there any more approaches generally followed for this requirement ?",
    "website_area": "discuss"
  },
  {
    "id": "6dc40729-7823-48a7-96c4-e48103c9ab80",
    "url": "https://discuss.elastic.co/t/problem-with-joining-data/214981",
    "title": "Problem with \"joining\" data",
    "category": [
      "Elasticsearch"
    ],
    "author": "Pieter_Agenbag",
    "date": "January 14, 2020, 11:19am January 14, 2020, 5:01pm January 15, 2020, 4:41am",
    "body": "I have a scenario where I have a bunch of indices (~100) each with thousands to several millions of documents each. My problem is each of these documents can be \"linked\" to one or more containers (Just a grouping name for simplicity). And I need to be able to filter documents in any of these indices based on a selected container. The obvious answer would be to carry the container name/id on each document , which works perfectly , except they can be linked or unlinked to containers frequently and the required update_by_query takes several hours I cant use parent child links , because parent child can only link in the same index (and can have only one parent and they have to be on the same shard) I tried indexing a separate document with and array of ids for each doc in the container ...and then using a terms lookup query, which also works great , but only for relatively small amounts of docs per container (I upped the max_terms to 10mil to test ) I tried the same thing with several smaller arrays and doing a bool-should with all the subsets , but that takes even longer. Since I use sequential numbering for ids , I tried to save the containers grouping as a set of id ranges , but the groups end up being waaay too scattered for that approach (ending up with 300 000 range queries) A typical query I run against the indices returns in ~100ms ,but the fastest (with the terms lookup) I've been able to get with the container filter is 10->30s which makes the application unusably unresponsive (I need sub second) Does anyone have ANY ideas for me ? I'm on ES 7.5 . I can reindex and change mappings and go crazy with the data as long as I can get the performance.",
    "website_area": "discuss"
  },
  {
    "id": "9456b5f2-368f-41fc-bdd6-05c9fc4190be",
    "url": "https://discuss.elastic.co/t/elasticsearch-sort-aggs-according-to-sibling-fields/215068",
    "title": "Elasticsearch, sort aggs according to sibling fields",
    "category": [
      "Elasticsearch"
    ],
    "author": "Noctis17",
    "date": "January 15, 2020, 3:11am",
    "body": "Elasticsearch v7.5 Hello and good day! We have 2 indices named socialmedia and influencers Sample contents: socialmedia: { '_id' : 1001, 'title' : \"Title 1\", 'smp_id' : 1, }, { '_id' : 1002, 'title' : \"Title 2\", 'smp_id' : 2, }, { '_id' : 1003, 'title' : \"Title 3\", 'smp_id' : 3, } //omitted other documents influencers { '_id' : 1, 'name' : \"John\", 'smp_id' : 1, 'smp_score' : 5 }, { '_id' : 2, 'name' : \"Peter\", 'smp_id' : 2, 'smp_score' : 10 }, { '_id' : 3, 'name' : \"Mark\", 'smp_id' : 3, 'smp_score' : 15 } //omitted other documents Now I have this simple query that determines which influencer has the most document in the socialmedia index GET socialmedia/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"INFLUENCERS\": { \"terms\": { \"field\": \"smp_id.keyword\" //smp_id is a **text** based field, that's why we have `.keyword` here } } } } SAMPLE OUTPUT: \"aggregations\" : { \"INFLUENCERS\" : { \"doc_count_error_upper_bound\" : //omitted, \"sum_other_doc_count\" : //omitted, \"buckets\" : [ { \"key\" : \"1\", \"doc_count\" : 87258 }, { \"key\" : \"2\", \"doc_count\" : 36518 }, { \"key\" : \"3\", \"doc_count\" : 34838 }, ] } } OBJECTIVE: My query is able to sort the influencers according to doc_count of their posts in the socialmedia index, now, is there a way for us to sort the INFLUENCERS according to their SMP_SCORE ? With that idea, smp_id 3 which is Mark, should be the first one to appear since he has an smp_score of 15 Thank you in advance for your help!",
    "website_area": "discuss"
  },
  {
    "id": "35ee9313-687b-4b77-8287-51834a33ce6b",
    "url": "https://discuss.elastic.co/t/find-similarity-between-two-fields-of-an-index/215060",
    "title": "Find similarity between two fields of an index",
    "category": [
      "Elasticsearch"
    ],
    "author": "prathako",
    "date": "January 15, 2020, 12:09am",
    "body": "My index: {\"var1\": \"abc\", \"var2\": \"ert\",\"var3\": \"ca\",\"var4\": \"sdf\"} {\"var1\": \"ert\", \"var2\": \"abc\",\"var3\": \"La\",\"var4\": \"abc\"} {\"var1\": \"vbn\", \"var2\": \"fgh\",\"var3\": \"ert\",\"var4\": \"vbh\"} {\"var1\": \"fgh\", \"var2\": \"tyu\",\"var3\": \"sdg\",\"var4\": \"sdf\"} I want to write a query to find the similarity between fields of my index. in this case, the intersection should be var1 & var2 -> 3 , var1&var3 ->1, var1&var4 -> 2, var2&var3->1, var2&var4->1, var3&var4->0 anything close to this is possible?",
    "website_area": "discuss"
  },
  {
    "id": "27470366-cd0f-4e67-bc5e-a1f1c720cafa",
    "url": "https://discuss.elastic.co/t/running-custom-docker-image-with-cloud-on-k8s-operator/215036",
    "title": "Running custom docker image with cloud-on-k8s operator",
    "category": [
      "Elasticsearch"
    ],
    "author": "sarthakn",
    "date": "January 14, 2020, 7:29pm",
    "body": "I'm trying to use the cloud-on-k8s operator to deploy ES on k8s. However, I need to use a different docker image as I need to use Ubuntu instead of CentOS for compatibility with some of our packages and I also need to use debian packaged ES to be compatible with our current plugin deployment workflow. I was able to create an ES docker image that satisfies both of the above criteria (runs Ubuntu and uses deb packaged ES), however I had some questions: The operator requires the Elastic license file to be present in /usr/share/elasticsearch (https://github.com/elastic/cloud-on-k8s/blob/1.0-beta/pkg/controller/elasticsearch/initcontainer/prepare_fs_script.go#L61), however the license file is not present in the debian distribution of ES. Is it alright if I add the license to the directory ourselves ? Is it fine to use the debian distribution with the Elastic operator (even if unsupported) ? I am using the docker-entrypoint file (https://github.com/elastic/dockerfiles/blob/7.5/elasticsearch/bin/docker-entrypoint.sh) in my Dockerfile as well, without any modifications. Am I allowed to do this ? 3.I'm also curious as to why Elastic decided to cover even Dockerfiles under Elastic license. This adds a lot of complications for us (in addition to the operator under Elastic license complicating things) even though we are just using it for internal purposes only.",
    "website_area": "discuss"
  },
  {
    "id": "ad27a236-a811-403d-afbe-68458dd736e7",
    "url": "https://discuss.elastic.co/t/checking-elasticsearch-health-during-rolling-restart/215023",
    "title": "Checking elasticsearch health during rolling restart",
    "category": [
      "Elasticsearch"
    ],
    "author": "lag13",
    "date": "January 14, 2020, 5:19pm January 14, 2020, 5:35pm January 14, 2020, 5:56pm",
    "body": "Hello! When doing a rolling upgrade (https://www.elastic.co/guide/en/elasticsearch/reference/7.3/rolling-upgrades.html), is checking that there are 0 initializing and relocating shards a reliable way to determine if you can restart the next node as opposed to checking that the \"status\" of the cluster is \"green\"? Background I'm writing a script to do a rolling upgrade on my 7.3.2 elasticsearch cluster deployed in AWS. For me that means (at a high level): for each ec2 instance: 1. terminate the instance 2. wait for an ASG to spin up an instance to replace the terminated one 3. wait for ES cluster to be \"healthy\" before terminating the next node My question revolves around (3). The rolling upgrade documentation seems to suggest to first wait for the cluster to become \"green\" but if it doesn't then you can continue the rolling upgrade if there are no initializing or relocating shards. Instead of first checking that the cluster is \"green\" can I just check that there are no initializing or relocating shards? That would make the script logic simpler. Excerpt from the documentation: Before upgrading the next node, wait for the cluster to finish shard allocation. You can check progress by submitting a _cat/health request. Wait for the status column to switch from yellow to green . Once the node is green , all primary and replica shards have been allocated. IMPORTANT: During a rolling upgrade, primary shards assigned to a node running the new version cannot have their replicas assigned to a node with the old version. The new version might have a different data format that is not understood by the old version. If it is not possible to assign the replica shards to another node (there is only one upgraded node in the cluster), the replica shards remain unassigned and status stays yellow . In this case, you can proceed once there are no initializing or relocating shards (check the init and relo columns). As soon as another node is upgraded, the replicas can be assigned and the status will change to green . Thanks for any and all advice!",
    "website_area": "discuss"
  },
  {
    "id": "127dc872-fc3e-4668-8985-e662d5ac5b3e",
    "url": "https://discuss.elastic.co/t/how-to-schedule-a-watcher-in-local-timezone/214953",
    "title": "How to schedule a Watcher in local timezone?",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 14, 2020, 8:48am January 14, 2020, 8:13pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "11f04a70-d71a-4603-9332-2d07ed86e2cf",
    "url": "https://discuss.elastic.co/t/latency-buffer-for-rollup-jobs-recommended-delay-and-or-what-it-depends-on/214819",
    "title": "Latency buffer for Rollup jobs - recommended delay and/or what it depends on",
    "category": [
      "Elasticsearch"
    ],
    "author": "alter",
    "date": "January 13, 2020, 11:51am January 14, 2020, 2:39pm January 14, 2020, 3:52pm January 14, 2020, 4:27pm",
    "body": "Hi! We are using real-time rollup job to aggregare events into metrics. The scenario: events are coming from logstash, peak ingestion flow is around 100k-120k events per minute rollup job is scheduled to run every minute and time bucket size is 1m We've noticed an anomaly with our scenario: If rollup job is running with no Latency buffer count of events in aggregated data is 5-7% less then in raw events. If rollup job is running with Latency buffer 1h - aggregated data perfectly matches raw events. Out goal is to have aggregated metrics as real-time as possible, which ideally means to figure out Latency buffer as small as possible while making sure that no events are lost in aggregations. And this is hard to achieve without clear understanding of what: is going on behind the cover of rollup job (in particular, how they deal with indexing delays, with events added with a past time stamp; etc) factors influencing the ability to aggregate all events (Harware specs/Indexing rate/Indexing delay/etc) So this is an open question - how to figure out the required latency buffer and influencing factors; what is currenlty the rollup ability to deal with indexing delays; who can share the experience of having real-time rollups; what are the recommendations for reducing required latency buffer. Any help and knowledge share is appreciated! Best regards, Andrey.",
    "website_area": "discuss"
  },
  {
    "id": "e93ababa-e3a7-461a-b60b-b12cd71bbe59",
    "url": "https://discuss.elastic.co/t/disable-machine-learning-process-creation/214913",
    "title": "Disable machine learning process creation",
    "category": [
      "Elasticsearch"
    ],
    "author": "musician",
    "date": "January 13, 2020, 10:26pm January 13, 2020, 10:31pm January 13, 2020, 11:01pm January 14, 2020, 7:37am January 14, 2020, 1:14pm January 14, 2020, 2:11pm January 14, 2020, 3:35pm",
    "body": "I am using Elasticsearch 7.1.1 and noticed that when I start it, a process controller.exe (part of Elasticsearch machine learning) is spawned automatically. How do I disable creation of this process since I do not use machine learning currently?",
    "website_area": "discuss"
  },
  {
    "id": "ffcbca04-1ed0-45c4-bb33-92d8a7b54271",
    "url": "https://discuss.elastic.co/t/need-help-in-setting-up-curator-in-the-current-elk-stack/214999",
    "title": "Need help in setting up curator in the current ELK stack",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 14, 2020, 2:02pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "574aa604-d5d8-402b-8ac9-1d2d3bc46c2a",
    "url": "https://discuss.elastic.co/t/failed-status-while-using-public-ip-on-my-office/214987",
    "title": "Failed status while using Public IP on my office",
    "category": [
      "Elasticsearch"
    ],
    "author": "gilang_bilbisri",
    "date": "January 14, 2020, 11:40am January 14, 2020, 12:34pm",
    "body": "hello everyone am new in elasticsearch program. i need your help for my installation progress. for my configuration in /etc/elasticsearch/elasticsearch.yml network.host : localhost http.port: 9200 it's successfull running after restarting elasticsearch.service but when i change the network.host : 100.100.200.150 as my public network its failed to start after i restart elasticsearch. please help. here's my configuration elasticsearch 1685687 93.1 KB Screenshot_1755683 91 KB",
    "website_area": "discuss"
  },
  {
    "id": "14256d3b-f96d-4bad-9535-b318358a0359",
    "url": "https://discuss.elastic.co/t/all-shards-failed-yellow-status/214919",
    "title": "All shards failed-yellow status",
    "category": [
      "Elasticsearch"
    ],
    "author": "Deny7",
    "date": "January 14, 2020, 12:32am January 14, 2020, 10:49am January 14, 2020, 11:12am January 14, 2020, 11:14am January 14, 2020, 11:16am",
    "body": "ELK: version 6.6.2 Hi, my elasticsearch stopped working after couple weeks. Im monitoring cca 10 tomcat logs ondifferent servers. I run ELK with Logstash on same server(SINGLE NODE). In log there is still repeating: [2020-01-14T00:00:26,348][WARN ][r.suppressed ] [wcnXOKN] path: /.kibana/_search, params: {ignore_unavailable=true, index=.kibana, filter_path=aggregations.types.buckets} org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed My memory: [root@HCgiKram bin]# free -g total used free shared buff/cache available Mem: 7 5 0 0 2 1 Swap: 3 0 2 In JVM option I have set -Xmx3g -Xms3g Output from health api: { \"cluster_name\" : \"elasticsearch\", \"status\" : \"yellow\", \"timed_out\" : false, \"number_of_nodes\" : 1, \"number_of_data_nodes\" : 1, \"active_primary_shards\" : 746, \"active_shards\" : 746, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 745, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 50.033534540576795 } Kibana wont even start, log error: FATAL [search_phase_execution_exception] all shards failed :: {\"path\":\"/.kibana/doc/_count\",\"query\":{},\"body\":\"{\"query\":{\"bool\":{\"should\":[{\"bool\":{\"must\":[{\"exists\":{\"field\":\"index-pattern\"}},{\"bool\":{\"must_not\":{\"term\":{\"migrationVersion.index-pattern\":\"6.5.0\"}}}}]}}]}}}\",\"statusCode\":503,\"response\":\"{\"error\":{\"root_cause\":,\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\": },\"status\":503}\"} I read that yellow status is normal on single node, but why is kibana not starting? Please can anybody help?",
    "website_area": "discuss"
  },
  {
    "id": "00ccd49b-90ad-4ed5-a57e-590b8469c1eb",
    "url": "https://discuss.elastic.co/t/external-version-control-with-update-by-query/214975",
    "title": "External version control with update_by_query",
    "category": [
      "Elasticsearch"
    ],
    "author": "uddhav",
    "date": "January 14, 2020, 10:51am",
    "body": "I have a use-case where I want to have an external version control with update_by_query. The same document might get updated a million times so retrying on version conflict is not an option. Any pointers on how to implement this.",
    "website_area": "discuss"
  },
  {
    "id": "d271f331-9f79-4691-880d-3e43d8b79966",
    "url": "https://discuss.elastic.co/t/elk-watcher-alarms/213566",
    "title": "ELK Watcher Alarms",
    "category": [
      "Elasticsearch"
    ],
    "author": "rj23495",
    "date": "January 2, 2020, 11:44am January 2, 2020, 9:31pm January 3, 2020, 6:56am January 6, 2020, 10:38am January 6, 2020, 3:10pm January 8, 2020, 10:27am January 14, 2020, 9:46am",
    "body": "Hey, I am trying to create alarms in elk watcher with the following parameters like 5 datapoints in 20 min for value > 10 but i cannot see anything apart from average median, etc. How can we achieve this type pf alarm config in elk watcher.",
    "website_area": "discuss"
  },
  {
    "id": "8fea96d9-869f-4f20-8f7c-850c06c0c130",
    "url": "https://discuss.elastic.co/t/getting-specific-repo-for-linux/214762",
    "title": "Getting specific Repo for Linux",
    "category": [
      "Elasticsearch"
    ],
    "author": "Sagar_Mandal",
    "date": "January 13, 2020, 6:05am January 13, 2020, 1:36pm January 14, 2020, 9:19am",
    "body": "Hi Team, How to get a specific version repo. of elasticsearch for Red Hat. thanks",
    "website_area": "discuss"
  },
  {
    "id": "5f1e45d8-443d-4b93-83dd-7df647a3703c",
    "url": "https://discuss.elastic.co/t/different-behavior-with-minimum-should-match-on-es2-4-and-es7-1/214960",
    "title": "Different behavior with minimum_should_match on ES2.4 and ES7.1",
    "category": [
      "Elasticsearch"
    ],
    "author": "oalfonso",
    "date": "January 14, 2020, 9:08am",
    "body": "Hi, First of all, disclaimer: we had to move long before to ES7.X but we are doing right now, from 2.4, shame on us. Well, my case: I've found that minimum_should_match has a different behavior when the param given is bigger than optionals. Query: ES 2.4: { \"query\": { \"bool\": { \"should\": [ { \"match\": { \"myfield\": { \"query\": \"my_search_value\", \"boost\": 150 } } } ], \"minimum_should_match\": 2 } } } Here, in ES2.4, giving \"minimum_should_match\": 2 and just 1 optional, it ignores the \"minimum_should_match\": 2 and returns everything that matches. BUT, now, the same in ES7.1 gives no results, I have to put \"minimum_should_match\": 1 if I want to get the results. Looks like ES2.4 was setting minimum_should_match at the maximum number of optionals and now it's not doing that anymore, is this possible? I tried to find this in documentation and change logs but I couldn't find anything. Any ideas? Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "463ef013-1104-4ad5-9ee0-3bed70f7873f",
    "url": "https://discuss.elastic.co/t/elasticsearch-7-5-error-of-connection-refused-when-curl-http-request-with-real-ip-address/214948",
    "title": "Elasticsearch(7.5) error of Connection refused when curl http request with real IP address",
    "category": [
      "Elasticsearch"
    ],
    "author": "longfei.li",
    "date": "January 14, 2020, 8:33am January 14, 2020, 8:33am January 14, 2020, 9:00am",
    "body": "After installing the elasticserach(7.5) on ubuntu 18.04 successfully, I try to send the curl request but get error message. |- curl http://10.13.2.111:9200 curl: (7) Failed to connect to 10.13.2.111 port 9200: Connection refused It does work when I send the command with localhost. |- curl localost:9200 How can I fix this issue? The elasticsearch config file is shown as below: =========== Elasticsearch Configuration ========================= NOTE: Elasticsearch comes with reasonable defaults for most settings. Before you set out to tweak and tune the configuration, make sure you understand what are you trying to accomplish and the consequences. The primary way of configuring a node is via this file. This template lists the most important settings you may want to configure for a production cluster. Please consult the documentation for further information on configuration options: https://www.elastic.co/guide/en/elasticsearch/reference/index.html ---------------------------------- Cluster ----------------------------------- Use a descriptive name for your cluster: cluster.name: labcar02 ------------------------------------ Node ------------------------------------ Use a descriptive name for the node: #node.name: node-1 Add custom attributes to the node: #node.attr.rack: r1 ----------------------------------- Paths ------------------------------------ Path to directory where to store the data (separate multiple locations by comma): path.data: /var/lib/elasticsearch Path to log files: path.logs: /var/log/elasticsearch ----------------------------------- Memory ----------------------------------- Lock the memory on startup: #bootstrap.memory_lock: true Make sure that the heap size is set to about half the memory available on the system and that the owner of the process is allowed to use this limit. Elasticsearch performs poorly when the system is swapping the memory. ---------------------------------- Network ----------------------------------- Set the bind address to a specific IP (IPv4 or IPv6): network.host: 0.0.0.0 Set a custom port for HTTP: http.port: 9200 For more information, consult the network module documentation. --------------------------------- Discovery ---------------------------------- Pass an initial list of hosts to perform discovery when this node is started: The default list of hosts is [\"127.0.0.1\", \"[::1]\"] #discovery.seed_hosts: [\"host1\", \"host2\"] Bootstrap the cluster using an initial set of master-eligible nodes: cluster.initial_master_nodes: [\"node-1\", \"node-2\"] For more information, consult the discovery and cluster formation module documentation. ---------------------------------- Gateway ----------------------------------- Block initial recovery after a full cluster restart until N nodes are started: #gateway.recover_after_nodes: 3 For more information, consult the gateway module documentation. ---------------------------------- Various ----------------------------------- Require explicit names when deleting indices: #action.destructive_requires_name: true http.cors.enabled: true http.cors.allow-origin: \"*\"",
    "website_area": "discuss"
  },
  {
    "id": "99f70991-683a-48ba-9363-12147af89db6",
    "url": "https://discuss.elastic.co/t/enrich-processor-lookups-how-to-update-once-through-enrich-pipeline-generated-document/214958",
    "title": "Enrich Processor, Lookups: how to Update once through Enrich Pipeline generated document",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mattteo",
    "date": "January 14, 2020, 8:59am",
    "body": "We have many Lookup-indexes and one enrich pipeline to decode some values. Data will be imported from a DB using logstash and a pipeline parameter within elasticsearch plugin. output { elasticsearch { index => \"customers\" pipeline => \"countries_lookup\" ... } } The question is: how can I update and reindex document using the same enrich pipeline? For example: I have a country ISO code within my document: { \"countrycode\":\"US\" } The pipeline adds a decoded nested object with a value for the country code: { \"countrycode\":\"CH\", \"countryname\" : { \"code\" : \"CH\", \"name\" : \"Schweiz\" } } Assume I want to update the document, change the country code, get new values from a lookup and reindex the document. How can I do that? is it possible to do it using logstash with input/http and output/elasticsearch plugins giving the same pipeline as a parameter? The \"_update\" feature from elasticsearch unfortunately has no \"pipeline\" parameter to make it easier.",
    "website_area": "discuss"
  },
  {
    "id": "c94c72fe-58d0-4a0c-91ee-67aaca632807",
    "url": "https://discuss.elastic.co/t/partial-search-of-attributes-substring-case-insensitive-and-special-chars-included/214883",
    "title": "Partial search of attributes(Substring, Case insensitive and special chars included)",
    "category": [
      "Elasticsearch"
    ],
    "author": "Shail",
    "date": "January 13, 2020, 5:49pm January 14, 2020, 8:41am",
    "body": "We are using ElasticSearch version 7.2 in our project with following characteristics: The main index mapping contains attributes, embedded objects as well as few nested (array) types. Analyzer : Standard analyzer The requirement is to be able to perform partial search such that it supports Case insensitive e.g. name: John Doe should match a search query like name: john doe Patterns based like e.g. name: John Doe should match a search query like name: ohn D Special characters(atleast the ones on standard English keyboard) e.g. name: John (star) Doe should match a search query like name: john (star We have tried using the match wildcard match_phrase_prefix and query_string but none of the above satisfies all the above 3 requirements together. Is there any specific query DSL that can fulfill our requirements ?",
    "website_area": "discuss"
  },
  {
    "id": "0e85a716-6bca-4314-a416-f3fab344c7e1",
    "url": "https://discuss.elastic.co/t/scripting-query-for-returning-cases-where-array-size-is-larger-than-x/214892",
    "title": "Scripting Query for returning cases where array size is larger than X",
    "category": [
      "Elasticsearch"
    ],
    "author": "spdoes",
    "date": "January 13, 2020, 6:57pm January 13, 2020, 10:51pm January 14, 2020, 8:36am",
    "body": "I want to make a query that counts how many elements there are in the field that is a array of jsons objects. When I run: {script: { script: \"doc['field_name'].values.length() > 0\"}} I get: \"No field found for [field_name] in mapping with types \" I have set mapping for field_name as nested, but still I am getting the same error I have tried multiple variations for the query but none of them worked: \"script\" : \"params['_source']['field_name'].length() > 1\" \"script\" : \"params['_source']['field_name'].size() > 1\" \"script\" : \"params['_source']['field_name'].values.size() > 1\" \"script\" : \"params['_source']['field_name.keyword'].values.size() > 1\" \"script\" : {\"source\": \"doc['ctx._source.field_name'].lenght() > 0\" }} and other variations. What am I doing wrong ?",
    "website_area": "discuss"
  },
  {
    "id": "95d6aa91-9f95-4837-b34c-97e7d1e15d27",
    "url": "https://discuss.elastic.co/t/error-reports-failures-when-exporting-documents/214717",
    "title": "Error reports failures when exporting documents",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 13, 2020, 1:57pm January 13, 2020, 1:56pm January 14, 2020, 8:03am January 14, 2020, 8:32am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "77692d70-5147-4306-8663-4ee09a45fa48",
    "url": "https://discuss.elastic.co/t/unable-to-make-any-request-to-elastic-cloud/214805",
    "title": "Unable to make any request to elastic cloud",
    "category": [
      "Elasticsearch"
    ],
    "author": "portal",
    "date": "January 13, 2020, 11:01am January 13, 2020, 1:27pm January 13, 2020, 3:00pm January 13, 2020, 3:07pm January 13, 2020, 10:32pm January 14, 2020, 8:28am",
    "body": "Hi, I have been working with elasticsearch for about a year now in development using version 7.1 and the high level rest client (java), deployed my application to azure and created a free trial elastic deployment but unable to create index or make any request at all. It keeps giving me the unknownHostException",
    "website_area": "discuss"
  },
  {
    "id": "2ae19ad4-9469-46a2-9de4-2389e8246a0d",
    "url": "https://discuss.elastic.co/t/possible-to-use-custom-dhparam-for-tls/214946",
    "title": "Possible to use custom dhparam for tls?",
    "category": [
      "Elasticsearch"
    ],
    "author": "asp",
    "date": "January 14, 2020, 8:17am",
    "body": "Hi, can I use an individual dhparm file with elasticserach tls? If possible I want to be able to use a cusrtom dhparam for rest api and for transport. Do I need to append dhparam fragment to the cert (if using pem mode), is there a parameter to load the dhparam file explicitly or will kibana just ignore custom dhparams? Is it posisble to use custom dhparam with .p12 files? Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "fb63de05-71e2-4682-a54a-296e58e59164",
    "url": "https://discuss.elastic.co/t/cpu-at-100-with-xpack-security-enabled-es-7-3/214544",
    "title": "CPU at 100% with XPACK security Enabled - ES 7.3",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "January 13, 2020, 12:00pm January 13, 2020, 12:06pm January 14, 2020, 7:18am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7eb5f983-0e7d-4817-82b5-4a75fd7d54b3",
    "url": "https://discuss.elastic.co/t/nodes-randomly-disconnected-from-the-es-cluster/23028",
    "title": "Nodes randomly disconnected from the ES cluster",
    "category": [
      "Elasticsearch"
    ],
    "author": "Anil_Karaka",
    "date": "June 16, 2015, 1:53pm June 16, 2015, 3:26pm June 16, 2015, 3:26pm June 16, 2015, 3:26pm June 16, 2015, 3:26pm June 16, 2015, 3:26pm February 1, 2016, 7:44am June 29, 2017, 1:46pm January 14, 2020, 6:39am January 14, 2020, 7:14am",
    "body": "I greped for \"removed\" in master node and these are the logs that I see. [2015-04-01 05:32:55,813][INFO ][cluster.service ] [ESBigNode3] removed {[ES30GBNode2][Yf8ODQh0TE2_0hQ35Y0M_w][ip-153-31-43-55][inet[/153.31.73.55:9300]],}, reason: zen-disco-node_failed([ES30GBNode2][Yf8ODQh0TE2_0hQ35Y0M_w][ip-153-31-73-55][inet[/153.31.73.55:9300]]), reason transport disconnected [2015-04-01 05:33:02,048][INFO ][cluster.service ] [ESBigNode3] removed {[ES30GBNode1][0CRaC261RXy8JfGc1XNLZA][ip-153-31-76-111][inet[/153.31.76.111:9300]],}, reason: zen-disco-node_failed([ES30GBNode1][0CRaC261RXy8JfGc1XNLZA][ip-153-31-36-101][inet[/153.31.36.101:9300]]), reason transport disconnected [2015-04-01 05:33:09,702][INFO ][cluster.service ] [ESBigNode3] removed {[ESBigNode5][PaNaDPwfSM-jUpGa8HQJmQ][esnode5][inet[/153.31.70.128:9300]],}, reason: zen-disco-node_failed([ESBigNode5][PaNaDPwfSM-jUpGa8HQJmQ][esnode5][inet[/153.31.70.128:9300]]), reason transport disconnected [2015-04-01 05:33:13,964][INFO ][cluster.service ] [ESBigNode3] removed {[ESBigNode1][ihJU17ToQVit9BxNzQjhnQ][esnode1][inet[/153.31.75.190:9300]],}, reason: zen-disco-node_failed([ESBigNode1][ihJU17ToQVit9BxNzQjhnQ][esnode1][inet[/153.31.35.190:9300]]), reason transport disconnected And in the data node, this is how the node leaving the cluster looks like in its log files. [2015-01-22 20:49:56,860][WARN ][discovery.ec2 ] [ESBigNode1] master left (reason = do not exists on master, act as master failure), current nodes: {[ESBigNode2][zVdCNza9Qk-v-Usu66jcvw][ip-153-31-73-29][inet[/153.31.73.29:9300]],[ESBigNode4][-8pj8n2sS5GB4XTIE0zudQ][ip-153-31-74-230][inet[/153.31.74.230:9300]],[ESBigNode1][nU6bkV-SSb6rvLHsth9AQg][ip-153-31-75-190][inet[/153.31.75.190:9300]],} That is 4 nodes leaving the 7 node cluster at at time.. and the cluster is in red state for few minutes, not just yellow state.. Although 4 nodes leaving the cluster is rare.. Single nodes leave the cluster very often. As discussed in this thread, https://groups.google.com/forum/#!msg/elasticsearch/ixoAF9Yur0E/CgX4Hbk1ynYJ I will change the discovery.zen.ping.timeout to 10sec, what else can I do. there is an older thread from 2012 that also suggests to change OS settings that deal with ipv4 TCP keep alive settings.. Do I also have to change this setting? https://groups.google.com/forum/#!msg/elasticsearch/c9JmaiVfBb0/9XZM6ZJpoBwJ -- You received this message because you are subscribed to the Google Groups \"elasticsearch\" group. To unsubscribe from this group and stop receiving emails from it, send an email to elasticsearch+unsubscribe@googlegroups.com. To view this discussion on the web visit https://groups.google.com/d/msgid/elasticsearch/727d0b5f-1dbf-4ce6-ab11-067b20513c76%40googlegroups.com. For more options, visit https://groups.google.com/d/optout.",
    "website_area": "discuss"
  },
  {
    "id": "f5a49f0a-ef7e-4c4f-9120-53c109e813b2",
    "url": "https://discuss.elastic.co/t/maximum-ram-recommended-for-data-node/214928",
    "title": "Maximum RAM recommended for data node",
    "category": [
      "Elasticsearch"
    ],
    "author": "malshan",
    "date": "January 14, 2020, 5:40am January 14, 2020, 6:19am January 14, 2020, 6:27am January 14, 2020, 6:47am January 14, 2020, 6:48am January 14, 2020, 7:12am",
    "body": "Hi all, I'm new here, for please forgive for any changes from expected format. ES version: 7.5 I'm going to deploy a hot-warm-cold architecture, with dedicated data, master and ingest nodes. However, even our 'hot' section of data is to be kept for 180 days, and would be around 50TB (total, including replicas). Keeping a disk to ram ratio of 24, i will still need about 40 nodes to handle this, if i'm to keep it below 64GB memory for a node ( i have heard somewhere not to go above 64GB per node). Is there an actually a recommended limit of 64GB, or can i go up to 256GB or so without issues per node? And that so called disk:ram ratio, is it calculated using replicated total size of disk or using a single replica only. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "f52505af-e711-4a5b-a560-56429156bef0",
    "url": "https://discuss.elastic.co/t/reset-elasticsearch-password/214281",
    "title": "Reset elasticsearch password",
    "category": [
      "Elasticsearch"
    ],
    "author": "talsantos",
    "date": "January 8, 2020, 4:59pm January 10, 2020, 3:59pm January 14, 2020, 5:53am",
    "body": "Hi, I need to reset the password of elasticsearch because I don't know it. And the person who know it, doesnt work at the company anymore. How can I reset the password? Thank you. Tnia",
    "website_area": "discuss"
  },
  {
    "id": "32ef3842-2323-495e-b634-87369b3ae45d",
    "url": "https://discuss.elastic.co/t/possibility-to-join-data-in-elasticsearch-kibana/214695",
    "title": "Possibility to 'join' data in Elasticsearch / Kibana",
    "category": [
      "Elasticsearch"
    ],
    "author": "Matthias_Seidl",
    "date": "January 14, 2020, 4:50am",
    "body": "Hi guys, this has been on my mind for a few days now. I have 2 indices, indA and indB. IndA has a field called \"Magic\" and indB has a field called \"Magic\". This is what I'd use in a relational DB to make the join between the indices. Why do I want to join? Because IndA has a field called \"Error\" which I want to visualize, but I want to filter this visual based on the field \"Target\" in indB. indA fields: Magic, Error indB fields: Magic, Target I can make an index pattern that covers both indices, but I don't know how to make the error visual filterable based on the \"Target\" field. Is there any way to do this? That'd be awesome",
    "website_area": "discuss"
  },
  {
    "id": "9720af41-ed77-4dba-94d9-f556937e7ab5",
    "url": "https://discuss.elastic.co/t/stackoverflow-rooms-for-discussion/214771",
    "title": "Stackoverflow rooms for discussion",
    "category": [
      "Elasticsearch"
    ],
    "author": "rakmo",
    "date": "January 13, 2020, 7:29am January 14, 2020, 3:21am",
    "body": "I have created Stackoverflow rooms for Elastic related discussions. I am completely aware that Elastic maintains it's own discussion platform which is awesome, and I am completely in favor of this platform. Stackoverflow will give an add on to this platform and extend the awareness of Elastic and Elasticsearch to one centralized organization/platform. We should not be limited to this one, and therefore extend our support and maintenance to Stackoverflow. I would like to take this initiative to start support on Stackoverflow. We can give it a try for 6 months if it seems to be helpful then we can continue maintaining it and elect more maintainers. Link to room Elastic: https://chat.stackoverflow.com/rooms/205849/elastic Elasticsearch: https://chat.stackoverflow.com/rooms/205848/elasticsearch",
    "website_area": "discuss"
  },
  {
    "id": "f7a86779-2cbe-4d44-a3d5-01f44e141dce",
    "url": "https://discuss.elastic.co/t/right-configuration-to-send-email-ses-alerts-in-elk/214893",
    "title": "Right configuration to send email (SES) alerts in ELK",
    "category": [
      "Elasticsearch"
    ],
    "author": "vpuvvala",
    "date": "January 13, 2020, 7:39pm January 13, 2020, 9:52pm January 13, 2020, 10:02pm",
    "body": "Hello, I'm trying to configure email notification alerts from Elasticsearch using SES SMTP server. Elasticsearch version: 7.4.2 Configuration: volumeMounts: - name: keystore mountPath: /usr/share/elasticsearch/config/elasticsearch.keystore subPath: elasticsearch.keystore volumes: - name: keystore secret: secretName: elastic-keystore I have created a secret: kubectl create secret generic elastic-keystore --from-literal=xpack.notification.email.account.ses_account.smtp.secure_password=xxxx and currently facing error: Exception in thread \"main\" org.elasticsearch.bootstrap.BootstrapException: java.io.IOException: Is a directory: SimpleFSIndexInput(path=\"/usr/share/elasticsearch/config/elasticsearch.keystore\") Likely root cause: java.io.IOException: Is a directory at java.base/sun.nio.ch.FileDispatcherImpl.read0(Native Method) at java.base/sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:48) at java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276) at java.base/sun.nio.ch.IOUtil.read(IOUtil.java:245) at java.base/sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:223) at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.readInternal(SimpleFSDirectory.java:178) at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342) at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54) at org.apache.lucene.store.BufferedChecksumIndexInput.readByte(BufferedChecksumIndexInput.java:41) at org.apache.lucene.store.DataInput.readInt(DataInput.java:101) at org.apache.lucene.codecs.CodecUtil.checkHeader(CodecUtil.java:194) at org.elasticsearch.common.settings.KeyStoreWrapper.load(KeyStoreWrapper.java:223) at org.elasticsearch.bootstrap.Bootstrap.loadSecureSettings(Bootstrap.java:234) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:305) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:125) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) Not sure where I'm going wrong, could any help me resolve this ? Thanks, Vin.",
    "website_area": "discuss"
  },
  {
    "id": "3206f304-e947-479e-af27-8e49a4ff9151",
    "url": "https://discuss.elastic.co/t/refresh-field-list/214899",
    "title": "Refresh Field List",
    "category": [
      "Elasticsearch"
    ],
    "author": "mannam14387",
    "date": "January 13, 2020, 8:47pm",
    "body": "Some times we fail to see any data in kibana for some indices but once i refresh the fields on that particular index it starts to show the data again , do i need to change the field types in this case or I am missing something here , please do let me know if any faced this issue in the past : Error during index fail to pull any data from hot node to show on kibana : Caused by: java.lang.IllegalArgumentException: Fielddata is disabled on text fields by default. Set fielddata=true on [child.child.child.child.child.child.child.child.child.child.details.message.text] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. at org.elasticsearch.index.mapper.TextFieldMapper$TextFieldType.fielddataBuilder(TextFieldMapper.java:767) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.index.fielddata.IndexFieldDataService.getForField(IndexFieldDataService.java:116) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.index.query.QueryShardContext.getForField(QueryShardContext.java:179) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.search.DefaultSearchContext.getForField(DefaultSearchContext.java:505) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.search.fetch.subphase.DocValueFieldsFetchSubPhase.hitsExecute(DocValueFieldsFetchSubPhase.java:98) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:177) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.search.SearchService.lambda$executeFetchPhase$3(SearchService.java:540) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.search.SearchService$3.doRun(SearchService.java:380) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:41) ~[elasticsearch-7.1.0.jar:7.1.0] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:751) ~[elasticsearch-7.1.0.jar:7.1.0]",
    "website_area": "discuss"
  },
  {
    "id": "5113572b-2d08-4d10-b3ac-3a4a353c7caf",
    "url": "https://discuss.elastic.co/t/adding-master-nodes-to-existing-cluster/214880",
    "title": "Adding master nodes to Existing cluster",
    "category": [
      "Elasticsearch"
    ],
    "author": "surya2",
    "date": "January 13, 2020, 5:14pm January 13, 2020, 5:49pm January 13, 2020, 6:29pm January 13, 2020, 6:44pm January 13, 2020, 6:48pm January 13, 2020, 6:55pm January 13, 2020, 6:55pm",
    "body": "Hello All, I have 14 node elasticsearch cluster in which all nodes are data,master,client. Now i want to have separate master nodes. How do i achieve this without causing splitbrain or down time. I can bring one node at one time down. Suggestions please. Thanks, Surya.",
    "website_area": "discuss"
  },
  {
    "id": "56fe4772-d6bf-4d3e-b35b-ca2484576544",
    "url": "https://discuss.elastic.co/t/ingest-pipeline-not-working-but-working-in-the-simulate-api/205322",
    "title": "Ingest pipeline not working but working in the simulate API",
    "category": [
      "Elasticsearch"
    ],
    "author": "themarcusaurelius",
    "date": "October 25, 2019, 8:01pm October 26, 2019, 4:05am October 26, 2019, 4:44am October 26, 2019, 3:13pm October 26, 2019, 3:14pm January 13, 2020, 6:42pm",
    "body": "I have a pipeline that is filtering Palo Alto logs through Grok patterns and other processors. Testing with the simulate API shows that it works but when I try to ingest into elasticsearch none of the documents go through. I did tweaking in the elasticsearch output which did send a few of the documents to but the pipeline was still being ignored. The dataset for these logs is quite a bit larger though than what I usually work with so I am wondering if the pipeline is just too slow to process each document. { \"version\": 1, \"processors\" : [ { \"grok\" : { \"field\" : \"message\", \"patterns\" : [\"%{TRAFFIC:traffic}\",\"%{SYSTEM:system}\"], \"pattern_definitions\": { \"TRAFFIC\" : \"%{GREEDYDATA:ignore} - - - - 1,%{DATA:receive_time},%{NUMBER:serial},%{WORD:type},%{WORD:subtype},%{NUMBER:config_version},%{DATA:time_generated},%{IP:src},%{IP:dst},%{IP:natsrc},%{IP:natdst},%{DATA:rule},%{DATA:srcuser},%{DATA:dstuser},%{WORD:app},%{WORD:vsys},%{WORD:from},%{DATA:to},%{DATA:inbound_if},%{DATA:outbound_if},%{DATA:logset},%{DATA:FUTURE_USE},%{NUMBER:sessionid},%{NUMBER:repeatcnt},%{NUMBER:sport},%{NUMBER:dport},%{NUMBER:natsport},%{NUMBER:natdport},%{WORD:flags},%{WORD:proto},%{WORD:action},%{NUMBER:bytes},%{NUMBER:bytes_sent},%{NUMBER:bytes_received},%{NUMBER:packets},%{DATA:start},%{NUMBER:sec},%{WORD:category},%{NUMBER:tpadding},%{NUMBER:seqno},%{WORD:actionflags},%{DATA:srcloc},%{DATA:dstloc},%{NUMBER:cpadding},%{NUMBER:pkts_sent},%{NUMBER:pkts_received},%{DATA:session_end_reason},%{NUMBER:dg_hier_level_1},%{NUMBER:dg_hier_level_2},%{NUMBER:dg_hier_level_3},%{NUMBER:dg_hier_level_4},%{DATA:vsys_name},%{DATA:device_name},%{DATA:action_source},%{DATA:src_uuid},%{DATA:dst_uuid},%{NUMBER:tunnelid},%{DATA:monitortag},%{NUMBER:parent_session_id},%{DATA:parent_start_time},%{DATA:tunnel},%{NUMBER:assoc_id},%{NUMBER:chunks},%{NUMBER:chunks_sent},%{NUMBER:chunks_received},%{DATA:rule_uuid},%{NUMBER:http}\", \"SYSTEM\" : \"%{GREEDYDATA:ignore} - - - - 1,%{DATA:receive_time},%{NUMBER:serial},%{WORD:type},%{WORD:subtype},%{NUMBER:config_version},%{DATA:time_generated},%{WORD:vsys},%{DATA:eventid},%{DATA:fmt},%{DATA:id},%{WORD:module},%{WORD:severity},%{QS:opaque},%{NUMBER:seqno},%{DATA:actionflags},%{DATA:dg_hier_level_1},%{DATA:dg_hier_level_2},%{DATA:dg_hier_level_3},%{DATA:dg_hier_level_4},%{DATA:vsys_name},%{GREEDYDATA:device_name}\" }, \"ignore_missing\" : true, \"ignore_failure\" : true } }, { \"grok\": { \"field\": \"message\", \"patterns\": [\"%{THREAT:threat}\"], \"pattern_definitions\": { \"THREAT\": \"%{GREEDYDATA:ignore} - - - - 1,%{DATA:receive_time},%{NUMBER:serial},%{WORD:type},%{WORD:subtype},%{NUMBER:config_version},%{DATA:time_generated},%{IP:src},%{IP:dst},%{IP:natsrc},%{IP:natdst},%{DATA:rule},%{DATA:srcuser},%{DATA:dstuser},%{WORD:app},%{WORD:vsys},%{DATA:from},%{DATA:to},%{DATA:inbound_if},%{DATA:outbound_if},%{DATA:logset},%{DATA:FUTURE_USE},%{NUMBER:sessionid},%{NUMBER:repeatcnt},%{NUMBER:sport},%{NUMBER:dport},%{NUMBER:natsport},%{NUMBER:natdport},%{WORD:flags},%{DATA:proto},%{WORD:action},%{DATA:misc},%{DATA:threatid},%{DATA:category},%{WORD:severity},%{DATA:direction},%{NUMBER:seqno},%{DATA:actionflags},%{DATA:srcloc},%{DATA:dstloc},%{NUMBER:cpadding},%{DATA:contenttype},%{DATA:pcap_id},%{DATA:filedigest},%{DATA:cloud},%{DATA:url_idx},%{DATA:user_agent},%{DATA:filetype},%{DATA:xff},%{DATA:referer},%{DATA:sender},%{DATA:subject},%{DATA:recipient},%{DATA:reportid},%{DATA:dg_hier_level_1},%{DATA:dg_hier_level_2},%{DATA:dg_hier_level_3},%{DATA:dg_hier_level_4},%{DATA:vsys_name},%{DATA:device_name},%{DATA:file_url},%{DATA:src_uuid},%{DATA:dst_uuid},%{DATA:http_method},%{NUMBER:tunnel_id},%{DATA:monitortag},%{DATA:parent_session_id},%{DATA:parent_start_time},%{WORD:tunnel},%{DATA:thr_category},%{DATA:six_flags},%{DATA:assoc_id},%{NUMBER:ppid},%{DATA:http_headers},%{QS:url_category},%{DATA:rule_uuid},%{NUMBER:http}\" }, \"ignore_missing\": true, \"ignore_failure\": true } }, { \"geoip\" : { \"field\" : \"dst\", \"ignore_failure\": true } }, { \"geoip\" : { \"field\" : \"src\", \"ignore_failure\" : true } }, { \"geoip\" : { \"field\" : \"natsrc\", \"ignore_failure\": true } }, { \"geoip\" : { \"field\" : \"natdst\", \"ignore_failure\": true } }, { \"convert\": { \"field\": \"bytes\", \"type\": \"float\", \"ignore_failure\": true, \"ignore_missing\": true } }, { \"convert\": { \"field\": \"bytes_received\", \"type\": \"float\", \"ignore_failure\": true, \"ignore_missing\": true } }, { \"convert\": { \"field\": \"bytes_sent\", \"type\": \"float\", \"ignore_failure\": true, \"ignore_missing\": true } }, { \"convert\": { \"field\": \"packets\", \"type\": \"float\", \"ignore_failure\": true, \"ignore_missing\": true } }, { \"convert\": { \"field\": \"pkts_received\", \"type\": \"float\", \"ignore_failure\": true, \"ignore_missing\": true } }, { \"convert\": { \"field\": \"pkts_sent\", \"type\": \"float\", \"ignore_failure\": true, \"ignore_missing\": true } }, { \"convert\": { \"field\": \"chunks\", \"type\": \"float\", \"ignore_failure\": true, \"ignore_missing\": true } }, { \"convert\": { \"field\": \"chunks_received\", \"type\": \"float\", \"ignore_failure\": true, \"ignore_missing\": true } }, { \"convert\": { \"field\": \"chunks_sent\", \"type\": \"float\", \"ignore_failure\": true, \"ignore_missing\": true } }, { \"date\" : { \"field\" : \"receive_time\", \"formats\" : [\"YYYY/MM/DD HH:mm:ss\"], \"ignore_failure\": true } }, { \"date\" : { \"field\" : \"start\", \"formats\" : [\"YYYY/MM/DD HH:mm:ss\"], \"ignore_failure\": true } }, { \"date\" : { \"field\" : \"time_generated\", \"formats\" : [\"YYYY/MM/DD HH:mm:ss\"], \"ignore_failure\": true } }, { \"remove\": { \"field\": \"ignore\", \"ignore_failure\": true } }, { \"remove\": { \"field\": \"threat\", \"ignore_failure\": true } }, { \"remove\": { \"field\": \"system\", \"ignore_failure\": true } }, { \"remove\": { \"field\": \"traffic\", \"ignore_failure\": true } } ] } SImulate is working when testing the messages. Capture.JPG1203851 176 KB",
    "website_area": "discuss"
  },
  {
    "id": "2e80d413-e2af-4614-bb5a-c0fa8662cc6d",
    "url": "https://discuss.elastic.co/t/native-controller-process-has-stopped-no-new-native-processes-can-be-started/214077",
    "title": "Native controller process has stopped : no new native processes can be started",
    "category": [
      "Elasticsearch"
    ],
    "author": "StepBat",
    "date": "January 7, 2020, 3:28pm January 7, 2020, 3:37pm January 7, 2020, 3:45pm January 7, 2020, 3:50pm January 13, 2020, 6:08pm January 13, 2020, 6:10pm",
    "body": "Hello support I need your help in order to fix a crash of elasticsearch just after the starting with the message 'Native controller process has stopped - no new native processes can be started\" Elasticsearch is started a systemd service. I don't get this issue when I start manually the process Elasticsearch release 6.7.1 OS : Redhat 7.1 Logs: [2020-01-07T15:40:59,709][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [sidre-bac-tac] [controller/5278] [Main.cc@109] controller (64 bit): Version 6.7.1 (Build e1f492de67a719) Copyright (c) 2019 Elasticsearch BV [2020-01-07T15:41:00,498][DEBUG][o.e.a.ActionModule ] [sidre-bac-tac] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security [2020-01-07T15:41:00,864][INFO ][o.e.d.DiscoveryModule ] [sidre-bac-tac] using discovery type [zen] and host providers [settings] [2020-01-07T15:41:01,686][INFO ][o.e.n.Node ] [sidre-bac-tac] initialized [2020-01-07T15:41:01,686][INFO ][o.e.n.Node ] [sidre-bac-tac] starting ... [2020-01-07T15:41:01,882][INFO ][o.e.t.TransportService ] [sidre-bac-tac] publish_address {10.229.3.157:9300}, bound_addresses {10.238.95.156:9300}, {10.229.3.157:9300} [2020-01-07T15:41:01,914][INFO ][o.e.b.BootstrapChecks ] [sidre-bac-tac] bound or publishing to a non-loopback address, enforcing bootstrap checks [2020-01-07T15:41:05,264][INFO ][o.e.c.s.ClusterApplierService] [sidre-bac-tac] detected_master {sidre-bac-rjs}{hm0uUOdvSR28mmeT6N4cFw}{UNjiXXzsSbmUaJ5JPHqPmw}{10.229.3.155}{10.229.3.155:9300}{ml.machine_memory=8189480960, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true}, added {{sidre-bac-rjs}{hm0uUOdvSR28mmeT6N4cFw}{UNjiXXzsSbmUaJ5JPHqPmw}{10.229.3.155}{10.229.3.155:9300}{ml.machine_memory=8189480960, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true},}, reason: apply cluster state (from master [master {sidre-bac-rjs}{hm0uUOdvSR28mmeT6N4cFw}{UNjiXXzsSbmUaJ5JPHqPmw}{10.229.3.155}{10.229.3.155:9300}{ml.machine_memory=8189480960, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true} committed version [14]]) [2020-01-07T15:41:05,572][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [sidre-bac-tac] Failed to clear cache for realms [] [2020-01-07T15:41:05,576][INFO ][o.e.x.s.a.TokenService ] [sidre-bac-tac] refresh keys [2020-01-07T15:41:05,843][INFO ][o.e.x.s.a.TokenService ] [sidre-bac-tac] refreshed keys [2020-01-07T15:41:05,881][INFO ][o.e.l.LicenseService ] [sidre-bac-tac] license [596d39b3-5ad0-422e-8b50-126d73681e02] mode [basic] - valid [2020-01-07T15:41:05,930][INFO ][o.e.h.n.Netty4HttpServerTransport] [sidre-bac-tac] publish_address {10.229.3.157:9200}, bound_addresses {10.238.95.156:9200}, {10.229.3.157:9200} [2020-01-07T15:41:05,930][INFO ][o.e.n.Node ] [sidre-bac-tac] started [2020-01-07T15:41:50,207][INFO ][o.e.x.m.p.NativeController] [sidre-bac-tac] Native controller process has stopped - no new native processes can be started [2020-01-07T15:41:50,209][INFO ][o.e.n.Node ] [sidre-bac-tac] stopping ... [2020-01-07T15:41:50,219][INFO ][o.e.x.w.WatcherService ] [sidre-bac-tac] stopping watch service, reason [shutdown initiated] [2020-01-07T15:41:50,674][INFO ][o.e.n.Node ] [sidre-bac-tac] stopped [2020-01-07T15:41:50,675][INFO ][o.e.n.Node ] [sidre-bac-tac] closing ... [2020-01-07T15:41:50,688][INFO ][o.e.n.Node ] [sidre-bac-tac] closed",
    "website_area": "discuss"
  },
  {
    "id": "858573be-670e-4a90-9d04-ae5e6a5d98b0",
    "url": "https://discuss.elastic.co/t/connection-reset-by-peer-in-6-8-es-with-6-1-4-client/209605",
    "title": "Connection reset by peer in 6.8 ES with 6.1.4 client",
    "category": [
      "Elasticsearch"
    ],
    "author": "nikuland",
    "date": "November 27, 2019, 4:35am November 27, 2019, 5:47am November 27, 2019, 7:52pm November 27, 2019, 7:53pm December 18, 2019, 10:17pm January 11, 2020, 12:26am January 13, 2020, 5:59pm",
    "body": "Often we are seeing \"Connection reset by Peer\" after migrating to ES 6.8. We are still using ES 6.1.4 client. It recovers, but often there is Connection reset by Peer. Any solution or how can this be debugged.",
    "website_area": "discuss"
  },
  {
    "id": "ae0b3ded-2601-4ba8-b7b3-719c21dcec34",
    "url": "https://discuss.elastic.co/t/random-indexing/214876",
    "title": "Random indexing",
    "category": [
      "Elasticsearch"
    ],
    "author": "shankarana",
    "date": "January 13, 2020, 4:41pm January 13, 2020, 5:58pm",
    "body": "Hi Team, My usecase to update index whenever the new json document received. Logstash will receive event and update ES index. example: 01.00 AM: index name: currencies inserted with 5 documents source-destination-offer% USD - GBP- 2% USD - EUR - 2% USD - YEN - 3% GBP - USD - 0.2% EUR - USD - 0.2% note: here we have only 3 documents for source=USD next at 01.10 AM: index name: currencies only received 6 rows. source-destination-offer% USD GBP 10% USD EUR 10% USD YEN 10% USD JPY 10% USD CNY 10% expected: i want to delete all old USD documents only and inserted newly received 5 USD + retain existing 2 non USD data I am not having fixed interval re-indexing - which means - there is no time specific indexing job, as long as data come in (irrelevant interval ) to elastic search i need to start indexing it and make it available for consumer search in that case i would like to only replace the existing document which is impacted. do i need to handle using primary key concepts ?",
    "website_area": "discuss"
  },
  {
    "id": "2cf9defe-c257-40b5-a178-f52fd2f1d78a",
    "url": "https://discuss.elastic.co/t/fielddata-or-nested-optimization-doubt/214877",
    "title": "FieldData or Nested optimization. Doubt",
    "category": [
      "Elasticsearch"
    ],
    "author": "EliuFlorez",
    "date": "January 13, 2020, 4:41pm",
    "body": "Hi guys, I find myself making some changes to an old scrits and I have the doubt that it would be more optimal if I continue using 'fielddata' or 'nested' that is to say the scripts do the searches or aggregations from a field like this: \"categoryId \":\" L: 1001 L: 1016 L: 1023 L: 1029 L: 1036 \" so what I want is to establish a 'nested' to search by ID within it. according to reading 'fielddata' consumes a lot of memory but nested consumes more or is it more optimal at the time of truth? Example FieldData: POST listing/_search { \"query\": { \"bool\": { \"must\": [ { \"term\": { \"status\": true } } ] } }, \"post_filter\": { \"bool\": { \"filter\": [ { \"terms\": { \"categoryId\": [ 1 ] } } ] } }, \"aggs\": { \"category\": { \"terms\": { \"field\": \"categoryId\" } } }, \"size\": 1 } Example Nested: POST listing/_search { \"query\": { \"bool\": { \"must\": [ { \"term\": { \"status\": true } } ] } }, \"post_filter\": { \"bool\": { \"must\": [ { \"nested\": { \"path\": \"categoryData\", \"query\": { \"bool\": { \"must\": [ { \"terms\": { \"categoryData.id\": [ 1 ] } } ] } } } } ] } }, \"aggs\": { \"aggsCategory\": { \"nested\": { \"path\": \"categoryData\" }, \"aggs\": { \"category\": { \"terms\": { \"field\": \"categoryData.id\", \"size\": 100 } } } } }, \"size\": 10 }",
    "website_area": "discuss"
  },
  {
    "id": "ea325748-1e7b-4979-be65-4fa111c357f4",
    "url": "https://discuss.elastic.co/t/retieve-element-in-an-array-object/214785",
    "title": "Retieve element in an array object",
    "category": [
      "Elasticsearch"
    ],
    "author": "pilo",
    "date": "January 13, 2020, 9:22am January 13, 2020, 4:11pm January 13, 2020, 10:12am January 13, 2020, 11:45am January 13, 2020, 4:11pm",
    "body": "Hi everyone, I have a question about array object in elasticsearch For example i have a mapping like this one: PUT test { \"settings\": { ...}, \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"levels\":{ \"properties\": { \"translations\":{ \"type\": \"text\", \"analyzer\": \"english\" }, \"id\":{ \"type\": \"keyword\" } } } } } } And then, i add documents in index test: { \"id\": 1, \"levels\":[ {\"id\": 1, \"translations\": \"lalaland\"}, {\"id\": 2, \"translations\": \"lili\"}, {\"id\": 3, \"translations\": \"lalaland\"} ] } Then, i launch some search GET test/_search { \"_source\": \"levels.translations\", \"query\": { \"match\": { \"levels.translations\": \"lalaland\" } }, \"highlight\": { \"fields\": {\"levels.translations\": {}} } } The result is: \"hits\" : [ { \"_index\" : \"test\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.3955629, \"_source\" : { \"levels\" : [ { \"id\": 1, \"translations\" : \"lalaland\" }, { \"id\": 2, \"translations\" : \"lili\" }, { \"id\": 3, \"translations\" : \"lalaland\" } ] }, \"highlight\" : { \"levels.translations\" : [ \"<em>lalaland</em>\", \"<em>lalaland</em>\" ] } } ] Are there anyway to retrieve just the first and the thirst element in levels field. I wonder if i can do it with nested ? Thank you all.",
    "website_area": "discuss"
  },
  {
    "id": "96b0d2a9-bb70-417a-b7e3-db4f13307c5c",
    "url": "https://discuss.elastic.co/t/ideal-data-ingester-node-count/214612",
    "title": "Ideal data/ingester node count",
    "category": [
      "Elasticsearch"
    ],
    "author": "Naseem",
    "date": "January 10, 2020, 2:58pm January 13, 2020, 1:43pm January 13, 2020, 2:38pm January 13, 2020, 2:51pm January 13, 2020, 3:11pm January 13, 2020, 3:36pm",
    "body": "tl;dr fewer data nodes with more disk space each vs more data nodes with smaller disk space each? Our current setup consists of 3 master nodes, and 4 data/ingest nodes with large amounts of disk size each. Our daily indices are roughly 400GB, and so we have 8 primary shards to obtain shard size of ~50GB, we also have 1 replica per shard. How is performance affected by the data node count? Our for 4 data/ingest nodes have large disks, we are thinking about doubling up the number of data nodes but halving their disk size. IS this beneficial?",
    "website_area": "discuss"
  },
  {
    "id": "f8ca15eb-585c-447a-b8b8-81ec43b584cf",
    "url": "https://discuss.elastic.co/t/how-to-potentially-flatten-a-nested-document-design/214650",
    "title": "How to potentially flatten a nested document design?",
    "category": [
      "Elasticsearch"
    ],
    "author": "cgremakes",
    "date": "January 10, 2020, 6:36pm January 13, 2020, 1:48pm January 13, 2020, 3:34pm",
    "body": "I am probably like many people that have more of a SQL background, and am attempting to branch out into using Elasticsearch. I am attempting to make a design choice that I'm hoping I can get some input on. We have a product database of around 50K SKU's (with a need to scale). We are a B2B wholesaler, so pricing varies by customer and by branch. From what I understand, nested documents have potential performance issues, so I'm considering if it would be better to flatten it and use the key to make the pricing unique? For example, I can do the following structure: product : { \"...\", \"...\", \"...\", \"attributes\": { \"price_<customer id>_<branch>\": <price> } } Would that likely be better than using nested documents to maintain the unique pricing for each customer/branch? It will always be a 1 to 1 mapping. Pricing doesn't change a ton, but it would need a periodic update as price changes come in.",
    "website_area": "discuss"
  },
  {
    "id": "dfd78332-f691-400a-8ee4-5cc44f82b6bc",
    "url": "https://discuss.elastic.co/t/elasticsearch-ngram-tokenizer/214398",
    "title": "Elasticsearch ngram tokenizer",
    "category": [
      "Elasticsearch"
    ],
    "author": "mirec",
    "date": "January 9, 2020, 9:51am January 9, 2020, 4:24pm January 13, 2020, 2:47pm January 13, 2020, 3:12pm",
    "body": "Hi, [Elasticsearch version 6.7.2] I am trying to index my data using ngram tokenizer but sometimes it takes too much time to index. What I am trying to do is to make user to be able to search for any word or part of the word. So if I have text - This is my text - and user writes \"my text\" or \"s my\", that text should come up as a result. using ngram tokenizer worked for me and it seemes like it is doing what I want but sometimes I have way too long text. because it is a description of something and it might be as long as it want to be (even 10000 chars) indexing this has to be a pain for elasticsearch I guess but I need it to be indexed the way I described. This is my init settings { \"settings\": { \"index\": { \"blocks\": {\"read_only_allow_delete\": \"false\"}, \"max_ngram_diff\": 150, \"number_of_shards\": 3, \"number_of_replicas\": 2 }, \"analysis\": { \"filter\":{ \"synonym\":{ \"type\":\"synonym\", \"synonyms_path\":\"thesaurus.conf\" } }, \"tokenizer\": { \"my_tokenizer\": { \"type\": \"ngram\", \"min_gram\": 2, \"max_gram\": 40 } }, \"analyzer\": { \"my_analyzer_lowercase\": { \"tokenizer\": \"my_tokenizer\", \"filter\": [ \"lowercase\", \"synonym\" ] }, \"my_analyzer_case_sensitive\": { \"filter\":[ \"synonym\" ], \"tokenizer\": \"my_tokenizer\" } } } }, \"mappings\": { \"modules\": { \"properties\": { \"module\": { \"type\": \"text\", \"analyzer\": \"my_analyzer_lowercase\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"organization\": { \"type\": \"text\", \"analyzer\": \"my_analyzer_lowercase\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"argument\": { \"type\": \"text\", \"fields\": { \"sensitive\": { \"type\": \"text\", \"analyzer\": \"my_analyzer_case_sensitive\" }, \"lowercase\": { \"type\": \"text\", \"analyzer\": \"my_analyzer_lowercase\" }, \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"description\": { \"type\": \"text\", \"fields\": { \"sensitive\": { \"type\": \"text\", \"analyzer\": \"my_analyzer_case_sensitive\" }, \"lowercase\": { \"type\": \"text\", \"analyzer\": \"my_analyzer_lowercase\" }, \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } } } So is there any better way how to do this? Would it hep to upgrade to new version to make the indexing faster? Also I am getting max_ngram_diff is too big warning. Which I ignored so far. Any suggestions and help is appreciated especialy why it failes to index long texts. I am indexing it using python library for key in yindexes: for success, info in parallel_bulk(es, yindexes[key], thread_count=int(threads), index='yindex', doc_type='modules', request_timeout=40): if not success: LOGGER.error('A elasticsearch document failed with info: {}'.format(info)) If I change the request_timeout to 300 then it will keep trying and crash the application. And again this happens only with long descritption texts Thank you Miroslav Kovac",
    "website_area": "discuss"
  },
  {
    "id": "a44ae3e7-2509-44d6-aa55-a06ce5e6fff4",
    "url": "https://discuss.elastic.co/t/about-the-kibana-category/22",
    "title": "About the Kibana category",
    "category": [
      "Kibana"
    ],
    "author": "Leslie_Hawthorn",
    "date": "April 22, 2015, 3:36pm November 25, 2016, 9:22pm July 6, 2017, 1:31pm",
    "body": "All things about visualizing data in Elasticsearch & Logstash, including how to use Kibana and extending the platform.",
    "website_area": "discuss"
  },
  {
    "id": "c9e38263-49b4-4a73-9cec-09d0e0650fd7",
    "url": "https://discuss.elastic.co/t/xpack-monitoring-issue/214312",
    "title": "XPack Monitoring Issue",
    "category": [
      "Kibana"
    ],
    "author": "aniaks",
    "date": "January 8, 2020, 10:12pm January 9, 2020, 4:55pm January 22, 2020, 3:25pm January 22, 2020, 4:23pm",
    "body": "In Kibana when i try to enable to monitoring i am getting the following error \" We checked the cluster defaults settings and found that xpack.monitoring.enabled is set to false set, which disables monitoring. Removing the xpack.monitoring.enabled: false setting from your configuration will put the default into effect and enable Monitoring.\" However when i get the cluster settings i am seeing that xpack is enabled { \"persistent\" : { \"xpack\" : { \"monitoring\" : { \"elasticsearch\" : { \"collection\" : { \"enabled\" : \"true\" } }, \"collection\" : { \"enabled\" : \"true\" } } } }, \"transient\" : { } }",
    "website_area": "discuss"
  },
  {
    "id": "a13408ff-a827-4b68-bf47-aceea03f4a80",
    "url": "https://discuss.elastic.co/t/losing-a-session-in-a-canvas/216095",
    "title": "Losing a session in a Canvas",
    "category": [
      "Kibana"
    ],
    "author": "Dennis_Rietvink",
    "date": "January 22, 2020, 4:10pm",
    "body": "Our Kibana has a canvas screen showing the uptime of various infrastructure components. Access to the portal is attained through single sign on handled by Keycloak OIDC. When we minimize the screen or go to other tabs and go back later the session is gone. The data feed has ended and warnings are shown. If we refresh the page Keycloak is shown. Do you know why this is happening since the session should be active for the lifetime of the browser window How can we ensure a session is kept alive for canvasses and dashboards? When these kind of timeouts happen how to automatically reload the page?",
    "website_area": "discuss"
  },
  {
    "id": "88f89c54-64ba-4649-8e19-e8c85056d8d5",
    "url": "https://discuss.elastic.co/t/single-logout-using-kibana-and-keycloak/216094",
    "title": "Single Logout using Kibana and Keycloak",
    "category": [
      "Kibana"
    ],
    "author": "Dennis_Rietvink",
    "date": "January 22, 2020, 4:09pm",
    "body": "We are integrating Kibana in our Keycloak identity management solution but have problems getting single logout working when triggered from another client. This is the scenario: User enters the portal but has to login in Keycloak first Keycloak handles authentication and redirects back to portal In the portal is a link to the Kibana dashboard and the user clicks it Kibana does OIDC single sign on with keycloak and the dashboard is presented User goes back to portal and clicks on logout in the portal Keycloak logoff is called and the portal session is gone The Kibana session with the user still exists In the normal situation the Single Logoff scenario would mean that the Keycloak server calls the Kibana logoff endpoint with the session-id used for single sign on. Does this work for Kibana? Do you have examples for Single or Global Logoff and Kibana where the action is performed from server to server.",
    "website_area": "discuss"
  },
  {
    "id": "d65bb089-e705-4364-b52b-e92cef6cf780",
    "url": "https://discuss.elastic.co/t/issue-in-graph/216020",
    "title": "Issue in graph",
    "category": [
      "Kibana"
    ],
    "author": "volcano",
    "date": "January 22, 2020, 9:33am January 22, 2020, 9:50am January 22, 2020, 9:58am January 22, 2020, 10:09am January 22, 2020, 10:22am January 22, 2020, 10:19am January 22, 2020, 11:26am January 22, 2020, 3:34pm January 22, 2020, 3:50pm",
    "body": "I need a simple graph. my search query is returning two fields, filed-1 and field-2 I want to plot field-1 in X-Axis and field-2 in Y-Axis. I have checked Visualization graphs i.e Line graph , Area graph , Vertical graph where I can not select field for Y-axis.....there is only metrics. How do I plot of X-Y axis fields graph then ?",
    "website_area": "discuss"
  },
  {
    "id": "e8a96ae6-faaf-46fc-8c72-d0311dd54d1b",
    "url": "https://discuss.elastic.co/t/change-the-daily-start-end-time-to-5am-instead-of-00-00/215957",
    "title": "Change the daily start/end time to 5am, instead of 00:00",
    "category": [
      "Kibana"
    ],
    "author": "Learning",
    "date": "January 21, 2020, 9:46pm January 22, 2020, 3:29pm",
    "body": "We have a processing facility here that runs an afternoon and night shift. When viewing the daily performance, we look at values from 5am (day 0) to 5am (day 1). Is there a way I can enable this in Kibana and more importantly change my 30-day (daily) chart to show daily values (but from 5am to 5am)?? Help please...",
    "website_area": "discuss"
  },
  {
    "id": "3d0f518e-aaf4-4559-97e0-129e1eee2ccd",
    "url": "https://discuss.elastic.co/t/can-we-get-the-result-column-1-multiply-or-divide-by-another-column-2-into-ew-column-in-visulization/215931",
    "title": "Can we get the result column 1 multiply or divide by another column 2 into ew column in visulization",
    "category": [
      "Kibana"
    ],
    "author": "anjilinga",
    "date": "January 21, 2020, 5:31pm January 22, 2020, 5:06am January 22, 2020, 11:59am January 22, 2020, 3:08pm",
    "body": "If data table in visulaization is having column 1 and column 2, can we display in column 3 as column 1 multiply or divide by column 2",
    "website_area": "discuss"
  },
  {
    "id": "db707cc0-7fa3-41ba-bde2-c154ab2dbb82",
    "url": "https://discuss.elastic.co/t/kibana-4-6-6-where-are-development-tools/216084",
    "title": "KIbana 4.6.6: Where are Development tools?",
    "category": [
      "Kibana"
    ],
    "author": "nicolasduminil",
    "date": "January 22, 2020, 3:08pm",
    "body": "Greetings, I'm using Kibana 4.6.6. The documentation refers to the Development Tolls menu option, but I don't find any in the console. Where is it ? Many thanks in advance. Nicolas",
    "website_area": "discuss"
  },
  {
    "id": "dd13dfcd-6572-4c7a-b3e2-fe4d5d217616",
    "url": "https://discuss.elastic.co/t/geo-point-property-timestamp-is-not-displaying-in-tooltip/216052",
    "title": "Geo_point property (timestamp) is not displaying in tooltip",
    "category": [
      "Kibana"
    ],
    "author": "Suren_ds",
    "date": "January 22, 2020, 1:54pm",
    "body": "Hello experts, I am a newbie to Elastic Maps. I have 3 dimension multiple geo_points i.e. lat, lon & timestamp. I would like to plot the co-ords in Maps and timestamp to be displayed in tool tip. Somehow i could not get the timestamp in tooltip. I don't know want is wrong. pls help Elastic version : 7.5.1 Kibana version : 7.5.1 Elastic Mapping: \"route\": { \"properties\": { \"coords\": { \"type\": \"geo_point\" }, \"timestamp\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } Sample data: { \"route\": [ { \"coords\": [ 132.35673166666666, 34.381421666666675 ], \"timestamp\": \"1574058343449148\" }, { \"coords\": [ 132.35688, 34.38138 ], \"timestamp\": \"1574058344444382\" }, { \"coords\": [ 132.35702833333335, 34.38133666666667 ], \"timestamp\": \"1574058345443957\" } ] } In tool tip: -------------",
    "website_area": "discuss"
  },
  {
    "id": "1d39da91-682f-4f2e-894b-2f151670a358",
    "url": "https://discuss.elastic.co/t/unable-to-connect-to-elasticsearch-error-request-timeout-after-30000ms/216054",
    "title": "Unable to connect to Elasticsearch. Error: Request Timeout after 30000ms",
    "category": [
      "Kibana"
    ],
    "author": "111249",
    "date": "January 22, 2020, 1:49pm",
    "body": "(topic withdrawn by author, will be automatically deleted in 24 hours unless flagged)",
    "website_area": "discuss"
  },
  {
    "id": "47fafb8c-65fd-43f4-8a26-1b8745d5135b",
    "url": "https://discuss.elastic.co/t/kibana-visualization-search-only-the-latest-value/215594",
    "title": "Kibana visualization search only the latest value",
    "category": [
      "Kibana"
    ],
    "author": "Ashutosh_Pachisia",
    "date": "January 19, 2020, 4:50am January 19, 2020, 12:26pm January 20, 2020, 12:19pm January 20, 2020, 9:12pm January 22, 2020, 12:00pm",
    "body": "Is there any way in kibana to search data only of the latest timestamp I have a dataset that repeats every (10min-15 min ) not fixed interval with a different set of values I want the historic data as well as the latest data in the visualization . Please help Eg : @timestamp | locations | status @now. ABC. 1 @now. DEF. 2 @now-5min. ABC. 4 @now-5min. DEF. 3 Final output @timestamp | locations | status @now. ABC. 1 @now. DEF. 2",
    "website_area": "discuss"
  },
  {
    "id": "38624f51-d3f6-40d0-a580-76c63dec4d64",
    "url": "https://discuss.elastic.co/t/unable-to-load-image-into-my-dashboard/215886",
    "title": "Unable to load image into my dashboard",
    "category": [
      "Kibana"
    ],
    "author": "prasannarajan",
    "date": "January 21, 2020, 1:32pm January 22, 2020, 11:56am",
    "body": "Hi, im trying to use markdown in kibana to load image from my local drive. This is the syntax im using. Im uable to load .Why?",
    "website_area": "discuss"
  },
  {
    "id": "aadb1a65-760d-4b0e-b4eb-1b618e4c3fba",
    "url": "https://discuss.elastic.co/t/coordinate-map-with-additional-layers-provided-by-elasticsearch/215997",
    "title": "Coordinate map with additional layers provided by elasticsearch",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 22, 2020, 6:52am January 22, 2020, 11:54am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "3c0989ce-9aa1-4b61-a08c-2e99befc7651",
    "url": "https://discuss.elastic.co/t/links-status-based-on-sysloghost-and-message/215544",
    "title": "Links status based on sysloghost and message",
    "category": [
      "Kibana"
    ],
    "author": "mushfiq",
    "date": "January 18, 2020, 5:19am January 20, 2020, 9:05am January 20, 2020, 10:36am January 21, 2020, 10:29am January 21, 2020, 11:03am January 22, 2020, 7:43am January 22, 2020, 8:40am January 22, 2020, 10:17am January 22, 2020, 10:44am",
    "body": "Hi, I am new ELK stack & started testing it from last week. I must say, I just love it. What I Have Basic SOHO dual WAN routers, first link leased line and second link broadband (no static IP) These routers support syslog. Example logs that appears on Kibana are below NSD FAIL WAN[2] (WAN1 is Leased line and WAN2 is Broadband) NSD SUCCESS WAN[2] What I did Installed and configured ELK stack and Rsyslog on single Ubuntu server. Redirected routers syslogs to this server. Configured filter to drop unwanted logs. What I want Kibana dashboard where I can monitor both the links, based on \"sysloghost\" unique entries and it's log message (logs example above) I tried to search but no luck, need help to accomplish this.",
    "website_area": "discuss"
  },
  {
    "id": "131de3b6-7b28-4a77-b046-9d0ff540095c",
    "url": "https://discuss.elastic.co/t/how-can-i-add-previous-results-in-a-line-chart-to-new-results/215762",
    "title": "How can I add previous results in a line chart to new results",
    "category": [
      "Kibana"
    ],
    "author": "111249",
    "date": "January 20, 2020, 3:04pm January 20, 2020, 5:23pm January 21, 2020, 9:47am January 22, 2020, 10:41am January 22, 2020, 10:41am",
    "body": "Hello. For example, I have a graph that shows the number of products added to the supermarket. I added 20 products in the first hour, 5 products in the second hour and 8 products in the third hour. I want to combine each point with the results of the previous points. First point must be 20 (for the first hour), second point must be 20 + 5 = 25 (for the second hour) and third point must be 20 + 5 + 8 = 32 (for the third hour). How can I do this? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "a9b3be88-66f8-495d-b236-4fc5647e5d6b",
    "url": "https://discuss.elastic.co/t/kibana-failed-to-load-indices/216034",
    "title": "Kibana Failed to load indices",
    "category": [
      "Kibana"
    ],
    "author": "Christophe_Journel",
    "date": "January 22, 2020, 10:38am",
    "body": "Hello. When i upgraded Kibana ( and the whole elk stack from 7.4.1 to 7.5.1), everything went fine. Except that now i cannot create a new index pattern The developper console shows: Capture175055 9.96 KB What can i do ? I already tried to execute this command in dev tool The first one: 2221622453 35.2 KB And another command: 33331391199 17.5 KB Thanks for yor help",
    "website_area": "discuss"
  },
  {
    "id": "3a19d821-bb76-485e-9eaf-27eb1a85a3d9",
    "url": "https://discuss.elastic.co/t/kibana-multitenancy/215982",
    "title": "Kibana Multitenancy",
    "category": [
      "Kibana"
    ],
    "author": "jaydengre",
    "date": "January 22, 2020, 5:00am January 22, 2020, 10:29am",
    "body": "Hi, we have Kibana Standard distribution hosted locally with elasticsearch service hosted on elastic cloud. Also we are using Auth0 as Identity Provider with SAML. Now, we have created roles and role-mappings according to users logging in, so that they have access to separate indices. Now what we want to achieve is - Set up landing page as default dashboard. But it should show data according to index permitted to logged in user. User from other role mapping should not see index patterns, visualizations, reports or dashboards from other users. How to achieve this? We have set <kibana.defaultAppId: \"dashboard/\"/> But it obviously shows same dashboard for all users and shows \"No Results Found\" for those who does not have permission to that specific index with which the visualizations in dashboard were built.",
    "website_area": "discuss"
  },
  {
    "id": "967038c6-760e-4e81-ad1c-9be56ca8fd15",
    "url": "https://discuss.elastic.co/t/ansible-role-for-kibana-2/215725",
    "title": "Ansible role for kibana #2",
    "category": [
      "Kibana"
    ],
    "author": "bernhard.fluehmann",
    "date": "January 20, 2020, 11:59am January 20, 2020, 2:11pm January 22, 2020, 8:14am",
    "body": "Please provide a official Ansible role for Kibana. This was already discussed here: Ansible role for Kibana. Regards Bernhard",
    "website_area": "discuss"
  },
  {
    "id": "feebdab6-8261-445e-a976-9e7939c0b226",
    "url": "https://discuss.elastic.co/t/continuous-failure-for-report-generation/215671",
    "title": "Continuous failure for report generation",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 20, 2020, 6:48am January 20, 2020, 12:05pm January 21, 2020, 7:04am January 21, 2020, 11:29am January 22, 2020, 7:00am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "eb3e4350-40ba-438c-b347-cc2a3d7f3e70",
    "url": "https://discuss.elastic.co/t/failed-to-reporting-csv-in-kibana-kibana-down/215998",
    "title": "Failed to Reporting CSV in Kibana, kibana down",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 22, 2020, 6:54am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "3ba434d7-e41f-4100-9ad6-f2b932ad2c7d",
    "url": "https://discuss.elastic.co/t/how-to-sum-all-numeric-values-of-a-particular-field/215838",
    "title": "How to sum all numeric values of a particular field",
    "category": [
      "Kibana"
    ],
    "author": "Kashif",
    "date": "January 21, 2020, 8:40am January 21, 2020, 10:49am January 22, 2020, 6:04am January 22, 2020, 6:10am",
    "body": "Hi, I configured the AWS Appstream log in ELK ( 7.5.1 ) I want to make visualization of two filed. user_id and session_duration_in_seconds fields example user_id kashif session_duration_in_seconds 300 How can I make a visualization that it will show the the sum of all values ( session_duration_in_seconds ) for that user according to selected time.",
    "website_area": "discuss"
  },
  {
    "id": "b65a56d1-e5e7-49d8-b96b-5ffbcd287e2b",
    "url": "https://discuss.elastic.co/t/custom-menu-bar-creation/215988",
    "title": "Custom menu bar creation",
    "category": [
      "Kibana"
    ],
    "author": "prasannarajan",
    "date": "January 22, 2020, 5:50am",
    "body": "Hi, Im trying to create a new icon in the menu bar . Mind if i get the steps of process of doing it... Thanks",
    "website_area": "discuss"
  },
  {
    "id": "e084b227-e253-40a5-a38b-f58ee85fe0f4",
    "url": "https://discuss.elastic.co/t/region-map-not-showing/214182",
    "title": "Region Map not showing",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 20, 2020, 10:30am January 8, 2020, 3:54pm January 9, 2020, 4:13am January 9, 2020, 4:34pm January 10, 2020, 2:41am January 10, 2020, 3:17am January 11, 2020, 12:24am January 11, 2020, 11:26am January 14, 2020, 12:35am January 14, 2020, 12:46am January 14, 2020, 12:50am January 14, 2020, 7:18am January 16, 2020, 12:34am January 16, 2020, 6:21am January 21, 2020, 8:31am January 21, 2020, 8:32am January 21, 2020, 10:10am January 21, 2020, 3:18pm January 22, 2020, 5:37am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "164fb683-3834-4498-8355-dab31a0f46de",
    "url": "https://discuss.elastic.co/t/numbers-on-yaxis-in-timelion-chart-is-not-getting-formatted-as-configured-in-kibana-settings/215571",
    "title": "Numbers on yaxis in Timelion chart is not getting formatted as configured in kibana settings",
    "category": [
      "Kibana"
    ],
    "author": "ajith198",
    "date": "January 18, 2020, 5:52pm January 21, 2020, 12:33am January 21, 2020, 2:45am January 21, 2020, 11:56pm",
    "body": "I am new to Kibana and using Kibana v 7.5.0. In index pattern, I have configured impressions field as number (0,0) but in timelion chart I created, its not showing the commas in the values whereas in other bar charts, numbers are shown with commas. Below is the script I used. .es(index=impressions-*,q='device.keyword:firetv', metric=sum:impressions,timefield='date').lines(width=2).color(color=#e6fff5).label(label='Fire TV').yaxis() Please find below the screenshot of the timelion chart. image941709 17.1 KB Any help to resolve the issue is much appreciated. Let me know if any additional information required.",
    "website_area": "discuss"
  },
  {
    "id": "e90bee66-2a9b-4d0b-a7ed-2dfaa41feb60",
    "url": "https://discuss.elastic.co/t/tsvb-markdown-dates/215916",
    "title": "TSVB markdown Dates",
    "category": [
      "Kibana"
    ],
    "author": "Mack_rogers",
    "date": "January 21, 2020, 9:28pm",
    "body": "Has anyone solved rendering dates in TSVB visualization? ( Time Series Visual Builder) Any field that was inputted as type: \"date\" with multiple random dates values. I only want 1 date shown just like the text field \"id\" is shown. @Elvis_Saravia @Alex_Piggott markdown problem32561422 203 KB markdown232521380 525 KB",
    "website_area": "discuss"
  },
  {
    "id": "becfb315-e29f-4a89-856a-adaa6f3465c3",
    "url": "https://discuss.elastic.co/t/where-does-the-page-load/215629",
    "title": "Where does the page load?",
    "category": [
      "Kibana"
    ],
    "author": "mr_searchng",
    "date": "January 19, 2020, 4:23pm January 20, 2020, 11:10am January 21, 2020, 5:47pm January 21, 2020, 5:47pm",
    "body": "http://localhost:5601/oki/app/kibana",
    "website_area": "discuss"
  },
  {
    "id": "c792d865-d2fd-4f7b-a5d2-a6e1990ec9ba",
    "url": "https://discuss.elastic.co/t/hoow-to-use-range-or-filter-aggragations-in-transforms/215755",
    "title": "Hoow to use range or filter aggragations in transforms",
    "category": [
      "Kibana"
    ],
    "author": "anjilinga",
    "date": "January 20, 2020, 2:38pm January 21, 2020, 8:04am January 21, 2020, 5:27pm",
    "body": "In Transforms, range and filter aggragations are not supported. can some one please suggest how to use these aggregations in transforms.",
    "website_area": "discuss"
  },
  {
    "id": "8c05a93c-94ee-4e5d-ba48-a9bf176a815f",
    "url": "https://discuss.elastic.co/t/cant-resolve-ui-registry-vis-types-7-5/215056",
    "title": "Can't resolve `ui/registry/vis_types` [7.5]",
    "category": [
      "Kibana"
    ],
    "author": "J_Gil",
    "date": "January 14, 2020, 10:41pm January 21, 2020, 1:39pm January 21, 2020, 5:13pm",
    "body": "According to the current (7.5) documentation for \"Developing Visualizations\": import { VisFactoryProvider } from 'ui/vis/vis_factory'; import { VisTypesRegistryProvider } from 'ui/registry/vis_types'; However, after generating a plugin via scripts/generate_plugin.js... and running yarn start, I get the error: Module not found: Error: Can't resolve 'ui/registry/vis_types' This appears to be caused by the removal of this file between the 7.4 -> 7.5 branch, which is from where the previous version of my visualization pulled VisTypesRegistryProvider. What am I doing wrong here?",
    "website_area": "discuss"
  },
  {
    "id": "65fe70b9-a579-413d-b3c1-1351420caa47",
    "url": "https://discuss.elastic.co/t/data-table-using-key-vs-key-as-string/215802",
    "title": "Data table using \"key\" vs. \"key_as_String\"",
    "category": [
      "Kibana"
    ],
    "author": "jfcantu",
    "date": "January 20, 2020, 10:04pm January 21, 2020, 5:56pm January 21, 2020, 4:54pm",
    "body": "Hi all, I'm having the same problem referenced in this thread: Kibana Data Table Date Histogram Aggregation Useless labels - namely, when I split a table and use a Date Histogram aggregation, the column labels appear in epoch time rather than a human-readable format, even when I explicitly specify format. Although, inspecting the response data, I think I see the problem - the formatted date is returned in key_as_string, but key is still in epoch time. I'm assuming that Kibana is using key as the column label rather than key_as_string. Is this intentional, and/or is there a way to force Kibana to use key_as_string?",
    "website_area": "discuss"
  },
  {
    "id": "76c08158-f504-471a-904f-4dd7a01f6c79",
    "url": "https://discuss.elastic.co/t/the-request-rate-makes-me-confused/215909",
    "title": "The Request Rate makes me confused",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 21, 2020, 3:45pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "cfb1163c-3210-43b9-af83-f507b2e86950",
    "url": "https://discuss.elastic.co/t/horizontal-bar-chart-label-location/215749",
    "title": "Horizontal bar chart label location",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 20, 2020, 3:29pm January 20, 2020, 3:47pm January 21, 2020, 12:02pm January 21, 2020, 2:57pm January 21, 2020, 2:57pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "fc5f8d4f-58d3-4efa-a718-80f5947b1268",
    "url": "https://discuss.elastic.co/t/kibana-user-login/215900",
    "title": "Kibana User Login",
    "category": [
      "Kibana"
    ],
    "author": "lquin1978",
    "date": "January 21, 2020, 2:45pm",
    "body": "Running ELK 7.5 and I want to enforce login for users, is this aspect free in X-pack or does this functionality require payment?",
    "website_area": "discuss"
  },
  {
    "id": "a3c80991-05b2-49bd-a588-c4c767fbbe16",
    "url": "https://discuss.elastic.co/t/kibana7-5-kibana-plugin-generator/215893",
    "title": "[kibana7.5] Kibana plugin generator",
    "category": [
      "Kibana"
    ],
    "author": "kwang_hee_Han",
    "date": "January 21, 2020, 1:59pm",
    "body": "Hello, I use Kibana 7.5 and make kiban plugin using \" [kbn-plugin-generator] . (https://github.com/elastic/kibana/tree/master/packages/kbn-plugin-generator) I checked manual kibana 7.5 - visualization factory. https://www.elastic.co/guide/en/kibana/current/development-visualization-factory.html I registered using 'ui/registry/vis_types' which mentioned above documentation. But, when I install my plugin to production site kibana7.5. I got a error. \"ui/registry/vis_types\" not found I check github source code 7.5 not exsits but \"ui/registry/_registry\" is exists.(Not exsits registry/vis_types!). I think I should use \"ui/registry/_registry\" but I don't know how to using them instead ui/registry/vis_types. import './index.less'; import { cvpComponent } from './cvp_plugin_vis_controller'; import { VisFactoryProvider } from 'ui/vis/vis_factory'; import { Schemas } from 'ui/vis/editors/default/schemas'; import { VisTypesRegistryProvider } from 'ui/registry/vis_types'; // register the provider with the visTypes registry VisTypesRegistryProvider.register(cvpCalcVisProvider); function cvpCalcVisProvider(Private, i18n) { const VisFactory = Private(VisFactoryProvider); return VisFactory.createReactVisualization({ type: 'metric', name: 'CVP-Calculator', title: 'CVP-Calculator', icon: 'visMetric', description: 'CVP Project caculator for compound`s intersection', visConfig: { component: cvpComponent, defaults: { type: 'metric', addTooltip: true, addLegend: false, metric: { percentageMode: false, useRange: false, colorSchema: 'Green to Red', metricColorMode: 'None', colorRange: [{ from: 0, to: 10000 }], labels: { show: true, }, invertColors: false, style: { bgFill: '#000', bgColor: false, labelColor: false, subText: '', fontSize: 60, }, }, }, }, editorConfig: { schemas: new Schemas([ { group: 'metrics', name: 'metric', title: i18n('metricVis.schemas.metricTitle', { defaultMessage: 'Metric' }), min: 1, aggFilter: [ '!std_dev', '!geo_centroid', '!derivative', '!serial_diff', '!moving_avg', '!cumulative_sum', '!geo_bounds', ], defaults: [{ type: 'count', schema: 'metric' }], }, { group: 'buckets', name: 'group', title: i18n('metricVis.schemas.splitGroupTitle', { defaultMessage: 'Split Group' }), min: 1, max: 3, aggFilter: ['!geohash_grid', '!filter'], }, ]), }, }); }",
    "website_area": "discuss"
  },
  {
    "id": "a9088c13-e1bb-42a0-a066-6e2512a8d647",
    "url": "https://discuss.elastic.co/t/use-different-columns-for-valuecolumn-and-filtercolumn-in-a-dropdown-filter/215890",
    "title": "Use different columns for valueColumn and filterColumn in a Dropdown Filter",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 21, 2020, 1:55pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e0b30848-cf6b-4a05-9493-83a4beb1833d",
    "url": "https://discuss.elastic.co/t/how-to-plot-vertical-graph/215872",
    "title": "How to plot vertical graph?",
    "category": [
      "Kibana"
    ],
    "author": "volcano",
    "date": "January 21, 2020, 1:45pm",
    "body": "I have a search query which returns request time and elapsed time. Now I want to plot a vertical bar graph with x-axis as request time and y-axis as elapsed time In Kibana , I opened Visualization > Vertical Bar and I get this How do I select my fields here ? I am stuck at this part.",
    "website_area": "discuss"
  },
  {
    "id": "ae44c642-3dd4-4947-97c6-5f0b67c8cb5b",
    "url": "https://discuss.elastic.co/t/last-refreshed-date-and-time/215863",
    "title": "Last refreshed Date and time",
    "category": [
      "Kibana"
    ],
    "author": "balaji_18",
    "date": "January 21, 2020, 10:56am January 21, 2020, 1:43pm",
    "body": "Is there any way to display the Last refreshed Date and Time of Data reloaded in kibana. For example: Last refreshed on : 20-01-2020 04.30.00 PM Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "0c94faaf-300e-45d9-920b-8ef73609ccf3",
    "url": "https://discuss.elastic.co/t/cvs-export/215859",
    "title": "CVS export",
    "category": [
      "Kibana"
    ],
    "author": "dao",
    "date": "January 21, 2020, 10:33am January 21, 2020, 11:02am January 21, 2020, 1:10pm",
    "body": "Hello, I have to export a saved search from a dashboard. The CSV is correct but the format of the @timestamp is not OK for my customer because excel does not accept 2020-01-21T07:52:51.000Z which is the format I get I tried to change the format in the GeneralSettings / Date format without effect thanks",
    "website_area": "discuss"
  },
  {
    "id": "ba49dd18-629d-46d6-abfd-d212cd8ee5c6",
    "url": "https://discuss.elastic.co/t/kibiana-time-range-and-interval-is-confusing/215598",
    "title": "Kibiana time range and interval is confusing",
    "category": [
      "Kibana"
    ],
    "author": "_App",
    "date": "January 19, 2020, 5:39am January 21, 2020, 1:02pm",
    "body": "Screen Shot 2020-01-19 at 2.34.26 PM1350698 57.3 KB first, I set time range to 2month ago to 2020-1-19 (which is now) However the graph start at 2019-10-31 (why?) and the last datapoint in the graph is 2019-12-21 (why?) How can I see the data for 28 days from the 28 days ago, and 28*2 days ago?",
    "website_area": "discuss"
  },
  {
    "id": "d2d38c4b-6452-4fd5-9c27-f5291778d1e0",
    "url": "https://discuss.elastic.co/t/kibana-virtualization-question-nested-documents-wont-aggregate/215801",
    "title": "Kibana virtualization question - nested documents won't aggregate",
    "category": [
      "Kibana"
    ],
    "author": "Chris_Belfield",
    "date": "January 20, 2020, 9:57pm January 21, 2020, 9:22am January 21, 2020, 9:40am January 21, 2020, 10:02am January 21, 2020, 10:20am January 21, 2020, 10:30am",
    "body": "Why can't I aggregate studies in this data? I'm ommitting other data which seems find to aggregate, but something in this dict / list I can not aggregate. I can search through the data with kquery, but I can't create charts etc. E.g. how many studies per account Screenshot from 2020-01-20 21-55-106241180 73.8 KB",
    "website_area": "discuss"
  },
  {
    "id": "4385df9f-74d1-4c98-af63-0eb1fd8c23c3",
    "url": "https://discuss.elastic.co/t/7-5-reporting-fails-randomly/215160",
    "title": "7.5 [reporting] fails randomly",
    "category": [
      "Kibana"
    ],
    "author": "dao",
    "date": "January 15, 2020, 3:01pm January 17, 2020, 3:35am January 17, 2020, 8:43am January 21, 2020, 10:27am",
    "body": "Hello, my users want to create some reports but the completion/fail is erratic. I am running on a debian 9 Could not find more hint to solve that. Can you help, please? best the message in the frontend: Unable to generate report Failed to decrypt report job data. Please ensure that xpack.reporting.encryptionKey is set and re-generate this report. Error: Unsupported state or unable to authenticate data My configuration (kibana.yml) xpack.reporting.encryptionKey: \"XXXEBHAIJXXXXXXXXXXXJESMXXXX79\" xpack.reporting.kibanaServer.port: 443 xpack.reporting.kibanaServer.protocol: https xpack.reporting.kibanaServer.hostname: mysubdomain.flightwatching.com xpack.reporting.index: \".reporting-mysubdomain\" xpack.reporting.capture.maxAttempts: 10 xpack.reporting.capture.loadDelay: 10000 The kibana.log: { \"type\":\"log\", \"@timestamp\":\"2020-01-15T13:53:42Z\", \"tags\":[\"reporting\",\"esqueue\",\"queue-worker\",\"warning\"], \"pid\":14559, \"message\": \"k5f3zsyv0b8f8bed02eycx6q - _claimPendingJobs encountered a version conflict on updating pending job k5fd8kpq0b8f8bed02au4mvk: Error: [version_conflict_engine_exception] [k5fd8kpq0b8f8bed02au4mvk]: version conflict, required seqNo [39], primary term [1]. current document has seqNo [41] and primary term [1], with { index_uuid=\\\"cPPJv-uMQx-kgFoblb_8-Q\\\" & shard=\\\"0\\\" & index=\\\".reporting-insight-val-2020.01.12\\\" } \\n at respond (/usr/share/kibana/node_modules/elasticsearch/src/lib/transport.js:349:15) \\n at checkRespForFailure (/usr/share/kibana/node_modules/elasticsearch/src/lib/transport.js:306:7) \\n at HttpConnector.<anonymous> (/usr/share/kibana/node_modules/elasticsearch/src/lib/connectors/http.js:173:7) \\n at IncomingMessage.wrapper (/usr/share/kibana/node_modules/elasticsearch/node_modules/lodash/lodash.js:4929:19) \\n at IncomingMessage.emit (events.js:194:15)\\n at endReadableNT (_stream_readable.js:1103:12) \\n at process._tickCallback (internal/process/next_tick.js:63:19)\"}",
    "website_area": "discuss"
  },
  {
    "id": "da9f83a7-2c56-4873-adda-d7222fe785fc",
    "url": "https://discuss.elastic.co/t/sample-data-kibana-sample-data-ecommerce-question/215411",
    "title": "Sample data (kibana_sample_data_ecommerce) Question",
    "category": [
      "Kibana"
    ],
    "author": "sung_il_Kim",
    "date": "January 17, 2020, 5:25am January 20, 2020, 4:30am January 21, 2020, 10:22am",
    "body": "Hello everyone. I'm a newbie in Korea. I've some questions about nested objects. Oh, I already know that Kibana doesn't support nested objects yet. But I'm curious about sample data - 'kibana_sample_data_ecommerce' on Elastic cloud There is a key (products) which has an array of the JSON type like below. { \"_index\": \"kibana_sample_data_ecommerce\", \"_type\": \"_doc\", \"_id\": \"IvjrrG8BD9puQIJ7dy9v\", \"_version\": 1, \"_score\": 0, \"_source\": { \"category\": [ \"Men's Clothing\" ], \"currency\": \"EUR\", \"customer_first_name\": \"Oliver\", \"customer_full_name\": \"Oliver Rios\", \"customer_gender\": \"MALE\", \"customer_id\": 7, \"customer_last_name\": \"Rios\", \"customer_phone\": \"\", \"day_of_week\": \"Monday\", \"day_of_week_i\": 0, \"email\": \"oliver@rios-family.zzz\", \"manufacturer\": [ \"Low Tide Media\", \"Elitelligence\" ], \"order_date\": \"2020-01-13T09:27:22+00:00\", \"order_id\": 565855, *\"products\"*: [ { \"base_price\": 20.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Low Tide Media\", \"tax_amount\": 0, \"product_id\": 19919, \"category\": \"Men's Clothing\", \"sku\": \"ZO0417504175\", \"taxless_price\": 20.99, \"unit_discount_amount\": 0, \"min_price\": 9.87, \"_id\": \"sold_product_565855_19919\", \"discount_amount\": 0, \"created_on\": \"2016-12-12T09:27:22+00:00\", \"product_name\": \"Shirt - dark blue white\", \"price\": 20.99, \"taxful_price\": 20.99, \"base_unit_price\": 20.99 }, { \"base_price\": 24.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Elitelligence\", \"tax_amount\": 0, \"product_id\": 24502, \"category\": \"Men's Clothing\", \"sku\": \"ZO0535205352\", \"taxless_price\": 24.99, \"unit_discount_amount\": 0, \"min_price\": 12.49, \"_id\": \"sold_product_565855_24502\", \"discount_amount\": 0, \"created_on\": \"2016-12-12T09:27:22+00:00\", \"product_name\": \"Slim fit jeans - raw blue\", \"price\": 24.99, \"taxful_price\": 24.99, \"base_unit_price\": 24.99 } ], \"sku\": [ \"ZO0417504175\", \"ZO0535205352\" ], \"taxful_total_price\": 45.98, \"taxless_total_price\": 45.98, \"total_quantity\": 2, \"total_unique_products\": 2, \"type\": \"order\", \"user\": \"oliver\", \"geoip\": { \"country_iso_code\": \"GB\", \"location\": { \"lon\": -0.1, \"lat\": 51.5 }, \"continent_name\": \"Europe\" } }, \"fields\": { \"order_date\": [ \"2020-01-13T09:27:22.000Z\" ], \"products.created_on\": [ \"2016-12-12T09:27:22.000Z\", \"2016-12-12T09:27:22.000Z\" ] } } but I found that \"products\" is not mapped to the nested type ... \"products\": { \"properties\": { \"_id\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"base_price\": { \"type\": \"half_float\" }, \"base_unit_price\": { \"type\": \"half_float\" }, \"category\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\" } } }, ... Even though that \"Products\" is not mapped to the nested type, there is a sample 'Cloud text' in Visualize named \"[eCommerce] Top Selling Products' which shows counting numbers of products and sorting it. and it returns the counted(aggregated) numbers of each product name very well. a111905918 78.1 KB Q1. As I know, it needs to be mapped to \"nested type\" to aggregate over the list. Am I wrong? but How does it works well? I've tried to put my new sample data like 'kibana_sample_data_ecommerce' using the elasticsearch API. (in dev tool) The data mapped automatically and everything is okay except visualize functions. I'm trying to show exactly the same Cloud text but it is not working properly. The thing what I expected is Shirt - dark blue white : 5 Slim fit jeans - raw blue : 2 but Kibana returns Shirt - dark blue white : 2 Slim fit jeans - raw blue : 2 I think it returns the record numbers of that index. Because nested object(\"products\") is not mapped properly. Also, I can see that docs count is \"2\" on my new sample data(in index management page) even if the products are 7 rows Of course, It works very well when I set \"nested type\" on that field, but Kibana is not supporting it Q. I'd like to show the cloud text chart using Kibana like 'kibana_sample_data_ecommerce. What should I do?",
    "website_area": "discuss"
  },
  {
    "id": "cf0d4843-abaf-4a83-90ae-ba5dac1ab59c",
    "url": "https://discuss.elastic.co/t/no-new-data-in-elasticsearch-after-upgrade-from-6-8-to-7-1/215731",
    "title": "No new data in elasticsearch after upgrade from 6.8 to 7.1",
    "category": [
      "Kibana"
    ],
    "author": "northy",
    "date": "January 20, 2020, 12:17pm January 21, 2020, 10:12am",
    "body": "Hi all, I have recently upgraded my elasticsearch cluster from version 6.8 to 7.1. Initially it seemed like the upgrade went well. Data continued to be indexed into elasticsearch and was available on kibana for the remainder of the day. However, the next morning the data suddenly stopped being indexed and hasn't appeared since. If I adjust the time period to the day of the upgrade and before, I can see data in discover. But nothing is appearing since then. What puzzles me is that data continued to be indexed for 12 - 14 hours after the upgrade, but then suddenly ceased the next day. I am fairly new to Elasticsearch and was looking to come here for some advise to see if I can find out the issue. Hope you can help, northy",
    "website_area": "discuss"
  },
  {
    "id": "47f39219-55b3-4b2b-8764-258a6b493db7",
    "url": "https://discuss.elastic.co/t/kibana-error-fetching-fields-forbidden/215851",
    "title": "Kibana \"error fetching fields\" forbidden",
    "category": [
      "Kibana"
    ],
    "author": "gerard1",
    "date": "January 21, 2020, 9:40am",
    "body": "I'm using version 7.5 on Elastic Cloud, and I've created a new user & role, to use instead of global admin \"elastic\" account, setting the permissions to read from the desired indexes. If that non-admin user access Kibana > Discovery BEFORE the admin \"elastic\" user, that error is triggered. Once the \"elastic\" user access Kibana > Discovery, the error goes away. This happens every 24h hours. I assume that some action is performed when accessing Discovery after a while, and the normal user doesn't have enough permissions to perform it, but I can't figure out what is it. I've also seen this error was reported here: Error fetching fields, but the solution doesn't make much sense to me.",
    "website_area": "discuss"
  },
  {
    "id": "ccd73314-1e41-4754-8a65-266b484cf841",
    "url": "https://discuss.elastic.co/t/filtering-on-nested-fields/215758",
    "title": "Filtering on nested fields",
    "category": [
      "Kibana"
    ],
    "author": "rgj",
    "date": "January 20, 2020, 2:42pm January 21, 2020, 8:55am January 21, 2020, 8:59am",
    "body": "I have a nested field called 'metadata' in my index with various fields inside. For some reason filtering in Kibana does not work. I have tried various data types but even 'exists' doesn't work correctly. Things show up in the dashboard nicely: image1705535 63.5 KB However, simply trying to filter by a certain value does not work: OjGyudMnNq1668530 138 KB What could I be doing wrong?",
    "website_area": "discuss"
  },
  {
    "id": "16aebb91-7d88-43e5-99ae-e2b849212eb9",
    "url": "https://discuss.elastic.co/t/kibana-error-client-request-error-parse-error/215638",
    "title": "Kibana error \"Client request error: Parse Error\",",
    "category": [
      "Kibana"
    ],
    "author": "Ramu_Kamath",
    "date": "January 19, 2020, 5:58pm January 20, 2020, 11:18am January 20, 2020, 11:53am January 20, 2020, 11:54am January 20, 2020, 11:54am January 20, 2020, 11:57am January 21, 2020, 3:37am January 21, 2020, 8:38am",
    "body": "Hi All, I have installed es and Kibana on ec2 cluster. everything works fine including search. I have strange issue where I have one query runs in ES but if I run same query in Kibana I get following error. I looked to Kibana and es logs but doesn't get any error info there. Do you know what could be the issue and where to look? { \"message\": \"Client request error: Parse Error\", \"statusCode\": 502, \"error\": \"Bad Gateway\" }",
    "website_area": "discuss"
  },
  {
    "id": "4773168f-2426-4003-9fa5-77285cc03a85",
    "url": "https://discuss.elastic.co/t/how-to-build-a-vega-tree-using-csv-data-stored-in-my-index/215717",
    "title": "How to build a Vega tree using csv data stored in my index",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 21, 2020, 7:50am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "32123da6-1f3e-4605-b46f-fafd7992e5a4",
    "url": "https://discuss.elastic.co/t/dashboard-repository/215625",
    "title": "Dashboard repository?",
    "category": [
      "Kibana"
    ],
    "author": "spacecabbie",
    "date": "January 19, 2020, 3:04pm January 20, 2020, 1:29pm January 20, 2020, 11:24pm January 21, 2020, 7:45am",
    "body": "Are there any atm ? I saw a post from 2014 but that repo is dead. If not are there any services that make them ?",
    "website_area": "discuss"
  },
  {
    "id": "1b44f9dc-0f47-4813-b7b4-c23f8c58bb50",
    "url": "https://discuss.elastic.co/t/features-in-xpack-opensource-6-8/215787",
    "title": "Features in XPACK opensource 6.8",
    "category": [
      "Kibana"
    ],
    "author": "nirav.gshah",
    "date": "January 20, 2020, 6:26pm January 21, 2020, 6:40am January 21, 2020, 2:21am January 21, 2020, 6:39am",
    "body": "hi some can confirm which all features of XpACK are available in 6.8 ELK stack.",
    "website_area": "discuss"
  },
  {
    "id": "d7fd66e9-ab9f-4501-90ce-74c5c8fadc02",
    "url": "https://discuss.elastic.co/t/filter-issue/215707",
    "title": "Filter issue",
    "category": [
      "Kibana"
    ],
    "author": "prasannarajan",
    "date": "January 20, 2020, 9:59am January 20, 2020, 11:09am January 20, 2020, 12:34pm January 21, 2020, 6:08am",
    "body": "When i select my pie sector , it isnt getting filtered but when editing it gets filtered. When i put in the dashboard it doesnt get filtered... Any help pls.",
    "website_area": "discuss"
  },
  {
    "id": "ab9be0cb-e87d-41c8-ad45-e681b2e7cfdb",
    "url": "https://discuss.elastic.co/t/is-there-a-way-use-vega-lite-to-implement-controls-panels/215820",
    "title": "Is there a way use Vega Lite to implement controls panels?",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 21, 2020, 5:24am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "3a44a4ba-0c9c-4d2b-bbda-b6d4d5d28944",
    "url": "https://discuss.elastic.co/t/warning-migrations-unable-to-connect-to-elasticsearch-error-request-timeout-after-30000ms/215472",
    "title": "[warning][migrations] Unable to connect to Elasticsearch. Error: Request Timeout after 30000ms",
    "category": [
      "Kibana"
    ],
    "author": "mparp",
    "date": "January 17, 2020, 1:52pm January 17, 2020, 6:02pm January 20, 2020, 5:13pm January 20, 2020, 8:54pm January 21, 2020, 4:49am",
    "body": "Hi all! Can you please help me to solve the problem with Kibana. I installed latest version of Elasticsearch and Kibana on the same server with minimum configuration. Installation is standalone. Server IP address 192.168.171.3 Elastic config: cluster.name: eleks-cluster node.name: elks-1 path.logs: E:\\Elastic\\ElasticLOGS network.host: 192.168.171.3 discovery.seed_hosts: [\"192.168.171.3\"] xpack.security.enabled: false Kibana config: server.host: \"192.168.171.3\" server.name: \"kibana-elks1\" elasticsearch.hosts: [\"http://192.168.171.3:9200\"] HTTP response from Elasticserach: { \"name\" : \"elks-1\", \"cluster_name\" : \"eleks-cluster\", \"cluster_uuid\" : \"na\", \"version\" : { \"number\" : \"7.5.1\", \"build_flavor\" : \"default\", \"build_type\" : \"zip\", \"build_hash\" : \"3ae9ac9a93c95bd0cdc054951cf95d88e1e18d96\", \"build_date\" : \"2019-12-16T22:57:37.835892Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.3.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Error from Kibana: PS E:\\Elastic\\kibana-7.5.1-windows-x86_64\\kibana-7.5.1-windows-x86_64\\bin> .\\kibana.bat log [12:49:01.674] [info][plugins-service] Plugin \"security\" is disabled. log [12:49:01.759] [info][plugins-system] Setting up [14] plugins: [licensing,code,timelion,features,spaces,translations,uiActions,newsfeed,expressions,inspector,embeddable,advancedUiActions,eui_utils,data] log [12:49:01.761] [info][licensing][plugins] Setting up plugin log [12:49:01.765] [info][code][plugins] Setting up plugin log [12:49:01.766] [info][plugins][timelion] Setting up plugin log [12:49:01.768] [info][features][plugins] Setting up plugin log [12:49:01.769] [info][plugins][spaces] Setting up plugin log [12:49:01.776] [info][plugins][translations] Setting up plugin log [12:49:01.777] [info][data][plugins] Setting up plugin log [12:49:08.248] [warning][licensing][plugins] License information could not be obtained from Elasticsearch for the [data] cluster. TypeError: Cannot use 'in' operator to search for 'type' in null log [12:49:08.658] [warning][legacy-plugins] Skipping non-plugin directory at E:\\Elastic\\kibana-7.5.1-windows-x86_64\\kibana-7.5.1-windows-x86_64\\src\\legacy\\core_plugins\\visualizations log [12:49:09.693] [info][plugins-system] Starting [7] plugins: [licensing,code,timelion,features,spaces,translations,data] log [12:49:31.848] [warning][licensing][plugins] License information could not be obtained from Elasticsearch for the [data] cluster. TypeError: Cannot use 'in' operator to search for 'type' in null log [12:49:39.704] [warning][migrations] Unable to connect to Elasticsearch. Error: Request Timeout after 30000ms log [12:50:01.875] [warning][licensing][plugins] License information could not be obtained from Elasticsearch for the [data] cluster. TypeError: Cannot use 'in' operator to search for 'type' in null As i suggest the primary error is: [warning][migrations] Unable to connect to Elasticsearch. Error: Request Timeout after 30000ms Any ideas? Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "dabc1af9-5a56-4659-84a9-6eb7fa783e0c",
    "url": "https://discuss.elastic.co/t/how-can-i-get-access-token-of-oidc-open-id-connect-provider/215080",
    "title": "How can I get access token of OIDC (Open ID Connect) provider?",
    "category": [
      "Kibana"
    ],
    "author": "RaghibHuda",
    "date": "January 15, 2020, 6:34am January 17, 2020, 4:51pm January 18, 2020, 8:26am January 21, 2020, 4:43am",
    "body": "Hi there, We are developing a custom plugin in Kibana. We integrate our authentication with OIDC. Now we want to make external API requests from our custom plugin. These requests need the access token of the same OIDC provider. How can I get the provider OIDC token? What should be the approach?",
    "website_area": "discuss"
  },
  {
    "id": "98f82d15-7e01-4e35-9a92-7256e23ff872",
    "url": "https://discuss.elastic.co/t/why-does-painless-script-doesnt-work-with-message-field/215814",
    "title": "Why does Painless Script doesn't work with message field?",
    "category": [
      "Kibana"
    ],
    "author": "Daniel_Gonzalez1",
    "date": "January 21, 2020, 2:41am",
    "body": "Hello Everyone! I want to do some basic stuff that I can do super easy with Splunk of extracting new fields from logs that are already indexed, and with the Painless Script in Kibana seems to be very difficult and almost impossible to do it. This is an example of my log in the field message 15Jan20 06:30:24.39 PRIFBLOQ MXP1 RACF ALTUSER success for PRIFBLOQ: ALTUSER X302034 Jobname + id: RFJPDABC RACF command: ALTUSER X302034 DATA(02651484000000000000000) Name : BLOQUEO Y BAJAS RACF Instdata : U PROC. BAJAS Y BLOQUEOS AUTOM. DE RACF Y want to extract some data of that message field with regular expressions: This is my Painless Script: def m = /RACF\\scommand:\\s\\w+\\s\\w+\\s(?P<activity>.+)\\sName/.matcher(doc['message.keyword'].value); if ( m.matches() ) { return m.group(1) } else { return \"no match\" } When I try to save the new field it says the script is invalid, I was wondering if the Painless Script can't work with the message field. Could you help me to figure out was going wrong? Best regards",
    "website_area": "discuss"
  },
  {
    "id": "f46bde8a-6c5a-4321-9c02-5d39a2bfb75b",
    "url": "https://discuss.elastic.co/t/kibana-login-cant-access-to-spaces/215496",
    "title": "Kibana login can't access to spaces",
    "category": [
      "Kibana"
    ],
    "author": "talsantos",
    "date": "January 17, 2020, 4:53pm January 20, 2020, 4:57pm",
    "body": "Hi all, After upgrade kibana to 7.5, when I login with a specific user that access to only one space, says that i don't have any space configured to user: This is the configuration: image458579 20.3 KB image954822 36 KB",
    "website_area": "discuss"
  },
  {
    "id": "a98dc3bf-8790-43b4-89ab-431096e72816",
    "url": "https://discuss.elastic.co/t/aggregate-visualize-based-on-substring-or-regex-query/215524",
    "title": "Aggregate/visualize based on substring or regex query",
    "category": [
      "Kibana"
    ],
    "author": "sdberts",
    "date": "January 17, 2020, 9:25pm January 20, 2020, 4:41pm",
    "body": "Hey all - I have a use case where I need to query for a substring or regex pattern from a field, then visualize it in Kibana. I have a field called \"description\" in an index, in which various data is stored. The data follows this pattern: application_name - metric : value application_name, metric, and value can all vary depending on the source of the data, so what I want to do is pull out application_name, then build a vertical bar chart showing the Count of different application_name values we receive. So for instance: application_name1 = 30 events last 24 hours application_name2 = 72 events last 24 hours application_name3 = 13 events last 24 hours Unfortunately I don't have a way to change the source, otherwise I would split up the data into individual fields and that would make things a lot easier. Is this feasible?",
    "website_area": "discuss"
  },
  {
    "id": "f0fb99ce-7e4f-423c-8ae6-5f4061791f4c",
    "url": "https://discuss.elastic.co/t/field-issue-in-visualization/215595",
    "title": "Field issue in visualization",
    "category": [
      "Kibana"
    ],
    "author": "volcano",
    "date": "January 19, 2020, 5:13pm January 20, 2020, 4:31pm",
    "body": "Why I don't find field in visualization? I see field.keyword only. What to do to see the field in visualization?",
    "website_area": "discuss"
  },
  {
    "id": "b4f7151a-32ed-47cf-95ee-039709b47151",
    "url": "https://discuss.elastic.co/t/watcher-one-field-value-in-a-json-array/215642",
    "title": "Watcher one field value in a json array",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 19, 2020, 6:41pm January 20, 2020, 4:21pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5302f99f-4b32-426a-bc2d-520968955d18",
    "url": "https://discuss.elastic.co/t/how-do-i-count-unique-prefixes/215751",
    "title": "How do I count unique prefixes",
    "category": [
      "Kibana"
    ],
    "author": "Diwike",
    "date": "January 20, 2020, 2:30pm January 20, 2020, 3:06pm",
    "body": "Hello, Im new to Kibana and this forum and hope to find some assistance. Im trying to create a unique counter of the first five digits of, ex: 47001834023049820397402 I can't for the life of me figure out how to do this. In my data I have thousands of prefixes as, ex: 57001834023049820397402 47001834023049820397402 24009700183402304982039 And I need to visualize how many of each prefix I have, ex: 57001: 12 47001: 9 24009: 35 If anyone have a good way of doing this Id appreciate the help, thanks!",
    "website_area": "discuss"
  },
  {
    "id": "7e003f0e-f442-4527-9f9f-6cb4e04b841f",
    "url": "https://discuss.elastic.co/t/how-to-make-these-fields-agrregators/215738",
    "title": "How to make these fields agrregators?",
    "category": [
      "Kibana"
    ],
    "author": "volcano",
    "date": "January 20, 2020, 12:42pm January 20, 2020, 2:04pm January 20, 2020, 2:44pm January 20, 2020, 2:42pm January 20, 2020, 2:52pm January 20, 2020, 2:51pm",
    "body": "This is the field list in my index pattern. How to make these fields aggregators? 850269 18.3 KB",
    "website_area": "discuss"
  },
  {
    "id": "0b6f37d0-08e8-4f62-9301-b850eaa52da4",
    "url": "https://discuss.elastic.co/t/kibana-dashboard-share-without-login-password-kibana-7-3-2/214674",
    "title": "Kibana dashboard share without login/Password - Kibana 7.3.2",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 10, 2020, 9:30pm January 17, 2020, 10:41am January 20, 2020, 2:35pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8a8df46b-36a3-479c-8494-e33c83372fac",
    "url": "https://discuss.elastic.co/t/kibana7-5-plugin-and-visualization-development-guide/215733",
    "title": "Kibana7.5 plugin and visualization development guide",
    "category": [
      "Kibana"
    ],
    "author": "zaynkorai",
    "date": "January 20, 2020, 12:19pm January 20, 2020, 2:30pm",
    "body": "Hey I am new to Elastic and trying to run this plugin inside my own plugins folder in cloned kibana directory. My goal is embed a visualization of kibana 7.5+ into my plugin and I am now looking for any documentation/step to run this plugin in kibana code. Any help will be highly appreciated // https://github.com/elastic/kibana/tree/master/test/plugin_functional/plugins/kbn_tp_custom_visualizations",
    "website_area": "discuss"
  },
  {
    "id": "82d09409-52eb-4c7c-8549-83993eaab4c9",
    "url": "https://discuss.elastic.co/t/fatal-class-cast-exception-class-java-lang-string-cannot-be-cast-to-class-java-util-map/215609",
    "title": "FATAL [class_cast_exception] class java.lang.String cannot be cast to class java.util.Map",
    "category": [
      "Kibana"
    ],
    "author": "Sudarshan013",
    "date": "January 19, 2020, 8:19am January 20, 2020, 1:38pm",
    "body": "FATAL [class_cast_exception] class java.lang.String cannot be cast to class java.util.Map (java.lang.String and java.util.Map are in module java.base of loader 'bootstrap') :: {\"path\":\"/.kibana_task_manager_1\",\"query\":{},\"body\":\"{\\\"mappings\\\":{\\\"dynamic\\\":\\\"strict\\\",\\\"properties\\\":{\\\"config\\\":{\\\"dynamic\\\":\\\"true\\\",\\\"properties\\\":{\\\"buildNum\\\":{\\\"type\\\":\\\"keyword\\\"}}},\\\"migrationVersion\\\":{\\\"dynamic\\\":\\\"true\\\",\\\"type\\\":\\\"object\\\"},\\\"type\\\":{\\\"type\\\":\\\"keyword\\\"},\\\"namespace\\\":{\\\"type\\\":\\\"keyword\\\"},\\\"updated_at\\\":{\\\"type\\\":\\\"date\\\"},\\\"references\\\":{\\\"type\\\":\\\"nested\\\",\\\"properties\\\":{\\\"name\\\":{\\\"type\\\":\\\"keyword\\\"},\\\"type\\\":{\\\"type\\\":\\\"keyword\\\"},\\\"id\\\":{\\\"type\\\":\\\"keyword\\\"}}},\\\"task\\\":{\\\"properties\\\":{\\\"taskType\\\":{\\\"type\\\":\\\"keyword\\\"},\\\"scheduledAt\\\":{\\\"type\\\":\\\"date\\\"},\\\"runAt\\\":{\\\"type\\\":\\\"date\\\"},\\\"startedAt\\\":{\\\"type\\\":\\\"date\\\"},\\\"retryAt\\\":{\\\"type\\\":\\\"date\\\"},\\\"interval\\\":{\\\"type\\\":\\\"text\\\"},\\\"attempts\\\":{\\\"type\\\":\\\"integer\\\"},\\\"status\\\":{\\\"type\\\":\\\"keyword\\\"},\\\"params\\\":{\\\"type\\\":\\\"text\\\"},\\\"state\\\":{\\\"type\\\":\\\"text\\\"},\\\"user\\\":{\\\"type\\\":\\\"keyword\\\"},\\\"scope\\\":{\\\"type\\\":\\\"keyword\\\"},\\\"ownerId\\\":{\\\"type\\\":\\\"keyword\\\"}}}},\\\"_meta\\\":{\\\"migrationMappingPropertyHashes\\\":{\\\"config\\\":\\\"87aca8fdb053154f11383fce3dbf3edf\\\",\\\"migrationVersion\\\":\\\"4a1746014a75ade3a714e1db5763276f\\\",\\\"type\\\":\\\"2f4316de49999235636386fe51dc06c1\\\",\\\"namespace\\\":\\\"2f4316de49999235636386fe51dc06c1\\\",\\\"updated_at\\\":\\\"00da57df13e94e9d98437d13ace4bfe0\\\",\\\"references\\\":\\\"7997cf5a56cc02bdc9c93361bde732b0\\\",\\\"task\\\":\\\"a7206eba37aaea59ba083a079fd80b65\\\"}}},\\\"settings\\\":{\\\"number_of_shards\\\":1,\\\"auto_expand_replicas\\\":\\\"0-1\\\"}}\",\"statusCode\":500,\"response\":\"{\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"class_cast_exception\\\",\\\"reason\\\":\\\"class java.lang.String cannot be cast to class java.util.Map (java.lang.String and java.util.Map are in module java.base of loader 'bootstrap')\\\"}],\\\"type\\\":\\\"class_cast_exception\\\",\\\"reason\\\":\\\"class java.lang.String cannot be cast to class java.util.Map (java.lang.String and java.util.Map are in module java.base of loader 'bootstrap')\\\"},\\\"status\\\":500}\"} Kibana and elastic version : 7.5.1 How could I resolve this issue?",
    "website_area": "discuss"
  },
  {
    "id": "83590083-3bf2-47d6-bd7c-3d8a8e556c67",
    "url": "https://discuss.elastic.co/t/kibana-read-only-dashboard/214792",
    "title": "Kibana Read only dashboard",
    "category": [
      "Kibana"
    ],
    "author": "DG13",
    "date": "January 13, 2020, 10:17am January 13, 2020, 11:51am January 13, 2020, 12:25pm January 13, 2020, 1:36pm January 13, 2020, 1:50pm January 20, 2020, 4:02am January 20, 2020, 1:28pm",
    "body": "Hi , I am using ELK 7.1.0 community edition . Would like to enable read only dashboard for the users , Is this feature available in community edition ? I am aware that it's available in ELK Enterprise edition . Thanks, Divya",
    "website_area": "discuss"
  },
  {
    "id": "924c0306-5fd7-4033-993f-a887d135db79",
    "url": "https://discuss.elastic.co/t/cloning-index/215716",
    "title": "Cloning Index",
    "category": [
      "Kibana"
    ],
    "author": "SpinDORK",
    "date": "January 20, 2020, 10:44am January 20, 2020, 1:30pm January 20, 2020, 11:02am",
    "body": "Hi, When I'm trying to clone an index with data, I've been getting errors, if I run this; POST /logflow-2019.11/_clone/robot-dsa_2 { \"settings\": { \"index.number_of_shards\": 5 }, \"aliases\": { \"my_search_indices\": {} } } Then I get \"type\": \"invalid_type_name_exception\", \"reason\": \"Document mapping type name can't start with '_', found: [_clone]\" Where am I going wrong? The index is locked with \"index.blocks.write\": true Thanks",
    "website_area": "discuss"
  },
  {
    "id": "d6eac41c-95c8-4066-a77b-9a865170b2fb",
    "url": "https://discuss.elastic.co/t/missing-split-series-option-on-heat-map-visualization/215710",
    "title": "Missing \"Split Series\" option on Heat Map Visualization",
    "category": [
      "Kibana"
    ],
    "author": "MARCO_RAMBALDI",
    "date": "January 20, 2020, 10:25am",
    "body": "Hi, I have noticed the lack of the SPLIT SERIES option since the 7.4 version. Why? Before this change, I was able to discriminate data on the y-axis. Now there is only SPLIT CHARTS option, so i can do a similar operation but what i see is the full name of the aggregation, instead of the key to split the data (view picture below). Screenshot_2020-01-20 cab_opened - Kibana1334760 71.4 KB In the picture below you can see the key to split the data: How can i do to show only the key? Why have split series otpion have been removed since the 7.4 version? Thanks, Marco",
    "website_area": "discuss"
  },
  {
    "id": "aa7d298e-cc2b-4c2f-94c8-88689c624861",
    "url": "https://discuss.elastic.co/t/how-to-apply-predefined-template-if-log-collection-is-done-in-a-different-way/215434",
    "title": "How to apply predefined template if log collection is done in a different way",
    "category": [
      "Kibana"
    ],
    "author": "karthiknpy",
    "date": "January 17, 2020, 9:29am January 17, 2020, 12:31pm January 17, 2020, 3:02pm January 19, 2020, 7:26am January 19, 2020, 4:29pm January 20, 2020, 9:37am",
    "body": "Hey Fellas, I need to collect suricata logs from a machine. By default suricata has a template in kibana if logs collected with filebeat and suricata filebeat module is installed, dashboards are created automatically. Stuffs are taken care...! But I dont have a provision to collect logs via Filebeat due to a special case. All I can afford is sending logs to a logstash syslog input plugin. How to apply the suricata default dashboard to interpret the logs collected by suricata automatically to a custom index name. Regards Karthik. K",
    "website_area": "discuss"
  },
  {
    "id": "ed4521bf-369b-4968-aa3e-14627a25b445",
    "url": "https://discuss.elastic.co/t/how-make-control-only-for-one-table-on-dashboard/215692",
    "title": "How make control only for one table on dashboard?",
    "category": [
      "Kibana"
    ],
    "author": "111249",
    "date": "January 20, 2020, 9:05am January 20, 2020, 2:43pm",
    "body": "Hello everyone, I have a dashboard and where are 2 tables: \"products by supermarket\" and \"products by category\". I have kibana control to filter by supermarket. I want to filter only the table with categories on this dashboard: then I select some supermarket in input, the table with categories must give only those categories and products that are in the selected supermarket. This works, but the \"products by supermarket\" table is filtered too. How can this behavior be avoided? How to filter only the \"products by category\" table. I hope you help me with this. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "f7b2f052-acf3-4cd2-a602-567b38e9d19d",
    "url": "https://discuss.elastic.co/t/user-access-privilege-on-specific-dashboard/215676",
    "title": "User access privilege on specific dashboard",
    "category": [
      "Kibana"
    ],
    "author": "Tennis",
    "date": "January 20, 2020, 7:14am January 20, 2020, 8:56am",
    "body": "I have multiple dashboards template (es. D1, D2, D3...etc). Can I control access privilege of each user correspond to dashboard name such as: user_1 >> can access D1, D2 user_2 >> can access D3, D4.",
    "website_area": "discuss"
  },
  {
    "id": "b19d55d1-ba5d-47da-b2bc-c86fa020cd53",
    "url": "https://discuss.elastic.co/t/using-eui-inside-fieldformatter-plugin/214032",
    "title": "Using EUI inside FieldFormatter plugin",
    "category": [
      "Kibana"
    ],
    "author": "Elaak",
    "date": "January 7, 2020, 11:06am January 16, 2020, 10:23am January 16, 2020, 10:23am January 16, 2020, 5:55pm January 17, 2020, 10:49am January 20, 2020, 8:23am",
    "body": "I have a FieldFormatter where I want to use EUI to create a html representation of some data. I want a modal dialog to open when I press on one field. This is a type of extended version for the \"Truncated String\" FieldFormatter. Here is an extract of the code: public.js: TruncatedField.prototype._convert = { text: function (value) { return value; }, html: function (value, _, document) { var htmlString = '<div>' + '<button class=\"euiButton\" type=\"button\">' + 'Truncated text example ...' + '</button></div>'; return htmlString; } }; This gives me a button that I can click, but I cannot figure how to open up an overlapping modal dialog from pressing the button. I tried look through the tutorials and examples at the EUI github page but I cannot find a complete example. How do I make this work, and how do I insert the full text of my truncated string into this modal dialog? Many thanks!",
    "website_area": "discuss"
  },
  {
    "id": "59daa1b9-6478-4c13-90bb-9f3af472bc7f",
    "url": "https://discuss.elastic.co/t/how-to-apply-filter-in-kibana-visualization/215431",
    "title": "How to apply filter in kibana visualization",
    "category": [
      "Kibana"
    ],
    "author": "Aurelien1609",
    "date": "January 17, 2020, 9:16am January 20, 2020, 8:10am January 20, 2020, 8:13am",
    "body": "Hi, I want to filter a specific visualization : a vertical bar chart in my Kibana Dashboard. I don't want to use Control visualization because the filter should be only applied on this bar chart. Example : I created a bar chart with some events : {\"name\": X, \"age\": 22, is_ok: True} {\"name\": Y, \"age\": 36, is_ok: True} {\"name\": Z, \"age\":24, is_ok: False} I need to print only value with field is_ok = True in the bar chart. Any idea ? Thanks a lot, Aurelien",
    "website_area": "discuss"
  },
  {
    "id": "7eb3bc83-e1f5-4cad-b0e7-3835618041bb",
    "url": "https://discuss.elastic.co/t/filter-some-value-on-visualize-control-function/215679",
    "title": "Filter some value on visualize \"control\" function",
    "category": [
      "Kibana"
    ],
    "author": "Tennis",
    "date": "January 20, 2020, 7:43am",
    "body": "I want to use the field: \"product\" to be a selection menu using the visualize feature \"control\". Data of field: \"product\" contains three types: keyboard, computer, screen. In default \"control\" function, my select menu will show these three types keyboard, computer, screen. Is there a filtering function to make my menu only show two types: keyboard, computer ? In the \"Line\" visualize, it support us to use KQL section to filter data, however, it doesn't appear in \"Control\" visualize.",
    "website_area": "discuss"
  },
  {
    "id": "cefdcb17-14e1-4748-a79a-e1fb0516e71e",
    "url": "https://discuss.elastic.co/t/dashboard-doesnt-keep-time-selected/215471",
    "title": "Dashboard doesn't \"keep\" time selected",
    "category": [
      "Kibana"
    ],
    "author": "Voula_Mikr",
    "date": "January 17, 2020, 1:48pm January 20, 2020, 6:58am January 20, 2020, 6:57am",
    "body": "Hi! I saved a dashboard with a specific time range selected in time filter but when I open again this dashboard, the time in time filter is \"Last 15 minutes\". So in the corresponding iframe I see data from last 15 minutes. Is there any way to save my time selection? Thank you in advance!",
    "website_area": "discuss"
  },
  {
    "id": "f7b0e47d-7da8-4ee7-aaca-925407814049",
    "url": "https://discuss.elastic.co/t/how-to-export-csv-in-kibana-7-5-with-more-then-1-million-row/214526",
    "title": "How to export csv in kibana 7.5 with more then 1 million row",
    "category": [
      "Kibana"
    ],
    "author": "umay_fb",
    "date": "January 10, 2020, 4:59am January 10, 2020, 6:45pm January 16, 2020, 6:56am January 16, 2020, 9:09pm January 16, 2020, 9:36pm January 17, 2020, 7:48am January 17, 2020, 9:11pm January 20, 2020, 4:29am",
    "body": "I need help... i have problem with reporting CSV on kibana. i want to get CSV with 1.000.000 row from discover but its always failed. i get notif that [Reporting generate CSV error:Unable to generate report Max attempts reached (3)]? what must i do to export CSV in large volume?",
    "website_area": "discuss"
  },
  {
    "id": "e0bf1b57-8905-4e98-a053-d1e4a156a6c3",
    "url": "https://discuss.elastic.co/t/documents-are-not-displayed-sequential-order-as-written-by-logstash-in-kibana/215417",
    "title": "Documents are not displayed sequential order as written by logstash in Kibana",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 17, 2020, 5:58am January 17, 2020, 2:00pm January 20, 2020, 4:17am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5ac4e034-2817-42a6-a6a6-b3a0eda6281b",
    "url": "https://discuss.elastic.co/t/max-bucket-aggregation-on-timestamp-s/213684",
    "title": "Max bucket aggregation (on @timestamp /s)",
    "category": [
      "Kibana"
    ],
    "author": "Petr.Simik",
    "date": "January 3, 2020, 9:36am January 3, 2020, 11:56pm January 19, 2020, 7:27pm",
    "body": "what is the correct approach to draw count of events (showing maximum events/seconds) I am trying to create visualisation (see picture below) show graf with maximum events/seconds Kibana is automatically scale @timestamp interval from 1sec to 1minute, i have to zoom in to see correct values. But zoomed in graph is not what I want . I was trying to use max bucket aggregation but it seems does not fit my case. (see picture below) histogram over few days image1148921 53.3 KB",
    "website_area": "discuss"
  },
  {
    "id": "73b3542d-e0a7-464d-aafa-c6c7d8eca3cd",
    "url": "https://discuss.elastic.co/t/monitoring-remote-es-in-local-kibana/214737",
    "title": "Monitoring remote ES in local Kibana",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 13, 2020, 8:29am January 14, 2020, 3:30pm January 19, 2020, 6:43pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "dc13b365-9c50-425c-8b6b-66c02781716a",
    "url": "https://discuss.elastic.co/t/import-ndjson-in-kibana-helm-deployment/215338",
    "title": "Import ndjson in kibana helm deployment",
    "category": [
      "Kibana"
    ],
    "author": "Voula_Mikr",
    "date": "January 16, 2020, 3:09pm January 17, 2020, 4:15am January 18, 2020, 8:45am January 19, 2020, 4:49pm",
    "body": "Hi, I have exported some dashboards (ndjson files) and I would like to have these exports imported in a new kibana-helm deployment. Is this possible? If yes, how can I do something like this? Thank you in advance!",
    "website_area": "discuss"
  },
  {
    "id": "29d1d56a-caec-492c-8957-277a1d1782fe",
    "url": "https://discuss.elastic.co/t/there-is-a-reward-for-those-who-know-how-baseurl-changed/215616",
    "title": "There is a reward for those who know how baseurl changed",
    "category": [
      "Kibana"
    ],
    "author": "mr_searchng",
    "date": "January 19, 2020, 10:51am January 19, 2020, 12:57pm January 19, 2020, 2:32pm January 19, 2020, 2:45pm January 19, 2020, 2:54pm",
    "body": "app/kibana# instead app/test# thanks",
    "website_area": "discuss"
  },
  {
    "id": "70e691b3-ecf1-4c33-83b8-fabe8236cda0",
    "url": "https://discuss.elastic.co/t/java-lang-string-cannot-be-cast-to-java-util-map-while-starting-kibana-7-5-1/213543",
    "title": "java.lang.String cannot be cast to java.util.Map while starting kibana 7.5.1",
    "category": [
      "Kibana"
    ],
    "author": "Arunan_Ramanathan",
    "date": "January 2, 2020, 9:30am January 3, 2020, 11:51pm January 4, 2020, 12:16pm January 6, 2020, 10:57am January 6, 2020, 11:31am January 6, 2020, 2:53pm January 19, 2020, 11:26am",
    "body": "I enabled xpack security in Elasticsearch and it is working fine, I can able to do CRUD in elasticsearch using username/password. Now I downloaded kibana and enabled the following, Blockquote elasticsearch.hosts: [\"http://localhost:9200\"] elasticsearch.username: \"kibana\" elasticsearch.password: \"elastic\" xpack.security.encryptionKey: \"asdfghjklzxcvbnmqwertyuiopqwertyuiop\" But kibana failes to start and throwing the following in the console, Blockquote log [09:19:38.209] [info][migrations] Creating index .kibana_task_manager_1. log [09:19:38.220] [info][migrations] Creating index .kibana_1. log [09:19:38.235] [warning][migrations] Unable to connect to Elasticsearch. Error: [class_cast_exception] java.lang.String cannot be cast to java.util.Map log [09:19:38.237] [fatal][root] { Error: [class_cast_exception] java.lang.String cannot be cast to java.util.Map at respond (c:\\Arunan\\tools\\kibana-7.5.1-windows-x86_64\\node_modules\\elasticsearch\\src\\lib\\transport.js:349:15) at checkRespForFailure (c:\\Arunan\\tools\\kibana-7.5.1-windows-x86_64\\node_modules\\elasticsearch\\src\\lib\\transport.js:306:7) at HttpConnector. (c:\\Arunan\\tools\\kibana-7.5.1-windows-x86_64\\node_modules\\elasticsearch\\src\\lib\\connectors\\http.js:173:7) at IncomingMessage.wrapper (c:\\Arunan\\tools\\kibana-7.5.1-windows-x86_64\\node_modules\\elasticsearch\\node_modules\\lodash\\lodash.js:4929:19) at IncomingMessage.emit (events.js:194:15) at endReadableNT (_stream_readable.js:1103:12) at process._tickCallback (internal/process/next_tick.js:63:19) status: 500, displayName: 'InternalServerError', message: '[class_cast_exception] java.lang.String cannot be cast to java.util.Map', path: '/.kibana_task_manager_1', query: {}, body: { error: { root_cause: [Array], type: 'class_cast_exception', reason: 'java.lang.String cannot be cast to java.util.Map' }, status: 500 }, statusCode: 500, response: '{\"error\":{\"root_cause\":[{\"type\":\"class_cast_exception\",\"reason\":\"java.lang.String cannot be cast to java.util.Map\"}],\"type\":\"class_cast_exception\",\"reason\":\"java.lang.String cannot be cast to java.util.Map\"},\"status\":500}', toString: [Function], toJSON: [Function] } log [09:19:38.252] [info][plugins-system] Stopping all plugins. log [09:19:38.257] [info][data][plugins] Stopping plugin log [09:19:38.259] [info][plugins][translations] Stopping plugin log [09:19:38.260] [info][plugins][spaces] Stopping plugin log [09:19:38.262] [info][features][plugins] Stopping plugin log [09:19:38.264] [info][plugins][timelion] Stopping plugin log [09:19:38.265] [info][code][plugins] Stopping plugin log [09:19:38.267] [info][licensing][plugins] Stopping plugin log [09:19:38.274] [info][plugins][security] Stopping plugin FATAL [class_cast_exception] java.lang.String cannot be cast to java.util.Map :: {\"path\":\"/.kibana_task_manager_1\",\"query\":{},\"body\":\"{\"mappings\":{\"dynamic\":\"strict\",\"properties\":{\"config\":{\"dynamic\":\"true\",\"properties\":{\"buildNum\":{\"type\":\"keyword\"}}},\"migrationVersion\":{\"dynamic\":\"true\",\"type\":\"object\"},\"type\":{\"type\":\"keyword\"},\"namespace\":{\"type\":\"keyword\"},\"updated_at\":{\"type\":\"date\"},\"references\":{\"type\":\"nested\",\"properties\":{\"name\":{\"type\":\"keyword\"},\"type\":{\"type\":\"keyword\"},\"id\":{\"type\":\"keyword\"}}},\"task\":{\"properties\":{\"taskType\":{\"type\":\"keyword\"},\"scheduledAt\":{\"type\":\"date\"},\"runAt\":{\"type\":\"date\"},\"startedAt\":{\"type\":\"date\"},\"retryAt\":{\"type\":\"date\"},\"interval\":{\"type\":\"text\"},\"attempts\":{\"type\":\"integer\"},\"status\":{\"type\":\"keyword\"},\"params\":{\"type\":\"text\"},\"state\":{\"type\":\"text\"},\"user\":{\"type\":\"keyword\"},\"scope\":{\"type\":\"keyword\"},\"ownerId\":{\"type\":\"keyword\"}}}},\"_meta\":{\"migrationMappingPropertyHashes\":{\"config\":\"87aca8fdb053154f11383fce3dbf3edf\",\"migrationVersion\":\"4a1746014a75ade3a714e1db5763276f\",\"type\":\"2f4316de49999235636386fe51dc06c1\",\"namespace\":\"2f4316de49999235636386fe51dc06c1\",\"updated_at\":\"00da57df13e94e9d98437d13ace4bfe0\",\"references\":\"7997cf5a56cc02bdc9c93361bde732b0\",\"task\":\"a7206eba37aaea59ba083a079fd80b65\"}}},\"settings\":{\"number_of_shards\":1,\"auto_expand_replicas\":\"0-1\"}}\",\"statusCode\":500,\"response\":\"{\"error\":{\"root_cause\":[{\"type\":\"class_cast_exception\",\"reason\":\"java.lang.String cannot be cast to java.util.Map\"}],\"type\":\"class_cast_exception\",\"reason\":\"java.lang.String cannot be cast to java.util.Map\"},\"status\":500}\"} I am using windows 10.",
    "website_area": "discuss"
  },
  {
    "id": "cba6d1cb-c04d-4972-8ed5-9a6520c496b7",
    "url": "https://discuss.elastic.co/t/sending-sms-push-notifications-emails-to-the-real-time-users-of-the-product-from-kibana-dashboard/215597",
    "title": "Sending SMS, Push Notifications, emails to the real time users of the product from Kibana Dashboard",
    "category": [
      "Kibana"
    ],
    "author": "Abhi12",
    "date": "January 19, 2020, 5:14am January 19, 2020, 11:15am",
    "body": "How can we send emails, SMS, push notifications to all the users(real time) directly from the kibana dashboard??Like all marketing companies send bulk mail to their users. I want to send mail and SMS to the all users of the specific time period(available on dashboard) directly with one click.",
    "website_area": "discuss"
  },
  {
    "id": "84215b63-536b-4776-ac87-e750fb75bfee",
    "url": "https://discuss.elastic.co/t/how-to-show-pie-chart/215562",
    "title": "How to show Pie chart?",
    "category": [
      "Kibana"
    ],
    "author": "volcano",
    "date": "January 18, 2020, 1:51pm January 18, 2020, 3:18pm January 19, 2020, 4:20am January 19, 2020, 4:26am January 19, 2020, 4:39am",
    "body": "I have a search query which shows records as Slow-Speed High-Speed High-Speed High-Speed High-Speed Slow-Speed Slow-Speed I want to create a pie chart Slow-Speed vs High-Speed. How to do this? I tried this way, I looked into Add Visualization and then selected the saved query How do I plot now Slow-Speed vs High-Speed pie chart. I am stuck at this part. shall I choose split rows , terms , add subbuckets ? I'm confused at this part. I need some help at this part. Could you please tell what is the correct way to plot the pie chart ?",
    "website_area": "discuss"
  },
  {
    "id": "568ccf5e-8c68-4ad1-847c-80f362e92628",
    "url": "https://discuss.elastic.co/t/kibana-split-json-field-shows-as-one-long-string/215514",
    "title": "Kibana - Split Json field shows as one long string",
    "category": [
      "Kibana"
    ],
    "author": "Newtoelastic",
    "date": "January 17, 2020, 8:13pm January 19, 2020, 2:14am",
    "body": "Hey there, as the name suggests I am new to elastic search and I am having kind of an odd problem. I have a list of links I am trying to have displayed (later on I need to use these links for alerting) and split in the following manner: Links: www.link1.com www.link2.com www.link3.com Now I split the text in the parsing on the log stash correctly and I am receiving the Json with an arry (pics as proof below) but in the table tab it still shows it as if it were 1 long string. Please assist ASAP as this is a pressing issue. The logs as they are parsed in the logstash: image1075169 2.54 KB The logs as they are shown in the json tab in Kibana image987261 59.9 KB The logs as they are shown in the table tab in Kibana image1268202 57.7 KB",
    "website_area": "discuss"
  },
  {
    "id": "98656f70-2910-4e8c-a115-c82a37617e56",
    "url": "https://discuss.elastic.co/t/kibana-keystore-no-such-directory-error/215549",
    "title": "Kibana Keystore No Such Directory Error",
    "category": [
      "Kibana"
    ],
    "author": "kiyapedia",
    "date": "January 18, 2020, 8:04am January 18, 2020, 11:12am January 18, 2020, 2:15pm",
    "body": "Hello  I've got Elasticsearch and Kibana running with docker-compose with no issues. In my kibana.yml file, my password is exposed as such: server.name: kibana server.host: \"0\" elasticsearch.hosts: [ \"http://elasticsearch:9200\" ] elasticsearch.username: kibana elasticsearch.password: password When trying to create a kibana-keystore to protect the password value, i get the following error: bash: bin/kibana-keystore: No such file or directory I am following this documentation to create the kibana-keystore: https://www.elastic.co/guide/en/kibana/6.8/secure-settings.html I've also tried storing the password in elasticsearch-keystore but could not reference it and it caused my elasticsearch cluster to fail. I assume this is the reason why: Only some settings are designed to be read from the keystore. However, there is no validation to block unsupported settings from the keystore and they can cause Elasticsearch to fail to start. Any insights on how i can protect the password in kibana.yml would be much appreciated. Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "c586d898-508b-4818-85b4-940459c5771b",
    "url": "https://discuss.elastic.co/t/display-data-as-metric-percentage-of-document-count/215508",
    "title": "Display data as metric percentage of document count",
    "category": [
      "Kibana"
    ],
    "author": "mattiacareddu",
    "date": "January 17, 2020, 7:25pm January 17, 2020, 8:06pm January 17, 2020, 9:42pm January 17, 2020, 9:57pm January 17, 2020, 10:00pm January 17, 2020, 10:08pm January 17, 2020, 10:15pm January 17, 2020, 11:47pm January 18, 2020, 8:40am",
    "body": "Hi, I want to display the increment between the counts of document in the same index with, of course, different period. So for example I want to calculate the increment between the total count of the last month and the previous month. Following this article I was able to get that working on a visualization with the filter ratio. I have two main problems. I have to put in the query string the fixed date, so every month or every period I have to manually change all that filter, which is not good. Is there any way to use the \"now\" variable and making operations with that, in the query string? The other problem is to use this filter ratio (or something similar) in the canvas. I didn't find anything. It seems that I was struggling too much with a so simple problem. A division between two counts of the same index. It seems I'm going in the wrong direction. Do you have any suggestion? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "1845826f-d1d6-4fe4-a574-d8e2f667c335",
    "url": "https://discuss.elastic.co/t/question-about-terms-join/215511",
    "title": "Question about terms join",
    "category": [
      "Kibana"
    ],
    "author": "flaviogoncalves",
    "date": "January 18, 2020, 7:57am",
    "body": "Im having and issue with terms join in Kibana for a map. Even with the exactly same term, (actually a code with 15 digits) in both index in some areas the map does not highlight. The map sector do not activate for counts below 10 Does anyone had the same problem?",
    "website_area": "discuss"
  },
  {
    "id": "46db6a65-a6b4-4ccf-9690-a6cf343b51d0",
    "url": "https://discuss.elastic.co/t/can-we-set-request-time-through-env-var/214635",
    "title": "Can we set request time through env var?",
    "category": [
      "Kibana"
    ],
    "author": "javadevmtl",
    "date": "January 10, 2020, 4:36pm January 10, 2020, 6:10pm January 10, 2020, 9:19pm January 17, 2020, 5:30pm January 17, 2020, 7:21pm January 18, 2020, 5:29am",
    "body": "Hi using 6.4.2, Can we set elasticsearch.requestTimeout as an env variable ELASTICSEARCH_REQUEST_TIMEOUT?? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "bedc99d2-d4a4-409b-a9de-bc233c654c3d",
    "url": "https://discuss.elastic.co/t/i-build-a-data-set-and-the-drop-down-filter-dont-work-with-some-fields-despite-work-with-others/215533",
    "title": "I build a data set and the drop down filter dont work with some fields despite work with others",
    "category": [
      "Kibana"
    ],
    "author": "nir9",
    "date": "January 17, 2020, 11:39pm January 18, 2020, 1:06am",
    "body": "how i can filter data table",
    "website_area": "discuss"
  },
  {
    "id": "cd387777-27ed-4049-9a2f-b74127737a42",
    "url": "https://discuss.elastic.co/t/timelion-wont-show-anything-in-the-graph-for-any-metric-other-than-count/215500",
    "title": "Timelion won't show anything in the graph for any metric other than \"count\"",
    "category": [
      "Kibana"
    ],
    "author": "rosamar",
    "date": "January 17, 2020, 5:27pm",
    "body": "I am using a version lower than 7.4 and most metrics (all but count) do not show any data in the diagram. I am working with Kibana (and ELK) 7.5. Any suggestions? Sample working query: .es(index=jos_data*, timefield=@timestamp, metric=count) Sample query not working: .es(index=jos_data*, timefield=@timestamp, metric=avg:message.payload.severity)",
    "website_area": "discuss"
  },
  {
    "id": "a6e057d3-440f-48e9-91c8-6dc7441bb256",
    "url": "https://discuss.elastic.co/t/index-pattern-at-discover-app/214851",
    "title": "Index Pattern at Discover App",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 13, 2020, 2:37pm January 17, 2020, 9:27am January 17, 2020, 4:15pm January 17, 2020, 4:56pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "23ddd20d-e472-4a80-918e-72f73b27678c",
    "url": "https://discuss.elastic.co/t/dealing-with-negative-spikes-when-data-source-maintains-metrics-that-always-increment/215494",
    "title": "Dealing with negative spikes when data source maintains metrics that always increment",
    "category": [
      "Kibana"
    ],
    "author": "sidhusaab",
    "date": "January 17, 2020, 4:34pm",
    "body": "I am building a TSVB visualization. Everything works great up until the point where if the application is restarted/restarts on its own the counters are reset to '0'. Since the visualizations use derivate to calculate difference in metrics, after this reboot, I get a huge negative spike in graphs which makes the rest of the metrics useless. (more details here on the use of derivative: Time Filter behaving strangely) image923605 35.6 KB So my plan of action was going to be drop the first event after counters reset to avoid this negative spike but i dont think it will work because the metrics would take time to get back to last value before restart. For instance: Last value of SUCCESS queries was 55,000 before counters reset and until the value becomes higher than 55,000 the graph would always be negative due to derivative calculating difference from last event. How should I tackle this? I am open to changing visualizations (5) completely if that would solve this. Additional information about the data: I am pulling in json data every 10 minutes using http poller input plugin. In the data, there are 2 fields: boot-time and current-time. Current Time refers to poll time and Boot Time is when the application was turned on. The metrics are maintained since boot time and always increment up. So there is no good way of showing a trend unless I use derivative. Also, is there something that can be done at logstash level that can calculate diff between the metric and pass that to ES?",
    "website_area": "discuss"
  },
  {
    "id": "56401d92-2538-4c9e-a587-a834984ae2ee",
    "url": "https://discuss.elastic.co/t/kibana-node-options/215018",
    "title": "Kibana NODE_OPTIONS",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 14, 2020, 4:57pm January 17, 2020, 3:30pm January 17, 2020, 3:31pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "83ac157e-ff6b-43c0-8574-d5ccbeb11aa9",
    "url": "https://discuss.elastic.co/t/kibana-not-fetching-fields-of-an-index-pattern-with-predefined-mapping-put-request/215349",
    "title": "Kibana not fetching fields of an index pattern with predefined mapping (PUT request)",
    "category": [
      "Kibana"
    ],
    "author": "AlekssM",
    "date": "January 16, 2020, 4:09pm January 17, 2020, 3:05pm January 17, 2020, 3:29pm",
    "body": "I have read the exact same question but the problem was automatically closed. \"When configuring an index pattern in Kibana, it's unable to fetch the field details of a particular index in Kibana, if I am using a predefined custom mapping when creating the index, and if i upload the data without predefined mapping it's working. Index exists in elastic search and have set of fields. I want to be able to create my own mapping with predefined data types for my object fields\" How do you know it's unable to fetch the field details? data types are unknown and no fields are visible for aggregations except te default ones _id, _source etc. Are there errors in Kibana? no errors in Kibana Are there errors in the Kibana log? -no errors in Kibana log Was the data loaded with one of our products like Logstash or a Beat? No, the index and default mapping with PUT request via Kibana console and the data objects via script with 'curl -H \"Content-Type: application/json\" -XPOST \"https://domain_endpoint/index_name/_bulk\" --data-binary \"@example.json\"';",
    "website_area": "discuss"
  },
  {
    "id": "6351ab07-2eaf-4cb1-80a0-a9016a4f656c",
    "url": "https://discuss.elastic.co/t/dns-lookup-result-in-discover-apache-logs/215253",
    "title": "DNS Lookup result in Discover Apache Logs",
    "category": [
      "Kibana"
    ],
    "author": "coolmac",
    "date": "January 16, 2020, 4:27pm January 17, 2020, 3:01pm",
    "body": "Hello. Faced with the need to see in the Apache log report the host name by ip-address (dns lookup reverse). I use Elastic Cloud and do not fully understand how this can be done. I added settings to filebeat.yml (on the server where the logs come from), indicated this: processors: - dns: type: reverse fields: source.ip: source.hostname destination.ip: destination.hostname - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ - add_kubernetes_metadata: ~ but the data did not appear in the report. Perhaps you need to somehow add a new field? I am new to this question. Perhaps someone knows why this does not work for me?",
    "website_area": "discuss"
  },
  {
    "id": "1f274488-b6eb-4a9a-a2b5-c9f100c985de",
    "url": "https://discuss.elastic.co/t/navigating-to-different-dashboard-on-clicking-the-data-of-a-visualization/215413",
    "title": "Navigating to different dashboard on clicking the data of a visualization",
    "category": [
      "Kibana"
    ],
    "author": "mruthyu",
    "date": "January 17, 2020, 5:30am January 17, 2020, 2:39pm",
    "body": "I am looking for an option to link the dashboards on clicking on a data of pie chart or histogram and it should route me to a new dashboard.",
    "website_area": "discuss"
  },
  {
    "id": "329b7843-3373-49ac-b548-cd054e20735a",
    "url": "https://discuss.elastic.co/t/dark-light-mode-in-dashboards/215191",
    "title": "Dark / Light mode in Dashboards",
    "category": [
      "Kibana"
    ],
    "author": "rosamar",
    "date": "January 15, 2020, 7:12pm January 17, 2020, 3:12am January 17, 2020, 2:29pm",
    "body": "Is there a way to have an option within the Dashboard to switch between dark and light mode, or even better to link it to the app state related to the same style-modification ability? I am working on a Vue/Vuetify/Vuex app that uses Kibana dashboards and Elastic-Search, and I'd like to have the option to change the dark/light mode of the dashboard either separately from the app or along with the app mode. Thank you in advance.",
    "website_area": "discuss"
  },
  {
    "id": "0d9e1384-6272-4861-878b-92ab46d99e3e",
    "url": "https://discuss.elastic.co/t/percentage-in-kibana/215389",
    "title": "Percentage in Kibana",
    "category": [
      "Kibana"
    ],
    "author": "elasticforme",
    "date": "January 16, 2020, 8:56pm January 17, 2020, 2:57am January 17, 2020, 2:10pm",
    "body": "How do I show percent of failed events and success event per user. I have 100 total document. 10 for user1 - 5 are success, 5 are failed 10 for user2 - 2 are sucesss, 8 are failed 80 for user3 - 40 are success, 40 are failed in this case user1 has 50% failed or of his total 10 user2 has 80% failed user3 has 50% failed and I want graph like that or even pie or anything, just want to show % image1283476 9.59 KB",
    "website_area": "discuss"
  },
  {
    "id": "71be3c5e-64b8-4235-822a-6c4405677ead",
    "url": "https://discuss.elastic.co/t/how-to-integrate-elastic-in-my-angular-application-with-out-i-frame/214578",
    "title": "How to integrate elastic in my angular application with out i frame",
    "category": [
      "Kibana"
    ],
    "author": "semmalai",
    "date": "January 10, 2020, 9:51am January 17, 2020, 9:43am",
    "body": "I have developed one application for log analysis. I will collect all the logs through elastic,logstashand and logbeats. I am using angular8 for the front end. spring-boot as back end . As per my organization requirement, i should not expose kibana/elastic endpoint to the webclient. How ever i can communicate elastic/kibana through my web application(ie through my spring boot-not through iframe). web client to elastic communication will beas follows webclient(angular based->webserver(springboot)->elastic (or kibana). provide is there any solution I gone through different sites for help.Please guide me below points 1.Weather i can use elastic chart for my front end.I am planning to Include the elastic-charts library to my exsisting application and backend i will comunicate elastic through my spring-boot application. Is it very difficult?.if I am doing this weather I am violating the licence policy?",
    "website_area": "discuss"
  },
  {
    "id": "b3660e35-2871-46a4-bb5a-3c81b5ec7c20",
    "url": "https://discuss.elastic.co/t/editable-field-on-dashboard/215318",
    "title": "Editable field on dashboard",
    "category": [
      "Kibana"
    ],
    "author": "giorgiogale",
    "date": "January 16, 2020, 1:44pm January 17, 2020, 2:54am January 17, 2020, 8:44am",
    "body": "Hi, is there a way to add a editable field on my data visualization? thanks",
    "website_area": "discuss"
  },
  {
    "id": "e5eb129b-1cc2-47a0-93f3-94d622f7136b",
    "url": "https://discuss.elastic.co/t/time-filter-for-the-current-month/215361",
    "title": "Time Filter for the current month",
    "category": [
      "Kibana"
    ],
    "author": "mattiacareddu",
    "date": "January 16, 2020, 4:56pm January 17, 2020, 2:41am January 17, 2020, 8:25am",
    "body": "I'm wondering if there is a way to filter the current visualization for the current month. What I need is a time range from the start of the current month month to now. I tried creating a new quick filter but I dind't find a way to calculate the first day of the current month. Is there a way to do that? Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "e2c6fabf-2703-434d-9459-1b593944c600",
    "url": "https://discuss.elastic.co/t/time-filter-on-iframe/215171",
    "title": "Time filter on iframe",
    "category": [
      "Kibana"
    ],
    "author": "Voula_Mikr",
    "date": "January 15, 2020, 4:15pm January 17, 2020, 3:25am",
    "body": "Hi! Is there a way to add time filter in an iframe ? I am using kibana 7.4.2. I have tried to add time filter by deleting embed=true from iframe url but this way the \"user\" of the iframe will have access to the whole kibana. Thank you in advance!",
    "website_area": "discuss"
  },
  {
    "id": "21779d64-c18f-4955-b825-de83e22b7c52",
    "url": "https://discuss.elastic.co/t/sync-kibana-settings-across-multiple-elasticsearch-clusters/215141",
    "title": "Sync Kibana settings across multiple elasticsearch clusters",
    "category": [
      "Kibana"
    ],
    "author": "raulgs",
    "date": "January 15, 2020, 5:19pm January 16, 2020, 6:50am January 17, 2020, 3:05am",
    "body": "I am running multiple kubernetes Clusters that run EFK-Stack. Each cluster runs its own elasticsearch cluster and a Kibana pod which includes the elaticsearch cluster of the other kubernetes clusters as remote cluster. Now I have the problem, that the Kibana settings like dashboards etc. are not synced. Is this possible somehow?",
    "website_area": "discuss"
  },
  {
    "id": "c611342f-3e66-433d-bf9a-19dbe1df725a",
    "url": "https://discuss.elastic.co/t/xls-table/215365",
    "title": "Xls Table",
    "category": [
      "Kibana"
    ],
    "author": "Peter_Balazovic",
    "date": "January 16, 2020, 5:30pm January 17, 2020, 2:38am",
    "body": "Hi I am new to Kinbana, I am looking for creating static Table (aka excel table) in Kibana. Is there a simple way to do it? It might look like picture herein. Note: that all data is there available already - I need to just format it to that kind of table. table855464 71.1 KB",
    "website_area": "discuss"
  },
  {
    "id": "2e6e5002-4071-4d5e-8d59-cf1452f9adda",
    "url": "https://discuss.elastic.co/t/unable-to-connect-to-elasticsearch-error-mapper-parsing-exception-no-handler-for-type-flattened-declared-on-field-state/215378",
    "title": "Unable to connect to Elasticsearch. Error: [mapper_parsing_exception] No handler for type [flattened] declared on field [state]",
    "category": [
      "Kibana"
    ],
    "author": "o1o1o11o1",
    "date": "January 16, 2020, 7:28pm January 16, 2020, 7:55pm January 16, 2020, 8:32pm January 16, 2020, 8:36pm January 16, 2020, 8:46pm January 16, 2020, 10:08pm",
    "body": "After upgrading Elasticsearch and Kibana from 7.1 to 7.5.1 Kibana is unable to start with the following error: Unable to connect to Elasticsearch. Error: [mapper_parsing_exception] No handler for type [flattened] declared on field [state] If I use OSS Kibana, then it starts. Any ideas on what is going on?",
    "website_area": "discuss"
  },
  {
    "id": "a77cb50c-476a-434b-8b16-8d438e9979f8",
    "url": "https://discuss.elastic.co/t/term-query-kibana/214592",
    "title": "Term query Kibana",
    "category": [
      "Kibana"
    ],
    "author": "rvanegmond",
    "date": "January 10, 2020, 11:27am January 10, 2020, 11:15pm January 15, 2020, 8:26am January 16, 2020, 9:15pm",
    "body": "I'm looking into a way to perform a term query straight from the search bar in Kibana. Found some stuff on this but last post is about a year old. I know I can then do this by editing the filter using the query DSL. When I use the defaults it is using a phrase query or a match but not a term query.",
    "website_area": "discuss"
  },
  {
    "id": "f3708e23-8bc7-4595-8fd0-c2e0ad7260ec",
    "url": "https://discuss.elastic.co/t/urgent-help-needed-filter-for-value-in-a-datatable-on-a-dashboard-not-responding/214951",
    "title": "Urgent Help needed: Filter for Value in a Datatable on a Dashboard not responding",
    "category": [
      "Kibana"
    ],
    "author": "Datakids",
    "date": "January 14, 2020, 8:41am January 14, 2020, 10:18am January 14, 2020, 10:27am January 14, 2020, 4:32pm January 16, 2020, 9:00pm",
    "body": "Hi Folks, ELK v7.5.1 Windows Server 16 Chrome, FF (same behaviour) I got a major issue here for unknown reason. I have 3 Indicies I Build for each a Datatable I create a Dashboard and bring them on But just one of them is responding if I do + or - to Filter for Values inside the table Now, if I switch to visualize Filter for Value is working well with each table but not on my Dashbord!? Please this is major Is there any Adjustment maybe inside Advanced Setting which handles this ? Thanks for any reply",
    "website_area": "discuss"
  },
  {
    "id": "d5944b41-8324-4315-a7dc-6475326e34bc",
    "url": "https://discuss.elastic.co/t/upgrade-to-kibana-7-5-failed-partially/215217",
    "title": "Upgrade to kibana 7.5 failed (partially)",
    "category": [
      "Kibana"
    ],
    "author": "bruce_clegg",
    "date": "January 15, 2020, 9:35pm January 16, 2020, 2:30am January 16, 2020, 6:21pm January 16, 2020, 8:10pm",
    "body": "I upgraded my kibana instance from 4.4 to 7.5 today on my CentOS7 server. I used the rpm method to install initially - so this is what I used to update. The upgrade appeared successful - but it was a bit of a mess. I had to manually create a kibana user and group. I still can't get the job to start using systemctl. to get it to work I have to run: sudo -u kibana /usr/share/kibana/bin/kibana -c /etc/kibana/kibana & The service IS set up to run as kibana in /etc/systemd/system/kibana.service. I believe the problem is perhaps related to the permissions for /usr/share/kibana/optimize/.babel_register_cache.json - I have tried several different permissions for the file and the optimize folder. Right now the permissions look like this: ls -ltra optimze/.babel_register_cache.json -rw-rw-r-- 1 kibana kibana 158M Jan 15 20:53 optimize/.babel_register_cache.json earlier, the ownership was for some deleted user/group 966:963 iirc Here are some of the errors I'm seeing: [root@ip-10-10-4-80 optimize]# systemctl status kibana  kibana.service - Kibana Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled) Active: failed (Result: start-limit) since Wed 2020-01-15 20:04:54 UTC; 1min 0s ago Process: 5716 ExecStart=/usr/share/kibana/bin/kibana -c /etc/kibana/kibana.yml (code=exited, status=1/FAILURE) Main PID: 5716 (code=exited, status=1/FAILURE) Jan 15 20:04:51 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: kibana.service: main process exited, code=exited, status=1/FAILURE Jan 15 20:04:51 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: Unit kibana.service entered failed state. Jan 15 20:04:51 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: kibana.service failed. Jan 15 20:04:54 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: kibana.service holdoff time over, scheduling restart. Jan 15 20:04:54 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: Stopped Kibana. Jan 15 20:04:54 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: start request repeated too quickly for kibana.service Jan 15 20:04:54 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: Failed to start Kibana. Jan 15 20:04:54 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: Unit kibana.service entered failed state. Jan 15 20:04:54 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: kibana.service failed. from journalctl: Jan 15 20:48:16 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: Started Kibana. Jan 15 20:48:17 ip-10-10-4-80.us-west-2.compute.internal kibana[3712]: /usr/share/kibana/node_modules/@babel/register/lib/cache.js:80 Jan 15 20:48:17 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: kibana.service: main process exited, code=exited, status=1/FAILURE Jan 15 20:48:17 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: Unit kibana.service entered failed state. Jan 15 20:48:17 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: kibana.service failed. Jan 15 20:48:20 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: kibana.service holdoff time over, scheduling restart. Jan 15 20:48:20 ip-10-10-4-80.us-west-2.compute.internal systemd[1]: Stopped Kibana. Any help is appreciated!",
    "website_area": "discuss"
  },
  {
    "id": "7b52c818-4384-43a8-b1f3-c5c301aaf6ec",
    "url": "https://discuss.elastic.co/t/selecting-multiple-time-frames-to-compare-data-performace/214644",
    "title": "Selecting multiple time-frames to compare data/performace",
    "category": [
      "Kibana"
    ],
    "author": "rosamar",
    "date": "January 10, 2020, 5:33pm January 10, 2020, 5:52pm January 10, 2020, 6:14pm January 10, 2020, 6:40pm January 10, 2020, 6:42pm January 16, 2020, 6:45pm",
    "body": "Is it possible to show the same data in a visualization/dashboard for 2 or more different time-frames to give a visual indication of production trends and similar details to the end-user? Thank you",
    "website_area": "discuss"
  },
  {
    "id": "377adcb3-c2d4-40ed-8b19-56e2d2d1d360",
    "url": "https://discuss.elastic.co/t/incorrect-values-order-on-the-x-axis/215003",
    "title": "Incorrect values order on the X-axis",
    "category": [
      "Kibana"
    ],
    "author": "Anne_Kim",
    "date": "January 14, 2020, 2:38pm January 14, 2020, 8:54pm January 15, 2020, 12:22pm January 15, 2020, 2:56pm January 16, 2020, 2:54pm January 17, 2020, 10:03am",
    "body": "Hello, The X-axis values are ordered Alphabetically (Ascending), but the visualization is incorrect (see image attached): 19201057 151 KB Here is the visState: { \"title\": \"Characters damages per a day after registration\", \"type\": \"line\", \"params\": { \"addLegend\": true, \"addTimeMarker\": false, \"addTooltip\": true, \"categoryAxes\": [ { \"id\": \"CategoryAxis-1\", \"labels\": { \"filter\": true, \"show\": true, \"truncate\": 100, \"rotate\": 0 }, \"position\": \"bottom\", \"scale\": { \"type\": \"linear\" }, \"show\": true, \"style\": {}, \"title\": {}, \"type\": \"category\" } ], \"dimensions\": { \"x\": { \"accessor\": 1, \"format\": { \"id\": \"terms\", \"params\": { \"id\": \"number\", \"otherBucketLabel\": \"Other\", \"missingBucketLabel\": \"Missing\" } }, \"params\": {}, \"aggType\": \"terms\" }, \"y\": [ { \"accessor\": 2, \"format\": { \"id\": \"number\" }, \"params\": {}, \"aggType\": \"avg\" }, { \"accessor\": 3, \"format\": { \"id\": \"number\" }, \"params\": {}, \"aggType\": \"avg\" } ], \"splitRow\": [ { \"accessor\": 0, \"format\": { \"id\": \"terms\", \"params\": { \"id\": \"string\", \"otherBucketLabel\": \"Other\", \"missingBucketLabel\": \"Missing\" } }, \"params\": {}, \"aggType\": \"terms\" } ] }, \"grid\": { \"categoryLines\": false, \"valueAxis\": \"ValueAxis-1\" }, \"labels\": { \"show\": true }, \"legendPosition\": \"top\", \"orderBucketsBySum\": false, \"radiusRatio\": 75, \"seriesParams\": [ { \"data\": { \"id\": \"1\", \"label\": \"Damage dealt\" }, \"drawLinesBetweenPoints\": true, \"interpolate\": \"linear\", \"mode\": \"normal\", \"show\": \"true\", \"showCircles\": true, \"type\": \"histogram\", \"valueAxis\": \"ValueAxis-1\" }, { \"data\": { \"id\": \"2\", \"label\": \"Damage received\" }, \"drawLinesBetweenPoints\": true, \"interpolate\": \"linear\", \"mode\": \"normal\", \"show\": true, \"showCircles\": true, \"type\": \"histogram\", \"valueAxis\": \"ValueAxis-1\" } ], \"times\": [], \"type\": \"line\", \"valueAxes\": [ { \"id\": \"ValueAxis-1\", \"labels\": { \"filter\": false, \"rotate\": 0, \"show\": true, \"truncate\": 100 }, \"name\": \"LeftAxis-1\", \"position\": \"left\", \"scale\": { \"defaultYExtents\": false, \"mode\": \"normal\", \"type\": \"linear\" }, \"show\": true, \"style\": {}, \"title\": { \"text\": \"Average damage_dealt_amount\" }, \"type\": \"value\" } ], \"thresholdLine\": { \"show\": false, \"value\": 10, \"width\": 1, \"style\": \"full\", \"color\": \"#34130C\" } }, \"aggs\": [ { \"id\": \"1\", \"enabled\": true, \"type\": \"avg\", \"schema\": \"metric\", \"params\": { \"field\": \"damage_dealt_amount\", \"customLabel\": \"Damage dealt\" } }, { \"id\": \"2\", \"enabled\": true, \"type\": \"avg\", \"schema\": \"metric\", \"params\": { \"field\": \"damage_received_amount\", \"customLabel\": \"Damage received\" } }, { \"id\": \"3\", \"enabled\": true, \"type\": \"terms\", \"schema\": \"segment\", \"params\": { \"field\": \"days_after_registration\", \"orderBy\": \"_key\", \"order\": \"asc\", \"size\": 99999, \"otherBucket\": true, \"otherBucketLabel\": \"Other\", \"missingBucket\": true, \"missingBucketLabel\": \"Missing\" } }, { \"id\": \"4\", \"enabled\": true, \"type\": \"terms\", \"schema\": \"split\", \"params\": { \"field\": \"character_name.keyword\", \"orderBy\": \"_key\", \"order\": \"asc\", \"size\": 12, \"otherBucket\": true, \"otherBucketLabel\": \"Other\", \"missingBucket\": true, \"missingBucketLabel\": \"Missing\", \"row\": true } } ] } What's the problem?",
    "website_area": "discuss"
  },
  {
    "id": "c7158242-8f0c-4727-8d16-0f949b57b645",
    "url": "https://discuss.elastic.co/t/blog-post-update-required/215077",
    "title": "Blog post update required",
    "category": [
      "Kibana"
    ],
    "author": "Rob_wylde",
    "date": "January 15, 2020, 6:26am January 16, 2020, 6:09pm",
    "body": "Blog Post on Custom Base Maps makes use of tilemap.url tilemap.options.attribution etc. but the these variables names have changed since this blog was posted, but i'm sure many are still looking to it as a guide. The new variable names have 'map' prepended as documented here The below code works while the example code in the above blog does not and will result in head bashing. map.tilemap.url: \"https://maps.tilehosting.com/styles/topo/{z}/{x}/{y}.png?key=<APIKEY>\" map.tilemap.options.attribution: \"&#xA9; [OpenMapTiles](http://www.openmaptiles.org/)|&#xA9; [OpenStreetMap contributors](http://www.openstreetmap.org/copyright)\" map.tilemap.options.maxZoom: 18",
    "website_area": "discuss"
  },
  {
    "id": "ef82a59c-7f43-4439-bba2-91574ae50a45",
    "url": "https://discuss.elastic.co/t/how-to-take-only-last-20-records-of-the-document-in-visualization/215117",
    "title": "How to take only last 20 records of the document in Visualization?",
    "category": [
      "Kibana"
    ],
    "author": "Utshab_Saha",
    "date": "January 15, 2020, 10:52am January 16, 2020, 3:37pm",
    "body": "I want to count of hits w.r.t the datetime in visualization but while I use visualization tool to build the chart It applies on the whole document. So when I plot the data with 10 objects (size of the bucket) in Asc order it sorts from the whole document and plots it bellow is the screen shot image1274478 43.2 KB And if I sort it in Desc order it applies on the whole document and shows such result image1261471 37.7 KB Hence I am not able to fetch todays records. Please help Thanks and Regards",
    "website_area": "discuss"
  },
  {
    "id": "9497d766-5f6c-4d65-bdde-f5ee133f5c3f",
    "url": "https://discuss.elastic.co/t/kibana-elasticsearch-cluster-communication/214559",
    "title": "Kibana elasticsearch Cluster communication",
    "category": [
      "Kibana"
    ],
    "author": "vivektsb",
    "date": "January 10, 2020, 8:15am January 10, 2020, 11:10pm January 16, 2020, 9:13am",
    "body": "Hi Team, We are currently using 7 node elasticsearch cluster with 6.7.1 with 5 master eligible nodes. Recently observed that whenever master changes KIbana is not losing connectivity without modifying elasticsearch ip in kibana.yml and automatically connecting to cluster .Is this change implemented in this version? Regards, Vivek",
    "website_area": "discuss"
  },
  {
    "id": "614bb159-49ac-4cec-8e39-04847870fa19",
    "url": "https://discuss.elastic.co/t/counting-unique-values-in-y-axis-of-a-line-bar-visualization/215272",
    "title": "Counting unique values in y axis of a line/bar visualization",
    "category": [
      "Kibana"
    ],
    "author": "Omer_Botenski",
    "date": "January 16, 2020, 9:00am",
    "body": "Hi there! I'm trying to create a line/bar visualization based upon a query which returns all tests I run on my system. The main goal is to show a count of unique test titles on the Y axis (since some tests run several times a day, and I only need to which ran that day, not how many times), while the x axis is date histogram with a day interval and split series of the test titles. The visualization shows all of the tests which ran that day. Looks like this: image33521618 329 KB image6101060 46.5 KB image6181270 50.1 KB What am I doing wrong?",
    "website_area": "discuss"
  },
  {
    "id": "31d7b6b2-c2bd-48c6-a51c-83b773a399c6",
    "url": "https://discuss.elastic.co/t/message-pattern-identification-in-kibana/215251",
    "title": "Message pattern Identification in kibana",
    "category": [
      "Kibana"
    ],
    "author": "manikandanid",
    "date": "January 16, 2020, 6:48am",
    "body": "On searching for a term \"took\" under discover i do get 10k messages. I am sure all these 10K messages fall under a total 15 message pattern. What i need is to get the different pattern available in the 10k messages. Is there any way i can get the pattern available in the search results.",
    "website_area": "discuss"
  },
  {
    "id": "0f28d042-3693-4354-904a-bdb8d346331f",
    "url": "https://discuss.elastic.co/t/kibana-discover-not-showing-data-for-any-time-range/212597",
    "title": "Kibana Discover not showing data for any time range",
    "category": [
      "Kibana"
    ],
    "author": "sharry007",
    "date": "December 20, 2019, 5:10am December 20, 2019, 1:16pm December 23, 2019, 5:20am January 6, 2020, 11:28am January 16, 2020, 6:07am",
    "body": "I have an index with timestamp field mapping as following: \"@timestamp\": { \"type\": \"date\", \"format\": \"EEE, dd MMM YYYY HH:mm:ss z\" } @timestamp field in document shows \"@timestamp\" : \"Wed, 26 Sep 2018 11:55:41 GMT\". When I create an index pattern for this index and and choose Time Filter field name as @timestamp, with default type as date, Kibana discover does not show any result. How can I get it to work?",
    "website_area": "discuss"
  },
  {
    "id": "cb7c0fe7-21c9-44d6-894c-64aa47d4d7b0",
    "url": "https://discuss.elastic.co/t/kibana-maps-for-custom-data/215241",
    "title": "Kibana Maps for custom data",
    "category": [
      "Kibana"
    ],
    "author": "katara",
    "date": "January 16, 2020, 5:42am",
    "body": "Hello all, I have a data like below in my ES NodeName State Country Value LS02 Virginia US 56 LS04 NewYork US 89 IN56 Delhi India 77 So I want to create a map based on the states, with its corresponding Value. I am working with a ELK 7.2.0 set. I'm rather new to Kibana, I'd very much like to have a kick start on what are the things I need to do. Hope I can get an Idea of working this out! Thanks in Advance",
    "website_area": "discuss"
  },
  {
    "id": "e765bdfc-881c-41ac-83db-28bc4eb3af13",
    "url": "https://discuss.elastic.co/t/any-way-to-do-something-similar-to-sqls-in-not-in-other-table/214750",
    "title": "Any way to do something similar to SQL's \"IN\"/\"NOT IN\" other table?",
    "category": [
      "Kibana"
    ],
    "author": "Wang_Hay_Ng",
    "date": "January 13, 2020, 2:07am January 13, 2020, 12:07pm January 14, 2020, 1:16am January 14, 2020, 11:44am January 14, 2020, 12:48pm January 14, 2020, 1:01pm January 14, 2020, 1:04pm January 14, 2020, 1:34pm January 15, 2020, 3:20pm January 16, 2020, 1:42am",
    "body": "i have never used elk before and not very familiar with the structure of the database, mostly work with mysql stuff before at the old place. since people want a visualization of the server log, i put the log into elk stack and gave them a nice looking dashboard. one of the thing they ask for is what \"access code\" is not in use using a data table. they are stored in a separated table in mysql and we were using something like \"SELECT * FROM access_code WHERE code NOT IN (SELECT access_code FROM server_log WHERE time>'start date' AND time<'end date')\" to search for them before. is there any way to do look up like that in kibana for visualization? should i import the lookup table as another index? if i do, how do i compare between 2 index?",
    "website_area": "discuss"
  },
  {
    "id": "881f2e13-264d-4ac3-b282-322ee404734e",
    "url": "https://discuss.elastic.co/t/my-plugin-will-build-and-install-but-kibana-wont-run/214126",
    "title": "My plugin will build and install, but kibana wont run",
    "category": [
      "Kibana"
    ],
    "author": "McKittrick_Kaminski",
    "date": "January 7, 2020, 8:34pm January 8, 2020, 5:08pm January 8, 2020, 5:48pm January 10, 2020, 10:13pm January 14, 2020, 3:28pm January 15, 2020, 8:50pm January 15, 2020, 9:21pm",
    "body": "I'm developing a plugin with Kibana v7.5.1, and I have it in a state where I want to build the plugin and install it into a docker container, and recommit the container and save it to my private registry. All of these steps works just fine yarn kbn bootstrap yarn start # for development yarn build builds just fine, then trying to install and run, the following steps are actually in a Dockerfile, but behave the same done this way. docker run --rm -it --name kb docker.elastic.co/kibana/kibana:7.5.1 bash In another shell: docker cp /path/to/built/plugin.zip kb:/plugin.zip back in kibana container bin/kibana-plugin install file:///plugin.zip ... # installs just fine bin/kibana the bin/kibana command will give me the following output: FATAL Error: Cannot find module '/usr/share/kibana/plugins/myPlugin' plugins/myPlugin exists, and has my code in it. Why cant it be found?",
    "website_area": "discuss"
  },
  {
    "id": "d4d5bf3f-8948-47b3-b8d8-270baf84ec0c",
    "url": "https://discuss.elastic.co/t/dashboard-user-and-dashboard-crator-user/215137",
    "title": "Dashboard user and dashboard crator user",
    "category": [
      "Kibana"
    ],
    "author": "alexolivan",
    "date": "January 15, 2020, 1:19pm January 19, 2020, 6:58pm January 15, 2020, 3:50pm January 15, 2020, 8:49pm",
    "body": "Hi all. I'm playing with the x-pack features, and, once I have some data on a few indices, I want to achieve the following: I would like to have an user that is able to create dashborads, so it is granted explore, visualization and dashboards from just its indices. Then, I would like to have another user that can just see the dashboards. So far, I've created two spaces that shows just 'show' relevant kibana tabs. I have created a pair of roles that have just read access to those Indices, and that grant read to the 'shown' kibana tabs denying access to the 'hidden' tabs I have created the two users, and have assigned each one its space. I'm facing two early issues: First and foremost, Kibana complains It needs an index pattern to retrieve data, which is logic, so ... how can I provide those users their pattern? The management tab is shown, although its areas are secured... is this tab really needed? can't it be hidden? Thank you very much in advance. Best regards.",
    "website_area": "discuss"
  },
  {
    "id": "91412ec0-9602-4b46-a9ea-dfa71c7078ce",
    "url": "https://discuss.elastic.co/t/date-format-error-in-tooltip-when-splitting-a-chart-with-date-histogram/215173",
    "title": "Date format error in tooltip when splitting a chart with date histogram",
    "category": [
      "Kibana"
    ],
    "author": "dcot",
    "date": "January 15, 2020, 4:25pm",
    "body": "Hello, When splitting chart with Date Histogram, the date is properly formatted in the axis label but it is in timestamp format inside the tooltip. tooltip-date-format19221018 111 KB I'm using Kibana 7.5.0 but I see the issue in Kibana demo which is currently 7.4.2 Kibana demo There is a possibly related issue fixing the axis label Is there a workaround for this? I would like to modify whats shown in the tooltip. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "6c791713-0881-4d0c-8488-e3b8f69edb68",
    "url": "https://discuss.elastic.co/t/display-data-in-kibana-table-from-an-elasticsearch-query/215108",
    "title": "Display data in kibana table from an elasticsearch query",
    "category": [
      "Kibana"
    ],
    "author": "reyhanadp",
    "date": "January 15, 2020, 9:51am January 15, 2020, 2:57pm",
    "body": "hi, i have an elasticsearch query as follows: GET default-2019.13/_search?size=0 { \"query\": { \"match_all\": {} }, \"aggs\": { \"group\": { \"terms\": { \"field\": \"jobId.keyword\", \"size\": 3 }, \"aggs\": { \"group_docs\": { \"top_hits\": { \"size\": 1, \"sort\": [ { \"@timestamp\": { \"order\": \"desc\" } } ] } } } } } } then the results of the above query are as follows: { \"took\" : 11, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 10000, \"relation\" : \"gte\" }, \"max_score\" : null, \"hits\" : }, \"aggregations\" : { \"group\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 79110, \"buckets\" : [ { \"key\" : \"4b976147-7463-4f3f-bff1-3841f1f92eef\", \"doc_count\" : 10764, \"group_docs\" : { \"hits\" : { \"total\" : { \"value\" : 10764, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"default-2019.13\", \"_type\" : \"_doc\", \"_id\" : \"YfaQDG8BpC-jBiiLGKw2\", \"_score\" : null, \"_source\" : { \"jobId\" : \"4b976147-7463-4f3f-bff1-3841f1f92eef\", \"fingerprint\" : \"a2fd3dc6-f309-47f8-ba3f-d87fea6a7e3d\", \"machineName\" : \"RVRPARS4ABC19WD\", \"tenantKey\" : \"27d1f088-1e52-4611-b90e-0d17007a5415\", \"logType\" : \"Default\", \"level\" : \"Trace\", \"path\" : \"D:/aplikasi_kerja/logstash-7.4.2/default-2019-20_csv-export.csv\", \"robotName\" : \"Robot3-dev\", \"host\" : \"IGS-RYHN\", \"@timestamp\" : \"2019-12-02T09:39:42.368Z\", \"fileName\" : \"Jaro_Processing\", \"machineId\" : 6, \"logF_BusinessProcessName\" : \"Framework\", \"processName\" : \"RoboticEnterpriseFramework\", \"processVersion\" : \"1.0.0\", \"message\" : \"Write Line Closed\", \"@version\" : \"1\", \"rawMessage\" : \"2019-12-02T09:39:42.3688078Z,Trace,Write Line Closed,0,Robot,,Default,a2fd3dc6-f309-47f8-ba3f-d87fea6a7e3d,RVRPARS4ABC19WD\\Administrator,RVRPARS4ABC19WD,RoboticEnterpriseFramework,1.0.0,4b976147-7463-4f3f-bff1-3841f1f92eef,Robot3-dev,6,,,Jaro_Processing,Framework,27d1f088-1e52-4611-b90e-0d17007a5415\", \"levelOrdinal\" : 0, \"Source\" : \"Robot\", \"windowsIdentity\" : \"RVRPARS4ABC19WD\\Administrator\" }, \"sort\" : [ 1575279582368 ] } ] } } }, { \"key\" : \"ff20ef37-fcfc-425e-a5de-d9c4103949ab\", \"doc_count\" : 7963, \"group_docs\" : { \"hits\" : { \"total\" : { \"value\" : 7963, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"default-2019.13\", \"_type\" : \"_doc\", \"_id\" : \"S_aODG8BpC-jBiiLgicq\", \"_score\" : null, \"_source\" : { \"jobId\" : \"ff20ef37-fcfc-425e-a5de-d9c4103949ab\", \"fingerprint\" : \"fe1c97c5-2a9e-4758-b022-904da3ac31c2\", \"machineName\" : \"RVRPARS4ABC19WD\", \"tenantKey\" : \"27d1f088-1e52-4611-b90e-0d17007a5415\", \"logType\" : \"Default\", \"level\" : \"Trace\", \"path\" : \"D:/aplikasi_kerja/logstash-7.4.2/default-2019-20_csv-export.csv\", \"robotName\" : \"Robot3-dev\", \"host\" : \"IGS-RYHN\", \"@timestamp\" : \"2019-12-02T13:50:33.304Z\", \"fileName\" : \"Jaro_Processing_Rules\", \"machineId\" : 6, \"logF_BusinessProcessName\" : \"Framework\", \"processName\" : \"RoboticEnterpriseFramework\", \"processVersion\" : \"1.0.0\", \"message\" : \"Execute Query Faulted\", \"@version\" : \"1\", \"rawMessage\" : \"2019-12-02T13:50:33.3043824Z,Trace,Execute Query Faulted,0,Robot,,Default,fe1c97c5-2a9e-4758-b022-904da3ac31c2,RVRPARS4ABC19WD\\Administrator,RVRPARS4ABC19WD,RoboticEnterpriseFramework,1.0.0,ff20ef37-fcfc-425e-a5de-d9c4103949ab,Robot3-dev,6,,,Jaro_Processing_Rules,Framework,27d1f088-1e52-4611-b90e-0d17007a5415\", \"levelOrdinal\" : 0, \"Source\" : \"Robot\", \"windowsIdentity\" : \"RVRPARS4ABC19WD\\Administrator\" }, \"sort\" : [ 1575294633304 ] } ] } } }, { \"key\" : \"5fe28a7a-558f-474d-b90e-d480b7fc36fe\", \"doc_count\" : 7617, \"group_docs\" : { \"hits\" : { \"total\" : { \"value\" : 7617, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"default-2019.13\", \"_type\" : \"_doc\", \"_id\" : \"8feSDG8BpC-jBiiLgXrD\", \"_score\" : null, \"_source\" : { \"jobId\" : \"5fe28a7a-558f-474d-b90e-d480b7fc36fe\", \"fingerprint\" : \"ca6d929f-fbff-4498-bf57-a5f6f35d19b3\", \"machineName\" : \"RVRPARS4ABC19WD\", \"tenantKey\" : \"27d1f088-1e52-4611-b90e-0d17007a5415\", \"logType\" : \"Default\", \"level\" : \"Info\", \"path\" : \"D:/aplikasi_kerja/logstash-7.4.2/default-2019-20_csv-export.csv\", \"robotName\" : \"Robot3-dev\", \"host\" : \"IGS-RYHN\", \"@timestamp\" : \"2019-12-03T02:09:27.938Z\", \"fileName\" : \"Main\", \"machineId\" : 6, \"logF_BusinessProcessName\" : \"Framework\", \"processName\" : \"RoboticEnterpriseFramework\", \"totalExecutionTimeInSeconds\" : 43219, \"processVersion\" : \"1.0.0\", \"message\" : \"RoboticEnterpriseFramework execution ended\", \"totalExecutionTime\" : \"12:00:19\", \"@version\" : \"1\", \"rawMessage\" : \"2019-12-03T02:09:27.9387254Z,Info,RoboticEnterpriseFramework execution ended,2,Robot,,Default,ca6d929f-fbff-4498-bf57-a5f6f35d19b3,RVRPARS4ABC19WD\\Administrator,RVRPARS4ABC19WD,RoboticEnterpriseFramework,1.0.0,5fe28a7a-558f-474d-b90e-d480b7fc36fe,Robot3-dev,6,43219,12:00:19,Main,Framework,27d1f088-1e52-4611-b90e-0d17007a5415\", \"levelOrdinal\" : 2, \"Source\" : \"Robot\", \"windowsIdentity\" : \"RVRPARS4ABC19WD\\Administrator\" }, \"sort\" : [ 1575338967938 ] } ] } } } ] } } } Can data from aggregate query results above be displayed in the Kibana table visualization?",
    "website_area": "discuss"
  },
  {
    "id": "14d13683-e925-44c5-b1df-308de79c75d7",
    "url": "https://discuss.elastic.co/t/view-time-series-for-top-x-keywords/215111",
    "title": "View time series for top X keywords",
    "category": [
      "Kibana"
    ],
    "author": "kurtgn",
    "date": "January 15, 2020, 10:56am January 15, 2020, 2:51pm",
    "body": "I have some time series data with revenue metrics and country codes on it. I want to plot a time series linechart with revenue per day, for top 5 countries. To do this, I create a visualization with sum revenue on Y axis, and then create buckets for X-axis with Date Histogram. Then I create sub-bucket aggregation based on country.keyword. But I do not get top 5 countries for the whole period. I get top 5 countries for each day, and more than 5 countries end up on my chart:   2020-01-15  12.57.2524661118 441 KB How do I split my series not by top 5 countries with highest revenue for each day, but by top 5 countries with highest revenue for the whole time period selected?",
    "website_area": "discuss"
  },
  {
    "id": "6d3d9182-c5d8-4275-b56e-5e10df599229",
    "url": "https://discuss.elastic.co/t/cant-log-in-in-kibana-gui/213106",
    "title": "Can't log in in Kibana GUI",
    "category": [
      "Kibana"
    ],
    "author": "emilio",
    "date": "December 26, 2019, 7:23pm December 26, 2019, 8:49pm December 27, 2019, 5:06am December 27, 2019, 2:53pm December 28, 2019, 6:37am December 28, 2019, 11:11am January 2, 2020, 1:09pm January 10, 2020, 9:58am January 10, 2020, 10:21am January 15, 2020, 12:12pm",
    "body": "Dear all, I need your support. I have installed ES 7.5, Kibana 7.5 I have switched license type to trial in order to use x-pack. Also, i have changed all built-in users passwords. Currently no one user can log in Kibana GUI. Here are ES and Kibana configs: ES: path.data: /d02/elastic/data Path to log files: path.logs: /d02/elastic/logs bootstrap.system_call_filter: false #bootstrap.seccomp: false bootstrap.memory_lock: true network.host: \"xx.x.xxx.xx\" Set a custom port for HTTP: http.port: 9200 discovery.type: single-node index.codec: best_compression #index.number_of_shards: 1 xpack.security.enabled: true Kibana server.port: 5601 server.host: \"xx.x.xx.xx\" server.defaultRoute: \"/etc/kibana\" server.maxPayloadBytes: 104857600 server.name: \"kibana\" elasticsearch.hosts: \"http://xx.x.xx.xx:9200\" elasticsearch.preserveHost: true kibana.index: \".kibana\" kibana.defaultAppId: \"discover\" elasticsearch.username: \"kibana\" elasticsearch.password: \"YYYYYYYY\" elasticsearch.pingTimeout: 1500 elasticsearch.requestTimeout: 30000 elasticsearch.requestHeadersWhitelist: elasticsearch.customHeaders: {} elasticsearch.shardTimeout: 0 elasticsearch.startupTimeout: 5000 pid.file: /var/run/kibana.pid logging.dest: /var/log/kibana/kibana.log logging.silent: false logging.quiet: false logging.verbose: true ops.interval: 5000 ---------------------------------- X-Pack ------------------------------------ xpack.security.enabled: true xpack.security.sessionTimeout: 600000 Also some superusers created by me and they can not login Kibana too. What is the reason i can not log in Kibana for built in users and other ones?",
    "website_area": "discuss"
  },
  {
    "id": "2348404e-fcd2-4487-85f0-1e9391e9b9a8",
    "url": "https://discuss.elastic.co/t/screenshots-in-kibana/214961",
    "title": "Screenshots in Kibana",
    "category": [
      "Kibana"
    ],
    "author": "Nickita1608",
    "date": "January 14, 2020, 9:09am January 14, 2020, 10:11am January 14, 2020, 10:19am January 15, 2020, 11:02am",
    "body": "Hello! Previously, I took screenshots using pageres in Kibana v6. * (I also tried other utilities). After updating to v7.2-7.5, the screenshot returns a white screen. It is as if something is limiting and does not allow you to take a screenshot of the dashboard, the main page or any other in Kibana. Do you have any ideas? P.S. The screen is 100% true, because when I boot Kibana, I see an inscription in the screenshot - \"Kibana has not booted yet.\" After loading - a white screen.",
    "website_area": "discuss"
  },
  {
    "id": "fdd011d2-ec17-4b69-97f8-7df3bb770160",
    "url": "https://discuss.elastic.co/t/additional-fields/214964",
    "title": "Additional Fields",
    "category": [
      "Kibana"
    ],
    "author": "amin224",
    "date": "January 14, 2020, 10:01am January 15, 2020, 10:32am",
    "body": "Hello, I am getting logs from a java application using logging.file, is it possible to add a field in kibana UI to see the IP address and port ? sorry i am new to elastic stack, Thank you in advance. fields1842956 111 KB",
    "website_area": "discuss"
  },
  {
    "id": "d2e9e5c2-3ae2-470f-8172-02e8c0acdabd",
    "url": "https://discuss.elastic.co/t/visualizing-non-aggregated-data-on-visualization-dashboard-menu/215047",
    "title": "Visualizing Non aggregated data on visualization/dashboard menu",
    "category": [
      "Kibana"
    ],
    "author": "sayeeguru",
    "date": "January 14, 2020, 9:29pm January 15, 2020, 9:28am",
    "body": "Hi, i know there is a thread stating that we can visualize only aggregated data on visualization option, Did that change? Would like to build trend charts on dashboard, can i do it without aggregation? thanks!",
    "website_area": "discuss"
  },
  {
    "id": "15249f1a-92cb-492d-a00c-cea99ddeb33f",
    "url": "https://discuss.elastic.co/t/visualization-count-every-occurrence-of-every-not-null-field/215101",
    "title": "Visualization : count every occurrence of every not null field",
    "category": [
      "Kibana"
    ],
    "author": "GabrielM",
    "date": "January 15, 2020, 9:18am",
    "body": "Hello ! Is there a way to count the number of occurrences of each not null field in a visualization (a table seems appropriate to me) Kibana please? Thank you in advance and have a nice day !",
    "website_area": "discuss"
  },
  {
    "id": "5b1f2324-514b-43b5-b46b-10eac6637603",
    "url": "https://discuss.elastic.co/t/ping-alert-only-in-specific-time-period-during-the-day/214429",
    "title": "Ping alert only in specific time period during the day",
    "category": [
      "Kibana"
    ],
    "author": "Alexandros888",
    "date": "January 9, 2020, 12:34pm January 9, 2020, 10:18pm January 14, 2020, 8:05am January 15, 2020, 9:21pm",
    "body": "Hello, I have implemented the following advanced watcher alert: > { > \"trigger\": { > \"schedule\": { > \"interval\": \"1m\" > } > }, > \"input\": { > \"search\": { > \"request\": { > \"search_type\": \"query_then_fetch\", > \"indices\": [ > \"heartbeat-*\" > ], > \"rest_total_hits_as_int\": true, > \"body\": { > \"size\": 0, > \"query\": { > \"bool\": { > \"must\": [ > { > \"term\": { > \"url.full\": \"https://shop.matik.ch:443\" > } > }, > { > \"term\": { > \"monitor.status\": { > \"value\": \"down\" > } > } > } > ], > \"filter\": [ > { > \"range\": { > \"@timestamp\": { > \"gte\": \"now-1m\" > } > } > } > ] > } > } > } > } > } > }, > \"condition\": { > \"compare\": { > \"ctx.payload.hits.total\": { > \"gt\": 0 > } > } > }, > \"actions\": { > \"send_email\": { > \"email\": { > \"profile\": \"standard\", > \"to\": [ > \"<alexandros.ananikidis@sag-ag.ch>,<Panayiotis.Stathis@sag-ag.ch>,<Manuel.Fischer@sag-ag.ch>,<D-GRPSAGInformatike-commerce@sag-ag.ch>,<helpdesk@sag-ag.ch>,<thi.nguyen@bbv.vn>,<franco.chiellino@umb.ch>\" > ], > \"subject\": \"https://shop.matik.ch is DOWN\", > \"body\": { > \"text\": \"The Watch [{{ctx.metadata.name}}] has occured ({{ctx.payload.hits.total}}) times the last 1 minute.\" > } > } > } > } > } BUT, i want the alert to make checks only during a specific time period. For example i want from 1:10 to 2:10 AM every night not to make ping checks at all because my server URL is down anyway that time and i receive alerts that i dont need. Help please? Thank you",
    "website_area": "discuss"
  },
  {
    "id": "a46b3147-049f-4b76-ac61-0cc3656872de",
    "url": "https://discuss.elastic.co/t/enable-disable-anonymous-usage-statistics-by-default/214778",
    "title": "Enable / Disable anonymous usage statistics by default?",
    "category": [
      "Kibana"
    ],
    "author": "asp",
    "date": "January 13, 2020, 8:47am January 15, 2020, 6:21am January 14, 2020, 7:19am January 14, 2020, 7:27am January 14, 2020, 11:10pm January 15, 2020, 6:21am",
    "body": "Hi, Is there a way to enable / disable anonymous usage statistics in kibana by configuration, so that no one has to answer this question manually? Any variable in kibana.yml? Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "530a16a4-8977-403a-a5a8-5db361d23278",
    "url": "https://discuss.elastic.co/t/setting-the-fragment-size-of-queries-made-by-a-dashboard/214896",
    "title": "Setting the fragment_size of queries made by a Dashboard",
    "category": [
      "Kibana"
    ],
    "author": "ChubbyRat",
    "date": "January 13, 2020, 8:00pm January 14, 2020, 2:12am January 14, 2020, 6:46pm January 14, 2020, 11:17pm",
    "body": "I want fragment_size to change on only some queries (on one specific Dashboard) so changing the fragment_size in Elastic isn't viable. Is it possible to set the fragment_size Kibana-side? Elastic and Kibana versions are both 6.8.5.",
    "website_area": "discuss"
  },
  {
    "id": "713feef0-00c2-4f13-9ade-951f1488db64",
    "url": "https://discuss.elastic.co/t/available-fields-in-kibana/214864",
    "title": "Available fields in kibana",
    "category": [
      "Kibana"
    ],
    "author": "volcano",
    "date": "January 13, 2020, 3:50pm January 14, 2020, 2:19am January 14, 2020, 4:39am January 14, 2020, 11:16pm",
    "body": "Here is this entry in logstash.conf mutate { split => [\"message\",\"Employee\"] add_field => {\"part1\" =>\"%{[message][0]}\"} add_field => {\"part2\" =>\"%{[message][1]}\"} } mutate { split => [\"part2\",\"#\"] add_field => {\"part2_1\" =>\"%{[part2][0]}\"} add_field => {\"part2_2\" =>\"%{[part2][1]}\"} } This adds part1 , part2 , part2_1 and part2_2 fields in Kibana's Available field list. But my requirement is to add only part2_2 field . rest of the fields are not required in Kibana. What changes I should make here so that only part2_2 field is added in Kibana's Available field list.",
    "website_area": "discuss"
  },
  {
    "id": "0f4f6651-86f4-402c-9ce9-78eae26b029e",
    "url": "https://discuss.elastic.co/t/kibana-7-5-1-unable-to-use-kibana-keystore-due-to-permissions-problem/215050",
    "title": "Kibana 7.5.1 Unable to use Kibana Keystore due to Permissions Problem",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 14, 2020, 9:38pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0fca0d46-07cd-4354-a4f2-63a1c181d6b9",
    "url": "https://discuss.elastic.co/t/getting-5-6-hours-of-data-delay-while-extracting-data-from-elastic-search-into-kibana-dev-tools-console/213990",
    "title": "Getting 5 - 6 hours of data delay while extracting data from elastic search into kibana dev tools/console",
    "category": [
      "Kibana"
    ],
    "author": "suraj888",
    "date": "January 7, 2020, 6:47am January 8, 2020, 5:12pm January 8, 2020, 9:10pm January 13, 2020, 6:12am January 13, 2020, 11:51am January 14, 2020, 2:35pm January 14, 2020, 8:12pm",
    "body": "Dear All, I am new to ELK and i apologize in advance for my question. I am trying to extract data from one of the index of elasticsearch to kibana dev tools, through query but i am getting data delay for nearly about 5-6 hours. While visualizing on the kibana dashboard it is showing correct data but in kibana dev tool/console it is showing 5-6 hours data dalay. I have used metricbeat to send data to elastic search. Can anyone please help me to understand why i am not able to get the current data on dev tool/console. Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "7edeaa0d-8985-410b-90d3-13e382bba4aa",
    "url": "https://discuss.elastic.co/t/error-on-discover/214906",
    "title": "Error on discover",
    "category": [
      "Kibana"
    ],
    "author": "vitornogueira",
    "date": "January 13, 2020, 9:33pm January 14, 2020, 1:59am January 14, 2020, 1:01pm January 14, 2020, 1:51pm January 14, 2020, 8:05pm",
    "body": "When I access the Discover screen the following error appears: image1000379 50.6 KB I ran the following command to rebuild the index pattern apm- *: apm-server setup Kibana version: 7.5",
    "website_area": "discuss"
  },
  {
    "id": "7c64767c-f5ed-48a8-a856-3c3bf8394167",
    "url": "https://discuss.elastic.co/t/removing-the-old-reports/215021",
    "title": "Removing the old reports",
    "category": [
      "Kibana"
    ],
    "author": "emnioj",
    "date": "January 14, 2020, 5:12pm",
    "body": "Hi Team, i have created so many reports and now would like to delete the old reports. Since i can not find any tab existing to deleting the old reports. Is there any way we could delete the reports only? all these are stored in .kibana index by default. would it possible to delete report details part from .kibana index. kibana 7 version is being used. regards. Manish",
    "website_area": "discuss"
  },
  {
    "id": "d57e49dc-a83d-45fd-bf85-02cde3f827e7",
    "url": "https://discuss.elastic.co/t/search-within-2-index-where-not-in/215012",
    "title": "Search within 2 index where 'not in'",
    "category": [
      "Kibana"
    ],
    "author": "pbc3199",
    "date": "January 14, 2020, 4:01pm",
    "body": "I have 2 index in elasticsearch, trxtxn ( transaction ) and trxrsp ( response ) Both index have same unique id call header.serialnum. Is it possible to display document only for trxtxn which not in trxrsp? So, I can know which transaction don't have response yet. TQ",
    "website_area": "discuss"
  },
  {
    "id": "cd22d5bb-ceec-42f4-95fb-6739c8b06b6b",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-have-one-dashboard-from-two-different-kibana-servers/214858",
    "title": "Is it possible to have one dashboard from two different Kibana servers?",
    "category": [
      "Kibana"
    ],
    "author": "emad101",
    "date": "January 13, 2020, 3:01pm January 13, 2020, 3:03pm January 13, 2020, 3:14pm January 13, 2020, 3:18pm January 14, 2020, 3:38pm January 14, 2020, 2:33pm January 14, 2020, 3:39pm",
    "body": "I have two different kibana servers and I wanted to know if it would be possible to create visualizations from both kibana servers into one dashboard for either server. Thank you in advance",
    "website_area": "discuss"
  },
  {
    "id": "7c617e39-1340-4758-9225-ddd7e0961cca",
    "url": "https://discuss.elastic.co/t/time-filter-behaving-strangely/214680",
    "title": "Time Filter behaving strangely",
    "category": [
      "Kibana"
    ],
    "author": "sidhusaab",
    "date": "January 10, 2020, 10:38pm January 13, 2020, 8:34am January 13, 2020, 11:57am January 13, 2020, 3:41pm January 13, 2020, 3:55pm January 13, 2020, 4:36pm January 13, 2020, 4:39pm January 13, 2020, 4:58pm January 13, 2020, 5:36pm January 13, 2020, 6:55pm January 13, 2020, 7:17pm January 13, 2020, 8:42pm January 13, 2020, 9:14pm January 13, 2020, 9:28pm January 13, 2020, 9:38pm January 14, 2020, 3:33pm January 14, 2020, 3:38pm",
    "body": "I have an index which contains data for the last 3 days (Jan 08 - Jan 10). I have a dashboard that shows different things from the index and it seems to only work if my time range is set to 7 days or more. If I set last 24 hours, last hour etc the charts do not show any data. I have attached the gif that shows this behavior. I am clueless on the cause and require help or suggestions on what could this be. elk_time937689 1.34 MB",
    "website_area": "discuss"
  },
  {
    "id": "7f4945b9-ecc3-48b5-9277-5e55b9faaa8d",
    "url": "https://discuss.elastic.co/t/efficient-aggregation-of-counting-nested-field-values/214869",
    "title": "Efficient aggregation of counting nested field values",
    "category": [
      "Kibana"
    ],
    "author": "Elaak",
    "date": "January 14, 2020, 8:55am January 13, 2020, 5:03pm January 13, 2020, 5:04pm January 14, 2020, 9:15am January 14, 2020, 3:20pm January 14, 2020, 3:21pm January 14, 2020, 3:27pm",
    "body": "Like so many others I run into the well-known \"too_many_buckets_exception\". I am creating a data table visualization which sorts my data over four different fields, and then finally does a count over a fifth field. A picture is worth a thousand words ... Because I do a nested aggregation first over my four fields, and then a combined aggregation of the results, I will quickly get too many buckets. Is there a more efficient way to aggregate my results to create the table I want? I think the biggest complication is that the final two fields are both date fields ...",
    "website_area": "discuss"
  },
  {
    "id": "0786cc28-82b3-4f22-b57e-64f08aec0e45",
    "url": "https://discuss.elastic.co/t/wrong-data-aggregation-in-a-table/212033",
    "title": "Wrong data aggregation in a table",
    "category": [
      "Kibana"
    ],
    "author": "Anne_Kim",
    "date": "December 16, 2019, 4:22pm December 16, 2019, 4:50pm December 16, 2019, 5:05pm December 16, 2019, 5:17pm December 17, 2019, 10:29am December 17, 2019, 4:07pm January 14, 2020, 2:21pm",
    "body": "Hello, I've created a table, but the data in the table differ from the elastic. For example, for player_id \"750\" there are 6 records from 2 different devices: 1426649 76.9 KB However it's impossible because each player's id is uniquely connected with a device: 1 device = 1 player_id and and vice versa. In Elastic with the id = 750 only one device is connected: 1886859 93.2 KB The same problem I've encountered at various IDs: e.g., id 744. In a data table we see 5 different devices connected with the id: 1739773 119 KB But in Elastic there is only one device: 1872732 89.5 KB What it can be caused by? Recently I've updated Kibana from 7.3 up to 7.4, so can the problem be connected either with the Kibana 7.4 version or with the incorrect upgrade from my side? Below I attach the request of the table vizualization: { \"aggs\": { \"2\": { \"terms\": { \"field\": \"device.keyword\", \"order\": { \"1\": \"desc\" }, \"missing\": \"__missing__\", \"size\": 5000 }, \"aggs\": { \"1\": { \"avg\": { \"field\": \"stage_duration\" } }, \"3\": { \"histogram\": { \"field\": \"player_id\", \"interval\": 6, \"min_doc_count\": 1 }, \"aggs\": { \"7\": { \"histogram\": { \"field\": \"tutorial_id\", \"interval\": 5, \"min_doc_count\": 1 }, \"aggs\": { \"6\": { \"histogram\": { \"field\": \"stage_id\", \"interval\": 1, \"min_doc_count\": 0 }, \"aggs\": { \"4\": { \"terms\": { \"field\": \"completed\", \"order\": { \"1\": \"desc\" }, \"size\": 5 }, \"aggs\": { \"1\": { \"avg\": { \"field\": \"stage_duration\" } }, \"5\": { \"terms\": { \"field\": \"created_at\", \"order\": { \"1\": \"desc\" }, \"size\": 50 }, \"aggs\": { \"1\": { \"avg\": { \"field\": \"stage_duration\" } } } } } } } } } } } } } } }, \"size\": 0, \"_source\": { \"excludes\": [] }, \"stored_fields\": [ \"*\" ], \"script_fields\": {}, \"docvalue_fields\": [ { \"field\": \"created_at\", \"format\": \"date_time\" }, { \"field\": \"player_created_at\", \"format\": \"date_time\" } ], \"query\": { \"bool\": { \"must\": [], \"filter\": [ { \"bool\": { \"filter\": [ { \"bool\": { \"should\": [ { \"match\": { \"is_tester\": false } } ], \"minimum_should_match\": 1 } }, { \"bool\": { \"should\": [ { \"match\": { \"build\": \"0.3.0.1650\" } } ], \"minimum_should_match\": 1 } } ] } }, { \"range\": { \"stage_id\": { \"gte\": 16, \"lt\": 19 } } }, { \"range\": { \"created_at\": { \"format\": \"strict_date_optional_time\", \"gte\": \"2019-12-10T22:00:00.000Z\", \"lte\": \"2019-12-13T21:30:00.000Z\" } } } ], \"should\": [], \"must_not\": [] } } }",
    "website_area": "discuss"
  },
  {
    "id": "9a0e7167-7702-4670-8130-e868a44767c5",
    "url": "https://discuss.elastic.co/t/auto-auth-for-kibana-visualization-using-nginx-not-working/215001",
    "title": "Auto Auth for kibana visualization using Nginx not working",
    "category": [
      "Kibana"
    ],
    "author": "Incauto",
    "date": "January 14, 2020, 2:16pm",
    "body": "Hi im trying to use nginx to auto authenticate in kibana shared visualizations and it isnt working. Im on a centos 8, installed nginx, installed httpd-tools ,I create a folder called: sites-available, with a filed called: default on it, wich contains this code: server { listen 80; server_name _; auth_basic \"Restricted Access\"; auth_basic_user_file /etc/nginx/htpasswd.users; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } Created a user and a password with this commands sudo sh -c \"echo -n 'sammy:' >> /etc/nginx/.htpasswd\" sudo sh -c \"openssl passwd -apr1 >> /etc/nginx/.htpasswd\" In my kibana.yml I have this line: server.host: \"localhost\" started nginx, load the shared visualization in browser, but the login page keeps on showing. what Im doing wrong?",
    "website_area": "discuss"
  },
  {
    "id": "cc5a2878-f0eb-4f78-8e50-ee7b95a9085a",
    "url": "https://discuss.elastic.co/t/timelion-extremities-not-showing/214992",
    "title": "Timelion extremities not showing",
    "category": [
      "Kibana"
    ],
    "author": "Antoine2",
    "date": "January 14, 2020, 1:24pm January 14, 2020, 2:04pm",
    "body": "Timelion data is not showing properly. I feel it's an issue with UTC vs local timezone but it's not only that. I did change my local time zone to UTC without success. When selecting a 24 hours time range under the \"absolute\" tab of the date picker , the beginning and the end of timelion's graph is blank (24h): Selection_0471192929 51 KB But there's no gap of that sort in my data (~48h): Selection_0481190939 76 KB kibana=true in the timelion expression. It seems I always get a little bit of missing data, it's just more obvious as the time range shorten. Kibana 6.6.1",
    "website_area": "discuss"
  },
  {
    "id": "8b21c69f-88ff-410d-926b-0da8b60e8bbb",
    "url": "https://discuss.elastic.co/t/count-within-field/214986",
    "title": "Count within field",
    "category": [
      "Kibana"
    ],
    "author": "treknado",
    "date": "January 14, 2020, 11:42am January 14, 2020, 1:46pm January 14, 2020, 1:38pm January 14, 2020, 3:31pm",
    "body": "Hello, i have documents with multiline fields in my.field and need to count the number of received filenames. example: document 1: my.field: { \"filename\": \"test.json\", \"type\" : \"File\"} document 2: my.field: { \"filename\": \"test2.json\", \"type\" : \"File\"}, { \"filename\": \"test3.json\", \"type\" : \"File\"} with my kibana skills, i can only count the number of documents --> 2 But i want the number filenames within my document: document1 (test.json) + document2 (test2.json, test3.json) --> 3 How can i handle it ? I already read articles about scripted fields. How can i use them here ? best regards treknado",
    "website_area": "discuss"
  },
  {
    "id": "40137c9f-6f50-4fb6-b302-c6bd7e7901f9",
    "url": "https://discuss.elastic.co/t/zoom-out/214996",
    "title": "Zoom out",
    "category": [
      "Kibana"
    ],
    "author": "Voula_Mikr",
    "date": "January 14, 2020, 1:30pm",
    "body": "Hi! I did zoom in at a histogram but there is no way to zoom out except pressing back botton. Is there any other way to zoom out that I am missing? Also when you zoom in in a visualization of an iframe there is no way to see that through the \"time filter\" or a filter addition at the top of the iframe . Is there a way to add a filter at the top of iframe when you do a zoom in? Thank you in advance!",
    "website_area": "discuss"
  },
  {
    "id": "b17dfebf-ab6e-4a9f-927f-ab17c5eed5b7",
    "url": "https://discuss.elastic.co/t/index-patterns-management/214854",
    "title": "Index Patterns management",
    "category": [
      "Kibana"
    ],
    "author": "Mauricio_Borges",
    "date": "January 13, 2020, 2:55pm January 14, 2020, 4:27am January 14, 2020, 11:57am January 14, 2020, 1:28pm",
    "body": "Hi Team! Where I can learn more about Index Pattern Management ? I'd like to know how I can group couple servers by indexes ( using Beats and ES ). For example: Server A, Server B and Server C ingest Logs into filebeat-ABC* index pattern. Server D, Server F and Server G ingest Logs into filebeat-DFG* index pattern. And the same idea for metrics, packages, heart beats logs. Thanks! Mauricio",
    "website_area": "discuss"
  },
  {
    "id": "8035a5ab-71f8-45df-9a9e-e9c06ae6f72a",
    "url": "https://discuss.elastic.co/t/ml-anomaly-detection-question/214090",
    "title": "ML anomaly detection question",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 7, 2020, 3:41pm January 7, 2020, 4:02pm January 7, 2020, 4:42pm January 7, 2020, 6:34pm January 8, 2020, 8:31am January 8, 2020, 10:01am January 8, 2020, 11:45am January 14, 2020, 11:43am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "86e6a631-3884-4715-94c8-1d564111dbd9",
    "url": "https://discuss.elastic.co/t/failed-to-authenticate-user-when-starting-kibana-service/214984",
    "title": "Failed to authenticate user when starting kibana service",
    "category": [
      "Kibana"
    ],
    "author": "Tennis",
    "date": "January 14, 2020, 11:36am",
    "body": "My Kibana version 7.5.1 and it seems that x-pack is a default setting. I added the configuration following in elasticsearch.yml: xpack.security.enabled: true xpack.security.transport.ssl.enabled: true In kibana.yml elasticsearch.username: \"elastic\" elasticsearch.password: \"pass\" xpack.security.sessionTimeout: 600000 When I started the kibana service and checked the status: Jan 14 19:15:27 node kibana[7744]: {\"type\":\"log\",\"@timestamp\":\"2020-01-14T11:15:27Z\",\"tags\":[\"warning\",\"legacy-plugins\"],\"pid\":7744,\"path\":\"/usr/share/kibana/src/legacy/core_plugins/visualizations\",\"message\":\"Skipping non-plugin directory at /usr/share/kibana/src/legacy/core_plugins/visualizations\"} Jan 14 19:15:27 node kibana[7744]: {\"type\":\"log\",\"@timestamp\":\"2020-01-14T11:15:27Z\",\"tags\":[\"warning\",\"plugins\",\"licensing\"],\"pid\":7744,\"message\":\"License information could not be obtained from Elasticsearch for the [data] cluster. [security_exception] failed to authenticate user [elastic], with { header={ WWW-Authenticate=\"Basic realm=\\\"security\\\" charset=\\\"UTF-8\\\"\" } } :: {\"path\":\"/_xpack\",\"statusCode\":401,\"response\":\"{\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"security_exception\\\",\\\"reason\\\":\\\"failed to authenticate user [elastic]\\\",\\\"header\\\":{\\\"WWW-Authenticate\\\":\\\"Basic realm=\\\\\\\"security\\\\\\\" charset=\\\\\\\"UTF-8\\\\\\\"\\\"}}],\\\"type\\\":\\\"security_exception\\\",\\\"reason\\\":\\\"failed to authenticate user [elastic]\\\",\\\"header\\\":{\\\"WWW-Authenticate\\\":\\\"Basic realm=\\\\\\\"security\\\\\\\" charset=\\\\\\\"UTF-8\\\\\\\"\\\"}},\\\"status\\\":401}\",\"wwwAuthenticateDirective\":\"Basic realm=\\\"security\\\" charset=\\\"UTF-8\\\"\"}\"} Jan 14 19:15:27 node kibana[7744]: {\"type\":\"log\",\"@timestamp\":\"2020-01-14T11:15:27Z\",\"tags\":[\"info\",\"plugins-system\"],\"pid\":7744,\"message\":\"Starting [8] plugins: [timelion,features,code,security,licensing,spaces,translations,data]\"} Jan 14 19:15:27 node kibana[7744]: {\"type\":\"log\",\"@timestamp\":\"2020-01-14T11:15:27Z\",\"tags\":[\"warning\",\"migrations\"],\"pid\":7744,\"message\":\"Unable to connect to Elasticsearch. Error: [security_exception] failed to authenticate user [elastic], with { header={ WWW-Authenticate=\"Basic realm=\\\"security\\\" charset=\\\"UTF-8\\\"\" } }\"} When I accessed http://node:5601 from the browser, It also showed Kibana server is not ready yet. In http://node:9200, It popped up an authentification required, but when I typed the user name: elastic; password: pass as I set on the kibana.yml. It still can't log in. How can I make my kibana service start with login page?",
    "website_area": "discuss"
  },
  {
    "id": "d8fb4090-f67e-46ab-b0ff-e24238673349",
    "url": "https://discuss.elastic.co/t/use-item-of-field-to-plot-chart/214826",
    "title": "Use item of field to plot chart",
    "category": [
      "Kibana"
    ],
    "author": "Tennis",
    "date": "January 13, 2020, 12:14pm January 14, 2020, 9:49am January 14, 2020, 9:49am",
    "body": "My index data: Item: Book, count: 1, time: ... Item: Car, count: 2, time: ... Item: Car, count: 3, time: ... Item: Book, count: 5, time: ... Item: Cat, count: 2, time: ... Item: Car, count: 7, time: ... I want to plot a chart using time as x axis and count as y axis with only Car value of Item field. Can kibana do this without change my index data structure or other better way to this.",
    "website_area": "discuss"
  },
  {
    "id": "cb562687-1160-42cb-82b2-b6ededb94f65",
    "url": "https://discuss.elastic.co/t/normalized-histograms/211450",
    "title": "Normalized Histograms",
    "category": [
      "Kibana"
    ],
    "author": "Aravinda_Kolla",
    "date": "December 11, 2019, 10:50am December 20, 2019, 1:07pm January 14, 2020, 9:03am",
    "body": "Say you are plotting a bar plot of top n items occurring in index 'A'. Additional information about these items, such as length of the item, is present in index 'B'. Plot a normalized histogram of count/length.",
    "website_area": "discuss"
  },
  {
    "id": "1347ce31-c99f-46be-be72-edc6d8d51c98",
    "url": "https://discuss.elastic.co/t/possible-to-use-custom-dhparam/214945",
    "title": "Possible to use custom dhparam?",
    "category": [
      "Kibana"
    ],
    "author": "asp",
    "date": "January 14, 2020, 8:15am",
    "body": "Hi, can I use an individual dhparm file with kibana's tls? Do I need to append dhparam fragment to the cert, is there a parameter to load the dhparam file explicitly or will kibana just ignore custom dhparams? Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "0e287003-268a-4450-b9c2-d08d50f0e29c",
    "url": "https://discuss.elastic.co/t/how-to-not-display-group-by-field/214934",
    "title": "How to not display group by field",
    "category": [
      "Kibana"
    ],
    "author": "reyhanadp",
    "date": "January 14, 2020, 6:31am",
    "body": "I have the following data: ss14161080 74.1 KB I want to display data group by jobId and display the latest processName and message based on jobId. but jobId doesn't need to be displayed. I have successfully displayed processName and message but jobId cannot be hidden: sq1742903 83.5 KB I want to display as follows : sw1603903 63.8 KB",
    "website_area": "discuss"
  },
  {
    "id": "500d304f-8270-4c1e-8558-9f5cd289c531",
    "url": "https://discuss.elastic.co/t/how-can-i-create-a-time-line/213982",
    "title": "How can i create a time line",
    "category": [
      "Kibana"
    ],
    "author": "prasannarajan",
    "date": "January 7, 2020, 5:28am January 8, 2020, 4:15pm January 14, 2020, 6:29am",
    "body": "i need to visualize a bar chart and have time frame of created date and updated date. but my X-axis can only hold either created or updated.i need to create where both can be seen",
    "website_area": "discuss"
  },
  {
    "id": "7b2ba152-ac98-426b-ac80-1be5de017e50",
    "url": "https://discuss.elastic.co/t/on-kibanas-display-of-super-long-fields/214701",
    "title": "On kibana's display of super long fields",
    "category": [
      "Kibana"
    ],
    "author": "ahahaex",
    "date": "January 11, 2020, 10:29am January 14, 2020, 5:23am",
    "body": "When there are very long fields in the dataKibana can't display data very well. So , I set truncated string: 100.Now kibana's discovery can display the abbreviated data normally. But when I want to view the detailed data Expanded document, the table tab still displays the abbreviated data. So I want to display the complete data in the table tab. What should I do? Help me,please. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "3e0e116a-ed59-4c55-b290-844e20143a25",
    "url": "https://discuss.elastic.co/t/metric-formatting-2-formats-simultaneously/214818",
    "title": "Metric formatting - 2 formats simultaneously",
    "category": [
      "Kibana"
    ],
    "author": "WishTripES",
    "date": "January 13, 2020, 11:47am January 14, 2020, 4:29am",
    "body": "I'd like to format a metric in a kibana index pattern for use in a visual using 2 formats .divide(1000) AND '0.0[0000]' I've tried all kinds of chain combinations and nothing works. Is this possible?",
    "website_area": "discuss"
  },
  {
    "id": "bb81cadc-2cfa-4d49-a7c6-6a7aab934865",
    "url": "https://discuss.elastic.co/t/in-kibana-how-to-draw-a-graph-which-should-give-me-average-time-given-with-2-dates-start-date-end-date/214861",
    "title": "In kibana how to draw a graph which should give me average time given with 2 dates(start date & end date)",
    "category": [
      "Kibana"
    ],
    "author": "Amarsha_N.G",
    "date": "January 13, 2020, 3:36pm January 14, 2020, 4:24am",
    "body": "I am having 2 date field one is PR open date other isPR closed date in my ES _source with PR num, project name, etc. Now In kibana I want to draw a graph which should give me average time to close a PR. how to achieve this? one more doubt how to do mathematical operation on 2 fields of same data type for ex: addition of filed1 + field 2 or subtraction of field1 - field 2",
    "website_area": "discuss"
  },
  {
    "id": "0efa99ce-2a4b-49f7-966f-4de3d5398181",
    "url": "https://discuss.elastic.co/t/visualizations-functions-in-canvas/214909",
    "title": "Visualizations functions in canvas",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 13, 2020, 10:06pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f35896fe-da00-4a22-902a-0a12664f054d",
    "url": "https://discuss.elastic.co/t/canvas-plugin-development/214875",
    "title": "Canvas plugin development",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 13, 2020, 4:38pm January 13, 2020, 8:28pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4300a757-ee8e-401b-9a7e-6319b5d8a629",
    "url": "https://discuss.elastic.co/t/timelion-offsetting-data-by-two-days-when-using-bucket-1d/214608",
    "title": "Timelion offsetting data by two days when using bucket 1d",
    "category": [
      "Kibana"
    ],
    "author": "viena",
    "date": "January 10, 2020, 2:33pm January 10, 2020, 7:47pm January 13, 2020, 5:54pm",
    "body": "Hello, We are having the same issue as discussed in topics Timelion offsetting data by two days Kibana Hello, I am trying to display one specific week's data in one of my Timelion graphs. However, when I make the bucket span wider (change from 1h to 1d), the entire data is being offset by 2 whole days. When I put my mouse pointer at any point on the graph, it shows the data point of two days after that day (for e.g., it shows 0 for Jan 24th, 2018 when in the dataset, the 0 is the value for Jan 26th, 2018). Is there any way to avoid this? Thanks. and Timelion default daily offset problem Kibana Hi, I have a very annoying problem using Timelion. Data comes in continuously, but I want to visualize my data with a day-granularity. However the data as it appears in Timelion is always off by a day. I fixed this using a workaround, by adding an offset of -1d, all days in Timelion now have the correct y-value, but this has the secondary problem that all data only appears in Timelion a day late. Very annoying, as I want to monitor the data real time. I've been looking everywhere for solutions Both topics are closed and I do not see any solutions. Kibana version 7.3.0 Please, is there anybody with a solution or a hint about what could be the trouble here? Thanks. Viena",
    "website_area": "discuss"
  },
  {
    "id": "abda5eca-15ca-4372-a5fe-ff927125577a",
    "url": "https://discuss.elastic.co/t/default-field-setting/214311",
    "title": "Default_field setting",
    "category": [
      "Kibana"
    ],
    "author": "Matt_McGovern",
    "date": "January 8, 2020, 10:05pm January 8, 2020, 10:14pm January 9, 2020, 12:19am January 9, 2020, 2:20pm January 12, 2020, 3:56am January 13, 2020, 2:19pm January 13, 2020, 2:25pm January 13, 2020, 4:21pm",
    "body": "Is there any way to set a default field for an index, such that when a user types an expression in kibana discover, the query will be to that default field and not every field in the index? I'm looking all over the documentation but cannot seem to find where this done in index creation, mapping, etc.",
    "website_area": "discuss"
  },
  {
    "id": "80e6c711-3f10-4a83-b403-f255859c4bcb",
    "url": "https://discuss.elastic.co/t/using-lte-now-in-watcher-date-range-giving-unusual-behaviour/214865",
    "title": "Using \"lte\" : \"Now\" in Watcher Date Range giving Unusual behaviour",
    "category": [
      "Kibana"
    ],
    "author": "Will_Monkeys",
    "date": "January 13, 2020, 4:02pm January 13, 2020, 4:06pm",
    "body": "Hey! I'm trying to set up a Watcher that fires any time a document enter a given index, with the property Command containing the phrase \"New Workset\". I got a query from the Discover tab > Inspect, then coped and pasted this into an advanced Watcher JSON like this (notice the fixed gte/lte date range): { \"trigger\": { \"schedule\": { \"interval\": \"5m\" } }, \"input\": { \"search\": { \"request\": { \"body\": { \"size\": 0, \"query\": { \"bool\": { \"must\": [], \"filter\": [ { \"bool\": { \"should\": [ { \"match_phrase\": { \"Command\": \"New Workset\" } } ], \"minimum_should_match\": 1 } }, { \"range\": { \"@timestamp\": { \"format\": \"strict_date_optional_time\", \"gte\": \"2020-01-13T13:40:34.449Z\", \"lte\": \"2020-01-13T16:40:34.449Z\" } } } ], \"should\": [], \"must_not\": [] }} }, \"indices\": [ \"*\" ] } } }, \"condition\": { \"compare\": { \"ctx.payload.hits.total\": { \"gte\": 1 } } }, \"actions\": { \"my-logging-action\": { \"logging\": { \"text\": \"There are {{ctx.payload.hits.total}} documents in your index. Threshold is 10.\" } } } } I simulated the Watcher and got 5 documents, as expected, and the Watcher fired. Then, I wanted to change the date ranges to make them dynamic using the \"now-1h\" syntax. I want the Watcher to trigger if there is at least one document added to the index that meets the condition. I thought this would be easy, but when I did it, I didn't get the results expected. When the date is set to: { \"range\": { \"@timestamp\": { \"format\": \"strict_date_optional_time\", \"gte\": \"now-2h\", \"lte\": \"now\" } } } I get the following, which is actually wrong (should be 5, and yes all these documents are definitely within the last 2 hours): \"condition\": { \"type\": \"compare\", \"status\": \"success\", \"met\": true, \"compare\": { \"resolved_values\": { **\"ctx.payload.hits.total\": 3** } } }, I think I've narrowed it down to the lte:\"Now\" line - not functioning properly, because when I try this Date Range, I get the correct ctx.payload.hits.total of 5 \"range\": { \"@timestamp\": { \"format\": \"strict_date_optional_time\", \"gte\": \"now-2h\", \"lte\": \"2020-01-13T16:40:34.449Z\" } } } Any ideas why this might be happening when I use lte:\"Now\"? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "b0046ce4-403c-4e05-985b-1a1936aa1639",
    "url": "https://discuss.elastic.co/t/percentage-metric-outside-of-visual-builder/214484",
    "title": "Percentage Metric outside of Visual Builder",
    "category": [
      "Kibana"
    ],
    "author": "SpinDORK",
    "date": "January 9, 2020, 8:04pm January 9, 2020, 8:41pm January 10, 2020, 10:17am January 13, 2020, 3:28pm",
    "body": "Hi, Will we ever get an easy to use Percentage tool for the Metric Visualisation and not simply the Visualisation Builder as this does not accurately show data over multiple years as we can only select the most recent d, w or y. This feature has been requested countless times on the Elastic Github, is there any reason why the developers cannot deliver this feature or is it in the works? Thanks, Steve",
    "website_area": "discuss"
  },
  {
    "id": "e1bc3a6e-cf4d-4ce7-8c4f-979876e3df86",
    "url": "https://discuss.elastic.co/t/kiban-vega-visualization-using-scripted-fields/214860",
    "title": "Kiban - Vega visualization using scripted fields",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 13, 2020, 3:26pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4a73055b-f0e7-4449-b304-ef65a53b82c3",
    "url": "https://discuss.elastic.co/t/alerts-and-passing-additional-detail-in-message-body/214857",
    "title": "Alerts and passing additional detail in message body",
    "category": [
      "Kibana"
    ],
    "author": "Michael_Schellhouse",
    "date": "January 13, 2020, 3:02pm",
    "body": "Currently have alerts setup based on extraction query. I have used aggregation to get summary counts by two fields. I would like to output the agg results in the body of the alert message. problem is, message will only send IF all ctx variables referenced actually resolve to a value. Not sure how to handle dynamic buckets in output. Any suggestions? Extraction Query Response: { \"_shards\": { \"total\": 2955, \"failed\": 0, \"successful\": 2955, \"skipped\": 2170 }, \"hits\": { \"hits\": , \"total\": 177, \"max_score\": 0 }, \"took\": 1436, \"num_reduce_phases\": 2, \"timed_out\": false, \"aggregations\": { \"1\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"2\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"doc_count\": 74, \"key\": \"10.9.2.156-consle.log\" }, { \"doc_count\": 74, \"key\": \"10.9.2.156-server.log\" }, { \"doc_count\": 7, \"key\": \"10.9.2.156-messages\" } ] }, \"doc_count\": 155, \"key\": \"ams-jboss\" }, { \"2\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"doc_count\": 10, \"key\": \"10.9.2.188-messages\" }, { \"doc_count\": 7, \"key\": \"10.9.1.151-messages\" }, { \"doc_count\": 5, \"key\": \"10.9.1.197-messages\" } ] }, \"doc_count\": 22, \"key\": \"annuity-services-b\" } ] } } } Looking to do something like this in message body: Details of Kibana search result: @log_group = {{ctx.results.0.aggregations.1.buckets.0.key}} - {{ctx.results.0.aggregations.1.buckets.0.doc_count}} total events across the following @log_streams: {{ctx.results.0.aggregations.1.buckets.0.2.buckets.0.key}} {{ctx.results.0.aggregations.1.buckets.0.2.buckets.1.key}} {{ctx.results.0.aggregations.1.buckets.0.2.buckets.2.key}} {{ctx.results.0.aggregations.1.buckets.0.2.buckets.3.key}}",
    "website_area": "discuss"
  },
  {
    "id": "4409624b-4e6a-4c34-8970-b5949a73ba9a",
    "url": "https://discuss.elastic.co/t/access-for-kibana-share-csv-report-generate-csv/214840",
    "title": "Access for Kibana ->Share -> CSV Report -> Generate CSV",
    "category": [
      "Kibana"
    ],
    "author": "ginu",
    "date": "January 13, 2020, 1:35pm January 13, 2020, 2:40pm January 13, 2020, 2:42pm",
    "body": "Hi I have a user with the role which can \"read\" and \"write\" access on an index. but when the user tries to Share the content of and index using Share -> CSV Report -> Generate CSV user get \"Reporting error \" > Forbidden what extra permission needs to be given to the role? Regards, Ginu",
    "website_area": "discuss"
  },
  {
    "id": "fe277f95-a893-4b6d-9586-286f324b76cf",
    "url": "https://discuss.elastic.co/t/max-size-in-option-list-for-non-string-fields/214804",
    "title": "Max size in option list for non string fields",
    "category": [
      "Kibana"
    ],
    "author": "Voula_Mikr",
    "date": "January 13, 2020, 1:28pm January 13, 2020, 2:21pm",
    "body": "Hi, I am trying to add a control in a control visualization where the field type is a non string (is an integer) so \"Dynamic Options\" is off and I have to insert the size. Is there any value (\"all\" for example) that will lead the visualization to take into consideration all the distinguished values that the field has? Is there any upper bound for this number?",
    "website_area": "discuss"
  },
  {
    "id": "d1785b8f-4d97-47ce-baac-832cf2c9e323",
    "url": "https://discuss.elastic.co/t/histogram-graph-no-showing-in-discover-view-module/214848",
    "title": "Histogram graph no showing in Discover view/module",
    "category": [
      "Kibana"
    ],
    "author": "sri",
    "date": "January 13, 2020, 2:07pm",
    "body": "I keep getting this error \"custom xDomain is invalid, custom minInterval is greater than computed minInterval\" in browser when 'Auto' is selected a time period for the graph in discover view, but i can view the graph when selecting 'Monthly' from dropdown. Have attached screenshots for reference Screenshot 2 (Monthly timeperiod)1881520 84.2 KB Screenshot 1 (Auto timeperiod)1881519 65.7 KB Screenshot 3 (Error in browser&#39;s console)1909322 18.1 KB",
    "website_area": "discuss"
  },
  {
    "id": "fa3d1137-8f81-41f2-ad53-24d4f9b70ffe",
    "url": "https://discuss.elastic.co/t/vega-stacked-bar-plot-group-by/214833",
    "title": "Vega stacked bar plot group by",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 13, 2020, 1:09pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1d0e9668-2406-4f60-80b9-ddcae9baa570",
    "url": "https://discuss.elastic.co/t/change-the-default-name-of-visualization/213709",
    "title": "Change the default name of visualization",
    "category": [
      "Kibana"
    ],
    "author": "chbas",
    "date": "January 3, 2020, 1:16pm January 3, 2020, 3:25pm January 13, 2020, 12:30pm January 13, 2020, 12:33pm",
    "body": "Hi everyone. What I want to achieve is to preview a visualization dynamically with some predefined data. I'm using the url to pass the values as follows: http://localhost:5601/gin/app/kibana#/visualize/create?type=gauge&indexPattern=somedata...,title:'Test%20Visualization',type:gauge)) The visualization is loaded with the defined parameters, with the exception of the title, which is always by default 'New visualization'. So, is there a way of changing the default name of this visualization dynamically ? I know that to change the default name, you should change this file: kibana-repo/src/legacy/core_plugins/kibana/public/visualize/saved_visualizations/_saved_vis.js but like this, all the default names of a visualisation will be changed. Thanks in advance!",
    "website_area": "discuss"
  },
  {
    "id": "e00d7865-1f22-478f-af37-d31cd37c1a25",
    "url": "https://discuss.elastic.co/t/can-we-display-the-creation-indice-date-in-kibana/214808",
    "title": "Can we display the creation indice date in Kibana?",
    "category": [
      "Kibana"
    ],
    "author": "missthery",
    "date": "January 13, 2020, 11:10am January 13, 2020, 11:46am",
    "body": "Hey here, I am a kibana end-user and I never use code. I have indices available that I can manage to create index pattern then smart data discovery, viz and dashboard. My question is how I can find the date when the indices have been created in Kibana UI? I have the same question for index-patterns... Thanks for your help",
    "website_area": "discuss"
  },
  {
    "id": "5e46b014-7b10-4bff-9235-8cbb09c89fff",
    "url": "https://discuss.elastic.co/t/can-we-update-the-messages-on-the-same-document-based-on-unique-field/214813",
    "title": "Can we update the messages on the same document based on unique field",
    "category": [
      "Kibana"
    ],
    "author": "anjilinga",
    "date": "January 13, 2020, 11:25am January 13, 2020, 11:38am",
    "body": "If we have 3 messages coming for each city, can we update the messages on same document based on city. is it possible through logstash or any otherway ?",
    "website_area": "discuss"
  },
  {
    "id": "edc84d2e-b657-4ce2-8405-fb03849f6afe",
    "url": "https://discuss.elastic.co/t/7-5-0-problem-with-reporting-feature-dependencies-most-likely/213686",
    "title": "[7.5.0] Problem with reporting feature (dependencies most likely)",
    "category": [
      "Kibana"
    ],
    "author": "Fabio-sama",
    "date": "January 3, 2020, 10:07am January 3, 2020, 10:08am January 3, 2020, 4:11pm January 3, 2020, 11:30pm January 7, 2020, 9:02am January 13, 2020, 11:36am",
    "body": "Hi there! I think the title is self-explanatory but I'll add here some further info. I'm trying to make some reports on a Kibana instance installed on a RHEL 6.3 (Santiago) machine. Now, it fails with main error Error spawning Chromium browser: [Error: Protocol error (Target.setDiscoverTargets): Target closed.] I try to dig deeper but without finding anything really interesting. I followed the instruction on some other similar posts and when I do ldd kibana/data/headless_shell-linux/headless_shell | grep not it returns data/headless_shell-linux/headless_shell: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by data/headless_shell-linux/headless_shell) data/headless_shell-linux/headless_shell: /lib64/libc.so.6: version `GLIBC_2.15' not found (required by data/headless_shell-linux/headless_shell) data/headless_shell-linux/headless_shell: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by data/headless_shell-linux/headless_shell) Supposing the machine I'm working with is not allowed to access the web, how do I install (if needed) such versions of GLIBC on it? Is there any chance there's a different cause to this error? For the sake of completeness I'll post down here the verbose log extracted from Kibana when trying to make a report of a Dashboard. Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "7e3f189c-5c14-4d8d-b6c9-b024d7a4ad2f",
    "url": "https://discuss.elastic.co/t/permalink-snapshot-und-saved-search-reference/214814",
    "title": "Permalink - snapshot und saved-search reference",
    "category": [
      "Kibana"
    ],
    "author": "kkensy",
    "date": "January 13, 2020, 11:29am",
    "body": "Hey, shouldn't the snapshot share links be independent from the saved search? This works for the changes on the saved search, but not if the saved search was deleted. The error message in this case is \"Saved object is missing, Could not locate that search (id: 41678a00-35ec-11ea-9b4e-19c48aaf7a7d)\" However, if i delete the saved search ID from the snapshot URL, the search is loading as expected. Is this a bug or a feature? Tested with Kibana 6.8 and 7.5.1.",
    "website_area": "discuss"
  },
  {
    "id": "8a8e23c8-983d-4bce-8255-94cdd8ef2789",
    "url": "https://discuss.elastic.co/t/kibana-url-change-name/213351",
    "title": "KBANA URL CHANGE NAME",
    "category": [
      "Kibana"
    ],
    "author": "mr_searchng",
    "date": "December 30, 2019, 12:36pm December 30, 2019, 1:27pm December 30, 2019, 1:37pm December 30, 2019, 1:39pm December 30, 2019, 1:44pm December 30, 2019, 1:49pm December 30, 2019, 1:55pm December 30, 2019, 1:58pm December 30, 2019, 1:59pm December 30, 2019, 2:02pm December 30, 2019, 2:03pm December 30, 2019, 2:05pm December 30, 2019, 2:11pm December 30, 2019, 2:20pm December 30, 2019, 2:22pm December 30, 2019, 2:26pm December 30, 2019, 2:36pm December 30, 2019, 2:37pm December 30, 2019, 2:44pm December 30, 2019, 3:01pm",
    "body": "Does anyone know if it is possible to change the 'kibana' text in Kibana URL to different name? We are using Kibana 6.3.0 version. image915464 57.6 KB",
    "website_area": "discuss"
  },
  {
    "id": "3c99ec07-50c2-4180-b6ed-c8463e594f5d",
    "url": "https://discuss.elastic.co/t/lucene-query-is-giving-error-in-canvas-metric/214624",
    "title": "Lucene query is giving error in canvas metric",
    "category": [
      "Kibana"
    ],
    "author": "anjilinga",
    "date": "January 10, 2020, 6:11pm January 10, 2020, 6:22pm January 13, 2020, 9:42am",
    "body": "when i tried to filter the documents from elastic search in canvas metric element giving an unexpected error. can some one please help here { \"bool\": { \"filter\": { \"term\": { \"field1\": \"130402\" }} } }",
    "website_area": "discuss"
  },
  {
    "id": "a586ac41-2d18-4fea-a54e-34259a47a7c5",
    "url": "https://discuss.elastic.co/t/usage-of-array-in-a-kibana-data-table-issue-with-count-behaving-as-a-unique-count/214195",
    "title": "Usage of Array in a Kibana Data Table : issue with Count behaving as a Unique Count",
    "category": [
      "Kibana"
    ],
    "author": "DavidDupuy",
    "date": "January 8, 2020, 9:24am January 8, 2020, 9:43am January 8, 2020, 2:01pm January 8, 2020, 3:26pm January 8, 2020, 3:52pm January 8, 2020, 5:30pm January 8, 2020, 6:39pm January 13, 2020, 8:36am",
    "body": "Hi, A am facing an issue when trying to visualise data coming from an array. For instance, my array 'Tableau_test' contains ths below PUT /discuss_test3/_doc/1 { \"Tableau_test\":[ \"Moto\", \"Voiture\", \"Voiture\", \"Velo\", \"Velo\", \"Velo\", \"Marche\", \"Marche\", \"Marche\", \"Marche\"] } I am expecting to see the below in a Kibana Data Table Marche 4 Velo 3 Voiture 2 Moto 1 But the 'count' metrics seems to behave as a 'Unique count' metrics... cf. below image1219349 20.6 KB If someone can explain/help Thanks ! David",
    "website_area": "discuss"
  },
  {
    "id": "e8ff0b57-3741-443e-9a5d-1ad9b9b34922",
    "url": "https://discuss.elastic.co/t/embedding-kibana-dashboards-in-my-web-app-that-can-be-altered-by-various-users/214761",
    "title": "Embedding kibana dashboards in my web app that can be altered by various users",
    "category": [
      "Kibana"
    ],
    "author": "fenster25",
    "date": "January 13, 2020, 6:29am January 13, 2020, 8:27am",
    "body": "I want to create a web app that would let users upload a json file and index it into elasticsearch and let them create custom kibana dashboards for themselves to visualize their org specific data. Can this be done using the Kibana API? The app will be centrally hosted and all the infrastructure will be maintained by us.",
    "website_area": "discuss"
  },
  {
    "id": "c70ee058-e14b-4179-9d95-033ec9a938b1",
    "url": "https://discuss.elastic.co/t/deviation-of-20-or-more-of-the-average-value/214768",
    "title": "Deviation of 20% or more of the average value",
    "category": [
      "Kibana"
    ],
    "author": "tamjid",
    "date": "January 13, 2020, 7:15am",
    "body": "ES and Kibana version: 6.8 I have some document in an index like (only showing _source field for simplicity): { \"name\": \"unique1\", \"amount\": 200 }, { \"name\": \"unique1\", \"amount\": 100 }, { \"name\": \"unique1\", \"amount\": 150 }, { \"name\": \"unique2\", \"amount\": 300 } For each name, I want to find all the amounts that deviates more than or equal to 20% from their avg. In the example above, I want to see the amount 200 and 100 because that deviates more than 20% of their avg, which is 150. How can I create a data table with these constraints?",
    "website_area": "discuss"
  },
  {
    "id": "d3379e09-837b-4559-9a86-e9c75408af10",
    "url": "https://discuss.elastic.co/t/kibana-how-to-customize-split-chart/214603",
    "title": "Kibana - How to customize split Chart",
    "category": [
      "Kibana"
    ],
    "author": "steven.l",
    "date": "January 10, 2020, 2:24pm January 10, 2020, 11:29pm January 13, 2020, 5:39am",
    "body": "I choose the Line Chart and use split chart function as following: image1062651 80 KB I have four items and each item has its value scale as the y-axis. For example, the value of item_1 is between 4~10, however, item_2 is between 40~80. When I use the split chart, the y-axis on all of these charts are locked between 20~60 liked the following: Q1. Is there any way to set the upper and lower bound of the y-axis for each subplot chart individually? Q2. Each subplot chart seems that size too small. Can I set the width, height on the split chart function? Or maybe some ways that can make the chart not so close. Q3. I also used the split series functions and each subplot has three lines. image1052117 21.6 KB Can the kibana support us to assign the color we want for each line? Q4. My y-axis setting screenshot liked the following: I choose Average on my field: Value, However, each y-axis on the subplot has the redundant word: image1071156 16.6 KB How can I eliminate these words? Q5. There is a function called Threshold Line on Panel settings image491597 13.3 KB When I use the function, for example, When I set the threshold value as 20, it will add a horizontal line at 20 (y-axis) on each subplot. However, I want to set the threshold line for each subplot chart individually. Is there any way can meet my requirements?",
    "website_area": "discuss"
  },
  {
    "id": "fbf9619c-1ef2-43ba-9429-8e1d3861b3ad",
    "url": "https://discuss.elastic.co/t/latency-in-kibana-performing-very-slow/213687",
    "title": "Latency in Kibana. Performing very slow",
    "category": [
      "Kibana"
    ],
    "author": "Saravana_Maadavan",
    "date": "January 3, 2020, 10:18am January 3, 2020, 11:36pm January 4, 2020, 10:00am January 4, 2020, 10:18am January 4, 2020, 10:18am January 4, 2020, 10:37am January 4, 2020, 10:43am January 4, 2020, 11:53am January 6, 2020, 12:36pm January 7, 2020, 1:07pm January 7, 2020, 1:49pm January 13, 2020, 4:48am",
    "body": "Hi Team, We 're using Kibana as \"Log Viewer\" for application logs. Right now we're using only discover feature and not visualizations etc. Discover feature is too slow to retrieve back the results. Even Kibana home page is taking around 10 seconds to load and whenever I navigate between each icon on the left pane like Visualizations, Monitoring, Settings, it takes more than 8 seconds ALWAYS. I would like to know where the bottleneck is. I have provided some information below for analyzing. > Kibana & Elastic version - 7.0 > Kibana host - 1 > Elastic host - 9 together in a single cluster (Master nodes - 3) > Logstash to Elastic - logs pushed to all three master nodes. > Kibana to Elastic - logs pulled from only one master node. > Disk space of Elastic - 4TB out of 100TB > Number of indices - 279. Indexing records based on week of the year and housekeeping done for any 15 days old index. > Shards - 2 (279 primary & 279 replica for 279 indices) > Total number of docs - 433,351,846 (XMLs stored as String) > Elastic heap - 128.1GB / 239GB > Kibana memory usage - 270 MB / 1.4 GB > In Kibana app monitoring, could see the response times only within 1000ms . > Every index has around 970 fields, every tag is been treated as field. Kindly feel free to ask for more details & please do help me in resolving this latency.",
    "website_area": "discuss"
  },
  {
    "id": "9094a0b3-28bb-4d65-8776-c5f5d0164d1d",
    "url": "https://discuss.elastic.co/t/hacks-in-kibana-7-5-1/214598",
    "title": "Hacks in Kibana 7.5.1",
    "category": [
      "Kibana"
    ],
    "author": "Aravinda_Kolla",
    "date": "January 10, 2020, 12:29pm January 10, 2020, 11:14pm January 13, 2020, 3:28am",
    "body": "Can I modify the visData object that a visualization renders using hacks ? How do hacks work? What kind of data objects does a hack handle?",
    "website_area": "discuss"
  },
  {
    "id": "d84a6b24-5008-4ca1-b32b-854049b32246",
    "url": "https://discuss.elastic.co/t/creating-metric-visualization-with-sum-of-top10-hits/211139",
    "title": "Creating metric visualization with sum of top10 hits",
    "category": [
      "Kibana"
    ],
    "author": "ahoffskov",
    "date": "December 9, 2019, 2:33pm December 30, 2019, 8:58am January 12, 2020, 9:05pm",
    "body": "Hello all, new user of the ELK platform here. I'm importing results from a Nessus-scanner, which I then present on a dashboard in order for different departments to use as a \"How are we doing in securing the environment\". I have a data table showing vulnerabilites sorted by no. of occurences. With a KPI defined as \"A 75% reduction in Top10 Critical/High Risk at the end of \", I would like to create a metric showing the sum of the top10 occurences. Example: Vulnerability 1: 75 unique hosts Vulnerability 2: 50 unique hosts Vulnerability 3: 49 unique hosts ... Vulnerability 10: 20 unique hosts SUM: X unique hosts Sum : Y unique hosts I believe the key here would be summing the Unique Count, but only for top10 - any ideas?",
    "website_area": "discuss"
  },
  {
    "id": "ec7e0c12-3fab-4dd0-a608-93ef107178b5",
    "url": "https://discuss.elastic.co/t/dashboard-not-showing-multi-year-data/214404",
    "title": "Dashboard not showing multi-year data",
    "category": [
      "Kibana"
    ],
    "author": "SpinDORK",
    "date": "January 9, 2020, 10:06am January 9, 2020, 12:22pm January 9, 2020, 10:14pm January 10, 2020, 10:22am January 12, 2020, 4:32pm January 12, 2020, 4:32pm",
    "body": "Exactly as the title suggests, our dashboards will not load data for both 2019 and 2020 but will show the correct data for each year separately. For example, I can set the time filter to Oct 11 2019 to Dec 31 2019 and can see 150 hours worth of processing, however when I change this to Oct 11 2019 to now, effectively previous 3 months, I can only see 5 hours worth of processing data which is the exact data from Jan 1 2020 to today. Is this something I can change in settings or config or have no control over? Thanks, Steve",
    "website_area": "discuss"
  },
  {
    "id": "34b110f3-fded-44ff-a976-4f4dca9c3be7",
    "url": "https://discuss.elastic.co/t/license-information-could-not-be-obtained-from-elasticsearch-for-the-data-cluster/213454",
    "title": "License information could not be obtained from Elasticsearch for the [data] cluster",
    "category": [
      "Kibana"
    ],
    "author": "Pushkala",
    "date": "December 31, 2019, 1:11pm December 31, 2019, 5:15pm January 2, 2020, 9:10am January 3, 2020, 9:26am January 11, 2020, 11:15pm",
    "body": "I have installed elasticsearch 7.5.1 and the same version of Kibana.. My es cluster seems fine, but Kibana is not able to connect to the elasticsearch.. Kiabana.yml is as below: server.port: 5601 server.host: \"\" server.name: \"\" elasticsearch.hosts: [ \"https://<IP of ES instance 1>:443\" , \"https://<IP of ES instance 2>:443\" ] elasticsearch.username: \"<kibana_user>\" elasticsearch.password: \"<kibana_user_password>\" server.ssl.enabled: true server.ssl.certificate: server.ssl.key: xpack.security.enabled: true xpack.reporting.kibanaServer.port: 443 xpack.reporting.kibanaServer.protocol: https elasticsearch.ssl.certificateAuthorities: [ \"\" ] elasticsearch.ssl.verificationMode: certificate logging.dest: /etc/kibana/log/kibana.log I have tried both kibana_oss and the non_oss, but I get the same error.. Thanks in advance for the suggestions!",
    "website_area": "discuss"
  },
  {
    "id": "6f082a8f-81c0-48b3-a32a-d51960570688",
    "url": "https://discuss.elastic.co/t/custom-field-for-visualization/213419",
    "title": "Custom field for visualization",
    "category": [
      "Kibana"
    ],
    "author": "Aadarsh_Mahala",
    "date": "December 31, 2019, 8:22am December 31, 2019, 4:55pm January 2, 2020, 5:30am January 11, 2020, 11:05pm January 11, 2020, 11:06pm",
    "body": "Is it possible to create the visualization in the kibana using the custom field like eventcount value from json data? <{ \"eventcount\" : 302932229, \"file\" : \"confidential\" }/>",
    "website_area": "discuss"
  },
  {
    "id": "f5bb220e-5403-49db-b5a8-309b1487aa60",
    "url": "https://discuss.elastic.co/t/how-to-sum-field/214471",
    "title": "How to sum field?",
    "category": [
      "Kibana"
    ],
    "author": "ja123",
    "date": "January 9, 2020, 4:55pm January 9, 2020, 6:03pm January 10, 2020, 8:53am January 10, 2020, 1:58pm January 10, 2020, 11:21pm",
    "body": "I've got many docs in elasticsearch and I want to sum the field Versptung by the field Linie. When I say Kibana to sum the Versptung , it sums all Versptung fields of result. So I got following : Linie Sum_versptung S5 82 EC218 82 But I want the following. Linie Sum_versptung S5 76 EC218 6 My docs in ES look like this: Example doc1: { \"@old_timestamp\" => \"2020-01-09T1346\", \"result\" => [ [ 0] { \"Versptung\" => 3, \"Linie\" => \"EC218\", \"ID\" => \"2020-01-09T14:04:00.000Z_8010101\" }, [ 1] { \"Bahntyp\" => \"S-Bahn\", \"Versptung\" => 56, \"Linie\" => \"S5\", \"ID\" => \"2020-01-09T14:48:00.000Z_8006698\", } [ 2] { \"Bahntyp\" => \"S-Bahn\", \"Versptung\" => 10, \"Linie\" => \"S5\", \"ID\" => \"2020-01-09T14:48:00.000Z_8006556\", } Example doc2: { \"@old_timestamp\" => \"2020-01-09T1386\", \"result\" => [ [ 0] { \"Versptung\" => 3, \"Linie\" => \"EC218\", \"ID\" => \"2020-01-09T14:04:00.000Z_8010101\" }, [ 1] { \"Bahntyp\" => \"S-Bahn\", \"Versptung\" => 10, \"Linie\" => \"S5\", \"ID\" => \"2020-01-09T14:48:00.000Z_8006698\", }",
    "website_area": "discuss"
  },
  {
    "id": "2100aee5-9fbd-44e7-b5e2-b4ac2cda4a9f",
    "url": "https://discuss.elastic.co/t/getting-response-from-a-restful-go-service/214602",
    "title": "Getting response from a RESTful Go service",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 10, 2020, 1:36pm January 10, 2020, 11:12pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d3f56d04-64fe-48bb-a9b5-4c465e69d887",
    "url": "https://discuss.elastic.co/t/how-to-introduce-variable-in-script-field/214375",
    "title": "How to introduce variable in script field",
    "category": [
      "Kibana"
    ],
    "author": "Ravin_Nadar",
    "date": "January 9, 2020, 8:48am January 10, 2020, 9:31am January 10, 2020, 3:30am January 10, 2020, 10:24pm",
    "body": "i want to add filter in the visualization for expected result let say: int variable !=270 i want to store this much in my query. what should i do. Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "4ea64aa6-0765-4265-8617-1d6f4765e65f",
    "url": "https://discuss.elastic.co/t/timelion-is-there-a-less-redundant-way/214677",
    "title": "Timelion - is there a less redundant way?",
    "category": [
      "Kibana"
    ],
    "author": "Skeeve",
    "date": "January 10, 2020, 10:07pm",
    "body": "I'm experimenting with timelion and I'm wondering whether there is a way to express this less redundant: .es( split=tenant_name.keyword:10, timefield=date, metric=sum:spam_complaints ) .divide( .es( split=tenant_name.keyword:10, timefield=date, metric=sum:total_volume ) ) .multiply(100) Is there a way to define the split and the timefield at one location? BTW: Why do I have to explicitly give the timefield? If I don't I get no data or even an error message Error in visualization [timelion_vis] > Timelion request error: Error: in cell #1: Cannot read property 'data' of undefined Error: in cell #1: Cannot read property 'data' of undefined BTW #2: Why are linebreaks (for readability) not possible in the expression field?",
    "website_area": "discuss"
  },
  {
    "id": "50acc031-5264-4d8f-9d9d-5f674a7d6f6f",
    "url": "https://discuss.elastic.co/t/how-can-we-showcase-multiple-hosts-metrics-in-single-dashboard/214676",
    "title": "How can we showcase multiple hosts metrics in single dashboard",
    "category": [
      "Kibana"
    ],
    "author": "mohitmehral",
    "date": "January 10, 2020, 10:05pm",
    "body": "I have multiple hosts enable on metricbeat. I want to show case hostname wise metrics in single dashboard. as example i attached I use selection bar to distingush multiple users however it wont work while i change value for first 2nd hostname also refreshed. dashboard-Elk1280463 48.8 KB",
    "website_area": "discuss"
  },
  {
    "id": "fa8ae3d9-6e67-4130-a29b-b20fd15c8720",
    "url": "https://discuss.elastic.co/t/visualizing-terms-in-a-field/214648",
    "title": "Visualizing Terms In A Field",
    "category": [
      "Kibana"
    ],
    "author": "wwalker",
    "date": "January 10, 2020, 6:16pm January 10, 2020, 8:19pm January 10, 2020, 9:01pm January 10, 2020, 9:06pm",
    "body": "I remember doing this once upon a time but can't figure it out anymore.... I have a field that is stored as both text and keyword. I would like to aggregate and visualize on a table the frequency of terms within the field. However, using the Bar graph visualizations, both horizontal and vertical, I only see the field as .keyword. If I go to Index Patterns under Advanced Settings, I see both the text field and keyword field rows, but the text row says it's not aggregatable. What am I doing wrong here? As far as what specifically I want. I have a bunch of URLs that look like default.aspx?search=pollling%20places. I want to identify top search requests, for example, this would be pollling and places. Perhaps this doesn't work here because Elasticsearch/Kibana doesn't see more than one term?",
    "website_area": "discuss"
  },
  {
    "id": "175be952-1314-49d6-a321-324bf7f2039c",
    "url": "https://discuss.elastic.co/t/access-to-user-only-to-view-and-create-dashboard/214351",
    "title": "Access to user only to view and create dashboard",
    "category": [
      "Kibana"
    ],
    "author": "Praveen_Yallala",
    "date": "January 9, 2020, 6:51am January 10, 2020, 8:10pm",
    "body": "Hi, I want to create a role where user should be only able to see dashboards tab, create new dashboards and also share option should be visible. I am using Kibana V6.8.4 in RHEL 7. How to do that? Please help. Regards, Praveen",
    "website_area": "discuss"
  },
  {
    "id": "7ef6e081-0b3b-4f54-8907-a43721fa285b",
    "url": "https://discuss.elastic.co/t/not-all-geo-points-are-shown-in-kibana-maps/214549",
    "title": "Not all geo points are shown in Kibana Maps",
    "category": [
      "Kibana"
    ],
    "author": "johnny1",
    "date": "January 10, 2020, 7:26am January 10, 2020, 8:04pm",
    "body": "Hello, I want to show about 30000 geo points in Kibana Maps but unfortunately the results are limited to first 10000 documents. How can I increase the size and display all points in Maps? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "35b4d50e-e73f-4efe-aa3f-5ac0dd6c662a",
    "url": "https://discuss.elastic.co/t/edit-query-as-dsl-in-kibana/214541",
    "title": "EDIT query as DSL in Kibana",
    "category": [
      "Kibana"
    ],
    "author": "Seetharaman_K",
    "date": "January 10, 2020, 6:07am January 10, 2020, 6:51pm",
    "body": "Hi i want to write queries directly in DSL to add filters in kibana vizualisations. for eg below screen shot talks about a query which is phrase type. i achieved this by just creating one filter using the default option of (if \"some field \" is Equal to \"some value\") and then adited the filter as query dsl. my question is can i edit this in DSL by adding some regular expression format inside it . now the current query type is phrase in this screen shot. image19201080 182 KB",
    "website_area": "discuss"
  },
  {
    "id": "8fd245e3-1396-4b5e-8af8-2f43b45bf108",
    "url": "https://discuss.elastic.co/t/how-to-sent-my-system-logs-to-remote-kibana-host-using-ssh/214527",
    "title": "How to sent my system logs to remote kibana host using ssh",
    "category": [
      "Kibana"
    ],
    "author": "SELVA_VIGNESH",
    "date": "January 10, 2020, 4:38am January 10, 2020, 6:47pm",
    "body": "My configuration yml files are here:https://github.com/Selvahere/ELK.git.",
    "website_area": "discuss"
  },
  {
    "id": "ec7ffbc7-765a-4199-bf0a-bba17999a4c0",
    "url": "https://discuss.elastic.co/t/source-filtering-for-all-index-patterns/214628",
    "title": "Source Filtering for all index patterns",
    "category": [
      "Kibana"
    ],
    "author": "lantern77",
    "date": "January 10, 2020, 3:57pm January 10, 2020, 6:06pm January 10, 2020, 6:13pm January 10, 2020, 6:19pm January 10, 2020, 6:41pm",
    "body": "Hello, So we have a very large nested field that contains alot of objects, and on the \"discover\" page for kibana whenever it appears, the application crashes. I am aware that source filtering exists for index patterns https://github.com/elastic/kibana/issues/4366 and it works perfectly fine. However is there a way to add in the source filter for every index pattern created from here on out automatically? Now we can also exclude the field from _source using https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html However we lose the ability to debug and see the data when querying in elasticsearch. Which is something we don't want. I know this is a specific use case, but does anyone have a solution for this? Regards",
    "website_area": "discuss"
  },
  {
    "id": "c34bf72c-c39f-4c26-8395-41206f66595e",
    "url": "https://discuss.elastic.co/t/device-uptime-metric-visualization/214651",
    "title": "Device Uptime Metric Visualization",
    "category": [
      "Kibana"
    ],
    "author": "joaoantoniopereira2",
    "date": "January 10, 2020, 6:38pm",
    "body": "Hi! I am doing some dashboards for an iot device. The entries count for the respective index is diplayed in the following visual: Screenshot 2020-01-10 18.27.541289337 38 KB Can we create a percentage metric for the uptime of this device considering any selected time frame directly in kibana? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "8dbf4711-29db-4813-8205-c05e25dab7b7",
    "url": "https://discuss.elastic.co/t/individual-default-route-for-different-spaces/214506",
    "title": "Individual default route for different spaces",
    "category": [
      "Kibana"
    ],
    "author": "Matthias_Seidl",
    "date": "January 9, 2020, 11:19pm January 10, 2020, 6:37pm January 10, 2020, 6:37pm",
    "body": "Hi, my understanding of the 7.5 release of Kibana is that I can set a different default route for each space. How can I do that? I only see that default route setting in the advanced settings and they only apply to my default Space. I would love to show different dashboards as default route depending on the user/space that is used. Thank you very much",
    "website_area": "discuss"
  },
  {
    "id": "93dfd04c-7fc0-4d14-9afe-5ed5e8aea563",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-set-advanced-settings-for-user-with-admin-user/214460",
    "title": "Is it possible to set advanced settings for user with admin user",
    "category": [
      "Kibana"
    ],
    "author": "mtudisco",
    "date": "January 9, 2020, 3:43pm January 10, 2020, 6:23pm January 10, 2020, 6:32pm January 10, 2020, 6:34pm",
    "body": "HI, i have created users, roles and spaces in kibana, and those users are not able to set or view advanced settings, but i'd like to set for them defaultRoute. Is it possible to set that setting for a user without being connected with that user? thanks",
    "website_area": "discuss"
  },
  {
    "id": "fd3734c7-e8a9-456b-b625-2423898bf015",
    "url": "https://discuss.elastic.co/t/process-binding-is-not-supported-when-running-kibana-7-5-1-with-custom-plugin-deployed/213699",
    "title": "Process.binding is not supported when running kibana 7.5.1 with custom plugin deployed",
    "category": [
      "Kibana"
    ],
    "author": "ylasri",
    "date": "January 3, 2020, 12:15pm January 3, 2020, 11:31pm January 4, 2020, 9:25am January 10, 2020, 6:31pm January 10, 2020, 6:31pm",
    "body": "I am using kibana tags/v7.5.1 to develop a custom plugin It's working fine on dev When i build and deploy the plugin into a stable release of kibana 7.5.1, i started Kibana but when running browser into http://kibana_stable:5601 i get this error does anyone have an idea about this error ? Thank You process.binding is not supported Version: 7.5.1 Build: 27610 Error: process.binding is not supported at Object.process.binding (http://kibana_stable:5600/built_assets/dlls/vendors.bundle.dll.js:6:426978) at Object.<anonymous> (http://kibana_stable:5600/bundles/commons.bundle.js:3:701069) at Object.<anonymous> (http://kibana_stable:5600/bundles/commons.bundle.js:3:707480) at __webpack_require__ (http://kibana_stable:5600/bundles/login.bundle.js:3:1463) at Object.<anonymous> (http://kibana_stable:5600/bundles/commons.bundle.js:10:14612) at __webpack_require__ (http://kibana_stable:5600/bundles/login.bundle.js:3:1463) at Object.<anonymous> (http://kibana_stable:5600/bundles/commons.bundle.js:10:13883) at __webpack_require__ (http://kibana_stable:5600/bundles/login.bundle.js:3:1463) at Object.<anonymous> (http://kibana_stable:5600/bundles/commons.bundle.js:3:2921691) at __webpack_require__ (http://kibana_stable:5600/bundles/login.bundle.js:3:1463)",
    "website_area": "discuss"
  },
  {
    "id": "209270f0-d6b3-405b-bf5f-7e51429e666b",
    "url": "https://discuss.elastic.co/t/callwithrequest-security-hasprivileges-not-available/214500",
    "title": "callWithRequest security.hasPrivileges not available",
    "category": [
      "Kibana"
    ],
    "author": "diniden",
    "date": "January 9, 2020, 9:58pm January 9, 2020, 10:36pm January 9, 2020, 10:36pm January 10, 2020, 4:52pm January 10, 2020, 6:15pm",
    "body": "I am attempting to use callWithRequest to fetch elastic privilieges so I can customize my user's plugin experience based on whether or not they have access to read/write to an index within elastic. However, I can't find a single example that explains how to use a sub-object portion of the API in conjunction with callWithRequest. For instance, I've tried: callWithRequest(req, 'security.hasPrivileges', { user: req.auth.credentials.username, body: { cluster: 'data', index: { names: [payload.index], privileges: ['write'] } } }).then(function (response) { return response; }).catch(err => { return { error: err, message: 'Invalid Check for Security Privileges' }; })",
    "website_area": "discuss"
  },
  {
    "id": "b6b93ba4-dc0d-46b3-a5e3-612a14e60dd4",
    "url": "https://discuss.elastic.co/t/plugin-extended-metric-under-7-5-1/214638",
    "title": "Plugin Extended Metric under 7.5.1",
    "category": [
      "Kibana"
    ],
    "author": "Matthias_Seidl",
    "date": "January 10, 2020, 4:46pm January 10, 2020, 5:54pm January 10, 2020, 5:59pm",
    "body": "I'm having trouble using this plugin (https://github.com/ommsolutions/kibana_ext_metrics_vis) under Kibana 7.5.1 I did try changing the version in the package.json, but it still wouldn't work. Is anyone using the plugin under 7.5.1?",
    "website_area": "discuss"
  },
  {
    "id": "c5575c26-02bd-4c3a-bb2a-d5490ee35a63",
    "url": "https://discuss.elastic.co/t/empty-uistate-in-a-custom-request-handler/214645",
    "title": "Empty uiState in a custom request handler",
    "category": [
      "Kibana"
    ],
    "author": "pchakour",
    "date": "January 10, 2020, 5:37pm January 10, 2020, 5:45pm",
    "body": "Hello, It's me again I'm migrating an old custom plugin from kibana 6.5 to kibana 7.5. The plugin is developed with AngularJS and has a custom request handler. In this request handler, I use the uiState from the interface RequestHandlerParameters to request data based on this. But, this object doesn't contain the data stored by the plugin using the vis.uiStateVal function from the scope of the controller. I capture the object content from the request handler: uistate_request_handler894261 21.7 KB And from the angular controller: uistate_from_angular_controller1141241 32.4 KB So, why the content is different ? Am I using the wrong uiState object in the request handler ?",
    "website_area": "discuss"
  },
  {
    "id": "2b4b5d3e-b9e0-4f8c-af81-fd949126d2db",
    "url": "https://discuss.elastic.co/t/index-alias-does-not-show-up-in-discover/213210",
    "title": "Index alias does not show up in discover",
    "category": [
      "Kibana"
    ],
    "author": "pgervais",
    "date": "December 27, 2019, 3:58pm December 27, 2019, 4:21pm December 27, 2019, 4:48pm December 27, 2019, 4:54pm December 27, 2019, 5:10pm December 27, 2019, 5:11pm December 27, 2019, 5:12pm December 30, 2019, 2:09am January 1, 2020, 4:24pm January 10, 2020, 5:40pm",
    "body": "I have created aliases to an existing index via update alias as shown in https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html in dev tools. When i use dev tools via GET / it creates a popup with all options allowed for GET including the index aliases i have created. All aliases operations in dev tools work as expected. When i go to discovery, only the actual index show up in drop down list and not the aliases i have created. I'm using the latest V 7.5.0 release. It states in the documentation that aliases are used just like index. My Kibana plugin is banking on the aliases showing up in discovery / kibaban lens as if they were index as stated in the documentation. Am i missing something? Do i need to modify Kibana configuration to enable this option? is there anything else to do to have this functionality?",
    "website_area": "discuss"
  },
  {
    "id": "48ad9af1-21ae-473e-9638-96124b7330a3",
    "url": "https://discuss.elastic.co/t/unique-filter-in-data-table-visualisation/214306",
    "title": "Unique filter in Data Table visualisation",
    "category": [
      "Kibana"
    ],
    "author": "ravnica1",
    "date": "January 8, 2020, 9:15pm January 9, 2020, 8:53pm January 10, 2020, 12:17pm January 10, 2020, 5:09pm January 10, 2020, 5:14pm",
    "body": "Hello Given the following set of documents: \"eventType\":\"Added\", \"movieId\":1, \"user\":\"Jane.Blogs@gmail.com\" \"name\":\"Terminator\", \"userId\":1, \"eventDate\":\"01-01-2020\" \"eventType\":\"Viewed\", \"movieId\":1, \"user\":\"Jane.Blogs@gmail.com\", \"name\":\"Terminator\", \"userId\":1, \"eventDate\":\"02-01-2020\" \"eventType\":\"Viewed\", \"movieId\":1, \"user\":\"Jane.Blogs@gmail.com\", \"name\":\"Terminator\", \"userId\":1, \"eventDate\":\"03-01-2020\" I am trying to create a Data Table visualisation that will have 2 metrics aggregated by email address as in the following example: ............email ................... added viewed Jane.Blogs@gmail.com ........1 .......... 1 So the count for a viewed should be distinct/unique for a movie. I can easily create aggregation to count all views but have not being able to figure out how to aggregate the distinct values in viewed column. Any help will be greatly appreciated",
    "website_area": "discuss"
  },
  {
    "id": "045c008d-1111-4871-a6ad-b20147d12e92",
    "url": "https://discuss.elastic.co/t/max-calculation-in-tsvb-over-interval/214418",
    "title": "Max Calculation in TSVB over interval",
    "category": [
      "Kibana"
    ],
    "author": "Matthias_Seidl",
    "date": "January 9, 2020, 10:55am January 9, 2020, 11:09pm January 9, 2020, 11:09pm January 10, 2020, 4:54pm",
    "body": "Hi, I need to visualize the maximum value of a field during the last 10s. I have to use the TSVB for it (because later I need that value in a bucket script). But I'm having problems with the calculation. Screenshot 2020-01-09 at 11.51.521349813 64.7 KB I'm using the Gauge visual (but it doesn't matter which one I'm using) and I set the timerange mode to last value and interval to 10s. But what happens now is, that every 10s the value jumps back to 0. It seems like it's not looking at the max value of the overall last 10 seconds but at the max value of the last 10s bucket. It's like it's opening ab a new bucket every 10s and is checking for the max value (that's why the value can only increase) and after 10s it resets to 0 and then it starts again increasing. Is there a way around this behavior?",
    "website_area": "discuss"
  },
  {
    "id": "4d93b34c-24c4-4c25-83c4-afa08cfffd4f",
    "url": "https://discuss.elastic.co/t/how-to-exhibit-last-record-for-each-different-possibility-in-a-field/213987",
    "title": "How to exhibit last record for each different possibility in a field?",
    "category": [
      "Kibana"
    ],
    "author": "b3owu1f",
    "date": "January 7, 2020, 6:30am January 8, 2020, 8:06pm January 9, 2020, 1:41am January 10, 2020, 4:42pm",
    "body": "Greetings, I need help to find a way to exhibit the following type of data, filtering Locomotive id field to not repeat, showing only the last available record for each different Locomotive id in a given period of time. kibana11294320 39.7 KB Thank you in advance for any help and directions. Kibana Version: 5.6.15",
    "website_area": "discuss"
  },
  {
    "id": "0e7b1c58-ca34-49cd-99ec-859bd1a10874",
    "url": "https://discuss.elastic.co/t/kibana-port-5601-is-already-in-use-another-instance-of-kibana-may-be-running/213804",
    "title": "Kibana - Port 5601 is already in use. Another instance of Kibana may be running!",
    "category": [
      "Kibana"
    ],
    "author": "Vinoth_kumar_Ramasam",
    "date": "January 5, 2020, 2:18pm January 8, 2020, 5:48pm January 8, 2020, 5:57pm January 8, 2020, 6:15pm January 10, 2020, 4:36pm",
    "body": "Im trying to setup ELK stack version 7.5.1 in my 64 bit windows machine. But, im always keep on getting the following error whenever i tried to start Kibana. Also, I checked using netstat -ano | findstr 5601. It returned no results. I just followed the steps given in the official website. Also, i have configured the elastic search hosts in the kibana yml config file. C:\\Softwares\\ELK\\kibana-7.5.1-windows-x86_64\\bin>netstat -ano | findstr :5601 C:\\Softwares\\ELK\\kibana-7.5.1-windows-x86_64\\bin>netstat -ano | findstr 5601 Error Log :- log [13:46:29.336] [info][status][plugin:reporting@7.5.1] Status changed from uninitialized to green - Ready log [13:46:29.378] [info][listening] Server running at http://localhost:5601 log [13:46:29.436] [fatal][root] Error: Port 5601 is already in use. Another instance of Kibana may be running! at Root.shutdown (C:\\Softwares\\ELK\\kibana-7.5.1-windows-x86_64\\src\\core\\serv er\\root\\index.js:67:18) at Root.start (C:\\Softwares\\ELK\\kibana-7.5.1-windows-x86_64\\src\\core\\server root\\index.js:57:18) at process._tickCallback (internal/process/next_tick.js:68:7) log [13:46:29.458] [info][plugins-system] Stopping all plugins. log [13:46:29.459] [info][data][plugins] Stopping plugin log [13:46:29.460] [info][plugins][translations] Stopping plugin log [13:46:29.461] [info][plugins][spaces] Stopping plugin log [13:46:29.462] [info][features][plugins] Stopping plugin log [13:46:29.463] [info][plugins][timelion] Stopping plugin log [13:46:29.464] [info][code][plugins] Stopping plugin log [13:46:29.465] [info][licensing][plugins] Stopping plugin log [13:46:29.467] [info][plugins][security] Stopping plugin FATAL Error: Port 5601 is already in use. Another instance of Kibana may be running! Its throwing the same error though no other process is using this 5601. Same happens when i give different port number. Please help me to resolve this. Any help is much appreciated!! Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "2ea03963-d685-4ae3-bbff-328be29f082d",
    "url": "https://discuss.elastic.co/t/range-filter-doesnt-work-with-ip-adresses/214599",
    "title": "Range filter doesn't work with IP-adresses",
    "category": [
      "Kibana"
    ],
    "author": "victor.nilsson",
    "date": "January 10, 2020, 12:30pm January 10, 2020, 3:58pm",
    "body": "Hi, I have the following message: <38>sshd[31656]: Accepted password for root from 192.168.2.180 port 51942 ssh2 I want to create a filter that limits the results to only the 192.168.2.0/24 adress span and so i've tried the following: message: \"Accepted password for\" AND NOT src_ip: 192.168.2.0/24<- i still get a hit message: \"Accepted password for\" AND NOT src_ip: [192.168.2.0 TO 192.168.2.255] <- still get a hit I don't understand what i am doing wrong. Could someone point me in the right direction?",
    "website_area": "discuss"
  },
  {
    "id": "ccba382c-b855-47f3-ac40-466398c605b5",
    "url": "https://discuss.elastic.co/t/kibana-canvas-share-copy-post-url-error/212694",
    "title": "Kibana Canvas Share: \"Copy POST URL\" error",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "December 20, 2019, 5:41pm December 26, 2019, 5:31pm January 10, 2020, 10:33am January 10, 2020, 2:25pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "86150919-707a-4389-a19e-5d1586fb7d55",
    "url": "https://discuss.elastic.co/t/nothing-showing-on-kibana-after-enabling-security-features/214234",
    "title": "Nothing showing on Kibana after enabling security features",
    "category": [
      "Kibana"
    ],
    "author": "Booooooo",
    "date": "January 8, 2020, 12:36pm January 9, 2020, 8:09pm January 10, 2020, 1:52pm January 10, 2020, 1:19pm January 10, 2020, 1:55pm January 10, 2020, 2:00pm January 10, 2020, 2:05pm",
    "body": "Hi, I just followed this tutorial on how to enable security features on the Elastic Stack. After restarting Elasticsearch and Kibana, add the password configured, I am logged in but there is nothing showing appart from an option to select your space and nothing else: How can I fix this?",
    "website_area": "discuss"
  },
  {
    "id": "de5e521e-1a88-4d37-83fe-d6172a02dafb",
    "url": "https://discuss.elastic.co/t/control-visualize-function-for-kibana-line-chart/214359",
    "title": "Control visualize function for kibana line chart",
    "category": [
      "Kibana"
    ],
    "author": "steven.l",
    "date": "January 9, 2020, 8:22am January 10, 2020, 1:33pm January 10, 2020, 1:33pm",
    "body": "My index has three Field: category, value, time. Data record format in elasticsearch liked the following: Record 1 - category: x1, value: 5, time: Jan 9, 2020 @ 13:28:00.000 Record 2 - category: x2, value: 6, time: Jan 9, 2020 @ 13:28:00.000 Record 3 - category: x3, value: 7, time: Jan 9, 2020 @ 13:28:00.000 Record 4 - category: y, value: 0, time: Jan 9, 2020 @ 13:28:00.000 Record 5 - category: x1, value: 2, time: Jan 9, 2020 @ 13:29:01.000 Record 6 - category: x2, value: 3, time: Jan 9, 2020 @ 13:29:01.000 Record 7 - category: x3, value: 4, time: Jan 9, 2020 @ 13:29:01.000 Record 8 - category: y, value: 1, time: Jan 9, 2020 @ 13:29:01.000 ... My category field contains four value types: x1, x2, x3, y and each type will be inserted to index every second with the same timestamp. My Goal: Can the control visualize function support to choose any of two value types (eg. x1,x3) of category field to plot double y-axis and timestamp as the x-axis. I have referred the kibana example data - [eCommerce], however, the control function used by this example seems like \"Select two fields for each record\" and then filter the data to plot. image1121292 13.6 KB In my case, It's not a good idea to create the field - filter which contains the value: x1-x2, x1-x3, x1-y, ... x3-y and insert multiple the same data record only with difference value of field - filter. It will contain lots of useless data in es only for filter function. I am not sure that the same function can be properly used in my scenario or not? Or should I change my insert data format to achieve my scenario? Also, does kibana have functions similar to subplot liked the following in one visualize chart? image850621 106 KB",
    "website_area": "discuss"
  },
  {
    "id": "f703469e-aa05-484b-8407-a6eae5564911",
    "url": "https://discuss.elastic.co/t/plotting-area-charts/214572",
    "title": "Plotting Area Charts",
    "category": [
      "Kibana"
    ],
    "author": "Karthigesu_Rajendran",
    "date": "January 10, 2020, 9:14am",
    "body": "I was able to get what I wanted , however I have a question. How to add a static line vertically to indicate the lower boundary and the upper boundary of the chart ? I was able to do so using the Timelion, but in the area chart I only see the option available for Y-Axis which a line called threshold which will draw a line as horizontally. Can you please guide me on this ? Area chart1288340 18 KB",
    "website_area": "discuss"
  },
  {
    "id": "4cc8a236-5019-4e02-b831-5cfb0a1866e4",
    "url": "https://discuss.elastic.co/t/kibana-throwing-errors-behind-a-load-balancer/214567",
    "title": "Kibana throwing errors behind a load balancer",
    "category": [
      "Kibana"
    ],
    "author": "gborg",
    "date": "January 10, 2020, 9:06am",
    "body": "Hello, I have been having some issues where Kibana is throwing me \"clear your session\" errors User enter's Kibana, loads a dashboard, a few minutes pass, error occurs Setup: F5 hardware load balancer ----- --------- > Server 1: Apache reverse proxy to localhost --- > Kibana | |______ > Server 2: Apache reverse proxy to localhost --- > Kibana Full error: Something went wrong Try refreshing the page. If that doesn't work, go back to the previous page or clear your session data. Clear your session Go back Courier fetch: Cannot read property 'timed_out' of null Version: 7.5.0 Build: 27521 TypeError: Cannot read property 'timed_out' of null at https://myurl/bundles/commons.bundle.js:3:4896083 at Function._module.service.Promise.try (https://myurl/bundles/commons.bundle.js:3:2501410) at https://myurl/bundles/commons.bundle.js:3:2500784 at Array.map () at Function._module.service.Promise.map (https://myurl/bundles/commons.bundle.js:3:2500741) at callResponseHandlers (https://myurl/bundles/commons.bundle.js:3:4895925) at https://myurl/bundles/commons.bundle.js:3:4878286 at processQueue (https://myurl/built_assets/dlls/vendors.bundle.dll.js:435:204190) at https://myurl/built_assets/dlls/vendors.bundle.dll.js:435:205154 at Scope.$digest (https://myurl/built_assets/dlls/vendors.bundle.dll.js:435:215159)",
    "website_area": "discuss"
  },
  {
    "id": "08b5ff2b-97d6-479b-8576-1af7afa6367a",
    "url": "https://discuss.elastic.co/t/control-visualization-and-filter-can-not-refresh-data-in-time/214556",
    "title": "Control visualization and filter can not refresh data in time",
    "category": [
      "Kibana"
    ],
    "author": "ming1997",
    "date": "January 10, 2020, 8:02am",
    "body": "I update my data in the elasticsearch. But I can not see the update data in the control visualization . it still show the old data. So I wonder if the control visualization need time to refresh?",
    "website_area": "discuss"
  },
  {
    "id": "4aadbc34-22cc-4c65-9693-1bc88ee1a8b8",
    "url": "https://discuss.elastic.co/t/geo-ip-not-appearing-in-kibana/214551",
    "title": "Geo ip not appearing in Kibana",
    "category": [
      "Kibana"
    ],
    "author": "Jayanth_S_N",
    "date": "January 10, 2020, 7:40am January 10, 2020, 7:56am",
    "body": "I am trying to load the logs into the elasticsearch from Kafka below is my config file. input { kafka { bootstrap_servers => \"localhost:9092\" topics => [\"elk104\"] }} filter{ mutate { add_field => { \"tmp_message\" => \"%{message}\" } split => [\"message\", \"|\"] add_field => { \"cef_version\" => \"%{[message][0]}\" } add_field => { \"cef_device_vendor\" => \"%{[message][1]}\" } add_field => { \"cef_device_product\" => \"%{[message][2]}\" } add_field => { \"cef_device_version\" => \"%{[message][3]}\" } add_field => { \"cef_sig_id\" => \"%{[message][4]}\" } add_field => { \"cef_sig_name\" => \"%{[message][5]}\" } add_field => { \"cef_sig_severity\" => \"%{[message][6]}\" } } kv { field_split => \",\" trim_key => \"<>\\[\\],\" trim_value => \"<>\\[\\],\" include_keys => [] }mutate { rename => [ \"src\", \"cef_traffic_src_ip\"] replace => { \"message\" => \"%{tmp_message}\" } remove_field => [ \"tmp_message\" ]} geoip { source => \"cef_traffic_src_ip\"}mutate { rename => { \"longitude\" => \"[location][lon]\" \"latitude\" => \"[location][lat]\"} convert => {\"[location][lat]\" => \"float\"} convert => {\"[location][lon]\" => \"float\"}}}output {elasticsearch { hosts => [\"localhost:9200\"] index => \"pioneer01-logstash\" template_name=>\"elk_template\" workers => 1 }} and for the template, I am using below configuration, PUT _template/elk_template { \"index_patterns\": [\"pioneer-*\"], \"settings\": { \"index\": { \"refresh_interval\": \"5s\" } }, \"mappings\": { \"dynamic_templates\": [ { \"message_field\": { \"path_match\": \"message\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"text\", \"norms\": false } } }, { \"string_fields\": { \"match\": \"*\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"text\", \"norms\": false, \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } ], \"properties\": { \"@timestamp\": { \"type\": \"date\" }, \"@version\": { \"type\": \"keyword\" }, \"location\": { \"type\": \"geo_point\" }, \"tags\": { \"type\": \"keyword\" }, \"latitude\" : { \"type\" : \"half_float\" }, \"longitude\" : { \"type\" : \"half_float\" }, \"ip\": { \"type\": \"ip\" } } }, \"aliases\": {} } Still, when I look at the output in elasticsearch I can get the GeoIP results, \"geoip\" : { \"city_name\" : \"Bucharest\", \"country_code3\" : \"RO\", \"region_code\" : \"B\", \"timezone\" : \"Europe/Bucharest\", \"ip\" : \"93.114.45.13\", \"latitude\" : 44.4354, \"continent_code\" : \"EU\", \"country_code2\" : \"RO\", \"region_name\" : \"Bucuresti\", \"postal_code\" : \"052822\", \"country_name\" : \"Romania\", \"longitude\" : 26.1033, \"location\" : { \"lat\" : 44.4354, \"lon\" : 26.1033 } } But, when I try to create the dashboard in Kibana, I don't get the mapping of the points on the map there. can somebody help me with this? Screenshot 2020-01-10 at 1.01.40 PM48381954 1.24 MB",
    "website_area": "discuss"
  },
  {
    "id": "b45a5e91-4a57-44c7-885d-22c41107a5e4",
    "url": "https://discuss.elastic.co/t/analyse-difference-of-count-parameters/214406",
    "title": "Analyse difference of count parameters",
    "category": [
      "Kibana"
    ],
    "author": "Manjula_Piyumal",
    "date": "January 9, 2020, 10:07am January 9, 2020, 9:06pm January 10, 2020, 2:42am January 10, 2020, 6:44am January 10, 2020, 7:20am January 10, 2020, 7:37am January 10, 2020, 7:45am",
    "body": "Hi All, I'm currently reporting user login and logout logs to Elasticsearch. Now I want to get the current logged i user count. Document structure is simply as below { \"userId\": \"user1\", \"eventType\": \"LOGIN\" ... } { \"userId\": \"user2\", \"eventType\": \"LOGIN\" ... } { \"userId\": \"user1\", \"eventType\": \"LOGOUT\" ... } So at a given point of time, current logged-in user count will be derived from the difference between record count of entityType=LOGIN and entityTyple=LOGOUT. I want to get the count and plot the trend in line graph. May I know is it possible to visualize this kind of count difference as a number and line graph? If so any tips would be really appreciated. Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "16a05750-3e87-4df3-857d-3281cda0e5ab",
    "url": "https://discuss.elastic.co/t/how-to-create-a-table-visualization-like-this/214033",
    "title": "How to create a table visualization like this",
    "category": [
      "Kibana"
    ],
    "author": "reyhanadp",
    "date": "January 7, 2020, 11:10am January 10, 2020, 7:19am",
    "body": "hi, I want to make a table visualization for server monitoring like this 12928544 128 KB",
    "website_area": "discuss"
  },
  {
    "id": "90c0680b-0306-4ca9-ac69-e8f35cbbc219",
    "url": "https://discuss.elastic.co/t/visualize-blocked-by-forbidden-12-index-read-only-allow-delete-api/214545",
    "title": "Visualize: blocked by: [FORBIDDEN/12/index read-only / allow delete (api)]",
    "category": [
      "Kibana"
    ],
    "author": "reyhanadp",
    "date": "January 10, 2020, 6:53am January 10, 2020, 7:12am",
    "body": "hi, i get the following error when i want to delete, change, and add a visualize. The error appears as follows: Visualize: blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];: [cluster_block_exception] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)]; Less Info OK Error: Forbidden at https://10.243.211.82:5601/bundles/commons.bundle.js:3:541446 at step (https://10.243.211.82:5601/built_assets/dlls/vendors.bundle.dll.js:522:4015) at Object.next (https://10.243.211.82:5601/built_assets/dlls/vendors.bundle.dll.js:522:3275) at fulfilled (https://10.243.211.82:5601/built_assets/dlls/vendors.bundle.dll.js:522:2669) please help, thank you image1920901 96.3 KB",
    "website_area": "discuss"
  },
  {
    "id": "e9f2cee4-8d7a-4df2-92a1-2822528296b1",
    "url": "https://discuss.elastic.co/t/shards-fail-in-high-amount-on-certain-index/214030",
    "title": "Shards fail in high amount on certain Index",
    "category": [
      "Kibana"
    ],
    "author": "Moritz_Kiesewetter",
    "date": "January 7, 2020, 10:55am January 8, 2020, 4:29pm January 10, 2020, 6:42am",
    "body": "Hi guys, when i take a look at one of my dashboards, which uses the Auditbeat-Index as an Input, i get an \"20/24 Shards failed\" Error like 20 Times on the side of my screen. It's stated as an illegal_arguement_expression with the following Message: Type illegal_argument_exception Reason Fielddata is disabled on text fields by default. Set fielddata=true on [user.name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. So i don't really know what exactly i'm supposed to do here, and where to do it. Any hellp will be appreciated. Cheers, Mo",
    "website_area": "discuss"
  },
  {
    "id": "4fdfac0f-af68-4c94-ac0c-9253398b0e04",
    "url": "https://discuss.elastic.co/t/tsvb-sort-by-bucket-scripts-value/214073",
    "title": "Tsvb - Sort by bucket script's value?",
    "category": [
      "Kibana"
    ],
    "author": "Skeeve",
    "date": "January 8, 2020, 7:42am January 9, 2020, 8:19pm January 10, 2020, 6:37am",
    "body": "My data contains informat about several companies sending mail. Each document contains company name (tenant_name) sending domain total amount of mails sent (total_volume) amount of mails considered spam (spam_complaints) PUT /myindex POST _bulk { \"index\": { \"_index\": \"myindex\" }} { \"date\": \"2020-01-08\", \"tenant_name\": \"company_a\", \"domain\": \"domain_1\", \"total_volume\": 33, \"spam_complaints\": 10 } { \"index\": { \"_index\": \"myindex\" }} { \"date\": \"2020-01-08\", \"tenant_name\": \"company_a\", \"domain\": \"domain_2\", \"total_volume\": 33, \"spam_complaints\": 5 } { \"index\": { \"_index\": \"myindex\" }} { \"date\": \"2020-01-08\", \"tenant_name\": \"company_a\", \"domain\": \"domain_3\", \"total_volume\": 34, \"spam_complaints\": 1 } { \"index\": { \"_index\": \"myindex\" }} { \"date\": \"2020-01-08\", \"tenant_name\": \"company_b\", \"domain\": \"domain_2\", \"total_volume\": 333, \"spam_complaints\": 1 } { \"index\": { \"_index\": \"myindex\" }} { \"date\": \"2020-01-08\", \"tenant_name\": \"company_b\", \"domain\": \"domain_3\", \"total_volume\": 333, \"spam_complaints\": 1 } { \"index\": { \"_index\": \"myindex\" }} { \"date\": \"2020-01-08\", \"tenant_name\": \"company_b\", \"domain\": \"domain_4\", \"total_volume\": 334, \"spam_complaints\": 1 } { \"index\": { \"_index\": \"myindex\" }} { \"date\": \"2020-01-08\", \"tenant_name\": \"company_c\", \"domain\": \"domain_3\", \"total_volume\": 3, \"spam_complaints\": 1 } { \"index\": { \"_index\": \"myindex\" }} { \"date\": \"2020-01-08\", \"tenant_name\": \"company_c\", \"domain\": \"domain_4\", \"total_volume\": 3, \"spam_complaints\": 1 } { \"index\": { \"_index\": \"myindex\" }} { \"date\": \"2020-01-08\", \"tenant_name\": \"company_c\", \"domain\": \"domain_1\", \"total_volume\": 4, \"spam_complaints\": 2 } What I like to get is the percentage of spam per company over the time. I already solved that using tsvb. image838488 20.3 KB But when it comes to grouping (by term \"tenant_name\") I can only order by total_volume or spam_complaints. Both is not sufficient as I would need to order by the percentage of spam_complaints (so by spam_complaints/total_volume). Looking at the data you can see that each metric (total_volume, spam_complaints, percentage) would give a different order: image1042380 108 KB I currently see no way how to achieve the ordering by percentage, in this case so that company_c would be top.",
    "website_area": "discuss"
  },
  {
    "id": "a89bb7c2-7c58-4de4-adf1-1df680910c15",
    "url": "https://discuss.elastic.co/t/tag-cloud-panel-is-not-filtering-the-data/214409",
    "title": "Tag Cloud panel is not filtering the data",
    "category": [
      "Kibana"
    ],
    "author": "Aadesh_Sharma",
    "date": "January 9, 2020, 10:13am January 9, 2020, 9:09pm",
    "body": "Hi, I have made a filter on tag cloud to filter the data we are showing in the dashboard. In visualize when click on a word it is working fine but when i add that panel into dashboard it stops working in it. When I click on any word in tag cloud panel nothing happens it didn't add any filter. Can anybody please help me out on this.",
    "website_area": "discuss"
  },
  {
    "id": "79c6006d-f62a-48d2-99c0-8e644696e986",
    "url": "https://discuss.elastic.co/t/how-to-install-kiban-plugins-which-is-hosted-on-kubernetes-pod/214383",
    "title": "How to install kiban-plugins which is hosted on kubernetes pod?",
    "category": [
      "Kibana"
    ],
    "author": "N_Lohidasu_Reddy",
    "date": "January 9, 2020, 9:16am January 9, 2020, 8:58pm",
    "body": "We have hosted the kibana service, we need to install a kibana-plugin. How to do that?",
    "website_area": "discuss"
  },
  {
    "id": "369c1319-23fd-4ae6-8883-2aa2dba6cb1c",
    "url": "https://discuss.elastic.co/t/problem-in-integrating-external-script-in-kibana-plugin/214170",
    "title": "Problem in integrating external script in Kibana Plugin",
    "category": [
      "Kibana"
    ],
    "author": "RaghibHuda",
    "date": "January 8, 2020, 7:20am January 9, 2020, 7:53pm",
    "body": "I am developing a plugin in Kibana with React. I need google map API for that. But when I render my component with mp API script it gives an error. I searched and found that and the reason behind it is the content security policy ( CSP ) of Kibana. The solution is to customize CSP rules though which is not recommended in Kibana documentation. I add csp.strict: true in kibana.yml but still throw the same error like Refused to load the script 'https://maps.googleapis.com/maps/api/js?key={MY_API_KEY}&libraries=places%2Cgeometry' because it violates the following Content Security Policy directive: \"script-src 'unsafe-eval' 'self'\". Note that 'script-src-elem' was not explicitly set, so 'script-src' is used as a fallback. How can I configure csp.rules in kibana.yml to only access the google map API plugin that is https://maps.googleapis.com domain and render my map? If anyone has any working code snippet of csp.rules for adding a particular script in the Kibana plugin will be very helpful for me.",
    "website_area": "discuss"
  },
  {
    "id": "395045f7-8a11-4e3e-86e5-fbf04bd63d89",
    "url": "https://discuss.elastic.co/t/content-security-policy-error/214257",
    "title": "Content Security Policy: error",
    "category": [
      "Kibana"
    ],
    "author": "oussama_ben_sassi",
    "date": "January 8, 2020, 2:41pm January 9, 2020, 5:07pm",
    "body": "Content Security Policy: Les paramtres de la page ont empch le chargement dune ressource  inline ( script-src ). please how to fix this error",
    "website_area": "discuss"
  },
  {
    "id": "0d4fa511-9e70-44ef-8132-05481adde7a6",
    "url": "https://discuss.elastic.co/t/kibana-optimization-and-load-balancing/214343",
    "title": "Kibana Optimization and Load Balancing",
    "category": [
      "Kibana"
    ],
    "author": "Purushoth_Kumar",
    "date": "January 9, 2020, 5:35am January 9, 2020, 5:07pm",
    "body": "Hi guys, I'm working on my 4 node ES cluster. I see that my kibana is lagging as many users are trying to access the kibana. I need to arrive at a solution to optimize the kibana performance. After going through some discussions I found setting up two kibana servers maybe a solution, in that case how can we do load balancing between the kibana servers and ES cluster. I'm using 7.4 version of ELK stack. I'm very new to the multiple kibana servers and load balancers, can any one help me with this. Thanks in Advance",
    "website_area": "discuss"
  },
  {
    "id": "49c3331d-d064-4f0e-8eaf-33e0040b9b24",
    "url": "https://discuss.elastic.co/t/setting-up-kibana-behind-aws-application-load-balancer/214348",
    "title": "Setting up Kibana behind AWS application load balancer",
    "category": [
      "Kibana"
    ],
    "author": "Amphagory",
    "date": "January 9, 2020, 5:55am January 9, 2020, 4:59pm",
    "body": "I am having trouble setting up multiple Kibana 7.3.2 instances behind an AWS ALB. Actually, I don't even get that far as the installation of Kibana is not going well. I follow the simple guide (https://www.elastic.co/guide/en/kibana/current/rpm.html) to install Kibana on an EC2 instance running AWS linux 2. I should be able to connect to this instance (http://public_ip:5601), but I get connection refuse... I've seen tutorials and videos connect with just setting this up. No reverse proxy (nginx or httpd) needed... I guess I side rant as the goal it to set up an application load balance with the Kibana instance behind it. I was wondering if you share your blog, experience, step by step guide if you have the same setup?",
    "website_area": "discuss"
  },
  {
    "id": "916151f7-dcf9-4bd5-810a-e788dd58bd6e",
    "url": "https://discuss.elastic.co/t/kibana-to-reload-certificates-on-change/214310",
    "title": "Kibana to reload certificates on change",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 8, 2020, 10:02pm January 9, 2020, 4:40pm January 9, 2020, 4:53pm January 9, 2020, 4:58pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "02196e9c-33d6-46cd-9704-6c2ebb6a1d7c",
    "url": "https://discuss.elastic.co/t/scatter-plot-using-a-field-from-filebeat-against-a-field-from-metricbeat/214454",
    "title": "Scatter plot using a field from filebeat against a field from metricbeat?",
    "category": [
      "Kibana"
    ],
    "author": "Shweta_Patil",
    "date": "January 9, 2020, 3:12pm",
    "body": "I am using Kibana 7.2.0, is it possible to create scatter plot using a field from filebeat against a field from metricbeat? E.g scatter plot for Requests Per Second (from Filebeat) vs CPU usage (from Metricbeat)",
    "website_area": "discuss"
  },
  {
    "id": "b8fed7c8-e51a-4b75-bc7e-88d4440a49e1",
    "url": "https://discuss.elastic.co/t/canvas-essql-data-table-query-to-select-most-recent-records-given-a-condition/214339",
    "title": "Canvas essql Data Table query to select most recent records given a condition",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 9, 2020, 2:59pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1f500002-0c69-44dc-9f9b-0639b3a95581",
    "url": "https://discuss.elastic.co/t/issue-with-kibana-log-data/214230",
    "title": "Issue with kibana log data",
    "category": [
      "Kibana"
    ],
    "author": "Guy_Rawsthorn",
    "date": "January 8, 2020, 2:42pm January 8, 2020, 3:36pm January 9, 2020, 2:37pm January 9, 2020, 12:08pm January 9, 2020, 2:40pm",
    "body": "Hi, I am using filebeat and logstash to parse the logs of a server to elastic search and then Kibana. elastic search version 7.5 & kibana version: latest. I am seeing logs being processed in kibana but the output seems to be encoded. image1540132 28.8 KB logstash.conf input { beats { port => 5000 codec => plain { charset => \"UTF-8\" } type => \"Jenkins Log\" } } filter { if [type] == \"Jenkins Log\" { grok { match => { \"message\" => [\"%{PF}\", \"%{DOVECOT}\" ] } } date { match => [ \"timestamp\", \"MMM dd HH:mm:ss\", \"MMM d HH:mm:ss\" ] } } } output { elasticsearch { hosts => [\"http://elasticsearch:9200\"] index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" } } filebeat.yaml filebeat.inputs: - type: log enabled: true encoding: utf-8 reload.enabled: true reload.period: 10s paths: - /var/log/jenkins/*.log output.logstash: hosts: [\"IPADDR:5000\"] console: pretty:true Do you have any suggestions to as to correct the config, Best",
    "website_area": "discuss"
  },
  {
    "id": "d3270da6-b802-4025-bfc6-940c582bf6e7",
    "url": "https://discuss.elastic.co/t/modify-internal-request-response-in-kibana/214446",
    "title": "Modify internal request response in Kibana",
    "category": [
      "Kibana"
    ],
    "author": "arseni-lipinski",
    "date": "January 9, 2020, 2:38pm",
    "body": "Hello I'm working on a Kibana plugin and the question I have is: is it possible to modify the internal request's response in Kibana, when loading the Discover page. The thing I'm trying to achieve is modifying response of request to saved objects so that the index pattern related to the user only is displayed. Is there a way to intercept such a request and if so, how would one do that? Thanks a lot",
    "website_area": "discuss"
  },
  {
    "id": "7dd0f867-1f05-42bd-ae11-219dc1e429c3",
    "url": "https://discuss.elastic.co/t/kibana-will-not-start-behind-reverse-proxy-with-x-content-type-options-nosniff/214438",
    "title": "Kibana will not start behind reverse proxy with X-Content-Type-Options: nosniff",
    "category": [
      "Kibana"
    ],
    "author": "WillemvdW",
    "date": "January 9, 2020, 1:53pm",
    "body": "We have an urgent problem in moving Kibana to a production environment where the proxy adds X-Content-Type-Options: nosniff to the response headers. If we run the kibana instance directly to bypass the proxy environment it starts and runs fine. But through the proxy we get the following line in the console: The resource from https://netefatsa.drdlr.gov.za/bundles/app/kibana/bootstrap.js was blocked due to MIME type (text/html) mismatch (X-Content-Type-Options: nosniff). This is in Firefox. In Chrome 1. Request URL: https://netefatsa.drdlr.gov.za/bundles/app/kibana/bootstrap.js, 2. Request Method: GET 3. Status Code: 404 Not Found. The console shows the following message Refused to execute inline script because it violates the following Content Security Policy directive: \"script-src 'unsafe-eval' 'self'\". Either the 'unsafe-inline' keyword, a hash ('sha256-SbBSU7MfZFnVMq4PuE/jbBz7pPIfXUTYDrdHl7Ckchc='), or a nonce ('nonce-...') is required to enable inline execution. The Reverse proxy environment runs nginx behind a corporate proxy server. As a result we cannot start Kibana at all. Please point us to some way. Please advise",
    "website_area": "discuss"
  },
  {
    "id": "d7c3503f-6c67-4770-8a72-6e5e1fc25786",
    "url": "https://discuss.elastic.co/t/show-only-the-average-values-that-are-more-than-80/214425",
    "title": "Show only the average values that are more than 80%",
    "category": [
      "Kibana"
    ],
    "author": "jtebbens",
    "date": "January 9, 2020, 12:14pm",
    "body": "Hi, I am trying to get a table with Average Utilization of interfaces that are more than 80%. It's like a TopN list but I need a limitation that shows only those interfaces that are more than 80%. I tried a filter option with \"value:[80 TO *]\" but that will only take the document values that are more than value:80 and take an average of that which is an unwanted behavior. image1896785 164 KB",
    "website_area": "discuss"
  },
  {
    "id": "c8345a61-df36-4812-aaaf-4b7f4457d293",
    "url": "https://discuss.elastic.co/t/applying-kibana-filter-to-vega-lite-visualization-with-url-body-query-set/212137",
    "title": "Applying kibana filter to Vega-lite visualization with url.body.query set",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "December 17, 2019, 10:19am December 20, 2019, 10:09am December 20, 2019, 1:50pm December 20, 2019, 2:03pm January 8, 2020, 9:22am January 8, 2020, 11:17pm January 9, 2020, 9:22am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5f9415ed-d9f5-44a0-9fb6-2703156c61f5",
    "url": "https://discuss.elastic.co/t/getting-error-when-i-am-running-kibana-on-multiple-elasticsearch-hosts/214346",
    "title": "Getting error when i am running kibana on multiple elasticsearch hosts",
    "category": [
      "Kibana"
    ],
    "author": "Bhayyasaheb_Koke",
    "date": "January 9, 2020, 5:44am",
    "body": "log [05:43:50.912] [warning][migrations] Unable to connect to Elasticsearch. Error: [index_not_found_exception] no such index [.kibana_task_manager], with { resource.type=\"index_or_alias\" & resource.id=\".kibana_task_manager\" & index_uuid=\"na\" & index=\".kibana_task_manager\" } log [05:43:50.913] [fatal][root] { Error: [index_not_found_exception] no such index [.kibana_task_manager], with { resource.type=\"index_or_alias\" & resource.id=\".kibana_task_manager\" & index_uuid=\"na\" & index=\".kibana_task_manager\" } at respond (C:\\ELK\\kibana\\node_modules\\elasticsearch\\src\\lib\\transport.js:349:15) at checkRespForFailure (C:\\ELK\\kibana\\node_modules\\elasticsearch\\src\\lib\\transport.js:306:7) at HttpConnector. (C:\\ELK\\kibana\\node_modules\\elasticsearch\\src\\lib\\connectors\\http.js:173:7) at IncomingMessage.wrapper (C:\\ELK\\kibana\\node_modules\\elasticsearch\\node_modules\\lodash\\lodash.js:4929:19) at IncomingMessage.emit (events.js:194:15) at endReadableNT (_stream_readable.js:1103:12) at process._tickCallback (internal/process/next_tick.js:63:19) status: 404, displayName: 'NotFound', message: '[index_not_found_exception] no such index [.kibana_task_manager], with { resource.type=\"index_or_alias\" & resource.id=\".kibana_task_manager\" & index_uuid=\"na\" & index=\".kibana_task_manager\" }', path: '/.kibana_task_manager/_count', query: {}, body: { error: { root_cause: [Array], type: 'index_not_found_exception', reason: 'no such index [.kibana_task_manager]', 'resource.type': 'index_or_alias', 'resource.id': '.kibana_task_manager', index_uuid: 'na', index: '.kibana_task_manager' }, status: 404 }, statusCode: 404, response: '{\"error\":{\"root_cause\":[{\"type\":\"index_not_found_exception\",\"reason\":\"no such index [.kibana_task_manager]\",\"resource.type\":\"index_or_alias\",\"resource.id\":\".kibana_task_manager\",\"index_uuid\":\"na\",\"index\":\".kibana_task_manager\"}],\"type\":\"index_not_found_exception\",\"reason\":\"no such index [.kibana_task_manager]\",\"resource.type\":\"index_or_alias\",\"resource.id\":\".kibana_task_manager\",\"index_uuid\":\"na\",\"index\":\".kibana_task_manager\"},\"status\":404}',",
    "website_area": "discuss"
  },
  {
    "id": "f56be7c0-4d52-4689-99fb-f4cab0e41f56",
    "url": "https://discuss.elastic.co/t/ui-for-search/214042",
    "title": "UI for search",
    "category": [
      "Kibana"
    ],
    "author": "rubpa",
    "date": "January 7, 2020, 11:54am January 8, 2020, 8:09pm January 9, 2020, 5:26am",
    "body": "We have an existing MySQL based application which has data over several years. We have created meaningful SQL queries (i.e. join/flatten the data) and indexed that data into elasticsearch. And we want to offer this data for free-form or structured search to our users. I chose to use Kibana (v7.4.2 currently) data tables to display the search results. See attached screenshots - the 3 visible tables are aggregations of the data on 3 different parameters which are meaningful for us - there are more visualizations below these 3 tables. One challenge I face is the scrollbars that appears and disappear based on the search results. It gives a feeling of an unprofessional GUI. If I choose a large height for the visualization on the dashboard - I end up with too much whitespace lots of times. Too little height and there is a scrollbar most of the times. I haven't found a good balance since our data has single line text as well several lines text. Do you have any suggestions or ideas to build a search UI using Kibana? A dynamically adapting data-table would be nice but Kibana's dashboard is fundamentally built around fixed areas for each widget. kibana-scrolls11674652 145 KB kibana-scrolls21674652 151 KB",
    "website_area": "discuss"
  },
  {
    "id": "83d384c6-7402-46e2-9f4e-d734f34b6526",
    "url": "https://discuss.elastic.co/t/how-to-use-the-connection-that-kibana-is-using-to-elasticsearch-in-kibana-plugin/213669",
    "title": "How to use the connection that kibana is using to Elasticsearch in kibana plugin?",
    "category": [
      "Kibana"
    ],
    "author": "jaytran",
    "date": "January 3, 2020, 7:18am January 3, 2020, 11:41pm January 4, 2020, 2:36am January 4, 2020, 5:55am January 6, 2020, 12:19pm January 9, 2020, 4:41am",
    "body": "Hi all, I'm building a kibana plugin with a new connection such as below: const elasticsearch = require('elasticsearch'); const client = new elasticsearch.Client({ host: '127.0.0.1:9200'}); const search = function search(index, body) { return client.search({index: index, body: body}); }; Now I want to use the connection that kibana is using Instead of creating new a new connection to elasticsearch from plugin. Please tell me know how to do it? thank you for any response.",
    "website_area": "discuss"
  },
  {
    "id": "4a575218-b20e-4413-b70c-e3182f389071",
    "url": "https://discuss.elastic.co/t/how-to-understand-the-monitor-metric-cpu-throttling/214157",
    "title": "How to understand the monitor metric cpu Throttling",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 8, 2020, 6:14am January 8, 2020, 6:48pm January 9, 2020, 2:15am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "edde7c16-f906-4a47-9cfc-a2daf805d80a",
    "url": "https://discuss.elastic.co/t/role-based-access-control/214300",
    "title": "Role based access control",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 8, 2020, 8:27pm January 8, 2020, 8:52pm January 8, 2020, 9:02pm January 8, 2020, 9:54pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "855f14d8-6c14-4905-9357-625e1aaac5ce",
    "url": "https://discuss.elastic.co/t/drill-down-kibana-visualization-with-another-chart/214140",
    "title": "Drill down Kibana visualization with another chart",
    "category": [
      "Kibana"
    ],
    "author": "MattEdwards",
    "date": "January 7, 2020, 11:27pm January 8, 2020, 8:11pm",
    "body": "Is there a way to create an option in Kibana where if user clicks on a visualization chart that it would drill down to a more in-depth chart. If so, can you please advise how to perform this? Thanks, Matt",
    "website_area": "discuss"
  },
  {
    "id": "5c1b0120-8870-4c07-bcfd-082cd730309d",
    "url": "https://discuss.elastic.co/t/kibana-upgrade/214167",
    "title": "Kibana Upgrade",
    "category": [
      "Kibana"
    ],
    "author": "balaji_18",
    "date": "January 8, 2020, 7:12am January 8, 2020, 8:08pm",
    "body": "How to upgrade kibana 7.4.2 to 7.5.1 in windows...",
    "website_area": "discuss"
  },
  {
    "id": "46da7055-96b1-4789-9335-85d2e8abed24",
    "url": "https://discuss.elastic.co/t/custom-y-axis-aggregation/214262",
    "title": "Custom Y-Axis aggregation",
    "category": [
      "Kibana"
    ],
    "author": "Xuan_Nam_Dang",
    "date": "January 8, 2020, 2:54pm January 8, 2020, 7:50pm",
    "body": "I have a custom field that i want to use to sort the data in line chart for Y-axis. Currently the options for Y-axis is either count or any other Metric aggregation. My field consists of integers only, which i want to implement for my Y-axis image1773737 116 KB",
    "website_area": "discuss"
  },
  {
    "id": "568e3047-a25d-4f9f-9655-d34bc278de88",
    "url": "https://discuss.elastic.co/t/cannot-generate-csv-on-nested-field/214062",
    "title": "Cannot generate CSV on nested field",
    "category": [
      "Kibana"
    ],
    "author": "JeremyP",
    "date": "January 7, 2020, 1:37pm January 8, 2020, 7:02pm January 8, 2020, 7:02pm",
    "body": "Hello, I have a nested field (Vulnerability Advisories) and I cannot generate a CSV unless I flatten the field out by converting it to a string. The problem with converting it, is it is no longer indexed. I'd like to maintain it being nested but at the same time, allow me to generate a CSV report. I hope I can have both options. The error: [illegal_argument_exception] field [Vulnerability Advisories] isn't a leaf field :: {\"path\":\"/testtest/_search\",\"query\":{\"scroll\":\"30s\",\"size\":500},\"body\":\"{\\\"stored_fields\\\":[\\\"Site Name\\\",\\\"Asset Name\\\",\\\"Asset IP Address\\\",\\\"Asset MAC Address\\\",\\\"Asset OS Name\\\",\\\"Asset OS Vendor\\\",\\\"Asset OS Version\\\",\\\"Asset OS Family\\\",\\\"Vulnerability Title\\\",\\\"Vulnerability Severity\\\",\\\"Vulnerability CVSSv3 Score\\\",\\\"Vulnerability CVSSv3 Vector\\\",\\\"Vulnerability Description\\\",\\\"Vulnerability Advisories\\\",\\\"Vulnerability Proof\\\",\\\"Vulnerability Fix\\\",\\\"Vulnerability Test Date\\\",\\\"Vulnerable Since\\\",\\\"Vulnerability Published Date\\\",\\\"Asset Scan Credential Status\\\",\\\"Service Name\\\",\\\"Service Port\\\",\\\"Service Protocol\\\"],\\\"query\\\":{\\\"bool\\\":{\\\"filter\\\":[{\\\"match_all\\\":{}}],\\\"must_not\\\":[],\\\"should\\\":[],\\\"must\\\":[]}},\\\"script_fields\\\":{},\\\"_source\\\":{\\\"excludes\\\":[],\\\"includes\\\":[\\\"Site Name\\\",\\\"Asset Name\\\",\\\"Asset IP Address\\\",\\\"Asset MAC Address\\\",\\\"Asset OS Name\\\",\\\"Asset OS Vendor\\\",\\\"Asset OS Version\\\",\\\"Asset OS Family\\\",\\\"Vulnerability Title\\\",\\\"Vulnerability Severity\\\",\\\"Vulnerability CVSSv3 Score\\\",\\\"Vulnerability CVSSv3 Vector\\\",\\\"Vulnerability Description\\\",\\\"Vulnerability Advisories\\\",\\\"Vulnerability Proof\\\",\\\"Vulnerability Fix\\\",\\\"Vulnerability Test Date\\\",\\\"Vulnerable Since\\\",\\\"Vulnerability Published Date\\\",\\\"Asset Scan Credential Status\\\",\\\"Service Name\\\",\\\"Service Port\\\",\\\"Service Protocol\\\"]},\\\"docvalue_fields\\\":[{\\\"field\\\":\\\"Vulnerability Published Date\\\",\\\"format\\\":\\\"date_time\\\"},{\\\"field\\\":\\\"Vulnerability Test Date\\\",\\\"format\\\":\\\"date_time\\\"},{\\\"field\\\":\\\"Vulnerable Since\\\",\\\"format\\\":\\\"date_time\\\"}],\\\"sort\\\":[{\\\"_score\\\":{\\\"order\\\":\\\"desc\\\"}}],\\\"version\\\":true}\",\"statusCode\":400,\"response\":\"{\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"field [Vulnerability Advisories] isn't a leaf field\\\"}],\\\"type\\\":\\\"search_phase_execution_exception\\\",\\\"reason\\\":\\\"all shards failed\\\",\\\"phase\\\":\\\"query\\\",\\\"grouped\\\":true,\\\"failed_shards\\\":[{\\\"shard\\\":0,\\\"index\\\":\\\"testtest\\\",\\\"node\\\":\\\"ejCJ8VBZSo2rPMwAmOtrtA\\\",\\\"reason\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"field [Vulnerability Advisories] isn't a leaf field\\\"}}],\\\"caused_by\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"field [Vulnerability Advisories] isn't a leaf field\\\",\\\"caused_by\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"field [Vulnerability Advisories] isn't a leaf field\\\"}}},\\\"status\\\":400}\"} The JSON of one the documents in question.... { \"_index\": \"testtest\", \"_type\": \"_doc\", \"_id\": \"NSC0001-1093-18804\", \"_version\": 1, \"_score\": 0, \"_source\": { \"Asset OS Name\": \"Linux\", \"Vulnerability Fix\": \"\\n<p>\\n Use `apt-get upgrade` to upgrade linux-image-generic to the latest version.\\n </p>\", \"@timestamp\": \"2020-01-07T13:27:55.848Z\", \"Asset OS Version\": \"16.04\", \"Vulnerability Advisories\": [ { \"Reference\": \"104606\", \"Source\": \"BID\" }, { \"Reference\": \"DSA-4187\", \"Source\": \"DEBIAN\" }, { \"Reference\": \"CVE-2018-1000004\", \"Source\": \"NVD\" }, { \"Reference\": \"RHSA-2018:0654\", \"Source\": \"REDHAT\" }, { \"Reference\": \"RHSA-2018:0676\", \"Source\": \"REDHAT\" }, { \"Reference\": \"RHSA-2018:1062\", \"Source\": \"REDHAT\" }, { \"Reference\": \"RHSA-2018:2390\", \"Source\": \"REDHAT\" }, { \"Reference\": \"3631-1\", \"Source\": \"UBUNTU\" }, { \"Reference\": \"3631-2\", \"Source\": \"UBUNTU\" }, { \"Reference\": \"3798-1\", \"Source\": \"UBUNTU\" }, { \"Reference\": \"3798-2\", \"Source\": \"UBUNTU\" } ], \"Vulnerability ID\": 18804, \"Service Protocol\": null, \"Vulnerability Test Date\": \"2019-11-14T16:53:33.515Z\", \"Asset IP Address\": \"1.2.3.4/32\", \"@version\": \"1\", \"Site Name\": \"test-NETWORKZ\", \"Vulnerability Published Date\": \"2018-01-16T05:00:00.000Z\", \"Asset OS Vendor\": \"Ubuntu\", \"Vulnerability CVSSv3 Vector\": \"CVSS:3.0/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H\", \"Vulnerability Proof\": \"<p><p>Vulnerable OS: Ubuntu Linux 16.04<p></p></p><p>Vulnerable software installed: Ubuntu linux-image-generic 4.4.0.87.93</p></p>\", \"Service Port\": null, \"Vulnerability Description\": \"\\n \\n<p>In the Linux kernel 4.12, 3.10, 2.6 and possibly earlier versions a race condition vulnerability exists in the sound system, this can lead to a deadlock and denial of service condition.</p>\\n \", \"Asset MAC Address\": \"00:50:56:a7:6b:50\", \"Asset Name\": \"ubuntu-1604\", \"Service Name\": null, \"type\": \"test\", \"Asset OS Family\": \"Linux\", \"Last Assessed for Vulnerabilities\": \"2019-11-14T16:53:33.515Z\", \"Vulnerable Since\": \"2019-11-14T16:53:33.515Z\", \"Vulnerability Severity\": \"Severe\", \"Asset Scan Credential Status\": \"All credentials successful\", \"Vulnerability Title\": \"Ubuntu: (Multiple Advisories) (CVE-2018-1000004): Linux kernel (Trusty HWE) vulnerabilities\", \"Vulnerability CVSSv3 Score\": 5.900000095367432, \"Asset ID\": 1093 }, \"fields\": { \"Vulnerability Published Date\": [ \"2018-01-16T05:00:00.000Z\" ], \"@timestamp\": [ \"2020-01-07T13:27:55.848Z\" ], \"Vulnerability Test Date\": [ \"2019-11-14T16:53:33.515Z\" ], \"Vulnerable Since\": [ \"2019-11-14T16:53:33.515Z\" ], \"Last Assessed for Vulnerabilities\": [ \"2019-11-14T16:53:33.515Z\" ] } I'd appreciate any help that can be offered! Thank you",
    "website_area": "discuss"
  },
  {
    "id": "d5065d4d-7930-4e9b-86d3-01f06cb0036f",
    "url": "https://discuss.elastic.co/t/single-dashboard/214161",
    "title": "Single dashboard",
    "category": [
      "Kibana"
    ],
    "author": "volcano",
    "date": "January 8, 2020, 6:34am January 8, 2020, 6:41pm",
    "body": "Goal: I want to display multiple search queries in a single dashboard. Currently, I have 2 saved search queries and I have created 2 dashboards from these individual saved search queries like below. These dashboards works fine. 963322 15.5 KB But this is inconvenient for the user to browse multiple dashboards. Is there any way to put all these in a single Dashboard. I want to do this so that the user can see everything in one place and no need to browse multiple dashboads how to do this?",
    "website_area": "discuss"
  },
  {
    "id": "ecac628a-0d3f-4321-b6cd-571de0adde28",
    "url": "https://discuss.elastic.co/t/kibana-is-very-slow-in-discover-and-visualisations/214066",
    "title": "Kibana is very slow in discover and visualisations",
    "category": [
      "Kibana"
    ],
    "author": "Angelos",
    "date": "January 7, 2020, 2:50pm January 8, 2020, 4:43pm",
    "body": "Hello, An index pattern is very slow when it comes to discover and even more in visualisation. Specifically in visualisation, when I use this index, kibana crashes sometimes. This pattern has 2.5b documents and 51 fields. I assume that the reason is that I have many documents. So, is this a resources' problem or can it be fixed in another way? Thank you very much in advance! Kind regards, Angelos",
    "website_area": "discuss"
  },
  {
    "id": "f6e6299b-ce81-4eaa-8cdd-a9b93cfbbbed",
    "url": "https://discuss.elastic.co/t/colorblindness-prevention-for-graphs/214064",
    "title": "Colorblindness prevention for graphs",
    "category": [
      "Kibana"
    ],
    "author": "Xuan_Nam_Dang",
    "date": "January 7, 2020, 1:54pm January 8, 2020, 4:36pm",
    "body": "Our team has several members with color blindness. Is there an update for latest Kibana version, which offers preventing against blindness in a chart (i.e. different icons in a line chart or bar chart with different areas)? How are people treating color blindness for their current charts?",
    "website_area": "discuss"
  },
  {
    "id": "2936859d-2a12-46b8-920a-f5fe3cca8b8e",
    "url": "https://discuss.elastic.co/t/kibana-security-settings/214005",
    "title": "Kibana Security Settings",
    "category": [
      "Kibana"
    ],
    "author": "ulas.keskin",
    "date": "January 7, 2020, 9:27am January 8, 2020, 4:25pm",
    "body": "Hi, While I'm using security tab on Kibana, I'm getting permission denied error. Kindly help on this thanks in advance image1878969 110 KB image1351547 22.6 KB",
    "website_area": "discuss"
  },
  {
    "id": "9bb5b0d0-22af-4f2d-8e47-3b5a2f3d102a",
    "url": "https://discuss.elastic.co/t/can-we-change-the-base-url-of-kibna-kibana/214175",
    "title": "Can we change the base URL of Kibna?(KBANA :))",
    "category": [
      "Kibana"
    ],
    "author": "mr_searchng",
    "date": "January 8, 2020, 7:44am January 8, 2020, 9:50am January 8, 2020, 4:22pm January 8, 2020, 4:22pm",
    "body": "http://localhost:5601/lqx/app/kibana#/dashboard I change kibana ,instead \"test\" text http://localhost:5601/lqx/app/test#/dashboard",
    "website_area": "discuss"
  },
  {
    "id": "96a26e4f-a3dd-42be-851a-a79d930984ed",
    "url": "https://discuss.elastic.co/t/can-we-change-kibana-base-url-thousands-of-people-are-developing-projects-on-kibana/214001",
    "title": "Can we change KBANA BASE URL ? (Thousands of people are developing projects on kibana.)",
    "category": [
      "Kibana"
    ],
    "author": "mr_searchng",
    "date": "January 7, 2020, 8:04am January 8, 2020, 4:20pm",
    "body": "thousands of people are developing projects on kibana. Someone sure knows how to change. There must be another way NOW: http://localhost:5601/lqx/app/kibana#/dashboard/ NSTEAD: http://localhost:5601/lqx/app/test#/dashboard/",
    "website_area": "discuss"
  },
  {
    "id": "4ff7f6cf-759f-4e54-a336-8b8baed7c6e8",
    "url": "https://discuss.elastic.co/t/unable-to-revive-connection-http-localhost-9200/213973",
    "title": "Unable to revive connection: http://localhost:9200/",
    "category": [
      "Kibana"
    ],
    "author": "pyerunka",
    "date": "January 7, 2020, 4:18am January 8, 2020, 4:13pm",
    "body": "Hello, I have ES cluster of 3 nodes. And i am trying to start kibana on one of the node. But it is giving me below error: [warning][admin][elasticsearch] Unable to revive connection: http://localhost:9200/ [warning][admin][elasticsearch] No living connections Can someone guide me on this? Regards, Priyanka",
    "website_area": "discuss"
  },
  {
    "id": "7ab1190c-0f6c-42cf-ab68-7587922cc8c8",
    "url": "https://discuss.elastic.co/t/how-to-use-gz-files-in-s3-to-kibana/213962",
    "title": "How to use gz files in s3 to kibana?",
    "category": [
      "Kibana"
    ],
    "author": "jamesyone",
    "date": "January 7, 2020, 1:59am January 15, 2020, 4:59am",
    "body": "Hi! I was able to successfully use kibana through .json files, but I was wondering if you are able to use .gz files on kibana as well? For some reason my .gz files dont work with kibana... Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "a362cdcd-eb3e-487c-a45f-bb745769e181",
    "url": "https://discuss.elastic.co/t/kibana-7-5-load-balancing/213775",
    "title": "Kibana 7.5 Load Balancing",
    "category": [
      "Kibana"
    ],
    "author": "gurdipe",
    "date": "January 4, 2020, 2:35pm January 8, 2020, 4:06pm",
    "body": "Hi, For Elasticsearch 7.5 does Kibana come with built in load balancing using the Kibana config YML file when you have multiple Kibana servers running? Its a feature that has been requested for a while but I do not know if its been implemented yet Kind Regards Gurdipe",
    "website_area": "discuss"
  },
  {
    "id": "1861a9d2-1d08-4350-8cd6-10c392e9220e",
    "url": "https://discuss.elastic.co/t/api-saved-objects-search-overwrite-true-getting-400-bad-request/214020",
    "title": "Api/saved_objects/search?overwrite=true getting 400 bad request",
    "category": [
      "Kibana"
    ],
    "author": "santhosh_kumar2",
    "date": "January 7, 2020, 9:55am January 7, 2020, 8:17pm January 8, 2020, 5:27am January 8, 2020, 3:29pm",
    "body": "When i try to hip below url i am getting 400 bad request. KIBANASERVER:/kibana/api/saved_objects/search?overwrite=true { \"attributes\": { \"title\": \"test\", \"description\": \"\", \"hits\": 0, \"columns\": [ \"adultCount\" ], \"sort\": [ \"@timestamp\", \"desc\" ], \"version\": 1, \"kibanaSavedObjectMeta\": { \"searchSourceJSON\": { \"highlightAll\": true, \"version\": true, \"query\": { \"query\": \"\", \"language\": \"kuery\" }, \"filter\": , \"indexRefName\": \"kibanaSavedObjectMeta.searchSourceJSON.index\" } } }, \"references\": [ { \"name\": \"kibanaSavedObjectMeta.searchSourceJSON.index\", \"type\": \"index-pattern\" } ] } getting below response: { \"statusCode\": 400, \"error\": \"Bad Request\", \"message\": \"child \"references\" fails because [\"references\" at position 0 fails because [child \"id\" fails because [\"id\" is required]]]\", \"validation\": { \"source\": \"payload\", \"keys\": [ \"references.0.id\" ] } } kindly help on this ..",
    "website_area": "discuss"
  },
  {
    "id": "63eae695-451f-4b5e-81a4-97ee04f68eec",
    "url": "https://discuss.elastic.co/t/show-different-index-for-different-fields/214116",
    "title": "Show different Index for different fields",
    "category": [
      "Kibana"
    ],
    "author": "Mehak_Bhargava",
    "date": "January 7, 2020, 7:19pm January 7, 2020, 8:13pm January 7, 2020, 8:25pm January 7, 2020, 9:50pm January 7, 2020, 9:29pm January 8, 2020, 3:26pm",
    "body": "I have a field called fields.log_type which has three values- access, errors, dispatch. I want each value to be its own index on discover tab. Is it possible? Time message fields.log_type Jan 7, 2020 @ 11:11:33.281 <incidentid>24749056</incidentid> access Jan 7, 2020 @ 11:11:33.139 Workflow.InIsDispatchPaused = true dispatch",
    "website_area": "discuss"
  },
  {
    "id": "06570ebb-18e3-4935-b348-fd120a6df068",
    "url": "https://discuss.elastic.co/t/kibana-cannot-list-saved-objects-fielddata-is-disabled-on-text-fields-by-default/214052",
    "title": "Kibana cannot list saved_objects. Fielddata is disabled on text fields by default",
    "category": [
      "Kibana"
    ],
    "author": "KapitanPlaneta",
    "date": "January 7, 2020, 1:19pm January 7, 2020, 9:43pm January 8, 2020, 12:42pm January 8, 2020, 3:26pm",
    "body": "Create an index pattern. Then go to saved objects - 'Unable find saved objects Error: Bad Request' popup shows up. This request: /api/kibana/management/saved_objects/_find?perPage=20&page=1&fields=id&type=config&type=map&type=canvas-workpad&type=canvas-element&type=query&type=index-pattern&type=visualization&type=search&type=dashboard&type=url&sortField=type fails with: {\"message\":\"all shards failed: [illegal_argument_exception] Fielddata is disabled on text fields by default. Set fielddata=true on [type] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\",\"statusCode\":400,\"error\":\"Bad Request\"} After some digging, I found out that the following query is executed: > GET /.kibana/_search > { > \"query\": { > \"terms\": { > \"type\": [ > \"dashboard\", > \"visualization\", > \"search\", > \"index-pattern\", > \"graph-workspace\", > \"timelion-sheet\" > ] > } > }, > \"aggs\": { > \"types\": { > \"terms\": { > \"field\": \"type\", > \"size\": 10 > } > } > } > } against .kibana index with following mapping: { \".kibana\" : { \"mappings\" : { \"properties\" : { \"config\" : { \"properties\" : { \"buildNum\" : { \"type\" : \"long\" }, \"defaultIndex\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } }, \"index-pattern\" : { \"properties\" : { \"fields\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"timeFieldName\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"title\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } }, \"migrationVersion\" : { \"properties\" : { \"index-pattern\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"space\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } }, \"namespace\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"space\" : { \"properties\" : { \"_reserved\" : { \"type\" : \"boolean\" }, \"color\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"description\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"name\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } }, \"telemetry\" : { \"properties\" : { \"enabled\" : { \"type\" : \"boolean\" } } }, \"type\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"ui-metric\" : { \"properties\" : { \"count\" : { \"type\" : \"long\" } } }, \"updated_at\" : { \"type\" : \"date\" } } } } } I wasn't touching the index in any way. Why is the autogenerated setup invalid? ES and Kibana both 7.4",
    "website_area": "discuss"
  },
  {
    "id": "12117542-8c78-457e-b6d2-696a2f660c86",
    "url": "https://discuss.elastic.co/t/making-dashboard-read-only/213853",
    "title": "Making Dashboard read only",
    "category": [
      "Kibana"
    ],
    "author": "Praveen_Yallala",
    "date": "January 6, 2020, 8:57am January 6, 2020, 11:28pm January 7, 2020, 5:07am January 7, 2020, 5:17pm January 7, 2020, 6:12pm January 8, 2020, 12:28pm",
    "body": "Hi, I have created a kibana Dashboard which our team will be using. As of now, everyone is able to view and edit the dashboards. Only I should be able to edit and rest of the team members should have read only access. I have installed Kibana and Elasticsearch both on RHEL 7. Please help me in creating a read only mode for Dashboards. Thank you in advance! Regards, Praveen",
    "website_area": "discuss"
  },
  {
    "id": "a5afd172-affd-489f-b565-7f7b900afe43",
    "url": "https://discuss.elastic.co/t/visualization-is-missing-an-index/214105",
    "title": "Visualization is missing an index",
    "category": [
      "Kibana"
    ],
    "author": "Xuan_Nam_Dang",
    "date": "January 7, 2020, 5:35pm January 8, 2020, 11:49am January 8, 2020, 11:49am",
    "body": "I have removed an index replacing it with identical one with one field changed. I forgot that it's mapped to a visualization and the visualization is missing that index now. How do i change the reference settings in the visualization to replace the missing index with the new one? image1145780 81.6 KB",
    "website_area": "discuss"
  },
  {
    "id": "aa96fe6a-d6a9-4928-8e19-027601e79888",
    "url": "https://discuss.elastic.co/t/create-dynamic-labels-from-a-part-of-es-request/212332",
    "title": "Create dynamic labels from a part of es request",
    "category": [
      "Kibana"
    ],
    "author": "tofmonaute",
    "date": "December 18, 2019, 1:26pm December 18, 2019, 7:06pm December 23, 2019, 12:14pm January 2, 2020, 5:42pm January 8, 2020, 11:38am",
    "body": "Hi, I try to create a vizualisation with Timelion with dynamic labels ( extracted from request response ) For example: the following search in Discover tools message: \"SQL ERROR\" results in following response: SQL ERROR occurs in DATABASE ORACLE_DFS01 at 15:01 SQL ERROR occurs in DATABASE ORACLE_DFS02 at 17:05 SQL ERROR occurs in DATABASE ORACLE_DFS01 at 19:01 SQL ERROR occurs in DATABASE ORACLE_DFS03 at 20:01 I would like to create a vizualization in which i would count all SQL ERRORS for each database: I would proceed as follow to extract dynamically the label from the result: .es(q='message: \"SQL ERROR\"', index=ref_rfnd*).label(\"$1\", \" DATABASE\\s([a-zA-Z0-9._-]+)\") But it did not work Can you help me ? Regards",
    "website_area": "discuss"
  },
  {
    "id": "e96db309-8c80-440d-b645-ad09107d5889",
    "url": "https://discuss.elastic.co/t/how-to-hide-count-in-data-table/213274",
    "title": "How to hide count in Data Table?",
    "category": [
      "Kibana"
    ],
    "author": "volcano",
    "date": "December 29, 2019, 6:49am December 30, 2019, 9:18am December 30, 2019, 11:00am December 30, 2019, 11:00am December 30, 2019, 11:15am December 30, 2019, 11:27am December 30, 2019, 11:57am December 30, 2019, 1:03pm January 8, 2020, 7:30am January 8, 2020, 7:33am January 8, 2020, 7:34am January 8, 2020, 7:35am January 8, 2020, 7:36am January 8, 2020, 9:01am January 8, 2020, 9:02am January 8, 2020, 9:05am January 8, 2020, 9:06am January 8, 2020, 9:18am January 8, 2020, 10:31am",
    "body": "I'm displaying a Data Table. I dont want to display default Count metric. How to hide it ? looking for an easy fix ?",
    "website_area": "discuss"
  },
  {
    "id": "14203b23-87ce-4ffe-8a08-e3648ab4b16a",
    "url": "https://discuss.elastic.co/t/top-left-logo-change-in-kibana-7-4/214188",
    "title": "Top left logo change in Kibana 7.4",
    "category": [
      "Kibana"
    ],
    "author": "Purushoth_Kumar",
    "date": "January 8, 2020, 8:39am January 8, 2020, 8:58am January 8, 2020, 9:00am January 8, 2020, 9:07am January 8, 2020, 9:14am",
    "body": "Hi I'm trying to change the kibana logo at top left ... I have tried searching through all possible post in elastic discuss regarding logo change. But couldn't find any usefull procedure for 7.4 version. Can someone who have successfully implement logo change in 7.4 version please share the step by step procedure. Thanks in Advance Note: I have gone through all post but couldn't find anything relevant for logo change in 7.4 version.",
    "website_area": "discuss"
  },
  {
    "id": "b92fa42f-bfd9-4174-a357-cfebb4014316",
    "url": "https://discuss.elastic.co/t/customizing-kibana-7-x/212082",
    "title": "Customizing Kibana 7.x",
    "category": [
      "Kibana"
    ],
    "author": "aarvee",
    "date": "December 17, 2019, 2:13am December 17, 2019, 4:36pm December 18, 2019, 8:22am December 24, 2019, 7:40am January 8, 2020, 7:43am",
    "body": "Hi All, I wanted to customize the logo of Kibana or probably add my company's logo in conjunction with kibana's logo itself on kibana7.x Can anyone please help me in finding a document for the same?",
    "website_area": "discuss"
  },
  {
    "id": "df97ce7d-52ed-4791-9fb5-8d6860f8e492",
    "url": "https://discuss.elastic.co/t/403-forbidden-error-when-trying-to-log-in-using-kibana-buitin-user/213922",
    "title": "403: forbidden error when trying to log in using 'kibana' buitin user",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 6, 2020, 4:55pm January 6, 2020, 5:24pm January 7, 2020, 6:13am January 7, 2020, 5:14pm January 8, 2020, 11:28am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "40b6a2b3-db78-4cd6-adf2-2c0e0206aea0",
    "url": "https://discuss.elastic.co/t/dashboard-timeout-data-might-be-incomplete-because-your-request-timed-out/212808",
    "title": "Dashboard Timeout - Data might be incomplete because your request timed out",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "December 23, 2019, 6:07am December 24, 2019, 1:25am December 24, 2019, 2:09am December 24, 2019, 3:18pm January 16, 2020, 4:53am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "129c2107-f56a-400a-93b3-181e84c212fd",
    "url": "https://discuss.elastic.co/t/can-we-change-the-base-url-of-kibana/213881",
    "title": "Can we change the base URL of Kibana?",
    "category": [
      "Kibana"
    ],
    "author": "mr_searchng",
    "date": "January 6, 2020, 11:36am January 6, 2020, 3:32pm January 7, 2020, 7:32am January 7, 2020, 3:49pm January 7, 2020, 7:31pm January 7, 2020, 7:34pm January 7, 2020, 7:35pm January 7, 2020, 7:37pm January 7, 2020, 7:45pm January 7, 2020, 7:51pm January 7, 2020, 9:02pm January 8, 2020, 2:13am January 8, 2020, 2:13am",
    "body": "Now: http://localhost:5601/lqx/app/kibana#/dashboard I change kibana ,instead \"test\" text http://localhost:5601/lqx/app/test#/dashboard",
    "website_area": "discuss"
  },
  {
    "id": "bf777640-ec74-405a-b077-20fbb7933b9d",
    "url": "https://discuss.elastic.co/t/changing-metric-visualization-filter-color/214041",
    "title": "Changing Metric Visualization Filter Color",
    "category": [
      "Kibana"
    ],
    "author": "lmit",
    "date": "January 7, 2020, 11:51am January 7, 2020, 10:13pm",
    "body": "I have a metric visualization that i would like to color the UP filter value GREEN and the DOWN filter Value RED. I only see the option to adjust Metric colors according to the the value in ranges, but not for the value of the filter? Is this possible?",
    "website_area": "discuss"
  },
  {
    "id": "3da17b7c-9dea-4564-9a5d-95685f34ea1a",
    "url": "https://discuss.elastic.co/t/sending-email-using-watcher/214025",
    "title": "Sending email using watcher",
    "category": [
      "Kibana"
    ],
    "author": "Seetharaman_K",
    "date": "January 7, 2020, 10:34am January 7, 2020, 9:00pm",
    "body": "Hi Team , to use this feature do we need to have licence ? i mean to create watcher do we need to buy the product ?",
    "website_area": "discuss"
  },
  {
    "id": "2dbd14ae-7c30-43c6-931e-df152f60b029",
    "url": "https://discuss.elastic.co/t/how-to-list-out-the-values-or-output-of-a-unique-count/213494",
    "title": "How to list out the values or output of a unique count",
    "category": [
      "Kibana"
    ],
    "author": "mosaku",
    "date": "January 1, 2020, 4:37pm January 2, 2020, 8:22am January 7, 2020, 8:27pm",
    "body": "I want to have a list of the distinct values that made up the unique count of 9 in the screenshot below. My aim is to have a list of the unique hostnames after getting the unique count. image1365578 40.1 KB",
    "website_area": "discuss"
  },
  {
    "id": "21771755-43f7-4f3f-af8f-a2650026cac9",
    "url": "https://discuss.elastic.co/t/are-there-any-thousands-of-kibana-users-making-changes-to-the-base-url-in-kibana-hey-friends/214124",
    "title": "Are there any thousands of kibana users making changes to the base url in kibana? hey friends",
    "category": [
      "Kibana"
    ],
    "author": "mr_searchng",
    "date": "January 7, 2020, 7:59pm January 7, 2020, 8:15pm January 7, 2020, 8:17pm January 8, 2020, 2:12am",
    "body": "Now: http://localhost:5601/lqx/app/kibana#/dashboard I change kibana ,instead \"test\" text http://localhost:5601/lqx/app/test#/dashboard",
    "website_area": "discuss"
  },
  {
    "id": "724e7f20-e2c6-49aa-b779-466bf8d14170",
    "url": "https://discuss.elastic.co/t/cant-open-kibana-page-in-my-web-browser/214108",
    "title": "Can't open Kibana page in my web browser",
    "category": [
      "Kibana"
    ],
    "author": "mxm",
    "date": "January 7, 2020, 6:07pm January 7, 2020, 6:14pm January 7, 2020, 6:17pm January 7, 2020, 6:18pm January 7, 2020, 6:19pm January 7, 2020, 6:34pm January 7, 2020, 6:38pm January 7, 2020, 6:40pm January 7, 2020, 6:43pm January 7, 2020, 6:44pm January 7, 2020, 6:45pm January 7, 2020, 6:46pm January 7, 2020, 6:47pm January 7, 2020, 6:49pm January 7, 2020, 6:50pm January 7, 2020, 6:51pm January 7, 2020, 6:52pm January 7, 2020, 6:57pm",
    "body": "Hi, I am not able to open Kibana page in my web browser. I get ERR_CONNECTION_REFUSED Elastic + Kibana installed on the same machine: NAME=\"Ubuntu\" VERSION=\"16.04.2 LTS (Xenial Xerus)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 16.04.2 LTS\" VERSION_ID=\"16.04\" Earlier I was able to see Kibana page in my browser but there are some issues with plugins that tried to fix. So I checked the status of elastic and it was running, I checked the logstash status and it was down so I started it and after it, I started Kibana Now I am not able to load Kibana page on my Browser. Could you please help me resolve it? localhost:9200 : { \"name\" : \"node1\", \"cluster_name\" : \"automation-\", \"cluster_uuid\" : \"-LFRoTUMQGNrrw\", \"version\" : { \"number\" : \"5.6.1\", \"build_hash\" : \"667b497\", \"build_date\" : \"2017-09-14T19:22:05.189Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.6.1\" }, \"tagline\" : \"You Know, for Search\" }",
    "website_area": "discuss"
  },
  {
    "id": "fcd7bd6b-750c-4dac-bb55-bfe821f073ec",
    "url": "https://discuss.elastic.co/t/painless-elasticsearch-cant-use-keyword-script-error/213878",
    "title": "Painless (Elasticsearch) can't use keyword - script error",
    "category": [
      "Kibana"
    ],
    "author": "Matthias_Seidl",
    "date": "January 6, 2020, 11:06am January 6, 2020, 11:42pm January 7, 2020, 7:55am January 7, 2020, 3:42pm January 7, 2020, 3:54pm January 7, 2020, 5:02pm",
    "body": "I'm trying to create a scripted field in Kibana, which checks whether the field \"Direction\" is \"I\" or not. if (doc['Direction'].value != \"I\") {return 1;} else {return 0;} But for some reason, it won't work. With all other fields .keyword, that aren't explicitly mentioned in the index mapping it works that way, but I had to mention Direction in the mapping because I also have an alias pointing to it. For Direction I put the following in the mapping file: \"Direction\": { \"type\": \"keyword\" } And there is also an alias pointing to Direction: \"ISDN_Direction\": { \"path\": \"Direction\", \"type\": \"alias\" } but both fields can't be used in the painless script. I don't get an error, but the result preview, for the first 10 results, is just empty. Can someone help me with this issue?",
    "website_area": "discuss"
  },
  {
    "id": "1a647cd1-dba8-4d1c-8308-6a6856908b6d",
    "url": "https://discuss.elastic.co/t/kibana-timelion-how-to-create-the-timelion-chart-based-on-the-values-of-a-field-dynamically/213857",
    "title": "[Kibana Timelion] how to create the timelion chart based on the values of a field dynamically",
    "category": [
      "Kibana"
    ],
    "author": "cheriemilk",
    "date": "January 6, 2020, 9:15am January 6, 2020, 11:45pm January 7, 2020, 6:27am January 7, 2020, 3:50pm",
    "body": "Hi Team, I need your help for timelion chart. Suppose I have below events. @timestamp, module, page, action t1, moduleA, pageA, moduleA.pageA.actionA t2, moduleA, pageB, moduleA.pageB.actionB t3, moduleA, pageC, moduleA.pageC.actionC t4, moduleA, pageA, moduleA.pageA.actionA t5, moduleA, pageB, moduleA.pageB.actionB t6, moduleA, pageA, moduleA.pageA.actionA ... ... And I want to create a timelion chart based on the action field to check how the number of usage for different actions changes as time goes by. But I didn't want to create the chart with below timelion expression as there're are many many actions in the real events and I am not able to list them one by one. .es(index=indexA,q=\"module:moduleA AND at:actionA\").label(\"actionA\" ).lines(fill=1,width=2).legend(columns=2, position=nw), .es(index=indexA,q=\"module:moduleA AND at:actionB\").label(\"actionA\" ).lines(fill=1,width=2).legend(columns=2, position=nw), .es(index=indexA,q=\"module:moduleA AND at:actionC\").label(\"actionA\" ).lines(fill=1,width=2).legend(columns=2, position=nw), .es(...) .title(\"Comparions - Key Actions\") Is there any convenient timelion expression to achieve this? For example, use regex like below (But It doesn't work as I am not familiar with regex) .es(index=indexA,q=\"module:moduleA AND at:\").label(\"moduleA.page[A|B] [$1]\", \"^. > type:(\\S+) > .*\") Please help and advice! Thanks, Cherie",
    "website_area": "discuss"
  },
  {
    "id": "2841c22b-527d-45dd-936d-e012bc32e07d",
    "url": "https://discuss.elastic.co/t/kibana-server-is-not-ready-yet-kibana-7-5/213923",
    "title": "Kibana server is not ready yet - kibana 7.5",
    "category": [
      "Kibana"
    ],
    "author": "talsantos",
    "date": "January 6, 2020, 4:57pm January 6, 2020, 5:16pm January 6, 2020, 5:18pm January 6, 2020, 5:21pm January 6, 2020, 5:58pm January 6, 2020, 6:27pm January 7, 2020, 9:50am January 7, 2020, 3:40pm",
    "body": "Hi, I upgrade elasticsearch and kibana from 7.1 to 7.5 version, keeping the previous settings. After restart the services both are running. Whem I call kibana on browser gives the message: \"Kibana server is not ready yet\" Can you help me?",
    "website_area": "discuss"
  },
  {
    "id": "1d8b1e2c-5c1d-4ead-b5e4-592815476014",
    "url": "https://discuss.elastic.co/t/cant-see-all-fields-in-lens/213900",
    "title": "Can't see all fields in lens",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 6, 2020, 2:20pm January 6, 2020, 3:30pm January 7, 2020, 1:40pm January 7, 2020, 3:38pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "901a08e3-3766-41a0-85c0-331dfdd0f0d7",
    "url": "https://discuss.elastic.co/t/histogram-in-kibana/213097",
    "title": "Histogram in Kibana",
    "category": [
      "Kibana"
    ],
    "author": "Aravinda_Kolla",
    "date": "December 26, 2019, 4:12pm December 26, 2019, 4:59pm December 27, 2019, 1:34pm December 27, 2019, 3:24pm December 30, 2019, 5:57am December 30, 2019, 8:12am January 6, 2020, 3:19pm January 7, 2020, 8:43am January 7, 2020, 3:04pm",
    "body": "I am trying to replicate the histogram visualization type in kibana using the plugin new visualization type functionality. I used VisFactory's createVislibVisualization. The visualization is not rendered. I encounter the following problem TypeError: Cannot read property 'splitColumn' of undefined. How can I overcome this error?",
    "website_area": "discuss"
  },
  {
    "id": "36e96fe6-0ecb-4fca-8c2d-57215fa24a54",
    "url": "https://discuss.elastic.co/t/in-computed-column-col1-col2-col3-col4-column-number-4-does-not-exist/210866",
    "title": "In computed column 'col1 + col2 +col3 + col4', column number 4 does not exist",
    "category": [
      "Kibana"
    ],
    "author": "swati_dupare",
    "date": "December 6, 2019, 10:24am December 30, 2019, 9:08am January 7, 2020, 10:43am",
    "body": "Hi, In Enhanced table -> Add computed column ,I want to make addition of 4 columns as col1,col2,col3,col4 . but my column col4 is dynamic sometimes I get only col1,col2,col3 Computed column formula - col1 + col2 +col3 + col4 that time it shows \"In computed column 'col1 + col2 +col3 + col4', column number 4 does not exist\"",
    "website_area": "discuss"
  },
  {
    "id": "cd9382b6-4033-429f-ad11-799fdce0846b",
    "url": "https://discuss.elastic.co/t/how-can-a-load-balancer-tell-if-a-kibana-intsance-is-usable/213924",
    "title": "How can a load balancer tell if a Kibana intsance is usable?",
    "category": [
      "Kibana"
    ],
    "author": "mikewillis",
    "date": "January 6, 2020, 5:10pm January 6, 2020, 5:31pm January 6, 2020, 5:34pm January 6, 2020, 6:06pm January 7, 2020, 10:13am",
    "body": "Kibana 6.8 Is there anything that a load balancer can look at that will tell it Kibana is really actually properly ready for use by a human being and not potentially route someone to a Kibana instance that has just been started up and is still in a state where it'll show a human an error message? Just doing a http request to / is no use because it will sometimes return a 200 status yet a web browser displays \"Kibana did not load properly. Check the server output for more information\". Looking for the absence of the message \"Kibana did not load properly. Check the server output for more information\" is no use because that message is in the response body even when the UI is ready for use. Parsing /api/status is no use because that says everything is green even before the UI is ready for use. Any useful UI element the existence of which would indicate the UI is ready for use seems to be thus invisible to anything but human eyes as they come in to existence by way of JavaScript rather than being in the response body.",
    "website_area": "discuss"
  },
  {
    "id": "f527324d-6f91-4fd1-b8a4-f31ad1be8507",
    "url": "https://discuss.elastic.co/t/kibana-7-5-0-error-when-starting-in-docker-kubernetes/211813",
    "title": "Kibana 7.5.0 error when starting in Docker/Kubernetes",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "December 13, 2019, 5:16pm December 16, 2019, 3:31pm December 16, 2019, 9:00pm January 7, 2020, 7:06am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ef01f264-3d7e-42ae-a0de-e8659a24c634",
    "url": "https://discuss.elastic.co/t/embed-kibana-dashboard-to-external-ui-app/213172",
    "title": "Embed kibana dashboard to external UI app",
    "category": [
      "Kibana"
    ],
    "author": "Narayanan",
    "date": "December 27, 2019, 10:03am December 30, 2019, 7:10pm January 1, 2020, 9:24am January 6, 2020, 8:08am January 7, 2020, 6:48am",
    "body": "HI Elastic Team, We are in process of embedding kibana dashboard (elastic + kibana setup we created using Azure marketplace) into separate web application (which was developed in angular 7). This can be done by sharing dashboard and embedding iframe into web app. This works perfectly but the problem is it prompts login page From different discussion threads from https://discuss.elastic.co/ we have come to know that it is possible to eliminate login page and access dashboard directly by auto-authentication (for this there are two possible ways) 1st method: Making POST request to api/security/v1/login with username and password which will return a cookie containing sid with that cookie iframe loads without prompting login page. But when we tried this, we got CORS issue and again from different discussion threads we came to know we need edit Kibana.yml file to overcome this CORS problem. We updated kibana.yml with server.cors = true and server.cors.origin = [*] This made options call success (status code 200 but http method OPTION) but still chrome console shows cors error. Now we are stuck at this point. 2nd method: Using Reverse Proxy Mechanism with header of basic authorization in nginx is another solution we tried. This time our kibana is at URL of localhost:5600 and we installed nginx server in our ubuntu machine where kibana is hosted. We are getting 502 http status code error when we try to access nginx url, below is config we have used in nginx. worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 4040; server_name xxxxxxxxxxxxx.centralus.cloudapp.azure.com; error_log /var/log/nginx/elasticsearch.proxy.error.log; access_log off; location / { proxy_pass http://127.0.0.1:5600/; proxy_redirect off; proxy_http_version 1.1; proxy_set_header Connection \"\"; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; # For CORS Ajax proxy_pass_header Access-Control-Allow-Origin; proxy_pass_header Access-Control-Allow-Methods; proxy_hide_header Access-Control-Allow-Headers; add_header Access-Control-Allow-Headers 'X-Requested-With, Content-Type'; add_header Access-Control-Allow-Credentials true; } location /test{ proxy_pass 127.0.0.1:5600/ } } }",
    "website_area": "discuss"
  },
  {
    "id": "dc7cc3d2-f038-41c3-bec9-a877c5de81bb",
    "url": "https://discuss.elastic.co/t/kibana-role-not-getting-reflected/213833",
    "title": "Kibana Role Not getting reflected",
    "category": [
      "Kibana"
    ],
    "author": "santhosh_kumar2",
    "date": "January 6, 2020, 6:29am January 6, 2020, 4:18pm January 7, 2020, 6:00am",
    "body": "While creating new user role. Getting 200 response but not reflecting in list of user role. i can't find any mislead in log as well. kindly help on this.",
    "website_area": "discuss"
  },
  {
    "id": "afe9e55e-dfa8-4645-bda4-0a2c8601a401",
    "url": "https://discuss.elastic.co/t/logstash-data-parsing-problem/213704",
    "title": "Logstash Data Parsing Problem",
    "category": [
      "Kibana"
    ],
    "author": "Syed.Ubaid",
    "date": "January 3, 2020, 12:44pm January 3, 2020, 3:44pm January 6, 2020, 4:33am January 6, 2020, 4:59am January 6, 2020, 5:07am January 6, 2020, 5:53pm January 7, 2020, 4:19am January 7, 2020, 5:34am",
    "body": "I have parse data through logstash i have 3 records in sql server the data is parse it shows image1366768 88 KB if you look the attached image it shows 24 records and logstash shutdown after parsing data from database is there any way to logstash not shutdown and i insert record in sql and new record is showing in kibana?? and why it shows 24 records instead of 3 records",
    "website_area": "discuss"
  },
  {
    "id": "78a82e27-f105-40a7-ad44-6f1e4f9d0ea1",
    "url": "https://discuss.elastic.co/t/calculate-difference-in-time-between-two-documents-in-same-bucket-on-data-table-visualization/213870",
    "title": "Calculate difference in time between two documents in same bucket on Data Table Visualization",
    "category": [
      "Kibana"
    ],
    "author": "lmit",
    "date": "January 6, 2020, 10:01am January 6, 2020, 4:11pm January 7, 2020, 4:43am",
    "body": "I am struggling to build a Data Table visualization below that captures the timestamp of a prior doc in the same bucket where a field value changed from the current doc, so that I can show the difference in time. For example below, show the time difference since the status field changed to UP in the Name field bucket?? Can anyone help point the way?",
    "website_area": "discuss"
  },
  {
    "id": "a7405189-44a6-4e59-8d4e-fb1c022e0930",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-integrate-javascript-and-kibana/213940",
    "title": "Is it possible to integrate JavaScript and Kibana?",
    "category": [
      "Kibana"
    ],
    "author": "anelson-edge",
    "date": "January 6, 2020, 7:37pm January 6, 2020, 7:41pm January 6, 2020, 10:59pm January 6, 2020, 11:21pm January 6, 2020, 11:54pm January 7, 2020, 12:05am",
    "body": "I am in a quandary here. I would like to have JavaScript abilities (like buttons) AND Kibana for global filters and the selected timerange. I could write my GUI using JS to talk directly to ES, but then I lose Kibana filters. I could write my GUI using Kibana + Vega, but then I can't have buttons AFAICT -- see this post: https://github.com/elastic/kibana/issues/30626. Or to put my question in more concrete terms, try this Vega example in Kibana and notice the lack of buttons: https://vega.github.io/editor/#/examples/vega/nested-bar-chart",
    "website_area": "discuss"
  },
  {
    "id": "35ba42e0-37c9-495b-a55d-0d60aab519e6",
    "url": "https://discuss.elastic.co/t/is-there-a-way-to-make-a-dashboard-default-from-space/213917",
    "title": "Is there a way to make a dashboard default from space?",
    "category": [
      "Kibana"
    ],
    "author": "Abdul_Jaleel",
    "date": "January 6, 2020, 4:25pm January 6, 2020, 11:41pm",
    "body": "I have multiple dashboards inside different spaces ? Is there a way to launch a dashboard on clicking on space ? Right now It will take to home page and from there to dashboards and then pick dashboard interested. Is there a way to avoid these multiple clicks ?",
    "website_area": "discuss"
  },
  {
    "id": "1bf92433-76c2-47c2-adbb-a01e063a98da",
    "url": "https://discuss.elastic.co/t/kibana-7-5-disable-authentication/213935",
    "title": "Kibana 7.5 disable authentication",
    "category": [
      "Kibana"
    ],
    "author": "J_Warner",
    "date": "January 6, 2020, 6:31pm January 6, 2020, 8:47pm January 6, 2020, 7:21pm January 6, 2020, 8:48pm January 6, 2020, 7:53pm January 6, 2020, 8:31pm January 6, 2020, 8:54pm January 6, 2020, 9:16pm January 6, 2020, 11:24pm",
    "body": "Hey I've configured Elasticsearch for Anonymous access which works. but because of this I can't login inside of my kibana instance is there a way to disable the login auth screen for kibana ?",
    "website_area": "discuss"
  },
  {
    "id": "8fb31633-1191-4a63-9e95-68c00b7255e5",
    "url": "https://discuss.elastic.co/t/how-can-i-solve-this-kibana-web-host-error/213014",
    "title": "How can i solve this kibana web host error",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "December 25, 2019, 11:01am December 26, 2019, 4:11pm January 5, 2020, 10:36am January 6, 2020, 9:28pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1a4d351e-b938-4bad-8833-da73dc0fe19c",
    "url": "https://discuss.elastic.co/t/what-is-the-event-hook-to-know-when-the-dashboard-dark-theme-changes/213927",
    "title": "What is the event hook to know when the dashboard dark theme changes?",
    "category": [
      "Kibana"
    ],
    "author": "brandtj",
    "date": "January 6, 2020, 5:26pm January 6, 2020, 6:22pm January 6, 2020, 6:27pm January 6, 2020, 6:33pm January 6, 2020, 6:34pm January 6, 2020, 6:36pm January 6, 2020, 6:43pm January 6, 2020, 6:46pm January 6, 2020, 7:16pm January 6, 2020, 9:18pm",
    "body": "I can't seem to find how to register a plugin to watch for the dark theme changing on a dashboard.",
    "website_area": "discuss"
  },
  {
    "id": "92143e2d-9352-44ca-9739-afd50ec3a88b",
    "url": "https://discuss.elastic.co/t/searcherror-internal-server-error-in-discover/212479",
    "title": "SearchError: Internal Server Error in 'Discover'",
    "category": [
      "Kibana"
    ],
    "author": "rdeshmukh",
    "date": "December 19, 2019, 2:50pm December 19, 2019, 1:06pm January 3, 2020, 10:32am January 6, 2020, 8:51pm",
    "body": "Hi, I'm facing this issue. I have indexed 200 records. In each record the body has huge amount of text data nearly 1000+ pages. I have tried increasing the heap size upto 16gb in jvm options xms16g and xmx16g. So there are very long strings in one field. I have set [index.highlight.max_analyzed_offset] more than it is required. But still I'm not able resolve. I think is it out of memory issue ? Please help me out of this.. Capture1920509 55.6 KB",
    "website_area": "discuss"
  },
  {
    "id": "f127bd03-38e7-481c-a8fc-c66d51d1c1e6",
    "url": "https://discuss.elastic.co/t/elastic-search-kibana-advanced-watcher-script/213934",
    "title": "Elastic Search - Kibana - Advanced Watcher Script",
    "category": [
      "Kibana"
    ],
    "author": "Maries",
    "date": "January 6, 2020, 6:26pm January 6, 2020, 7:28pm January 6, 2020, 7:40pm January 6, 2020, 7:44pm January 6, 2020, 7:48pm",
    "body": "I have created an Advanced Watcher Script to trigger an action if the specific index does not receive data for the last 60 minutes. Script is working fine and alarms are getting triggered when the index does not receive any data in the last 60 minutes and able to view the alarms in Kibana UI. Created an advanced watcher script with action type as log. Please let me know whether i can see these alarms or log files in back-end server.",
    "website_area": "discuss"
  },
  {
    "id": "fb51a0bc-8adc-4b70-afa4-57be2365d580",
    "url": "https://discuss.elastic.co/t/custom-visualization-in-typescript-not-showing-up-in-new-visualization-list/213939",
    "title": "Custom Visualization in Typescript not showing up in New Visualization list",
    "category": [
      "Kibana"
    ],
    "author": "illeatmyhat",
    "date": "January 6, 2020, 7:19pm January 6, 2020, 7:33pm",
    "body": "I made an extremely basic visualization using Typescript but it won't show up in the list of visualizations I can create. I generated the plugin using node scripts/generate_plugin my_plugin without any additional generated components. I used the Markdown plugin as a reference Does anyone have a basic example of an actually working custom kibana visualization using typescript? index.js: export default function (kibana) { return new kibana.Plugin({ require: ['elasticsearch'], name: 'my_plugin', uiExports: { visTypes: [ 'plugins/my_plugin/vis_type', ] }, config(Joi) { return Joi.object({ enabled: Joi.boolean().default(true), }).default(); }, }); } vis_type.tsx: import React from 'react'; // @ts-ignore import { visFactory } from 'ui/vis/vis_factory'; const Editor = () => <p>hello editor</p> const Visualization = () => <p>hello visualization</p> export const CustomVisType = visFactory.createReactVisualization({ name: 'my_plugin', title: 'My Plugin', icon: 'graphApp', isAccessible: true, description: 'A really basic plugin', visConfig: { component: Visualization, defaults: {} }, editor: 'default', editorConfig: { optionTabs: [ { name: 'editor', title: 'Editor', editor: Editor } ], enableAutoApply: true, }, requestHandler: 'none', responseHandler: 'none', });",
    "website_area": "discuss"
  },
  {
    "id": "1332e599-41ed-4beb-bddc-bd2e6d1b9543",
    "url": "https://discuss.elastic.co/t/unable-to-generate-report-max-attempts-reached/213720",
    "title": "Unable to generate report / Max attempts reached",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 3, 2020, 3:08pm January 3, 2020, 3:21pm January 3, 2020, 6:40pm January 6, 2020, 7:16pm January 6, 2020, 7:16pm January 6, 2020, 7:16pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "43e56ef1-8960-42b9-9109-bc0695d45db3",
    "url": "https://discuss.elastic.co/t/interface-bandwidth-calculation/213742",
    "title": "Interface bandwidth calculation",
    "category": [
      "Kibana"
    ],
    "author": "Richard_Neely",
    "date": "January 3, 2020, 7:33pm January 3, 2020, 11:29pm January 6, 2020, 6:05pm",
    "body": "I'm using the logstash snmp plugin to get ifInOctect and ifOutOctect for all my interfaces. How can I create a scripted field to show the actual bandwidth of the interface from these numbers?",
    "website_area": "discuss"
  },
  {
    "id": "fba7de11-d416-42e6-bfcc-e2f161ea150a",
    "url": "https://discuss.elastic.co/t/kibana-control-in-visualization-not-showing-all-values-than-expected/213896",
    "title": "Kibana control in visualization not showing all values than expected",
    "category": [
      "Kibana"
    ],
    "author": "renu_jessi",
    "date": "January 6, 2020, 2:10pm January 6, 2020, 4:41pm January 6, 2020, 5:22pm January 6, 2020, 5:37pm",
    "body": "kibana control in visualization not showing all values than expected.iam using 7.2 kibana",
    "website_area": "discuss"
  },
  {
    "id": "48ea3868-5c1c-4da0-9fbf-53740398d8d1",
    "url": "https://discuss.elastic.co/t/manage-cross-clusters-index-life-cycle-policy/213867",
    "title": "Manage cross-clusters index-life-cycle-policy",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "January 6, 2020, 9:53am January 6, 2020, 4:35pm January 6, 2020, 5:33pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8e3d090a-20a3-4bdf-8b26-1df3409f5e2f",
    "url": "https://discuss.elastic.co/t/users-roles-migration/213786",
    "title": "Users & Roles Migration",
    "category": [
      "Kibana"
    ],
    "author": "karnamonkster",
    "date": "January 5, 2020, 3:17am January 14, 2020, 9:53am",
    "body": "Hi, What would be the best way to migrate all my User and Roles from a cluster running 6.8.0 to a new cluster running 7.5?",
    "website_area": "discuss"
  },
  {
    "id": "f3fa81de-6423-4ea8-aac9-a591d544b42c",
    "url": "https://discuss.elastic.co/t/how-to-know-the-supported-es-version-for-a-specific-kibina-version/213836",
    "title": "How to know the supported es version for a specific kibina version?",
    "category": [
      "Kibana"
    ],
    "author": "Kramer_Li",
    "date": "January 6, 2020, 7:06am January 6, 2020, 4:16pm",
    "body": "I downloaded a kibina 7.5.1. And want to know if my elasticsearch version are supported by this kibina version Is there any doc about this?",
    "website_area": "discuss"
  },
  {
    "id": "a6fd99f2-69cb-4d85-b91a-0bee3034c4e8",
    "url": "https://discuss.elastic.co/t/kibana-changes-require-logout-login-to-be-visible/213744",
    "title": "Kibana changes require logout login to be visible",
    "category": [
      "Kibana"
    ],
    "author": "anelson-edge",
    "date": "January 3, 2020, 7:42pm January 3, 2020, 7:43pm January 3, 2020, 9:57pm January 6, 2020, 4:06pm",
    "body": "I am not using Kibana Spaces (yet). I am using Kibana version 7.4.2. I am logged as user:admin. My collaborator is logged in as user:kibana5. When I make changes to the dashboard that we are working on together, she only sees my changes if she logs out and then logs in again. Neither a reload, or SHIFT-reload shows her my new changes. Is this a known issue? Is there any easy workaround?",
    "website_area": "discuss"
  },
  {
    "id": "93959129-f079-4f63-90cb-8a335b88591e",
    "url": "https://discuss.elastic.co/t/kibana-6-5-custom-visualisation-not-receiving-filters/199846",
    "title": "Kibana 6.5 custom visualisation not receiving filters",
    "category": [
      "Kibana"
    ],
    "author": "reillye",
    "date": "September 18, 2019, 10:51am September 17, 2019, 7:15pm September 23, 2019, 2:04pm September 25, 2019, 1:18pm October 16, 2019, 2:02pm October 22, 2019, 12:11pm October 24, 2019, 8:51am October 30, 2019, 2:13pm October 30, 2019, 2:25pm November 11, 2019, 10:04am November 15, 2019, 11:12am November 19, 2019, 11:40am November 19, 2019, 1:27pm November 20, 2019, 9:54am November 28, 2019, 1:55pm December 2, 2019, 1:42pm December 12, 2019, 4:03pm January 6, 2020, 2:45pm",
    "body": "Hi, I have written a custom visualisation for Kibana 6.5, which has its own custom request and response handlers. It queries elasticsearch and returns all the data for the chosen time period in its raw format. I want to be able to filter this data by using the built-in kibana filters, however I do not believe I have written my requestHandler.js class to handle these. I know that the requestHandler takes 3 parameters: params , queryFilter and timeFilter . I already use timeFilter to search for the given time period, but can I use the other two to pass the values from chosen filters from kibana into the query? Is that how it would work? I have seen in this documentation on filter context that you can add a filter to the query. Is this the correct approach? My code is below for my current visualisation and the request handler, can anyone advise on how I can include filters into my query to ES? self_changing_vis.js import { VisFactoryProvider } from 'ui/vis/vis_factory'; import { VisTypesRegistryProvider } from 'ui/registry/vis_types'; import { SelfChangingEditor } from './self_changing_editor'; import { SelfChangingComponent } from './self_changing_components'; import { Schemas } from 'ui/vis/editors/default/schemas'; import optionsTemplate from './options_template.html'; import { RequestHandlerProvider } from './requestHandler.js'; import handleResponse from './responseHandler.js'; const myResponseHandler = (vis, response) => { // transform the response (based on vis object?) console.log('1: ', response); console.log('2 : ', vis); return response; }; function SelfChangingVisType(Private) { const VisFactory = Private(VisFactoryProvider); const RequestHandler = Private(RequestHandlerProvider); return VisFactory.createReactVisualization({ name: 'self_changing_vis', title: 'Skyplot', icon: 'visControls', description: 'This visualization is able to change its own settings, that you could also set in the editor.', visConfig: { component: SelfChangingComponent, defaults: { counter: 0, constellation: 'GPS' }, }, editor: 'default', editorConfig: { optionsTemplate: optionsTemplate, schemas: new Schemas([ { group: 'metrics', name: 'metric', title: 'Metric', min: 1, aggFilter: ['!derivative', '!geo_centroid'], defaults: [ { type: 'count', schema: 'metric' } ] }, { group: 'buckets', name: 'segment', title: 'Bucket Split', min: 0, max: 1, aggFilter: ['!geohash_grid', '!filter'] } ]), }, requestHandler: RequestHandler.handle, responseHandler: handleResponse, }); } VisTypesRegistryProvider.register(SelfChangingVisType); requestHandler.js const getRequestBody = (params, queryFilter, timeFilter) => { console.log('params: ', params); console.log('queryFilter: ', queryFilter); const requestBody = { size: 10000, 'query': { 'bool': { 'must': [ { 'range': { 'timestamp': { 'gte': timeFilter.from, 'lte': timeFilter.to } } } ] } }, 'sort': [ { 'timestamp': { 'order': 'asc' } } ] }; return requestBody; }; export function RequestHandlerProvider(Private, es) { return { handle(vis) { const { timeFilter, queryFilter } = vis.API; return new Promise(resolve => { const params = vis.params; const requestBody = getRequestBody(params, queryFilter, timeFilter._time); es.search({ index: 'observed_range_error', body: requestBody }).then(result => resolve(result)); }); } }; }",
    "website_area": "discuss"
  },
  {
    "id": "5c95c0b3-d616-4aa0-b6d7-f4555e77dfe3",
    "url": "https://discuss.elastic.co/t/trigger-reload-of-es-data-from-a-plugin/212027",
    "title": "Trigger reload of ES data from a plugin",
    "category": [
      "Kibana"
    ],
    "author": "Elaak",
    "date": "December 16, 2019, 3:19pm December 20, 2019, 12:58pm January 6, 2020, 12:59pm",
    "body": "I have a plugin that is used to edit values in ES using the REST API. I get a success or fail response back and would like to trigger a refresh of the data if the update is successful. In essence, I want to with my script essentially \"press\" the Refresh button in the upper-right corner of Kibana when I am in my Dashboard or \"Discover\". I have tried to go through the code to find the method that is called by pressing Refresh, but so far have not been able to find it.",
    "website_area": "discuss"
  },
  {
    "id": "c6d65ab5-6346-4720-b929-1982cfe99a10",
    "url": "https://discuss.elastic.co/t/timelion-how-to-avoid-error-of-split-when-there-is-not-data-during-the-statistical-period/213849",
    "title": "Timelion: How to avoid error of split when there is not data during the statistical period?",
    "category": [
      "Kibana"
    ],
    "author": "piratf",
    "date": "January 6, 2020, 8:34am",
    "body": "Hey guys, I'm using Kibana version: 6.4.3 I have a query to find the top N functions that generated error logs which been hit most today and compare it with yesterday, here is an example: image2482885 197 KB The grey line is top N functions of today, and the other lines are the functions which have error logs newly generated today that we need to pay attention. In use we will choose the time range as last 1d. Here is the query content, I simply used split and if to complete this query, I'm not sure if it's the right approach: // cLogLevel:1 means findout the error log, level 1 means error. // dwUserDef1 means line number, need to filter out reports containing line numbers // sUserDef2 means function name. // first line draws top N functions today .es(q='cLogLevel:1 AND dwUserDef1:>0', split=sUserDef2:10).label().color(\"#c6c6c6\"), // second line draws with comparison .es(q='cLogLevel:1 AND dwUserDef1:>0', split=sUserDef2:10).if(operator=gt, if=.es(offset=-1d, q='cLogLevel:1 AND dwUserDef1:>0', split=sUserDef2:10), then=.es(q='cLogLevel:1 AND dwUserDef1:>0', split=sUserDef2:10)).label(\"[$1]\", regex=\"^.* > sUserDef2:([\\S:\\(\\)_ ]+) > .*\").lines(fill=5, steps=0), Now the problem is: If there is no error log during the statistical period, using split will report an error: image2484835 95 KB I want to draw all the data today when there is no data yesterday, since all the data today can be considerd newly generated, and draw null on the no data part. I have tried adding a new if condition to draw only if the total value greater than 0, but I still get this error, it seems like I can not skip the calculation even if the condition not met. .es(offset=-1d, q='data today').if(operator=gt, if=0, then=.es(q='the query above'), else=null) Is there any way to solve this problem",
    "website_area": "discuss"
  },
  {
    "id": "1feac797-1cb6-464e-bb19-836b11df7e79",
    "url": "https://discuss.elastic.co/t/kibana-sql-server/213666",
    "title": "Kibana Sql server",
    "category": [
      "Kibana"
    ],
    "author": "Syed.Ubaid",
    "date": "January 3, 2020, 7:11am January 3, 2020, 11:58pm January 6, 2020, 4:27am January 6, 2020, 5:15am",
    "body": "I have 2 records in my sql server and i have integrate the sql server with elk stack now when i run my logstash file it works fine. Can is this possible if i enter the third record in sql server the data will parse in kibana automatically without running the logstash file ? or everytime i dump data in sql server i have to run the logstash file for parsing is any way around for this? what i want that when i dump the data in the sql server it will show in the kibana.",
    "website_area": "discuss"
  },
  {
    "id": "324ed4cd-75b2-4dbc-920b-e39e1380b446",
    "url": "https://discuss.elastic.co/t/kibana7-5-1-yarn-kbn-bootstrap-failed/213662",
    "title": "Kibana7.5.1 yarn kbn bootstrap failed",
    "category": [
      "Kibana"
    ],
    "author": "Todd",
    "date": "January 3, 2020, 7:00am January 4, 2020, 12:01am January 6, 2020, 4:31am",
    "body": "hi, when I try to build kibana-7.5.1 source code, it failed my yarn and node version is: the error message is: Installs completed, linking package executables: [kbn_tp_embeddable_explorer] plugin-helpers -> ../../../../packages/kbn-plugin-helpers/bin/plugin-helpers.js [kbn_tp_sample_panel_action] plugin-helpers -> ../../../../packages/kbn-plugin-helpers/bin/plugin-helpers.js [x-pack] plugin-helpers -> ../packages/kbn-plugin-helpers/bin/plugin-helpers.js Linking executables completed, running `kbn:bootstrap` scripts @elastic/datemath: $ yarn build --quiet @kbn/elastic-idx: $ yarn build @kbn/config-schema: $ yarn build @kbn/utility-types: $ tsc @elastic/datemath: $ babel src --out-dir target --copy-files --quiet @kbn/elastic-idx: $ tsc @kbn/config-schema: $ tsc @kbn/dev-utils: $ yarn build @kbn/dev-utils: $ tsc @kbn/dev-utils: src/proc_runner/proc.ts(103,7): error TS2345: Argument of type 'MonoTypeOperatorFunction<unknown>' is not assignable to parameter of type 'OperatorFunction<unknown, [number]>'. @kbn/dev-utils: Type 'Observable<unknown>' is not assignable to type 'Observable<[number]>'. @kbn/dev-utils: Type 'unknown' is not assignable to type '[number]'. @kbn/dev-utils: src/proc_runner/proc.ts(122,10): error TS2345: Argument of type 'MonoTypeOperatorFunction<number | null>' is not assignable to parameter of type 'OperatorFunction<unknown, number | null>'. @kbn/dev-utils: Types of parameters 'source' and 'source' are incompatible. @kbn/dev-utils: Type 'Observable<unknown>' is not assignable to type 'Observable<number | null>'. @kbn/dev-utils: Type 'unknown' is not assignable to type 'number | null'. @kbn/dev-utils: Type 'unknown' is not assignable to type 'number'. @kbn/dev-utils: src/proc_runner/proc.ts(144,47): error TS2554: Expected 1 arguments, but got 2. @kbn/dev-utils: src/proc_runner/proc.ts(152,47): error TS2554: Expected 1 arguments, but got 2. @kbn/dev-utils: info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.  @kbn/dev-utils: error Command failed with exit code 2. @kbn/dev-utils: info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.  @kbn/dev-utils: error Command failed with exit code 2. [bootstrap] failed: Error: Command failed: yarn run kbn:bootstrap error Command failed with exit code 2. error Command failed with exit code 2. $ yarn build $ tsc src/proc_runner/proc.ts(103,7): error TS2345: Argument of type 'MonoTypeOperatorFunction<unknown>' is not assignable to parameter of type 'OperatorFunction<unknown, [number]>'. Type 'Observable<unknown>' is not assignable to type 'Observable<[number]>'. Type 'unknown' is not assignable to type '[number]'. src/proc_runner/proc.ts(122,10): error TS2345: Argument of type 'MonoTypeOperatorFunction<number | null>' is not assignable to parameter of type 'OperatorFunction<unknown, number | null>'. Types of parameters 'source' and 'source' are incompatible. Type 'Observable<unknown>' is not assignable to type 'Observable<number | null>'. Type 'unknown' is not assignable to type 'number | null'. Type 'unknown' is not assignable to type 'number'. src/proc_runner/proc.ts(144,47): error TS2554: Expected 1 arguments, but got 2. src/proc_runner/proc.ts(152,47): error TS2554: Expected 1 arguments, but got 2. info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command. info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command. at makeError (/data/kibana-7.5.1/packages/kbn-pm/dist/index.js:19328:9) at Promise.all.then.arr (/data/kibana-7.5.1/packages/kbn-pm/dist/index.js:19432:16) at process._tickCallback (internal/process/next_tick.js:68:7) error Command failed with exit code 1. info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command. how can I fix it? thanks!",
    "website_area": "discuss"
  },
  {
    "id": "9a3786da-ac5a-4d16-8278-18c6fd6f1fc1",
    "url": "https://discuss.elastic.co/t/how-to-configure-apm-of-elastic-search-in-asp-net-framework/216086",
    "title": "How to configure APM of elastic search in ASP.NET Framework?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 5:02pm January 22, 2020, 5:06pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c1f224f8-fa74-430b-83d8-0568d02ab27b",
    "url": "https://discuss.elastic.co/t/integrating-a-application-logs-using-apm-server-in-existing-setup-of-kibana/216059",
    "title": "Integrating a application logs using APM server in existing setup of Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 1:30pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "3938e909-ef20-467c-90f7-a5ddea930b63",
    "url": "https://discuss.elastic.co/t/does-apm-support-microsoft-dynamics-ax/216038",
    "title": "Does APM support Microsoft Dynamics AX?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 2:07pm January 22, 2020, 10:43am January 22, 2020, 10:56am January 22, 2020, 11:08am January 22, 2020, 11:10am January 22, 2020, 11:18am January 22, 2020, 11:26am January 22, 2020, 11:31am January 22, 2020, 11:48am January 22, 2020, 12:01pm January 22, 2020, 12:36pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d32d93bb-436d-4b15-ac64-437410732c50",
    "url": "https://discuss.elastic.co/t/kibana-apm-ui-not-displaying-cpu-memory-metric-for-kubernetes-pods-containers-shipping-apm-data/216011",
    "title": "Kibana APM UI not displaying CPU & Memory metric for Kubernetes pods/containers shipping APM data",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 1:28pm January 22, 2020, 11:27am January 22, 2020, 12:02pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7d317b21-fc1f-4e02-9751-84b02750b4fe",
    "url": "https://discuss.elastic.co/t/apm-angular-js-unknown-transaction/216026",
    "title": "APM Angular JS - Unknown transaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 12:47pm January 22, 2020, 10:20am January 22, 2020, 10:20am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "75ae04ff-4d82-43a6-89a8-4a9e0352ab66",
    "url": "https://discuss.elastic.co/t/incorrect-error-grouping/215939",
    "title": "Incorrect Error Grouping",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 9:18am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e17da89e-5c1d-47ea-a7ed-7c79b1298707",
    "url": "https://discuss.elastic.co/t/added-label-to-apm-which-has-the-incorrect-index-type/215650",
    "title": "Added label to APM which has the incorrect index 'type'",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 20, 2020, 12:19pm January 21, 2020, 1:51pm January 21, 2020, 1:56pm January 22, 2020, 1:55am January 22, 2020, 9:15am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a40787e8-dc2b-40aa-b81a-f1809c60e285",
    "url": "https://discuss.elastic.co/t/how-to-reduce-apm-transactions-and-metrics/215206",
    "title": "How to reduce APM transactions and metrics?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 16, 2020, 11:47pm January 16, 2020, 2:34am January 16, 2020, 3:17am January 16, 2020, 6:31am January 17, 2020, 12:06am January 17, 2020, 4:08pm January 20, 2020, 9:19pm January 21, 2020, 4:24pm January 21, 2020, 7:36pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "358d6407-4346-4599-992d-1a95783615c0",
    "url": "https://discuss.elastic.co/t/apm-rum-js-agent-service-worker/215862",
    "title": "APM RUM JS Agent + Service worker",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 21, 2020, 11:24am January 21, 2020, 1:57pm January 21, 2020, 3:04pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "36f03fee-777a-4515-babf-49946610e6ca",
    "url": "https://discuss.elastic.co/t/please-ask-when-apm-server-supports-php-agent/215674",
    "title": "Please ask when APM Server supports PHP agent?",
    "category": [
      "APM"
    ],
    "author": "mikecui",
    "date": "January 20, 2020, 6:57am January 20, 2020, 7:23am January 21, 2020, 3:22am January 21, 2020, 7:55am",
    "body": "If you are asking about a problem you are experiencing, please use the following template, as it will help us help you. If you have a different problem, please delete all of this text Kibana version: 7.5.1 Elasticsearch version:7.5.1 APM Server version:7.5.1 APM Agent language and version: Browser version: Original install method (e.g. download page, yum, deb, from source, etc.) and version: Fresh install or upgraded from other version? Is there anything special in your setup? For example, are you using the Logstash or Kafka outputs? Are you using a load balancer in front of the APM Servers? Have you changed index pattern, generated custom templates, changed agent configuration etc. Description of the problem including expected versus actual behavior. Please include screenshots (if relevant): Steps to reproduce: 1. 2. 3. Errors in browser console (if relevant): Provide logs and/or server output (if relevant):",
    "website_area": "discuss"
  },
  {
    "id": "dbe406b7-51d6-4fe6-b37b-c5a4a7dfb874",
    "url": "https://discuss.elastic.co/t/default-apm-server-docker-did-not-create-apm-7-5-1-transaction-and-apm-7-5-1-span-with-suffix-number-behind/215815",
    "title": "Default APM Server docker did not create apm-7.5.1-transaction and apm-7.5.1-span with suffix number behind",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 21, 2020, 7:07am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b5acb676-f4cb-4b69-b521-5a8994eed1a5",
    "url": "https://discuss.elastic.co/t/apm-grails/215346",
    "title": "APM + Grails",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 16, 2020, 4:05pm January 17, 2020, 12:56pm January 17, 2020, 1:30pm January 20, 2020, 5:04pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "414313f9-359b-4ed5-ac21-50adccd5987e",
    "url": "https://discuss.elastic.co/t/dotnet-apm-not-displaying-data-on-the-span-chart/215719",
    "title": "Dotnet APM not displaying data on the span chart",
    "category": [
      "APM"
    ],
    "author": "asmutti",
    "date": "January 20, 2020, 11:23am January 20, 2020, 11:49am January 20, 2020, 11:56am",
    "body": "Kibana version: 7.5.1 Elasticsearch version: 7.5.1 APM Server version: 7.5.1 APM Agent language and version: dotnet 1.2.0 Browser version: Chrome Is there any extra config that I must set up to show the span data? I'm pretty sure the spans info are sent to the ES.",
    "website_area": "discuss"
  },
  {
    "id": "bee7ccbf-8b50-4c48-8822-55a7b607088e",
    "url": "https://discuss.elastic.co/t/how-can-i-access-headers-or-custom-headers-in-apm/213673",
    "title": "How can I access headers or custom headers in APM?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 3, 2020, 9:56am January 3, 2020, 9:55am January 20, 2020, 7:11am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "daea7b86-a4ca-4d5b-a60b-fcf58ab9e5c9",
    "url": "https://discuss.elastic.co/t/elastic-apm-rum-js-agent-grouping-http-request-transactions/215380",
    "title": "Elastic APM RUM JS Agent - Grouping http-request transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 16, 2020, 7:32pm January 17, 2020, 6:58pm January 17, 2020, 8:05pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e581555f-d566-44e3-a706-dfee1738a47e",
    "url": "https://discuss.elastic.co/t/elastic-apm-cant-find-service-name-mentioned-in-elastic-apm-agent-on-kibana-ui-or-in-elastic-index/215440",
    "title": "Elastic apm : can't find service name mentioned in elastic apm agent on kibana ui or in elastic index",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 17, 2020, 10:44am January 17, 2020, 12:20pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b66c368e-5d01-4ae5-b643-8327f4699d6c",
    "url": "https://discuss.elastic.co/t/hosts-value-i-comment-apm-server-do-not-start/214822",
    "title": "Hosts value I comment , apm-server do not start",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 13, 2020, 1:51pm January 14, 2020, 6:31am January 17, 2020, 6:44am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a82b4f4b-559a-488d-972e-560ef5281941",
    "url": "https://discuss.elastic.co/t/websphere-application-server-support/215048",
    "title": "Websphere Application Server support",
    "category": [
      "APM"
    ],
    "author": "ledgpat",
    "date": "January 14, 2020, 9:31pm January 15, 2020, 6:34am January 15, 2020, 7:18pm January 16, 2020, 3:50pm January 16, 2020, 3:50pm",
    "body": "Hello, I know that the Supported technologies page only lists Websphere Liberty as being officially supported, but I have also seen references to future Websphere classic/full support being on the radar. It's been a while since I have seen any updates, is this still something that is expected to be added officially? We have had success for the most part so far using the java agent with Websphere (classic) 8.5.5 in our development environment but would feel more comfortable with official support for production! Thanks,",
    "website_area": "discuss"
  },
  {
    "id": "039bc14d-bef0-4ba2-a494-3cf78fbfa58c",
    "url": "https://discuss.elastic.co/t/elastic-apm-rum-not-working-with-angular-8/214274",
    "title": "Elastic APM RUM not working with Angular 8",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 8, 2020, 4:12pm January 8, 2020, 4:34pm January 9, 2020, 2:56pm January 10, 2020, 1:02pm January 10, 2020, 3:03pm January 14, 2020, 9:42am January 15, 2020, 8:24pm January 16, 2020, 10:36am January 16, 2020, 1:41pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6dea92a6-81cc-4129-ac1d-fc5d06318bfc",
    "url": "https://discuss.elastic.co/t/transactions-tab-empty-but-data-with-transaction-id-available-in-apm-indices/214057",
    "title": "Transactions tab empty but data with `transaction.id` available in `apm-*` indices",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 7, 2020, 5:09pm January 7, 2020, 5:59pm January 8, 2020, 3:10pm January 12, 2020, 7:54pm January 14, 2020, 1:41pm January 14, 2020, 1:54pm January 14, 2020, 3:23pm January 15, 2020, 8:32am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "fa57029c-c39d-4bd2-b604-ec2916e9851a",
    "url": "https://discuss.elastic.co/t/limit-data-retention-on-the-hosted-product/214606",
    "title": "Limit data retention on the hosted product?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 11, 2020, 10:03am January 10, 2020, 2:57pm January 10, 2020, 6:52pm January 13, 2020, 7:56pm January 14, 2020, 8:20am January 14, 2020, 8:33am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a1efb2cf-8f31-44c7-90e0-b492ed8dbdb9",
    "url": "https://discuss.elastic.co/t/capturing-stack-traces-from-graphqlerrors-in-apollo-server/212563",
    "title": "Capturing stack traces from GraphQLErrors in Apollo Server",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 20, 2019, 2:28pm January 9, 2020, 5:12pm January 14, 2020, 7:06am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7e2e7c4d-2d97-4d18-8e68-2963a5aa1adf",
    "url": "https://discuss.elastic.co/t/apm-agent-not-able-to-connect-apm-server/214874",
    "title": "APM agent not able to connect apm server",
    "category": [
      "APM"
    ],
    "author": "sattishv",
    "date": "January 13, 2020, 4:36pm January 13, 2020, 5:05pm",
    "body": "Hi Team, We were able to install elastic search, kibana and apm server successfully on MacOS and apm agent was hooked with the application deployed in ubuntu but not able to connect to apm server. Kibana version: 7.5.1 Elasticsearch version: 7.5.1 APM Server version: 7.5.1 APM Agent language and version: Java 7.5.1 Fresh install or upgraded from other version? Fresh install **Description of the problem including expected versus actual behavior. Please include screenshots config used: -javaagent:/elasticapmagent/elastic-apm-agent-1.12.0.jar -Delastic.apm.service_name=sampleserv -Delastic.apm.server_url=http://apm-serveripaddress:8200 -Delastic.apm.log_level=DEBUG Steps to reproduce: 1. 2. 3. Provide logs and/or server output (if relevant): INFO | jvm 1 | 2020/01/13 08:32:24 | java.net.SocketTimeoutException: connect timed out INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.PlainSocketImpl.socketConnect(Native Method) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.Socket.connect(Socket.java:589) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.NetworkClient.doConnect(NetworkClient.java:175) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.http.HttpClient.openServer(HttpClient.java:463) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.http.HttpClient.openServer(HttpClient.java:558) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.http.HttpClient.(HttpClient.java:242) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.http.HttpClient.New(HttpClient.java:339) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.http.HttpClient.New(HttpClient.java:357) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.startRequest(IntakeV2ReportingEventHandler.java:200) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.handleEvent(IntakeV2ReportingEventHandler.java:138) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:116) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:50) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.lang.Thread.run(Thread.java:748) INFO | jvm 1 | 2020/01/13 08:32:24 | 2020-01-13 08:32:24.872 [apm-reporter] INFO co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Backing off for 36 seconds (+/-10%) INFO | jvm 1 | 2020/01/13 08:31:21 | 2020-01-13 08:31:20.991 [apm-remote-config-poller] DEBUG co.elastic.apm.agent.configuration.ApmServerConfigurationSource - Reloading configuration from APM Server http://10.229.100.178:8200/config/v1/agents INFO | jvm 1 | 2020/01/13 08:31:26 | 2020-01-13 08:31:25.992 [apm-remote-config-poller] DEBUG co.elastic.apm.agent.report.ApmServerClient - Exception while interacting with APM Server, trying next one. INFO | jvm 1 | 2020/01/13 08:31:31 | 2020-01-13 08:31:30.997 [apm-remote-config-poller] ERROR co.elastic.apm.agent.report.HttpUtils - Exception when closing input stream of HttpURLConnection. INFO | jvm 1 | 2020/01/13 08:31:31 | 2020-01-13 08:31:30.997 [apm-remote-config-poller] ERROR co.elastic.apm.agent.configuration.ApmServerConfigurationSource - connect timed out INFO | jvm 1 | 2020/01/13 08:31:31 | 2020-01-13 08:31:30.997 [apm-remote-config-poller] DEBUG co.elastic.apm.agent.configuration.ApmServerConfigurationSource - Scheduling next remote configuration reload in 300s INFO | jvm 1 | 2020/01/13 08:31:40 | 2020-01-13 08:31:40.632 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Receiving METRICS event (sequence 25) INFO | jvm 1 | 2020/01/13 08:31:40 | 2020-01-13 08:31:40.633 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Starting new request to http://10.229.100.178:8200/intake/v2/events INFO | jvm 1 | 2020/01/13 08:31:45 | 2020-01-13 08:31:45.638 [apm-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Failed to handle event of type METRICS with this error: connect timed out INFO | jvm 1 | 2020/01/13 08:31:45 | 2020-01-13 08:31:45.638 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Event handling failure INFO | jvm 1 | 2020/01/13 08:31:45 | java.net.SocketTimeoutException: connect timed out INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.PlainSocketImpl.socketConnect(Native Method) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.Socket.connect(Socket.java:589) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.NetworkClient.doConnect(NetworkClient.java:175) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.http.HttpClient.openServer(HttpClient.java:463) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.http.HttpClient.openServer(HttpClient.java:558) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.http.HttpClient.(HttpClient.java:242) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.http.HttpClient.New(HttpClient.java:339) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.http.HttpClient.New(HttpClient.java:357) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.startRequest(IntakeV2ReportingEventHandler.java:200) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.handleEvent(IntakeV2ReportingEventHandler.java:138) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:116) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:50) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.lang.Thread.run(Thread.java:748) INFO | jvm 1 | 2020/01/13 08:31:45 | 2020-01-13 08:31:45.639 [apm-reporter] INFO co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Backing off for 36 seconds (+/-10%) INFO | jvm 1 | 2020/01/13 08:32:19 | 2020-01-13 08:32:19.869 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Receiving METRICS event (sequence 26) INFO | jvm 1 | 2020/01/13 08:32:19 | 2020-01-13 08:32:19.869 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Starting new request to http://10.229.100.178:8200/intake/v2/events INFO | jvm 1 | 2020/01/13 08:32:24 | 2020-01-13 08:32:24.872 [apm-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Failed to handle event of type METRICS with this error: connect timed out INFO | jvm 1 | 2020/01/13 08:32:24 | 2020-01-13 08:32:24.872 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Event handling failure",
    "website_area": "discuss"
  },
  {
    "id": "54bca6cf-33ba-4c33-9829-9beb3fd95ef2",
    "url": "https://discuss.elastic.co/t/transaction-sample-rate-minimun-value-allowed/214870",
    "title": "Transaction_sample_rate minimun value allowed",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 13, 2020, 4:32pm January 13, 2020, 4:42pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0aba0ef7-d2f5-428b-9630-4b1bf7fa772e",
    "url": "https://discuss.elastic.co/t/load-apm-data-into-kibana-dashboard/214790",
    "title": "Load APM data into Kibana Dashboard",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 13, 2020, 10:04am January 13, 2020, 10:17am January 13, 2020, 10:54am January 13, 2020, 3:45pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "614b1bb2-5dcf-4583-a9af-f15e8644e39f",
    "url": "https://discuss.elastic.co/t/access-metric-of-a-transaction/214815",
    "title": "Access metric of a transaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 13, 2020, 11:34am January 13, 2020, 2:52pm January 13, 2020, 2:54pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "04fc9d04-f601-446b-bc5e-408caca944a7",
    "url": "https://discuss.elastic.co/t/elastic-apm-agent-attached-to-java-application-running-on-jboss-7-2-2-eap/214588",
    "title": "Elastic apm agent attached to java application running on JBoss 7.2.2 EAP",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 10, 2020, 11:09am January 12, 2020, 4:46am January 12, 2020, 7:18pm January 13, 2020, 6:31am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8c698eaf-59b8-4dd1-a351-93c473d1089d",
    "url": "https://discuss.elastic.co/t/elastic-apm-dropping-spans-from-large-distributed-transactions/214241",
    "title": "Elastic APM dropping spans from large distributed transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 11, 2020, 10:03am January 8, 2020, 3:35pm January 8, 2020, 3:44pm January 9, 2020, 9:19am January 10, 2020, 2:53pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e5f2c3d5-116e-4512-a918-8e04043763cb",
    "url": "https://discuss.elastic.co/t/ignoring-certain-http-verbs-from-apm/214286",
    "title": "Ignoring certain HTTP verbs from APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 10, 2020, 1:02pm January 9, 2020, 2:27pm January 9, 2020, 4:05pm January 9, 2020, 4:21pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "969575e9-bc33-4244-9f69-8f84a36d4843",
    "url": "https://discuss.elastic.co/t/is-it-possible-advisable-to-use-apm-in-my-scenario/214392",
    "title": "Is it possible / advisable to use APM in my scenario?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 9, 2020, 9:36am January 9, 2020, 9:55am January 9, 2020, 10:05am January 9, 2020, 1:22pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5469e54c-6b34-41e2-bff5-7bd2294bb883",
    "url": "https://discuss.elastic.co/t/apm-correlate-logs-jboss-6-0-1/214271",
    "title": "APM correlate logs jboss 6.0.1",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 9, 2020, 5:48am January 9, 2020, 5:58am January 9, 2020, 8:21am January 9, 2020, 9:20am January 9, 2020, 11:52am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8ae41086-b130-41db-a279-a747462e8dbf",
    "url": "https://discuss.elastic.co/t/is-apm-server-port-8200-secure-by-default-how-to-make-it-secure/213780",
    "title": "Is APM Server (port 8200) secure by default? How to make it secure?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 6, 2020, 7:30am January 8, 2020, 3:35am January 8, 2020, 3:35am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "82d23ad8-5749-4581-aafd-f379be769640",
    "url": "https://discuss.elastic.co/t/apm-not-creating-aliases/212427",
    "title": "APM not creating aliases",
    "category": [
      "APM"
    ],
    "author": "jmcpherson",
    "date": "December 19, 2019, 2:58am December 19, 2019, 9:10am December 19, 2019, 1:10pm December 20, 2019, 1:54pm January 7, 2020, 3:47pm",
    "body": "Kibana version: 7.5.0 Elasticsearch version: 7.5.0 APM Server version: 7.5.0 Original install method (e.g. download page, yum, deb, from source, etc.) and version: ECK Manifest Fresh install or upgraded from other version? Fresh Description of the problem including expected versus actual behavior. Please include screenshots (if relevant): I am running ECK eck/eck-operator:1.0.0-beta1 and have a elastic/apm/kibana stack setup across approximately 30 clusters. On some of these everything seems to be working fine: Kibana_12286712 85.5 KB On others it looks like so: Kibana2288612 76.4 KB and I start seeing illegal_argument_exception: index.lifecycle.rollover_alias [apm-7.5.0-error] does not point to index [apm-7.5.0-error] in kibana and the following in the apm server logs: 2019-12-19T02:49:56.716Z ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(https://monitoring-es-http.client-env-monitoring.svc:9200)): Connection marked as failed because the onConnect callback failed: resource 'apm-7.5.0-error' exists, but it is not an alias 2019-12-19T02:49:56.716Z INFO pipeline/output.go:93 Attempting to reconnect to backoff(elasticsearch(https://monitoring-es-http.client-env-monitoring.svc:9200)) with 3734 reconnect attempt(s) I'm assuming these aliases should be getting creating but I am not completely sure on that so I guess that's my first question, do I need to create these aliases manually? If not, I could use a bit of direction in figuring out what is going on. Steps to reproduce: Unsure",
    "website_area": "discuss"
  },
  {
    "id": "9bed3f3a-694a-4dbd-9668-236715b26e08",
    "url": "https://discuss.elastic.co/t/activerecord-profiling-with-sinatra/214010",
    "title": "ActiveRecord profiling with Sinatra",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 7, 2020, 9:21am January 7, 2020, 9:36am January 7, 2020, 11:43am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a82f73a1-09c0-4ec4-9457-e29a3d28d465",
    "url": "https://discuss.elastic.co/t/elastic-apm-not-displaying-scheduler-jobs/213956",
    "title": "Elastic APM Not Displaying Scheduler jobs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 8, 2020, 3:35am January 7, 2020, 9:48am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c9e4eb58-30c9-4344-be36-59959ef37652",
    "url": "https://discuss.elastic.co/t/apm-server-unable-to-get-data-from-my-servlet-based-application/213126",
    "title": "APM Server unable to get data from my servlet based application",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 27, 2019, 8:03am December 27, 2019, 6:06am December 27, 2019, 6:15am December 27, 2019, 6:20am December 27, 2019, 8:21am December 27, 2019, 8:29am December 30, 2019, 10:56am December 30, 2019, 10:59am December 30, 2019, 11:00am January 6, 2020, 7:44am January 7, 2020, 9:33am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1ee23c84-d0c9-4412-ba65-a78c50b2fea7",
    "url": "https://discuss.elastic.co/t/apm-nodejs-default-code-is-resulting-in-error/213886",
    "title": "APM NodeJS default code is resulting in error",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 7, 2020, 8:48am January 6, 2020, 12:33pm January 6, 2020, 12:33pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "44dd116c-47cf-455d-b041-49c7e8dab22c",
    "url": "https://discuss.elastic.co/t/how-to-add-and-process-private-custom-attributes-in-elastic-apm-traceparent/213033",
    "title": "How to add and process private custom attributes in elastic-apm-traceparent?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 26, 2019, 3:38am December 26, 2019, 3:39am January 6, 2020, 8:19am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "dd5f37d7-fc10-483e-997e-d75fa497f8a9",
    "url": "https://discuss.elastic.co/t/apm-traces-tab-doesnt-split-out-different-services/213605",
    "title": "APM Traces Tab Doesn't Split out different services",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 2, 2020, 6:40pm January 2, 2020, 8:39pm January 2, 2020, 8:39pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f9eb0159-bda8-452c-bc6c-cb31182c7dd3",
    "url": "https://discuss.elastic.co/t/apm-net-agent-dont-capture-http-status-2xx/211339",
    "title": "APM .NET Agent don't Capture Http Status 2xx",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 11, 2019, 2:07am December 31, 2019, 1:02pm January 2, 2020, 8:17pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "708a5aa2-3bbf-4279-97c8-7f38a9802b39",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-embed-user-id-for-apm-messages-in-django/212805",
    "title": "Is it possible to embed user-id for apm messages in Django?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 23, 2019, 6:12am January 2, 2020, 9:08am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4ed0b08a-251d-465c-b1e7-aafd64b1ee8e",
    "url": "https://discuss.elastic.co/t/elastic-cloud-more-than-100-apm-indices-how-to-automate-the-deletion/213485",
    "title": "Elastic Cloud - More than 100 APM indices - How to automate the deletion?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 1, 2020, 10:56am January 1, 2020, 5:42pm January 22, 2020, 1:38pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "fc9a1d00-e424-47d6-bdd9-41ce222096cc",
    "url": "https://discuss.elastic.co/t/questions-about-transaction/213069",
    "title": "Questions about transaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 26, 2019, 11:07am December 29, 2019, 7:25am December 29, 2019, 8:22am December 29, 2019, 8:45am December 31, 2019, 9:12pm January 1, 2020, 6:17am January 22, 2020, 2:17am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f57c9862-cdc9-4522-b937-9b5a69d87d9b",
    "url": "https://discuss.elastic.co/t/run-apm-server-with-docker-compose/213345",
    "title": "Run APM Server with Docker Compose",
    "category": [
      "APM"
    ],
    "author": "drnextgis",
    "date": "December 30, 2019, 12:07pm December 30, 2019, 4:09pm December 31, 2019, 5:26pm January 21, 2020, 1:26pm",
    "body": "Is there any docker-compose.yml file that can be used to run APM server without building it?",
    "website_area": "discuss"
  },
  {
    "id": "a86f9a20-0706-4491-8c8d-3a55a54e5498",
    "url": "https://discuss.elastic.co/t/cannot-create-apm-watcher-email-error-report/212537",
    "title": "Cannot create APM watcher email error report",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 23, 2019, 6:15am December 30, 2019, 1:11pm December 30, 2019, 1:11pm January 20, 2020, 9:19am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "05450d47-70e8-4c59-ab5c-286e4df45bc3",
    "url": "https://discuss.elastic.co/t/uncertainty-unit-test-cases-in-apm-java-agent/212163",
    "title": "Uncertainty unit test cases in apm java agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 17, 2019, 12:54pm December 17, 2019, 1:15pm January 20, 2020, 9:10am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6885c1c0-aefd-4418-b361-c51117091c3d",
    "url": "https://discuss.elastic.co/t/how-to-config-apm-output-to-kafka-with-topic-name-using-events-processor-event-name/213289",
    "title": "How to config APM output to kafka with topic name using event's processor event name",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 30, 2019, 7:08am December 30, 2019, 8:04am December 30, 2019, 7:47am December 30, 2019, 8:09am January 20, 2020, 4:06am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c72c4cc4-3d1d-4669-90f9-b7c8454994c7",
    "url": "https://discuss.elastic.co/t/can-i-view-custom-error-exception-which-is-appearing-in-tomcat-logs-at-apm-error-listing/213157",
    "title": "Can I view custom error/exception which is appearing in tomcat logs at APM error listing",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 29, 2019, 4:37am December 27, 2019, 8:32am December 27, 2019, 9:11am December 27, 2019, 9:31am December 29, 2019, 4:36am January 19, 2020, 12:36am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f462f227-e238-4655-a5fc-9980cd46f833",
    "url": "https://discuss.elastic.co/t/new-threads-are-blocked-after-enabling-elastic-agent/213145",
    "title": "New Threads are blocked after enabling elastic agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 27, 2019, 6:49am December 29, 2019, 4:06am January 19, 2020, 12:06am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "3f32baab-e2b7-4af3-b780-27e2817ae4b1",
    "url": "https://discuss.elastic.co/t/instrument-incoming-http-requests/213242",
    "title": "Instrument incoming HTTP requests",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 28, 2019, 4:02am January 18, 2020, 12:08am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a9622905-f850-48a6-9c68-20634d25564f",
    "url": "https://discuss.elastic.co/t/the-unit-of-the-max-queue-size-reporter-config-option/213159",
    "title": "The unit of the max_queue_size reporter config option",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 27, 2019, 9:01am December 27, 2019, 9:09am December 27, 2019, 9:36am December 27, 2019, 10:12am January 17, 2020, 5:59am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "581911c0-6555-4bd9-a7d5-6bebf35f3f82",
    "url": "https://discuss.elastic.co/t/application-high-response-time-with-nodejs-agent/213101",
    "title": "Application high response time with Nodejs agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 26, 2019, 4:25pm January 16, 2020, 12:26pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5a419da4-7657-4c56-b7c4-fbddde773ef0",
    "url": "https://discuss.elastic.co/t/send-alert-only-if-the-error-has-http-status-code-500/212689",
    "title": "Send alert only if the error has http status code 500",
    "category": [
      "APM"
    ],
    "author": "Edgar_Peixoto",
    "date": "December 20, 2019, 4:44pm December 20, 2019, 8:59pm January 10, 2020, 4:59pm",
    "body": "I can send error alerts to a slack channel when an Java application send an error to APM. But I want to send only errors with status code 500. I tried to put the following in the bool filter but now the alert does not send anything at all. I also tried to use http.response.status_code with the same result. { \"term\": { \"context.response.status_code\": 500 } } I am using Elastic Cloud. Everything is in 7.5.1 version.",
    "website_area": "discuss"
  },
  {
    "id": "233d7ae8-631f-4cbd-8e55-dcc7ed0ee782",
    "url": "https://discuss.elastic.co/t/watch-creation-failed-make-sure-your-user-has-permission-to-create-watches/212656",
    "title": "Watch creation failed - Make sure your user has permission to create watches",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 20, 2019, 12:47pm December 20, 2019, 4:07pm December 20, 2019, 4:07pm January 10, 2020, 12:07pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d079fb2b-d71b-495b-bfd2-007bd9373b9a",
    "url": "https://discuss.elastic.co/t/apm-errors-not-showing-in-kibana/212158",
    "title": "APM Errors not Showing in Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 17, 2019, 12:12pm December 20, 2019, 12:01pm January 10, 2020, 6:50am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "72560dd4-b619-4513-9a12-82589b1ecde0",
    "url": "https://discuss.elastic.co/t/apm-server-not-creating-separate-indices/212142",
    "title": "APM server not creating separate indices",
    "category": [
      "APM"
    ],
    "author": "sharry007",
    "date": "December 17, 2019, 11:13am December 17, 2019, 3:19pm December 18, 2019, 11:04am December 18, 2019, 12:27pm December 19, 2019, 4:31am December 19, 2019, 2:04pm December 19, 2019, 2:01pm January 9, 2020, 10:01am",
    "body": "Kibana version: 7.4.0 Elasticsearch version: 7.4.0 APM Server version: 7.4.0 Upgraded from other version: From 6.7.1 I upgraded elastic stack and APM server to 7.4.0. I installed APM server from deb package. Earlier SSL was enabled for output.elasticsearch in apm-server.yml but now I have removed it. Other settings are same. I am facing below three problems: Separate indices are not created for different processor.event. I see that only one index is created per day, for example today's index is apm-7.4.0-2019.12.17. I am using default setting for example index: \"apm-%{[beat.version]}-error-%{+yyyy.MM.dd}\" when.contains: processor.event: \"error\" Whenever I try to see results in Kibana discover for index pattern apm-*, I get shard failed issue and the response is No field found for [context.service.name] in mapping with types []. I can see that the fields.yml file is different in 7.4.0 than 6.7.1 and there is no field for context. Should I copy the old fields.yml file and restart APM server? When I start APM server using systemctl start apm-server, logs are not being created in /var/log/apm-server/ but I can see logs in journalctl. When I start APM server using /usr/share/apm-server/bin/apm-server -c /etc/apm-server/apm-server.yml --path.home /usr/share/apm-server --path.config /etc/apm-server --path.data /var/lib/ap -server --path.logs /var/log/apm-server manually, I can see logs in /var/log/apm-server/. How can I configure APM server to send logs to default location when using systemctl command?",
    "website_area": "discuss"
  },
  {
    "id": "a763cba6-1245-4543-b8bd-cefe567d9825",
    "url": "https://discuss.elastic.co/t/how-to-find-customer-schema-in-apm/211952",
    "title": "How to find customer schema in APM?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 16, 2019, 7:48am December 16, 2019, 12:37pm December 17, 2019, 6:23am December 17, 2019, 8:32am December 18, 2019, 6:26am December 18, 2019, 8:46am January 8, 2020, 4:46am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7dfbbbf0-14bc-42a2-ae8d-3d1443a38076",
    "url": "https://discuss.elastic.co/t/apm-7-5-0-not-resolving-api-paths-in-transactions/212061",
    "title": "APM 7.5.0 not resolving API paths in Transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 16, 2019, 9:35pm December 17, 2019, 8:56am December 17, 2019, 1:55pm January 7, 2020, 9:54am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9ecec7d0-6efd-4001-be94-22220ca1fc59",
    "url": "https://discuss.elastic.co/t/elastic-apm-with-windows-services/212110",
    "title": "Elastic APM with windows services",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 17, 2019, 7:55am December 17, 2019, 1:08pm December 17, 2019, 1:08pm January 7, 2020, 9:22am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f75f6025-649c-4932-a49a-e69302896542",
    "url": "https://discuss.elastic.co/t/do-you-support-apm-link-tracking-for-websocket/212087",
    "title": "Do you support APM link tracking for Websocket?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 17, 2019, 8:30am December 17, 2019, 3:42am December 17, 2019, 5:17am December 17, 2019, 7:01am December 17, 2019, 7:53am December 17, 2019, 9:19am December 17, 2019, 10:39am December 17, 2019, 10:08am January 7, 2020, 6:08am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1ec1221b-313f-4c8a-9e41-6ac457780676",
    "url": "https://discuss.elastic.co/t/apm-data-not-showing-in-kibana/212035",
    "title": "APM Data not showing in Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 17, 2019, 8:26am December 17, 2019, 8:29am January 7, 2020, 4:29am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4721a599-a86c-4079-80b3-a7db4a314710",
    "url": "https://discuss.elastic.co/t/errors-per-min-column-in-apm-shows-0-err-even-when-there-are-errors-inside/211950",
    "title": "Errors per min column in APM shows 0 err. even when there are errors inside",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 16, 2019, 12:34pm December 16, 2019, 1:46pm December 17, 2019, 5:51am January 7, 2020, 1:51am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "2548d13e-cd7b-4702-938b-49875a42e672",
    "url": "https://discuss.elastic.co/t/server-fail-over-suggested-method/211843",
    "title": "Server fail over, suggested method",
    "category": [
      "APM"
    ],
    "author": "LordMathis",
    "date": "December 13, 2019, 8:16pm December 16, 2019, 6:03pm January 5, 2020, 6:25pm",
    "body": "Kibana version: 7.3 Elasticsearch version: 7.3 APM Server version: 7.3 APM Agent language and version: Intake API Our team was wondering what you all believe is the best method of fail over for the APM servers. We are alright with loosing the packets when the server goes down, that is not the issue, but our agent implementation was using HTTP calls. This meant that when the server went down all of our applications started doing blocking http calls that persisted until the http timeout. We mitigated this problem by putting all of our http sends into a thread. This allows our send to go down without effecting the user. Is this how it is handled in the other agents? What do you recommend to make certain that the server going down doesn't effect the code APM is monitoring?",
    "website_area": "discuss"
  },
  {
    "id": "8c760f52-8a05-4e6c-9e12-f904f0c975ba",
    "url": "https://discuss.elastic.co/t/how-to-use-traces-with-the-intake-api/211830",
    "title": "How to use traces with the intake API",
    "category": [
      "APM"
    ],
    "author": "LordMathis",
    "date": "December 13, 2019, 6:09pm December 15, 2019, 10:59pm January 5, 2020, 6:56pm",
    "body": "Kibana version: 7.3 Elasticsearch version: 7.3 APM Server version: 7.3 APM Agent language and version: New agent built for all versions of Delphi Original install method (e.g. download page, yum, deb, from source, etc.) and version: Web download I'd like to preface this discussion with the knowledge that I don't exactly quite understand what traces are used for. I believe it is some kind of method to clump all your transactions together so you can observe the flow of the entire application not just the flow of your transaction. My issue is essentially this, the intake API docs don't specify how to properly create a Trace, so I assumed it would be as simple as generating a trace ID and passing that trace id in on all my JSON, but it would appear as though this does not work. When I go to my traces tab there is nothing there and the % of trace on the transaction is N/A. What is it that I need to do to properly create a trace using the intake API?",
    "website_area": "discuss"
  },
  {
    "id": "8df6c2e6-e74f-4b9a-aadc-9c1c005e09b2",
    "url": "https://discuss.elastic.co/t/get-currentspan-in-net-apm-agent/211741",
    "title": "Get CurrentSpan in .Net APM Agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 13, 2019, 4:14am December 13, 2019, 6:56am December 13, 2019, 6:44am December 13, 2019, 4:04pm December 13, 2019, 4:38pm December 14, 2019, 12:33pm December 14, 2019, 12:39pm January 4, 2020, 8:37am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6127bf66-1167-4fe7-b984-3f22dcba89fe",
    "url": "https://discuss.elastic.co/t/golang-apmchi-library-question/211738",
    "title": "Golang apmchi library question",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 13, 2019, 2:44am December 13, 2019, 3:19am January 2, 2020, 11:19pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a2ffbe28-130a-457f-951a-d7ab2fa2fed7",
    "url": "https://discuss.elastic.co/t/cannot-find-logs-for-net-agent/211350",
    "title": "Cannot find logs for .NET agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 10, 2019, 6:02pm December 10, 2019, 6:20pm December 10, 2019, 6:50pm December 12, 2019, 9:17am December 12, 2019, 4:05pm January 2, 2020, 12:05pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "20a0f6e4-8b37-4fb8-b869-1a2832ef6f3b",
    "url": "https://discuss.elastic.co/t/create-multiple-secret-tokens/211458",
    "title": "Create Multiple Secret Tokens",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 11, 2019, 11:27am December 12, 2019, 9:40am December 11, 2019, 5:03pm December 12, 2019, 9:40am December 12, 2019, 9:40am January 2, 2020, 5:40am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9f6d875a-a986-44cc-ab45-d4c88ae8e614",
    "url": "https://discuss.elastic.co/t/opentracing-net-elastic-apm-bridge/211573",
    "title": "OpenTracing .NET Elastic APM Bridge",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 12, 2019, 6:47am December 12, 2019, 9:30am January 2, 2020, 5:30am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d5533c79-b9c7-48df-9c11-f7047aacc45c",
    "url": "https://discuss.elastic.co/t/apm-custom-metrics-from-node-js-app/211559",
    "title": "APM custom metrics from node.js app",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 12, 2019, 4:53am January 2, 2020, 12:53am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "2d80fe71-2e55-4755-acdc-7806ed309045",
    "url": "https://discuss.elastic.co/t/elastic-apm-java-agent-sanitize-fields-names-on-application-json-data/211209",
    "title": "Elastic APM Java agent - sanitize_fields_names on application/json* data",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 10, 2019, 2:05am December 10, 2019, 8:56am December 11, 2019, 4:36pm January 1, 2020, 12:36pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e0967d41-5d68-419b-a42e-53937f40f5ee",
    "url": "https://discuss.elastic.co/t/how-to-monitore-cassandra-queries-in-the-dotnet-agent/211471",
    "title": "How to monitore Cassandra queries in the Dotnet Agent?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 11, 2019, 1:37pm December 11, 2019, 2:21pm December 11, 2019, 2:38pm January 1, 2020, 10:38am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "61bdb2b6-d038-4d46-88bd-8a0b7b4da10f",
    "url": "https://discuss.elastic.co/t/ruby-apm-custom-intrumentation-not-showing-span-in-cronilogical-order/211335",
    "title": "Ruby: APM custom intrumentation not showing span in cronilogical order",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 10, 2019, 4:33pm December 11, 2019, 12:27pm January 1, 2020, 8:27am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "86e10421-845b-48bb-bf1e-23bb4ac08cd5",
    "url": "https://discuss.elastic.co/t/using-apm-with-latest-apollo-server-and-apollo-server-express-results-into-unknown-routes/209080",
    "title": "Using APM with latest Apollo Server and Apollo Server Express results into \"Unknown routes\"",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 25, 2019, 9:53am December 11, 2019, 11:14am January 1, 2020, 7:15am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4dc7eafa-01c3-47e6-b94c-1cd83e1c2fd9",
    "url": "https://discuss.elastic.co/t/after-updating-apm-agent-from-1-1-2-to-1-2-transactions-are-empty/211081",
    "title": "After updating Apm Agent from 1.1.2 to 1.2 transactions are empty",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 9, 2019, 9:32am December 9, 2019, 9:54am December 9, 2019, 10:01am December 9, 2019, 11:24am December 9, 2019, 11:59am December 9, 2019, 5:55pm December 10, 2019, 6:57pm December 11, 2019, 4:36am December 11, 2019, 11:34am January 1, 2020, 6:52am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b77ad20a-f946-4097-aeb7-5b4f91baad18",
    "url": "https://discuss.elastic.co/t/using-apm-to-monitor-a-java-application/211412",
    "title": "Using APM to monitor a JAVA application",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 11, 2019, 5:24am December 11, 2019, 8:27am January 1, 2020, 4:29am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e57635af-88ca-448c-b262-da4f755bc30f",
    "url": "https://discuss.elastic.co/t/custom-transaction-naming/211317",
    "title": "Custom Transaction Naming",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 10, 2019, 2:30pm December 10, 2019, 4:49pm December 31, 2019, 12:48pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b3015b5c-fd21-4049-b966-d92437f90b08",
    "url": "https://discuss.elastic.co/t/cannot-make-apm-client-work-on-iis/211179",
    "title": "Cannot make APM Client work on IIS",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 9, 2019, 7:41pm December 10, 2019, 1:47pm December 10, 2019, 1:51pm December 31, 2019, 9:51am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "fb42f6e7-d370-4326-b434-1f4aa04ff511",
    "url": "https://discuss.elastic.co/t/apm-jms-tibco/211185",
    "title": "APM JMS -Tibco",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 10, 2019, 7:14am December 10, 2019, 10:20am December 10, 2019, 10:39am December 10, 2019, 12:42pm December 31, 2019, 8:42am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4f405051-a333-4430-9ad3-b246aa0a13aa",
    "url": "https://discuss.elastic.co/t/working-matrix-of-apm/211078",
    "title": "Working matrix of APM",
    "category": [
      "APM"
    ],
    "author": "srinarayanant",
    "date": "December 9, 2019, 9:21am December 30, 2019, 5:21am",
    "body": "<dependency> <groupId>io.opentracing.contrib</groupId> <artifactId>opentracing-spring-cloud-starter</artifactId> <version>0.2.3</version> </dependency> <dependency> <groupId>co.elastic.apm</groupId> <artifactId>apm-opentracing</artifactId> <version>1.8.0</version> </dependency> <dependency> <groupId>io.opentracing</groupId> <artifactId>opentracing-api</artifactId> <version>0.31.0</version> </dependency> <dependency> <groupId>io.opentracing</groupId> <artifactId>opentracing-util</artifactId> <version>0.31.0</version> </dependency>",
    "website_area": "discuss"
  },
  {
    "id": "59ebb246-9f2c-49e0-902a-a739ab09a490",
    "url": "https://discuss.elastic.co/t/instrumenting-react-component-using-withtransaction-doesnt-work/210437",
    "title": "Instrumenting React component using withTransaction doesn't work",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 3, 2019, 11:31pm December 4, 2019, 7:35pm December 4, 2019, 7:59pm December 6, 2019, 9:45am December 6, 2019, 4:42pm December 27, 2019, 12:42pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9f638bc1-d427-4c5a-b93a-28fa6b6b3ff0",
    "url": "https://discuss.elastic.co/t/apm-for-spring-webflux/210637",
    "title": "APM for spring webflux",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 5, 2019, 8:31am December 5, 2019, 5:09pm December 6, 2019, 7:02am December 6, 2019, 1:21pm December 6, 2019, 2:38pm December 6, 2019, 3:18pm December 27, 2019, 11:19am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4db04d26-02bd-4909-bb1c-b96d2f897c4d",
    "url": "https://discuss.elastic.co/t/support-for-istio-service-mesh/210829",
    "title": "Support for Istio service mesh",
    "category": [
      "APM"
    ],
    "author": "surenraju",
    "date": "December 6, 2019, 6:18am December 6, 2019, 2:05pm December 27, 2019, 10:05am",
    "body": "Elastic Search Version: 7.3.1 APM Server Version: 7.3.1 APM Client Version: APM Java Agent: 1.x (current) APM Agent language and version: Java 1.8 Is there any plan to provide integration with Istio for distributed tracing, metrics etc? Datadog and Stackdriver provides integration with Istio by providing Mixer adapter. Istio Datadog Adapter to deliver metrics to a dogstatsd agent for delivery to DataDog. Istio Stackdriver Adapter to deliver logs, metrics, and traces to Stackdriver. Thanks, Suren",
    "website_area": "discuss"
  },
  {
    "id": "fdd1108c-c554-4bc5-bd76-cf957d80e489",
    "url": "https://discuss.elastic.co/t/how-to-add-label-in-java-apm-agent/210540",
    "title": "How to add label in Java apm agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 4, 2019, 1:34pm December 4, 2019, 1:40pm December 4, 2019, 2:11pm December 6, 2019, 7:03am December 6, 2019, 1:36pm December 27, 2019, 9:36am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "51e8a59e-ec70-481c-99a5-6e59df061091",
    "url": "https://discuss.elastic.co/t/how-to-send-apm-agent-data-to-two-different-apm-servers/210602",
    "title": "How to send apm agent data to two different apm-servers",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 5, 2019, 8:33am December 5, 2019, 9:05am December 5, 2019, 5:54pm December 6, 2019, 1:30pm December 27, 2019, 9:30am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f7b72142-2c73-4c01-9c21-3e23519f35c7",
    "url": "https://discuss.elastic.co/t/apm-config-overriding/207854",
    "title": "APM config overriding",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 14, 2019, 10:12am November 14, 2019, 10:43am November 14, 2019, 10:57am November 14, 2019, 12:18pm November 14, 2019, 4:12pm November 14, 2019, 4:23pm November 18, 2019, 11:11am December 4, 2019, 3:01pm December 5, 2019, 4:56pm December 26, 2019, 12:56pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "05fd48b4-5f38-4e68-a252-191e354bbbe5",
    "url": "https://discuss.elastic.co/t/aggregated-distributed-tracing-view/210559",
    "title": "Aggregated Distributed Tracing View?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 4, 2019, 3:27pm December 5, 2019, 2:59am December 5, 2019, 4:15pm December 26, 2019, 12:15pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b02cd222-124b-4c28-b19b-cc823154104b",
    "url": "https://discuss.elastic.co/t/after-enabling-x-pack-security-transactions-doesnt-appear-in-apm/210696",
    "title": "After Enabling X-Pack security,transactions doesn't appear in APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 5, 2019, 12:17pm December 5, 2019, 12:45pm December 5, 2019, 3:31pm December 26, 2019, 11:31am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1dd06a42-b067-4bed-8b87-46504292eb28",
    "url": "https://discuss.elastic.co/t/kibana-doesnt-show-apm-data/209996",
    "title": "Kibana doesn't show APM data",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 29, 2019, 7:05pm December 2, 2019, 7:53am December 4, 2019, 6:02pm December 25, 2019, 2:02pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "494c0622-16a5-44af-b714-6defa3221df4",
    "url": "https://discuss.elastic.co/t/not-see-sql-transactions/210202",
    "title": "Not see sql transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 2, 2019, 2:42pm December 2, 2019, 2:58pm December 2, 2019, 3:48pm December 3, 2019, 9:25am December 3, 2019, 9:50am December 3, 2019, 11:03am December 3, 2019, 11:10am December 3, 2019, 11:10am December 3, 2019, 11:12am December 3, 2019, 11:12am December 3, 2019, 11:27am December 3, 2019, 1:36pm December 3, 2019, 3:51pm December 4, 2019, 2:06pm December 25, 2019, 10:06am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "13518661-7be5-491d-ace3-224045f0347c",
    "url": "https://discuss.elastic.co/t/forward-errors-to-sentry/210386",
    "title": "Forward Errors to Sentry",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 3, 2019, 3:18pm December 3, 2019, 3:58pm December 24, 2019, 11:58am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "efe5b859-c19b-4dad-b107-687d62059d27",
    "url": "https://discuss.elastic.co/t/sourcemaps-dont-show-for-all-errors/209978",
    "title": "SourceMaps don't show for all Errors",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 29, 2019, 2:40pm December 2, 2019, 7:51am December 2, 2019, 12:19pm December 3, 2019, 8:00am December 3, 2019, 11:42am December 3, 2019, 1:23pm December 24, 2019, 9:23am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "868ed6ab-778d-4404-9306-cd0f89c58d62",
    "url": "https://discuss.elastic.co/t/elasticapm-integeration-py-did-not-see-the-place-where-we-can-point-apm-server-and-port-no/208302",
    "title": "Elasticapm_integeration.py, did not see the place where we can point APM server and port no",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 18, 2019, 11:50am November 18, 2019, 12:24pm November 19, 2019, 11:50am November 20, 2019, 11:49am November 25, 2019, 11:16am November 25, 2019, 2:45pm November 26, 2019, 3:39am December 2, 2019, 5:09pm December 23, 2019, 1:09pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "01048128-f700-45ea-83a3-f373d6d7b816",
    "url": "https://discuss.elastic.co/t/apm-tracer-doesnt-show-python-code-time/210063",
    "title": "APM Tracer doesn't show Python code time",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 1, 2019, 1:51pm December 1, 2019, 2:18pm December 1, 2019, 2:39pm December 1, 2019, 9:21pm December 22, 2019, 5:21pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a7e0c054-38b0-4cc3-92fd-587684447716",
    "url": "https://discuss.elastic.co/t/multiple-service-in-same-jvm/208569",
    "title": "Multiple \"service\" in same JVM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 19, 2019, 7:20pm November 21, 2019, 5:08am November 21, 2019, 8:56am November 28, 2019, 2:10pm December 19, 2019, 10:10am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a378bea6-c7c3-4fce-8a12-221488f229a0",
    "url": "https://discuss.elastic.co/t/custom-distributed-transactions-linked-with-instrumented-spans/209584",
    "title": "Custom Distributed transactions linked with instrumented spans",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 27, 2019, 9:37pm November 27, 2019, 9:30pm December 18, 2019, 5:30pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "aeb72eab-dba6-443c-a38c-55700cc5f597",
    "url": "https://discuss.elastic.co/t/distributed-tracing-question-about-how-to-implement/204734",
    "title": "Distributed Tracing question about how to implement",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 25, 2019, 6:22am October 23, 2019, 1:13am October 23, 2019, 3:36pm October 23, 2019, 8:15pm October 24, 2019, 5:02pm November 7, 2019, 2:57pm November 7, 2019, 2:57pm November 27, 2019, 9:08pm December 18, 2019, 5:08pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "391cf5ca-a60c-44c8-85e4-2368fd05c1df",
    "url": "https://discuss.elastic.co/t/number-of-transactions-per-minute-reported-by-apm-is-significantly-different-than-request-count-in-monitoring-possible-reasons-for-the-discrepancy/209367",
    "title": "Number of transactions per minute reported by APM is significantly different than Request Count in monitoring. Possible reasons for the discrepancy?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 26, 2019, 8:23am November 26, 2019, 10:09am November 26, 2019, 1:50pm November 26, 2019, 4:34pm November 26, 2019, 4:34pm November 27, 2019, 10:53am December 18, 2019, 6:53am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a86068c4-d4b2-4e8f-947d-069f9451234b",
    "url": "https://discuss.elastic.co/t/elasticsearch-aggregation-request-for-apm-trace-data/208328",
    "title": "Elasticsearch aggregation request for APM trace data",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 26, 2019, 5:25pm December 9, 2019, 10:18am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "97b2748e-8e5a-405e-9f0d-e0a349e5d383",
    "url": "https://discuss.elastic.co/t/how-solve-get-unknown-route-from-apm-java/209364",
    "title": "How solve \"GET unknown route\" from APM Java?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 25, 2019, 7:33pm November 26, 2019, 11:17am December 17, 2019, 7:29am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "09869d4d-badf-42f7-b760-3b271e82c50f",
    "url": "https://discuss.elastic.co/t/python-agent-how-to-track-various-counters-values-evolution-over-time/209176",
    "title": "Python Agent how to track various counters/values evolution over time?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 24, 2019, 12:04pm November 25, 2019, 1:41am November 25, 2019, 9:41pm November 26, 2019, 3:02am November 26, 2019, 10:05am December 17, 2019, 6:05am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ca01f082-649e-49b6-b909-d72ec1b8478b",
    "url": "https://discuss.elastic.co/t/span-log-with-opentracing/209366",
    "title": "Span.log with opentracing",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 26, 2019, 8:42am November 26, 2019, 2:03am December 16, 2019, 10:04pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9d86c4be-be17-4d43-a3eb-c200475dcf0d",
    "url": "https://discuss.elastic.co/t/error-publishing-app-with-dotnet-fullframework-apm/209303",
    "title": "Error publishing app with dotnet fullframework APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 25, 2019, 12:45pm November 25, 2019, 1:51pm November 25, 2019, 1:28pm December 16, 2019, 9:30am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "48afa79d-b237-4bca-aa32-693282e44372",
    "url": "https://discuss.elastic.co/t/sql-spans-with-db2-jdbc-4/207319",
    "title": "SQL Spans with DB2 (JDBC 4)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 11, 2019, 9:54am November 12, 2019, 10:12am November 25, 2019, 12:57pm December 16, 2019, 8:57am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "793c4c56-05b6-4e84-b6e4-db6a642f7f61",
    "url": "https://discuss.elastic.co/t/monitoring-docker-application/209196",
    "title": "Monitoring Docker application",
    "category": [
      "APM"
    ],
    "author": "gopikrishnan",
    "date": "November 24, 2019, 7:17pm November 25, 2019, 1:16am December 15, 2019, 9:16pm",
    "body": "Hi Team, Kindly help me to understand whether elastic APM is capable of monitoring Docker application running on Linux machine and if yes then how to instrument it. Thanks, Gopikrishnan",
    "website_area": "discuss"
  },
  {
    "id": "35323d58-4c03-4125-b9cd-4d49b8656555",
    "url": "https://discuss.elastic.co/t/apm-agent-cannot-connect-to-remote-apm-server/209117",
    "title": "Apm agent cannot connect to remote apm server",
    "category": [
      "APM"
    ],
    "author": "he11oworld",
    "date": "November 22, 2019, 7:39pm November 24, 2019, 7:49am December 15, 2019, 3:49am",
    "body": "Kibana version: 7.4.0 Elasticsearch version: 7.4.0 APM Server version: 7.4.0 APM Agent language and version: Java, 1.11.0 Is there anything special in your setup? For example, are you using the Logstash or Kafka outputs? Are you using a load balancer in front of the APM Servers? Have you changed index pattern, generated custom templates, changed agent configuration etc. my apm-server is on Kubernetes, and has nginx-proxy in front. Description of the problem including expected versus actual behavior. Please include screenshots (if relevant): The elastic-apm-agent and the java application reside on my local system, I am able to send logs when I run kubectl port forward apm-server 8200 but I get 502 status when I specify nginx-proxy loadbalancer url in elasticapm.properties file. Provide logs and/or server output (if relevant): my elasticapm.properties file looks like this: service_name=demo application_packages=ca.com.elastic server_urls=https://172.20.11.188 verify_server_cert=false log_level=debug apm-server.yaml file: apm-server.yml: | host: \"0.0.0.0:8200\" setup.template.settings: index: number_of_shards: 1 codec: best_compression setup.kibana: host: \"< kibana endpoint >\" output.elasticsearch: hosts: [\"elasticsearch:9200\"] logs from elastic-apm-agent: 2019-11-22 11:25:03.188 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Starting new request to https://172.20.11.188/intake/v2/events 2019-11-22 11:25:03.224 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Scheduling request timeout in 10s 2019-11-22 11:25:13.225 [apm-request-timeout-timer] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Request flush because the request timeout occurred 2019-11-22 11:25:13.235 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Receiving FLUSH event (sequence 1) 2019-11-22 11:25:13.238 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Flushing 1386 uncompressed 601 compressed bytes 2019-11-22 11:25:13.240 [apm-reporter] INFO co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Backing off for 0 seconds (+/-10%) 2019-11-22 11:25:13.240 [apm-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Error sending data to APM server: Server returned HTTP response code: 502 for URL: https://172.20.11.188/intake/v2/events, response code is 502 2019-11-22 11:25:13.240 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Sending payload to APM server failed java.io.IOException: Server returned HTTP response code: 502 for URL: https://172.20.11.188/intake/v2/events at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1894) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492) at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:263) at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.flush(IntakeV2ReportingEventHandler.java:224) at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.handleEvent(IntakeV2ReportingEventHandler.java:128) at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:116) at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:50) at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) at java.lang.Thread.run(Thread.java:748) 2019-11-22 11:25:13.243 [apm-reporter] WARN co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - 502 Bad Gateway 502 Bad Gateway nginx/1.13.12 Logs from nginx proxy pod: 2019/11/22 19:24:35 [error] 10#10: *309 connect() failed (111: Connection refused) while connecting to upstream, client: 10.20.6.1, server: , request: \"POST /config/v1/agents HTTP/1.1\", upstream: \"http://10.19.72.106:8200/config/v1/agents\", host: \"172.20.11.188\" 10.20.6.1 - - [22/Nov/2019:19:24:35 +0000] \"POST /config/v1/agents HTTP/1.1\" 502 174 \"-\" \"elasticapm-java/1.11.0\" \"-\" 2019/11/22 19:24:44 [info] 10#10: *309 client 10.20.6.1 closed keepalive connection 2019/11/22 19:25:43 [error] 10#10: *311 connect() failed (111: Connection refused) while connecting to upstream, client: 10.20.6.1, server: , request: \"POST /intake/v2/events HTTP/1.1\", upstream: \"http://10.19.72.106:8200/intake/v2/events\", host: \"172.20.11.188\" 10.20.6.1 - - [22/Nov/2019:19:25:43 +0000] \"POST /intake/v2/events HTTP/1.1\" 502 174 \"-\" \"elasticapm-java/1.11.0\" \"-\" 2019/11/22 19:25:49 [info] 10#10: *311 client 10.20.6.1 closed keepalive connection 2019/11/22 19:26:13 [error] 10#10: *313 connect() failed (111: Connection refused) while connecting to upstream, client: 10.20.6.1, server: , request: \"POST /intake/v2/events HTTP/1.1\", upstream: \"http://10.19.72.106:8200/intake/v2/events\", host: \"172.20.11.188\" 10.20.6.1 - - [22/Nov/2019:19:26:13 +0000] \"POST /intake/v2/events HTTP/1.1\" 502 174 \"-\" \"elasticapm-java/1.11.0\" \"-\" 2019/11/22 19:26:26 [info] 10#10: *313 client 10.20.6.1 closed keepalive connection 10.20.6.1 - - [22/Nov/2019:19:27:13 +0000] \"POST /intake/v2/events HTTP/1.1\" 502 174 \"-\" \"elasticapm-java/1.11.0\" \"-\" 2019/11/22 19:27:13 [error] 10#10: *315 connect() failed (111: Connection refused) while connecting to upstream, client: 10.20.6.1, server: , request: \"POST /intake/v2/events HTTP/1.1\", upstream: \"http://10.19.72.106:8200/intake/v2/events\", host: \"172.20.11.188\" 2019/11/22 19:27:33 [info] 10#10: *315 client 10.20.6.1 closed keepalive connection 2019/11/22 19:27:43 [error] 10#10: *317 connect() failed (111: Connection refused) while connecting to upstream, client: 10.20.6.1, server: , request: \"POST /intake/v2/events HTTP/1.1\", upstream: \"http://10.19.72.106:8200/intake/v2/events\", host: \"172.20.11.188\" Any ideas?",
    "website_area": "discuss"
  },
  {
    "id": "ea814245-d09d-451e-92fc-3749a5f1c19e",
    "url": "https://discuss.elastic.co/t/apm-server-logs/208998",
    "title": "Apm-server logs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 22, 2019, 3:03am November 22, 2019, 11:39am November 22, 2019, 4:56pm November 22, 2019, 11:01pm December 13, 2019, 7:01pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "797975f2-0dc1-414a-ac06-e6cc62a85838",
    "url": "https://discuss.elastic.co/t/apm-dashboard-transaction-overview-bug/208899",
    "title": "APM dashboard transaction overview bug?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 22, 2019, 7:28pm November 25, 2019, 3:55pm December 20, 2019, 7:58pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5143dbeb-5183-4de1-865e-5e5a38326f62",
    "url": "https://discuss.elastic.co/t/nodejs-apm-does-not-report-postgres-queries/208984",
    "title": "NodeJS APM does not report postgres queries",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 21, 2019, 10:05pm November 22, 2019, 3:54am November 22, 2019, 3:54am November 22, 2019, 5:19am December 13, 2019, 1:23am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a804857f-6228-4cca-a5fe-99a7832b0f98",
    "url": "https://discuss.elastic.co/t/span-start-using-withstarttimestamp-long-micro/208992",
    "title": "Span start using withStartTimestamp(Long micro)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 22, 2019, 6:19am November 22, 2019, 4:43am December 12, 2019, 9:56pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5c486214-f6f9-4128-8d2c-550c1f353807",
    "url": "https://discuss.elastic.co/t/creating-spans-using-elastic-apm-bridge/208614",
    "title": "Creating spans using Elastic APM bridge",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 20, 2019, 3:48pm November 20, 2019, 3:41pm November 20, 2019, 5:11pm November 21, 2019, 8:24pm December 12, 2019, 4:22am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "64d11c64-e587-4550-acdf-d80e19e55f4f",
    "url": "https://discuss.elastic.co/t/can-apm-monitor-what-user-actions-in-the-web-page/205516",
    "title": "Can APM monitor what user actions in the web page?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 21, 2019, 12:44am October 30, 2019, 6:33pm October 30, 2019, 7:03pm October 31, 2019, 2:24am November 14, 2019, 5:47pm November 14, 2019, 5:59pm November 19, 2019, 9:16am November 21, 2019, 12:43am November 21, 2019, 8:09am November 21, 2019, 10:20pm November 21, 2019, 10:37pm December 12, 2019, 6:37pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "837bf8da-8744-4d93-bb2e-eb514bcd8399",
    "url": "https://discuss.elastic.co/t/kinaba-apm-error-occurrences-is-inaccurate/208770",
    "title": "Kinaba APM error occurrences is inaccurate",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 20, 2019, 9:20pm November 21, 2019, 5:15am December 12, 2019, 1:15am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c1e206b0-2972-4d9a-a708-cb29d5cc84b7",
    "url": "https://discuss.elastic.co/t/elastic-rum-js-agent-not-tracking-client-ip-address/208597",
    "title": "Elastic RUM JS agent not tracking client IP address",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 20, 2019, 2:15am November 20, 2019, 10:03am November 20, 2019, 9:22pm December 11, 2019, 5:22pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "643fe78b-5f82-48c0-ab42-024ba516a036",
    "url": "https://discuss.elastic.co/t/disableinstrumentations-for-route-change/208598",
    "title": "disableInstrumentations for route-change",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 20, 2019, 2:18am November 20, 2019, 9:03am November 20, 2019, 5:22pm December 11, 2019, 1:28pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "402d60b2-8f30-43cd-aaf5-808a8b469a19",
    "url": "https://discuss.elastic.co/t/apm-server-has-still-not-connected-to-elasticsearch-2-previous-was-closed/206491",
    "title": "APM Server has still not connected to Elasticsearch (2, previous was closed)",
    "category": [
      "APM"
    ],
    "author": "fraballi",
    "date": "November 5, 2019, 2:03pm November 6, 2019, 11:05am November 6, 2019, 1:48pm November 7, 2019, 7:16pm November 7, 2019, 7:33pm November 11, 2019, 9:07am November 12, 2019, 4:05pm November 18, 2019, 3:23pm November 19, 2019, 8:29am November 19, 2019, 8:38am November 19, 2019, 2:26pm November 20, 2019, 11:12am December 11, 2019, 7:12am",
    "body": "APM Server version: APM-Server 7.4.1 Elasticsearch: 7.4.0 Kibana: 7.4.1 I'm having this error, yet I ran: docker exec -it elastic-apm-server bash -c \"./apm-server test config\" Config OK docker exec -it elastic-apm-server bash -c \"./apm-server test output\" elasticsearch: http://elasticsearch:9200... parse url... OK connection... parse host... OK dns lookup... OK addresses: 192.x.x.x dial up... OK TLS... WARN secure connection disabled talk to server... OK version: 7.4.0 But nothing happens. Even, the kibana container sees the apm-server one: docker exec -it kibana-ui bash -c \"curl -i http://elastic-apm-server:8200/\" HTTP/1.1 200 OK Content-Type: application/json X-Content-Type-Options: nosniff Date: Mon, 04 Nov 2019 22:01:52 GMT Content-Length: 124 { \"build_date\": \"2019-10-22T15:25:16Z\", \"build_sha\": \"26c6b9c55d4753495d8aa571a21b8a97f9693b4b\", \"version\": \"7.4.1\" } So, what's next? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "f6fb95c6-2504-4885-be75-569fd1dd8443",
    "url": "https://discuss.elastic.co/t/how-to-hot-load-agent/208411",
    "title": "How to hot load agent?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 19, 2019, 3:25am November 19, 2019, 6:06am November 19, 2019, 6:45am November 19, 2019, 8:22am November 19, 2019, 10:11am November 19, 2019, 11:46am November 20, 2019, 12:43am December 10, 2019, 8:43pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0148b660-076a-4cdb-adc1-5b42c82dc37b",
    "url": "https://discuss.elastic.co/t/apm-server-aws-elasticsearch-service-cannot-retrieve-the-elasticsearch-license-unauthorized-access-could-not-connect-to-the-xpack-endpoint-verify-your-credentials/207759",
    "title": "APM Server -> AWS Elasticsearch Service: \"cannot retrieve the elasticsearch license: unauthorized access, could not connect to the xpack endpoint, verify your credentials\"",
    "category": [
      "APM"
    ],
    "author": "hickeroar",
    "date": "November 13, 2019, 6:34pm November 13, 2019, 8:31pm November 13, 2019, 9:24pm November 13, 2019, 10:08pm November 13, 2019, 9:28pm November 25, 2019, 8:26pm November 25, 2019, 8:26pm December 10, 2019, 12:22pm",
    "body": "Kibana version: 7.1.1 Elasticsearch version: 7.1.1 APM Server version: 7.1.1 Original install method (e.g. download page, yum, deb, from source, etc.) and version: DEB: curl -L -O https://artifacts.elastic.co/downloads/apm-server/apm-server-7.1.1-amd64.deb sudo dpkg -i apm-server-7.1.1-amd64.deb Fresh install or upgraded from other version? Fresh Is there anything special in your setup? It's AWS's elasticsearch service. apm-server is running on an ubuntu 18.04 instance. They're in the same VPC. Description of the problem including expected versus actual behavior. Please include screenshots (if relevant): I may have some misconceptions or issues with my setup, so please correct me if I'm wrong on any of this. The log output, trying to connect to my elasticsearch instance displays: Connection marked as failed because the onConnect callback failed: cannot retrieve the elasticsearch license: unauthorized access, could not connect to the xpack endpoint, verify your credentials It appears that AWS runs on port 443 instead of 9200: curl -XGET 'https://<snip>.us-east-1.es.amazonaws.com:443' Yields: \"name\" : \"<snip>\", \"cluster_name\" : \"<snip>\", \"cluster_uuid\" : \"<snip>\", \"version\" : { \"number\" : \"7.1.1\", \"build_flavor\" : \"oss\", \"build_type\" : \"tar\", \"build_hash\" : \"7a013de\", \"build_date\" : \"2019-09-05T07:25:23.525600Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.0.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Does anyone know what I may be doing wrong here?",
    "website_area": "discuss"
  },
  {
    "id": "7b1b27d7-a7a4-4203-bdad-4cc97eff1f63",
    "url": "https://discuss.elastic.co/t/net-core-span-apm/208104",
    "title": ".net Core Span APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 15, 2019, 5:02pm November 15, 2019, 5:06pm November 19, 2019, 11:57am November 19, 2019, 12:21pm December 10, 2019, 8:21am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4600af93-3176-4ce5-ba16-310628428412",
    "url": "https://discuss.elastic.co/t/rest-api-for-elastic-apm/208115",
    "title": "Rest API for elastic apm",
    "category": [
      "APM"
    ],
    "author": "gopikrishnan",
    "date": "November 15, 2019, 5:53pm November 16, 2019, 8:29am November 16, 2019, 9:32am November 18, 2019, 12:54am November 19, 2019, 6:40am December 10, 2019, 2:53am",
    "body": "Hi Team, Am new to elastic and trying to explore elastic apm and its capabilities..So is there a way to export data from elastic apm via rest api or something? Regards, Gopikrishnan",
    "website_area": "discuss"
  },
  {
    "id": "d6aab313-d9be-4713-8232-fa606236b395",
    "url": "https://discuss.elastic.co/t/apm-is-not-show-transaction/208033",
    "title": "APM is not show transaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 20, 2019, 5:36am November 18, 2019, 4:34am December 9, 2019, 12:34am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "925a3061-eb07-4751-9212-ad50091f6459",
    "url": "https://discuss.elastic.co/t/apm-transactions-from-one-specific-service-arent-shipping/206853",
    "title": "APM transactions from one specific service aren't shipping",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 6, 2019, 8:13pm November 7, 2019, 11:28am November 8, 2019, 8:07pm November 12, 2019, 2:57am November 15, 2019, 4:49pm November 18, 2019, 1:06am December 8, 2019, 9:07pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "28c737a3-9ab4-4bd9-a453-f921bdda4c6b",
    "url": "https://discuss.elastic.co/t/mongoose-spans-are-missing-when-i-add-custom-spans-i-can-see-also-the-mongo-spans/205744",
    "title": "Mongoose spans are missing(When i add custom spans i can see also the mongo spans)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 29, 2019, 8:05pm October 30, 2019, 6:59pm October 30, 2019, 9:04pm October 30, 2019, 10:35pm October 31, 2019, 7:09am November 14, 2019, 9:59am November 14, 2019, 11:07am November 16, 2019, 8:07pm December 7, 2019, 4:07pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "70992451-2362-4467-a15d-f14c4468f193",
    "url": "https://discuss.elastic.co/t/how-to-trace-call-from-one-grpc-service-to-the-other-grpc-service/207667",
    "title": "How to trace call from one Grpc service to the other Grpc service?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 13, 2019, 9:44am November 15, 2019, 2:08am November 14, 2019, 2:42am November 15, 2019, 2:08am November 15, 2019, 1:10pm December 6, 2019, 9:11am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6c075fa1-6a5d-4d96-8553-eb64068d9eab",
    "url": "https://discuss.elastic.co/t/how-to-instrument-a-different-xmlhttprequest-object/207662",
    "title": "How to instrument a different XmlHttpRequest Object",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 13, 2019, 9:11am November 15, 2019, 10:14am November 15, 2019, 11:04am December 6, 2019, 7:04am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "26f94fcf-3529-40a9-8c47-c3a80cd93f6a",
    "url": "https://discuss.elastic.co/t/apm-transaction-and-error-json/206716",
    "title": "APM Transaction and Error JSON",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 6, 2019, 3:33pm November 6, 2019, 11:08am November 6, 2019, 11:32am November 12, 2019, 10:27am November 15, 2019, 8:43am December 6, 2019, 4:43am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d11f65a1-f012-4608-bcce-0927bd5df867",
    "url": "https://discuss.elastic.co/t/apm-self-instrumentation-outputs/207765",
    "title": "APM Self Instrumentation outputs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 13, 2019, 7:11pm November 14, 2019, 1:55am November 14, 2019, 5:18pm November 15, 2019, 1:19am December 5, 2019, 9:19pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e182222c-56c2-4ced-870f-c214673f72c1",
    "url": "https://discuss.elastic.co/t/python-rq/206045",
    "title": "Python RQ",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 31, 2019, 1:33pm November 11, 2019, 10:59pm November 14, 2019, 8:48am November 14, 2019, 5:02pm November 14, 2019, 6:18pm December 5, 2019, 2:18pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f6e1b4c4-71a0-415a-8337-a313221ab3fc",
    "url": "https://discuss.elastic.co/t/progress-on-apm-for-java-8-based-erp/207699",
    "title": "Progress on APM for Java 8 based ERP",
    "category": [
      "APM"
    ],
    "author": "Joseph_John",
    "date": "November 13, 2019, 12:28pm November 13, 2019, 1:16pm November 14, 2019, 2:47am November 14, 2019, 3:38am December 4, 2019, 11:38pm",
    "body": "Continuing the discussion from Is APM perfromance like this because of Java version 1.7 at the Application side: Hi All, A small update I was able to setup APM for my Adempiere which is using Java 8, in the APM I am able to get results, but there is no information on the sections of transaction and error, I am getting information on the metrics side. I am wondering why I am not able to get anything on the transaction side, may be there is no transactions on the Adempiere ERP side, the instance is only for testing so I think there was no transaction happening when I tested and so I did not get the detail on transaction and on the error My JAVA_OPT options is export JAVA_OPTS=\"${JAVA_OPTS} -Delastic.apm.application_packages=org.compiere,org.adempiere\" and my screen shot of the metric side I am attaching Tomorrow I will try to do some transactions and check if I am getting results in transaction and error menus APM-heap.png1767826 55.8 KB This post is only an update on the progress, just sharing my progress Thanks Joseph John Blockquote",
    "website_area": "discuss"
  },
  {
    "id": "140571de-2752-4704-baa9-5299e50b65ae",
    "url": "https://discuss.elastic.co/t/startup-error-already-been-instantiated-using-full-framework-elastic-apm-agent/207707",
    "title": "Startup error (\"already been instantiated\") using Full Framework Elastic APM Agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 13, 2019, 12:44pm November 13, 2019, 1:40pm December 4, 2019, 9:40am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1acb3655-7419-41e1-b524-6bcc148cf54e",
    "url": "https://discuss.elastic.co/t/instrument-any-net-core-application/207680",
    "title": "Instrument any .NET Core application",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 13, 2019, 10:26am November 13, 2019, 10:40am November 13, 2019, 11:16am November 13, 2019, 11:25am November 13, 2019, 11:39am November 13, 2019, 11:43am November 13, 2019, 11:47am November 13, 2019, 12:53pm November 13, 2019, 12:55pm December 4, 2019, 8:59am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6ca6a455-0114-4625-b212-1f832b169b0d",
    "url": "https://discuss.elastic.co/t/how-traceid-combines-with-kibana-discover/207429",
    "title": "How traceid combines with kibana discover",
    "category": [
      "APM"
    ],
    "author": "wajika",
    "date": "November 12, 2019, 1:58am November 13, 2019, 7:17am November 13, 2019, 7:17am December 4, 2019, 3:17am",
    "body": "Now that we can query logs on discover, why can't we associate traceid with logs? For example, when I find an exception under APM, I can copy traceid to kibana discover to query the detailed log of the exception. _20191112095818.png1078379 41.2 KB",
    "website_area": "discuss"
  },
  {
    "id": "601ac0ce-4082-49c6-9b32-f4c09ff1a8f7",
    "url": "https://discuss.elastic.co/t/apm-cant-connect-to-elasticsearch/207578",
    "title": "APM Can't connect to ElasticSearch",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 12, 2019, 7:39pm November 12, 2019, 9:41pm November 12, 2019, 9:41pm December 3, 2019, 5:41pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ba7aba20-f690-4b53-9dba-a164990c0753",
    "url": "https://discuss.elastic.co/t/how-to-trace-non-web-transactions/207052",
    "title": "How to trace non web transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 11, 2019, 9:21am November 8, 2019, 11:07am November 11, 2019, 6:47am November 11, 2019, 7:02am November 11, 2019, 1:01pm November 12, 2019, 11:25am December 3, 2019, 7:25am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0a2fe73a-6aac-435e-ad43-466330923a76",
    "url": "https://discuss.elastic.co/t/how-to-filter-certain-span-in-a-transaction/206042",
    "title": "How to filter certain span in a transaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 31, 2019, 1:14pm November 12, 2019, 8:52am December 3, 2019, 4:52am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1cac683f-6cd6-4611-bc62-31a3cdd6495e",
    "url": "https://discuss.elastic.co/t/trying-to-push-os-details-to-apm-server-using-python-module/205374",
    "title": "Trying to push OS details to APM server using python module",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 27, 2019, 4:26am October 27, 2019, 8:51am October 27, 2019, 9:29am November 11, 2019, 11:13pm November 12, 2019, 2:29am December 2, 2019, 10:29pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b1c8d046-8f00-40cd-99a0-2b5eaf103af6",
    "url": "https://discuss.elastic.co/t/apm-not-showing-data-in-kibana-when-running-app-through-gunicorn-wsgi/206372",
    "title": "APM not showing data in Kibana when running app through gunicorn wsgi",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 4, 2019, 10:40am November 6, 2019, 6:23am November 11, 2019, 11:16pm December 2, 2019, 7:16pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c4c022c1-4f2d-4de1-95cf-bd58ad349645",
    "url": "https://discuss.elastic.co/t/why-timeout-of-agent-load-balanced-and-apm-server-should-be-incremental/206247",
    "title": "Why timeout of agent, load balanced and apm server should be incremental?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 11, 2019, 9:28am November 11, 2019, 11:07pm December 2, 2019, 7:07pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "dcd9d829-0203-4246-b34b-ad20f9969ef2",
    "url": "https://discuss.elastic.co/t/apm-an-internal-server-error-occurred/205526",
    "title": "APM An internal server error occurred",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 28, 2019, 5:49pm October 29, 2019, 9:09am October 29, 2019, 9:40pm October 30, 2019, 8:38am October 31, 2019, 9:23am October 31, 2019, 10:53am October 31, 2019, 11:13pm November 11, 2019, 7:29pm November 11, 2019, 7:31pm December 2, 2019, 3:31pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8ac536c3-ed2f-40bf-8764-db211230e4d8",
    "url": "https://discuss.elastic.co/t/apm-server-agent-deployment-questions/207128",
    "title": "APM Server/Agent Deployment Questions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 8, 2019, 2:26pm November 9, 2019, 1:52pm November 11, 2019, 3:34pm November 11, 2019, 5:23pm December 2, 2019, 1:23pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7cf1b7fc-bbc9-452c-b781-4441c4f9ec9d",
    "url": "https://discuss.elastic.co/t/browser-was-not-responsive-enough-during-the-transaction/206400",
    "title": "\"Browser was not responsive enough during the transaction\"",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 4, 2019, 12:31pm November 11, 2019, 12:58pm December 2, 2019, 8:58am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "22f3829b-a3d1-4370-83ea-a51385db284e",
    "url": "https://discuss.elastic.co/t/errors-not-getting-labels-from-spans/207334",
    "title": "Errors not getting labels from spans",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 11, 2019, 11:00am November 11, 2019, 11:34am November 11, 2019, 11:34am December 2, 2019, 7:34am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "463f1fc3-2ea2-48c0-bea5-05f45c544d30",
    "url": "https://discuss.elastic.co/t/infrastructure-ui-kubernetes-apm-traces-integration/207096",
    "title": "Infrastructure UI (kubernetes) - APM Traces Integration",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 8, 2019, 10:41pm November 8, 2019, 10:41pm November 9, 2019, 1:48am November 11, 2019, 7:19am November 11, 2019, 7:45am November 11, 2019, 9:05am November 11, 2019, 10:15am December 2, 2019, 6:15am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a0a3c54d-b3a7-42bf-9ef4-e7e16821f2c3",
    "url": "https://discuss.elastic.co/t/im-new-to-elk-i-have-a-doubt-on-apm/206898",
    "title": "I'm new to ELK, I have a doubt on APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 7, 2019, 9:38am November 7, 2019, 4:37pm November 8, 2019, 6:42am November 9, 2019, 6:12am November 20, 2019, 1:35am November 11, 2019, 8:57am November 20, 2019, 1:35am November 11, 2019, 9:35am December 2, 2019, 5:35am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f043a4cf-3314-4d41-8684-ad4195aee3e4",
    "url": "https://discuss.elastic.co/t/performance-tuning-help/207121",
    "title": "Performance tuning help",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 11, 2019, 9:18am November 10, 2019, 11:09pm December 1, 2019, 7:09pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f135dc0f-99b3-4d58-a40d-5d3e12344708",
    "url": "https://discuss.elastic.co/t/current-transaction-is-empty-aspnetfullframework/206812",
    "title": "Current transaction is empty - AspNetFullFramework",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 6, 2019, 2:57pm November 10, 2019, 11:54am December 1, 2019, 7:45am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "abf764f4-edf0-48f2-b37a-97283294bb65",
    "url": "https://discuss.elastic.co/t/apm-dotnet-1-1-2-error-validating-json-document-against-schema-i-s-doesnt-validate-with-metricset/207101",
    "title": "APM dotnet 1.1.2 error validating JSON document against schema: I[#] S[#] doesn't validate with \\\"metricset#\\",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 8, 2019, 12:00pm November 9, 2019, 2:54pm November 30, 2019, 10:54am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b4c65f6f-05ab-4cef-8fac-19f7ee95d553",
    "url": "https://discuss.elastic.co/t/apm-not-showing-data-in-kibana/206791",
    "title": "APM not showing data in Kibana",
    "category": [
      "APM"
    ],
    "author": "ceriania",
    "date": "November 6, 2019, 12:40pm November 6, 2019, 3:35pm November 27, 2019, 11:36am",
    "body": "I'm using APM Real User Monitoring JavaScript Agent Reference [4.x] with my React application. When I try to send error to APM Server with apm.captureError(new Error('...')); the POST request is correctly sended to the APM server and I receive a 202 response, but I don't see that error in Kibana.",
    "website_area": "discuss"
  },
  {
    "id": "4836e185-f8fc-407e-874b-ee7da34b8c80",
    "url": "https://discuss.elastic.co/t/custom-transactions-no-longer-detect-external-http-spans-automatically/202702",
    "title": "Custom Transactions no longer detect external.http spans automatically",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 8, 2019, 4:45pm October 9, 2019, 8:37am October 10, 2019, 12:47am November 20, 2019, 1:41am October 29, 2019, 10:04pm October 30, 2019, 12:26am October 30, 2019, 9:13am November 5, 2019, 11:04pm November 26, 2019, 7:04pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "558112ad-3557-448e-912d-d02391c56480",
    "url": "https://discuss.elastic.co/t/how-to-set-a-custom-configuration-reader/206443",
    "title": "How to set a custom configuration reader?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 4, 2019, 4:54pm November 5, 2019, 7:31pm November 26, 2019, 10:03am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "222e66a1-ee23-40f1-9c94-924863b729f8",
    "url": "https://discuss.elastic.co/t/customize-apm-json/206339",
    "title": "Customize APM JSON",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 4, 2019, 7:18am November 4, 2019, 7:46am November 5, 2019, 10:30am November 26, 2019, 6:30am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "27be3ec2-e8c1-4f37-acfd-6806c2b6de00",
    "url": "https://discuss.elastic.co/t/no-data-has-been-received-from-agents-yet/206466",
    "title": "No data has been received from agents yet",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 5, 2019, 7:58pm November 4, 2019, 10:20pm November 25, 2019, 6:20pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5ee672f0-690d-46a6-a90c-50314bdb13bd",
    "url": "https://discuss.elastic.co/t/how-to-use-ecs-logging-java/206348",
    "title": "How to use \"ECS logging Java\"?",
    "category": [
      "APM"
    ],
    "author": "wajika",
    "date": "November 4, 2019, 7:55am November 4, 2019, 8:06am November 4, 2019, 8:30am November 4, 2019, 8:31am November 4, 2019, 9:08am November 25, 2019, 5:08am",
    "body": "I add log4j2-ecs-layout dependency to the application apm.png929834 30.5 KB log4j2.xml logapm.png886446 12.5 KB COMMAND:  java -javaagent:/root/elastic-apm-agent-1.11.0.jar -Delastic.apm.service_name=my-cool-service -Delastic.apm.enable_log_correlation=true -Delastic.apm.application_packages=org.example,org.another.example -Delastic.apm.server_urls=http://localhost:8200 -jar target/dj-development-system-0.0.3-RELEASE.jar I don't know how to reference \"ecsjsonserializer\" How can I solve this error? LOG  2019-11-04 02:24:03.536 [apm-server-healthcheck] INFO co.elastic.apm.agent.report.ApmServerHealthChecker - Elastic APM server is available: { \"build_date\": \"2019-09-27T06:58:27Z\", \"build_sha\": \"971d864356e4438bf4a799a1fa052cfd0ce680b4\", \"version\": \"7.4.0\"} 2019-11-04 02:24:03.635 [main] INFO co.elastic.apm.agent.configuration.StartupInfo - Starting Elastic APM 1.11.0 as my-cool-service on Java 11.0.5 (Oracle Corporation) Linux 3.10.0-862.el7.x86_64 2019-11-04 02:24:03.649 [apm-remote-config-poller] INFO co.elastic.apm.agent.configuration.ApmServerConfigurationSource - Received new configuration from APM Server: {transaction_sample_rate=1} Exception in thread \"main\" java.lang.reflect.InvocationTargetException at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) Caused by: java.lang.NoClassDefFoundError: co/elastic/logging/EcsJsonSerializer at co.elastic.logging.log4j2.EcsLayout.(EcsLayout.java:97) at co.elastic.logging.log4j2.EcsLayout.(EcsLayout.java:63) at co.elastic.logging.log4j2.EcsLayout$Builder.build(EcsLayout.java:408) at co.elastic.logging.log4j2.EcsLayout$Builder.build(EcsLayout.java:324) at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:122) at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:958) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:898) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:890) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:890) at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:513) at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:237) at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:249) at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:545) at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:617) at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:634) at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:229) at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153) at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45) at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194) at org.apache.commons.logging.LogFactory$Log4jLog.(LogFactory.java:199) at org.apache.commons.logging.LogFactory$Log4jDelegate.createLog(LogFactory.java:166) at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:109) at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:99) at org.springframework.boot.SpringApplication.(SpringApplication.java:189) at com.djcps.djfileserver.DjdevelopmentsysApplication.main(DjdevelopmentsysApplication.java:34) ... 8 more Caused by: java.lang.ClassNotFoundException: co.elastic.logging.EcsJsonSerializer at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588) at org.springframework.boot.loader.LaunchedURLClassLoader.loadClass(LaunchedURLClassLoader.java:93) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521) ... 33 more",
    "website_area": "discuss"
  },
  {
    "id": "74bb32fd-1456-440a-8109-9a9c3bbe771d",
    "url": "https://discuss.elastic.co/t/steps-to-reproduce/206163",
    "title": "Steps to reproduce",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 1, 2019, 3:23pm November 22, 2019, 8:44am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "39a3471f-9dff-45d1-afe2-437da0865256",
    "url": "https://discuss.elastic.co/t/apm-config-file-location/203887",
    "title": "APM config_file location",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 16, 2019, 4:52pm October 16, 2019, 5:15pm October 21, 2019, 7:04am October 31, 2019, 2:11pm October 31, 2019, 2:24pm November 21, 2019, 10:24am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0404646f-43d9-436c-8764-7636e89bb7e8",
    "url": "https://discuss.elastic.co/t/apms-metrics-tab-doesnt-show-when-using-kibana-spaces/205983",
    "title": "APM's metrics tab doesn't show when using Kibana spaces",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 31, 2019, 10:03am October 31, 2019, 9:37am October 31, 2019, 10:02am October 31, 2019, 12:12pm November 21, 2019, 8:12am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "29db9021-145e-42e5-b409-407816a7efb8",
    "url": "https://discuss.elastic.co/t/apm-on-grape-api-and-sidekiq/206016",
    "title": "APM On Grape API and Sidekiq",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 31, 2019, 10:07am October 31, 2019, 10:26am October 31, 2019, 10:45am October 31, 2019, 10:49am October 31, 2019, 11:26am November 21, 2019, 7:26am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0988b11e-1da8-4d00-b586-9f2dfd15a685",
    "url": "https://discuss.elastic.co/t/discuss-instrumentation-for-nuxtjs/203094",
    "title": "Discuss: Instrumentation for NuxtJs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 10, 2019, 6:33pm October 30, 2019, 7:01pm November 20, 2019, 3:01pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "fea225d3-158c-4b9b-9e30-de982d62feaa",
    "url": "https://discuss.elastic.co/t/child-spans-not-aligned-properly-with-parent/205743",
    "title": "Child spans not aligned properly with parent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 31, 2019, 7:37am October 31, 2019, 4:12am November 19, 2019, 11:54pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7963259e-f54d-4f19-b4a2-2be1527a472a",
    "url": "https://discuss.elastic.co/t/java-apm-agent-cannot-be-added-as-a-dependency/205762",
    "title": "Java APM agent cannot be added as a dependency",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 30, 2019, 9:12am October 30, 2019, 3:36am November 19, 2019, 11:36pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "bb5618fc-ca4b-48fb-8430-9111281251cf",
    "url": "https://discuss.elastic.co/t/keep-ilm-policy-on-upgrade/205279",
    "title": "Keep ILM policy on upgrade",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 28, 2019, 3:33am October 25, 2019, 2:46pm October 28, 2019, 7:43am October 29, 2019, 2:09pm November 19, 2019, 10:10am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "da2f9309-5bb9-4b72-bf78-6de5405e737c",
    "url": "https://discuss.elastic.co/t/apm-error-forbidden-request-endpoint-is-disabled/205399",
    "title": "APM error: forbidden request: endpoint is disabled",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 28, 2019, 8:49am October 28, 2019, 5:56pm October 28, 2019, 5:56pm November 18, 2019, 1:56pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "697bb4c9-9a0a-47f3-83ae-9d45748d085b",
    "url": "https://discuss.elastic.co/t/handle-span-drop-message-in-apm-java-agent/205414",
    "title": "Handle Span drop message in APM Java agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 28, 2019, 8:05am October 28, 2019, 8:41am October 28, 2019, 9:45am November 18, 2019, 5:45am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f9db8b85-34b8-4317-9b8e-2c9118a41a1e",
    "url": "https://discuss.elastic.co/t/couldnt-verify-apm-server-status/202511",
    "title": "Couldn't verify APM server status",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 7, 2019, 12:48pm October 7, 2019, 3:51pm October 8, 2019, 11:17am October 8, 2019, 12:27pm October 8, 2019, 6:03pm October 9, 2019, 9:50am October 18, 2019, 2:12pm October 22, 2019, 2:52pm October 25, 2019, 11:09am October 25, 2019, 12:15pm November 15, 2019, 8:15am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "57a9b4df-2615-4cb0-b143-22f5adbb657b",
    "url": "https://discuss.elastic.co/t/how-to-group-spans-metadata-and-transactions-together-for-visualization-part-2/204527",
    "title": "How to Group Spans, Metadata and Transactions Together for Visualization, Part 2",
    "category": [
      "APM"
    ],
    "author": "LordMathis",
    "date": "October 21, 2019, 5:43pm October 23, 2019, 2:55am October 24, 2019, 1:27pm October 25, 2019, 7:07am November 15, 2019, 3:07am",
    "body": "Kibana version: 7.3 Elasticsearch version: 7.3 APM Server version: 7.3 APM Agent language and version: Self created agent for Delphi 5 and above Hello, I have proposed this question before but the answer I got back didn't solve the issue in its entirety. Here is a link to the original discussion: How to Group Spans, Metadata and Transactions Together for Visualization At the time we had a work around and the problem wasn't that large but as we developed further, this has only grown in a bigger and bigger issue. Essentially the issue is that there is no legitimate link between Spans, Metadata, Metrics, Transactions, or Errors and therefore APM visualization in Kibana is completely useless. Lets take this example. I want to know which customer experienced an error in my calculator app. To do this I create a label in each span that is \"CustomerName\". In my visualization I filter by the CustomerName = Matt's Car Shop. This will give me all the spans that had the customer name Matt's Car Shop. Now I want to get a count of all the errors in each spans associated transaction. Kibana however has no way of linking the span and the transaction. This means I can't say \"how many errors did Matt's Car Shop get\". The only way to do this would be to search for all spans with the customer name \"Matt's Car Shop\", record all the transaction IDs of each span, then filter for exceptions with a particular transaction ID, for every ID I found. This means I can't make appropriate graphs or charts. The other option would be to add a label to the metadatas context but again, no way to link the metadata back to the transaction and then filter for exceptions by those IDs, in one search. My use case seems like something everyone would want to be able to know. How many errors did a particular customer receive in a given time period. I would like to be able to create a bar chart that shows each customer and how many errors they received but there is no option to do this. Can anyone help? Any work arounds that people are aware of? The answer in the original post was that I could create a new index but I don't know what this means.",
    "website_area": "discuss"
  },
  {
    "id": "6d50cacf-fe19-4a21-837d-f79e06990c3e",
    "url": "https://discuss.elastic.co/t/while-running-the-service-after-injecting-nodejs-agent-wrapper-my-metrics-are-seen-on-discover-tab-but-does-not-shows-on-apm-screen/204575",
    "title": "While running the service after injecting nodejs agent wrapper, my metrics are seen on Discover tab, but does not shows on APM screen",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 22, 2019, 5:57am October 23, 2019, 12:25am October 23, 2019, 4:03am October 24, 2019, 3:16am October 24, 2019, 9:29pm November 14, 2019, 5:29pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9b895259-ed97-4476-85e7-58849ad99245",
    "url": "https://discuss.elastic.co/t/look-at-more-than-one-sampled-trace/203920",
    "title": "Look at more than one sampled trace",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 21, 2019, 3:10pm October 17, 2019, 1:04am October 21, 2019, 9:00am October 21, 2019, 1:52pm October 21, 2019, 3:10pm October 22, 2019, 7:54pm October 24, 2019, 7:27pm November 14, 2019, 3:26pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0a033586-9090-43c5-bd02-1c07d9a51fbb",
    "url": "https://discuss.elastic.co/t/typeerror-cannot-read-property-filter-of-undefined-in-elastic-cloud-apm/205097",
    "title": "\"TypeError: Cannot read property 'filter' of undefined\" In elastic cloud APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 24, 2019, 2:53pm October 24, 2019, 4:18pm October 24, 2019, 4:39pm October 24, 2019, 7:02pm November 14, 2019, 3:02pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9c78381a-bd36-4a1e-a085-356b486a12d8",
    "url": "https://discuss.elastic.co/t/apm-from-file/204806",
    "title": "APM From file",
    "category": [
      "APM"
    ],
    "author": "cosmo",
    "date": "October 23, 2019, 7:35am October 23, 2019, 11:10am October 23, 2019, 12:50pm October 24, 2019, 11:39am November 14, 2019, 7:39am",
    "body": "Hello So I have a PHP app, but it doesn't have access to my internal elasticsearch cluster. I'm wondering how feasible it would be to do something like the following; Custom PHP agent in app writes events to file Custom agent (that does have access to elasticsearch/apm server) pulls in files from app, and pushes them to the apm server Clearly not recommended, but could something like this work? If not, does anyone have any suggestions on using APM with a web app that doesn't have access to the APM server?",
    "website_area": "discuss"
  },
  {
    "id": "265a2cf4-69ac-49ea-ad67-b30834ed0800",
    "url": "https://discuss.elastic.co/t/do-we-have-to-use-entity-framework-to-monitor-sql-on-the-c-agent/204958",
    "title": "Do we have to use entity framework to monitor SQL on the C# agent",
    "category": [
      "APM"
    ],
    "author": "LordMathis",
    "date": "October 23, 2019, 8:38pm October 24, 2019, 5:52am November 14, 2019, 1:45am",
    "body": "Kibana version: 7.3 Elasticsearch version: 7.3 APM Server version: 7.3 APM Agent language and version: C# The C# agent uses entity framework to monitor sql connections. All of our applications are currently using the SqlConnection object from .net. Is there parody between the C# agent and this object or do we need to use entity framework to monitor sql connections.",
    "website_area": "discuss"
  },
  {
    "id": "f499003c-ec10-4acf-9dce-800dd1a6fb88",
    "url": "https://discuss.elastic.co/t/apm-503-queue-is-full-server-sleeping-nothing-helps/203180",
    "title": "APM: 503 Queue is full, server sleeping, nothing helps",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 11, 2019, 10:26am October 13, 2019, 7:19am October 14, 2019, 8:20am October 14, 2019, 9:50am October 15, 2019, 8:08am October 21, 2019, 3:20pm October 21, 2019, 5:42pm October 23, 2019, 3:08pm October 23, 2019, 5:46pm October 23, 2019, 6:09pm November 13, 2019, 2:09pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "77eb4ec6-9f2b-4ba0-94a7-8d7b02b3df30",
    "url": "https://discuss.elastic.co/t/capture-all-traces-spans-and-custom-configuration-doubt/204362",
    "title": "Capture all traces & spans and custom configuration doubt",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 22, 2019, 6:56am October 21, 2019, 1:20am October 21, 2019, 9:53am October 22, 2019, 3:46am October 22, 2019, 10:48am October 22, 2019, 12:31pm October 23, 2019, 7:19am October 23, 2019, 7:52am November 13, 2019, 3:52am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "dfc44f21-b15a-4dbf-b726-27eb72b6c826",
    "url": "https://discuss.elastic.co/t/monitoring-grpc-servers-on-jvm-kotlin/204392",
    "title": "Monitoring GRPC servers on JVM (Kotlin)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 21, 2019, 8:34am October 21, 2019, 8:37am October 23, 2019, 5:52am October 23, 2019, 7:47am November 13, 2019, 3:47am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f5087baa-9b2a-46f3-acb4-3108277ba5f6",
    "url": "https://discuss.elastic.co/t/capture-method-arguments-and-return-value/204360",
    "title": "Capture method arguments and return value",
    "category": [
      "APM"
    ],
    "author": "gopikrishnan",
    "date": "October 20, 2019, 3:25pm October 21, 2019, 1:09am October 21, 2019, 10:03am October 22, 2019, 5:12am October 22, 2019, 9:54am October 22, 2019, 1:00pm November 12, 2019, 9:00am",
    "body": "Hi All, Am totally new to elastic apm as well as elastic search and am trying to evaluate elastic apm and hence subscribed to trail version of elastic cloud. I want to understand whether i can capture any method arguments or return value and if possible then how to do the same. Regards, Gopikrishnan",
    "website_area": "discuss"
  },
  {
    "id": "1d744c56-7a7d-4e38-b48e-12795cbf9bf1",
    "url": "https://discuss.elastic.co/t/elastic-apm-bt-details-not-capture/203644",
    "title": "Elastic APM BT details not capture",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 21, 2019, 9:07am October 16, 2019, 2:05am October 16, 2019, 4:53am October 18, 2019, 8:16am October 22, 2019, 6:13am November 12, 2019, 2:13am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "00e73384-228b-49ab-9326-8a61f444dc7b",
    "url": "https://discuss.elastic.co/t/improving-analysis-of-gaps-between-spans/204482",
    "title": "Improving analysis of gaps between spans",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 22, 2019, 6:22am October 21, 2019, 2:52pm October 21, 2019, 4:30pm November 11, 2019, 12:30pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "52bc8c14-e386-42bb-a765-f1fa20e0c587",
    "url": "https://discuss.elastic.co/t/reduce-the-amount-of-data-for-spans-and-transactions/204287",
    "title": "Reduce the amount of data for spans and transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 18, 2019, 10:16pm October 21, 2019, 2:47pm October 21, 2019, 7:55am October 21, 2019, 12:16pm November 11, 2019, 8:16am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "68de9492-c4ab-41f6-9c28-5e29235ec941",
    "url": "https://discuss.elastic.co/t/apm-java-agent-jmx-metrics-for-springboot-application-capture-jmx-metrics/202700",
    "title": "APM Java Agent JMX Metrics for Springboot application (capture_jmx_metrics)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 8, 2019, 4:21pm October 8, 2019, 5:19pm October 8, 2019, 7:41pm October 9, 2019, 3:15pm October 10, 2019, 2:38pm October 10, 2019, 4:39pm October 10, 2019, 5:40pm October 14, 2019, 12:20am October 14, 2019, 12:20am October 14, 2019, 12:20am October 14, 2019, 12:18am October 21, 2019, 9:21am November 11, 2019, 5:21am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5f017d59-1cf9-4347-8d27-325d4f2d14b9",
    "url": "https://discuss.elastic.co/t/no-traces-or-transactions-showing-up-in-kibana/203324",
    "title": "No Traces or Transactions showing up in Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 21, 2019, 9:18am October 17, 2019, 5:20am October 18, 2019, 12:29am October 21, 2019, 9:18am November 11, 2019, 5:18am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5f00b05e-f73e-40a5-bde9-088a759d7710",
    "url": "https://discuss.elastic.co/t/ember-js-integration-with-apm-agent/203796",
    "title": "Ember JS integration with APM agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 18, 2019, 10:13am October 21, 2019, 9:11am November 11, 2019, 5:18am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "eba611e6-bc11-4172-ad28-77703e60efe5",
    "url": "https://discuss.elastic.co/t/has-anyone-been-able-to-create-custom-transaction-using-the-new-angular-integration/202072",
    "title": "Has anyone been able to create custom transaction using the new angular integration?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 2, 2019, 11:51pm October 3, 2019, 10:47am October 3, 2019, 6:04pm October 3, 2019, 10:41pm October 21, 2019, 9:01am November 11, 2019, 5:03am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ece43913-3470-4e73-bdf0-1b7abdc2c162",
    "url": "https://discuss.elastic.co/t/need-to-change-default-time-range-for-apm-monitoring-in-kibana/202775",
    "title": "Need to change default time range for apm monitoring in kibana",
    "category": [
      "APM"
    ],
    "author": "Shashank_Sah",
    "date": "October 9, 2019, 4:02pm October 10, 2019, 1:01pm October 21, 2019, 8:43am October 21, 2019, 8:59am November 11, 2019, 5:03am",
    "body": "The default time range for apm monitoring in kibana is 24 hours and we want to change it to last 1 hour. I tried changing the 'Time picker defaults ' from kibana settings, but apm dashboard still had the same time range of last 24 hours. How can I configure this? Thanks in advance Kibana version: 7.1.0 Elasticsearch version: 7.1.0 APM Server version: 7.1.0",
    "website_area": "discuss"
  },
  {
    "id": "38c7d9a8-c355-4ea4-a85d-bccd8d3122a2",
    "url": "https://discuss.elastic.co/t/dissable-connection-to-apm-server-elastic-apm-disable-send/204124",
    "title": "Dissable connection to APM Server (ELASTIC_APM_DISABLE_SEND)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 17, 2019, 8:28pm October 21, 2019, 8:46am November 11, 2019, 4:47am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "96a7dca7-de79-4752-ba11-0010c8c0d47b",
    "url": "https://discuss.elastic.co/t/ie-11-support-apm-rumjs-issue/203990",
    "title": "IE 11 Support - APM Rumjs Issue",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 17, 2019, 7:18am October 17, 2019, 7:33am October 21, 2019, 6:57am November 11, 2019, 2:57am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "20818133-e9bc-4051-b6fc-8d1c9c9fc63e",
    "url": "https://discuss.elastic.co/t/nhibernate-supported-by-net-agent/204224",
    "title": "nHibernate supported by .NET agent?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 22, 2019, 6:13am October 18, 2019, 12:40pm October 18, 2019, 12:54pm October 18, 2019, 1:01pm November 8, 2019, 9:01am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1df09835-671e-44c6-b1c9-697845005674",
    "url": "https://discuss.elastic.co/t/apm-apmsql-pq-golang-does-not-support-parameterized-sql-statement/203931",
    "title": "[apm/apmsql/pq][GoLang] Does NOT support parameterized SQL statement",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 17, 2019, 4:51am October 17, 2019, 12:56am October 17, 2019, 2:13pm October 18, 2019, 10:28am November 8, 2019, 6:28am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "77c8fa86-ecd5-4897-9112-0f7dfcfd3d3b",
    "url": "https://discuss.elastic.co/t/filter-apm-metrics-with-logstash-in-elastic-cloud/204055",
    "title": "Filter APM metrics with Logstash in Elastic Cloud",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 17, 2019, 2:50pm October 18, 2019, 7:36am November 8, 2019, 3:37am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b0cd852b-8823-4de5-baaf-584a11e1d9ee",
    "url": "https://discuss.elastic.co/t/setting-start-timestamp-on-transaction-to-an-earlier-time-is-always-added-to-app-type/203615",
    "title": "Setting start timestamp on Transaction to an earlier time is always added to \"app\" type",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 15, 2019, 10:12am October 15, 2019, 5:34pm October 16, 2019, 9:05am October 16, 2019, 10:21am October 17, 2019, 9:24am October 17, 2019, 11:03am November 7, 2019, 7:03am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "46c7016c-b896-41da-8057-eaab738133bc",
    "url": "https://discuss.elastic.co/t/how-does-grouping-key-get-assigned/203959",
    "title": "How does grouping_key get assigned?",
    "category": [
      "APM"
    ],
    "author": "ntavendale",
    "date": "October 17, 2019, 3:24am October 17, 2019, 5:24am November 7, 2019, 1:24am",
    "body": "Have attempted to do my own assignment like so: {\"error\":{\"id\":\"4367DBC77C9A6AE7\",\"timestamp\":1571282318000000,\"trace_id\":\"70C8BA2D755B5AE570DBA0807F14A684\",\"transaction_id\":\"5E3768BB0EF5FBAC\",\"parent_id\":\"5E3768BB0EF5FBAC\",\"culprit\":\"TTransactionWithSpansTest.Load\",\"grouping_key\":\"403\",\"exception\":{\"code\":\"403\",\"message\":\"HTTP/1.1 403 Forbidden\",\"attributes\":null,\"stacktrace\":null,\"type\":\"EIdHTTPProtocolException\",\"handled\":true}}} and I get a 200 back. However when I look up the error in ES the grouping_key has been changed. And for some reason all my errors are being assigned the same grouping_key.",
    "website_area": "discuss"
  },
  {
    "id": "aab7dc9a-5556-4c4f-a611-9269f798c596",
    "url": "https://discuss.elastic.co/t/span-start-gives-abstraction-error-using-elasticapmtracer/203513",
    "title": "SPAN start gives abstraction error using ElasticApmTracer",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 15, 2019, 7:58am October 15, 2019, 9:10am October 15, 2019, 3:13pm October 15, 2019, 3:14pm October 15, 2019, 3:19pm October 15, 2019, 11:38pm October 16, 2019, 3:48am October 16, 2019, 3:51am October 16, 2019, 4:09am October 16, 2019, 4:16am October 16, 2019, 4:57am October 16, 2019, 7:14pm October 16, 2019, 3:13pm October 16, 2019, 7:14pm November 6, 2019, 3:14pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9f50aab2-e15c-4328-8d54-c3c050c2dc97",
    "url": "https://discuss.elastic.co/t/elasticsearch-rest-high-level-client-is-not-traced/203223",
    "title": "Elasticsearch Rest High Level Client is not traced",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 11, 2019, 11:21am October 15, 2019, 7:08am October 16, 2019, 4:44pm October 16, 2019, 5:04pm November 6, 2019, 1:04pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4e40c90a-7d7a-4c53-839d-83da09367d4e",
    "url": "https://discuss.elastic.co/t/how-to-exclude-signalr-requests-from-collected-transactions/203848",
    "title": "How to exclude SignalR requests from collected transactions?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 16, 2019, 1:26pm October 16, 2019, 1:52pm November 6, 2019, 9:52am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "792eee44-35ff-44a5-844f-266686ed6636",
    "url": "https://discuss.elastic.co/t/any-plans-for-extended-metrics-events/203348",
    "title": "Any plans for extended metrics / events?",
    "category": [
      "APM"
    ],
    "author": "ethrbunny",
    "date": "October 13, 2019, 3:04pm October 14, 2019, 3:06am October 14, 2019, 10:52pm October 15, 2019, 1:19am October 15, 2019, 1:40pm November 5, 2019, 9:40am",
    "body": "Curious if things like counters, stack traces, custom metrics, events, etc are anywhere on the APM roadmap.",
    "website_area": "discuss"
  },
  {
    "id": "15602483-275b-45f7-a21a-76a7d965c823",
    "url": "https://discuss.elastic.co/t/spring-boot-starter-for-api-setup-to-self-attach/203333",
    "title": "Spring boot starter for API setup to self-attach",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 13, 2019, 9:02am October 15, 2019, 7:27am October 15, 2019, 11:58am October 15, 2019, 1:05pm November 5, 2019, 9:05am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ff5f23c8-0c5b-414b-85aa-b332fc4f5ade",
    "url": "https://discuss.elastic.co/t/rum-starttransaction-not-writing-to-apm-server-in-javascript/203140",
    "title": "[RUM] StartTransaction not writing to apm server in javascript",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 15, 2019, 6:06am October 15, 2019, 10:29am November 5, 2019, 6:29am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "63abab58-594d-45ee-979e-54518a8a0158",
    "url": "https://discuss.elastic.co/t/shipping-java-apm-agent-logs/203505",
    "title": "Shipping java apm agent logs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 15, 2019, 7:39am October 15, 2019, 7:58am November 5, 2019, 3:58am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "75364ab4-8dd3-454e-b3ff-d8703a9b557a",
    "url": "https://discuss.elastic.co/t/does-capturespan-work-on-private-methods/203492",
    "title": "Does @CaptureSpan work on private methods?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 14, 2019, 4:43pm October 15, 2019, 7:35am November 5, 2019, 3:35am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "7c4b0f12-bc29-42c9-974a-3072e0d999d7",
    "url": "https://discuss.elastic.co/t/apm-java-solutions-best-practices-and-apm-test-guidelines/203207",
    "title": "[APM][JAVA] Solutions, best practices and APM test guidelines",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 11, 2019, 10:18am October 15, 2019, 6:43am November 5, 2019, 2:43am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0ad131d8-8044-4b5a-9ac9-be410bc8d311",
    "url": "https://discuss.elastic.co/t/apm-failed-to-publish-events-temporary-bulk-send-failure-queue-is-full-503-error/202884",
    "title": "APM Failed to publish events: temporary bulk send failure / Queue is full 503 error",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 10, 2019, 6:26am October 11, 2019, 7:17am October 11, 2019, 12:31pm October 14, 2019, 5:07am October 14, 2019, 8:49am October 14, 2019, 9:59am October 15, 2019, 8:09am October 14, 2019, 12:49pm November 4, 2019, 8:49am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1c6c09ce-c35f-4e79-b0e2-ac567c74a69c",
    "url": "https://discuss.elastic.co/t/apm-sidekiq-maxium-duration-for-trace/203092",
    "title": "APM Sidekiq Maxium duration for trace?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 15, 2019, 6:05am October 11, 2019, 10:46am October 11, 2019, 12:03pm October 11, 2019, 12:25pm October 14, 2019, 12:06pm November 4, 2019, 8:06am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "cc70155a-eb7d-4e79-959a-285e398f8336",
    "url": "https://discuss.elastic.co/t/monitoring-sap-netweaver/202793",
    "title": "Monitoring SAP Netweaver",
    "category": [
      "APM"
    ],
    "author": "RichardH",
    "date": "October 9, 2019, 8:42am October 11, 2019, 10:44am October 14, 2019, 6:14am November 4, 2019, 2:15am",
    "body": "Hi All, Has anyone tried using Elastic APM to monitor SAP Netweaver on SLES (as an alternative to using the Wily Introscope built into SAP Solman)?",
    "website_area": "discuss"
  },
  {
    "id": "02047e29-cfe6-462a-be14-b995bccc2b00",
    "url": "https://discuss.elastic.co/t/traefik-integration/203377",
    "title": "Traefik integration",
    "category": [
      "APM"
    ],
    "author": "trajano",
    "date": "October 14, 2019, 4:34am October 14, 2019, 5:01am October 14, 2019, 5:04am October 14, 2019, 5:08am October 14, 2019, 5:11am November 4, 2019, 1:16am",
    "body": "Traefik supports five tracing backends according to https://docs.traefik.io/observability/tracing/overview/ Jaeger Zipkin Datadog Instana Haystack I was wondering if there's a way of using any of those to integrate with APM? Like some sort of adapter.",
    "website_area": "discuss"
  },
  {
    "id": "da71b163-da52-4ca8-a8bc-e09caf78dba9",
    "url": "https://discuss.elastic.co/t/httpclient-calls-not-getting-captured/202605",
    "title": "HttpClient calls not getting captured",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 8, 2019, 1:31am October 11, 2019, 9:17am October 12, 2019, 10:57am October 13, 2019, 8:41pm November 3, 2019, 4:41pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "53bad3aa-0104-4efa-9bf2-06dcdae94698",
    "url": "https://discuss.elastic.co/t/apm-environment-variables-misconfigured/200841",
    "title": "APM environment variables misconfigured",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 24, 2019, 10:46am September 25, 2019, 6:05am September 25, 2019, 10:28am September 26, 2019, 2:26am October 13, 2019, 11:50am November 3, 2019, 7:50am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0053a0b5-9dd8-4f70-a28a-8feada7e3f96",
    "url": "https://discuss.elastic.co/t/referencing-to-an-environment-variable-for-usage-inside-apm-properties-file/202410",
    "title": "Referencing to an environment variable for usage inside apm properties file",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 5, 2019, 5:27pm October 5, 2019, 11:42pm October 6, 2019, 5:57am October 6, 2019, 2:21pm October 6, 2019, 2:47pm October 6, 2019, 3:15pm October 11, 2019, 2:53pm November 1, 2019, 10:53am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f2b91aa8-b067-4e71-b105-77c6788d3256",
    "url": "https://discuss.elastic.co/t/exclude-certain-classes-from-being-instrumented/203027",
    "title": "Exclude certain classes from being instrumented",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 10, 2019, 12:59pm October 10, 2019, 1:37pm October 10, 2019, 1:38pm October 31, 2019, 9:49am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8c5ef944-5398-49c3-8f06-5a830bfd7dc3",
    "url": "https://discuss.elastic.co/t/ephemeral-jobs/202047",
    "title": "Ephemeral Jobs",
    "category": [
      "APM"
    ],
    "author": "jasonmoo-we",
    "date": "October 2, 2019, 8:13pm October 4, 2019, 11:43am October 8, 2019, 5:52pm October 8, 2019, 11:48pm October 9, 2019, 6:20pm October 9, 2019, 6:48pm October 9, 2019, 6:50pm October 9, 2019, 7:07pm October 9, 2019, 8:08pm October 9, 2019, 8:29pm October 10, 2019, 7:13am October 31, 2019, 3:13am",
    "body": "Hi I'm just curious if the APM libraries can be used for short lived jobs (ie. a cron job that processes some entries in a queue and puts them in a database and exits). I haven't seen any special configuration for this. Do the APM libs flush before exit automatically or are they more suited for long-running daemons? Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "dbefffdb-f297-40d1-8018-1e955bd550c9",
    "url": "https://discuss.elastic.co/t/securing-apm-agent-comms/202764",
    "title": "Securing APM agent comms",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 9, 2019, 8:18am October 9, 2019, 7:21am October 9, 2019, 8:18am October 9, 2019, 8:28am October 9, 2019, 8:48am October 9, 2019, 12:16pm October 10, 2019, 6:13am October 31, 2019, 2:13am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "65873a02-fdd0-4d46-bca6-70f363593817",
    "url": "https://discuss.elastic.co/t/apm-setup-success-but-is-not-showing-information-into-kibana/202061",
    "title": "APM Setup Success but is not showing information into Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 2, 2019, 9:49pm October 3, 2019, 7:44am October 8, 2019, 3:42pm October 10, 2019, 3:24am October 30, 2019, 11:25pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "3b30c6c2-c374-49e9-9afd-7abba6676aaf",
    "url": "https://discuss.elastic.co/t/elastic-apm-js-agent-for-angular-throws-console-error-cannot-read-property-events/202405",
    "title": "Elastic APM js agent for angular throws console error Cannot read property 'events'",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 5, 2019, 12:11am October 7, 2019, 9:01am October 8, 2019, 4:36pm October 9, 2019, 8:27am October 10, 2019, 12:49am October 30, 2019, 8:49pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0fe7dd10-5501-4293-a136-0b385b9fe1fc",
    "url": "https://discuss.elastic.co/t/unable-to-remote-debug-server-when-apm-agent-is-enabled/202821",
    "title": "Unable to remote debug server when APM agent is enabled",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 9, 2019, 11:55am October 9, 2019, 12:42pm October 9, 2019, 1:01pm October 9, 2019, 2:27pm October 30, 2019, 10:27am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f4c9d7cc-19a9-4204-b6b4-8482c624baf6",
    "url": "https://discuss.elastic.co/t/apm-go-client/202785",
    "title": "APM Go Client",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 9, 2019, 8:19am October 9, 2019, 8:16am October 9, 2019, 8:45am October 9, 2019, 8:51am October 9, 2019, 9:24am October 30, 2019, 5:24am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "01ff48f4-1601-4ced-802f-32ffad588d6a",
    "url": "https://discuss.elastic.co/t/looking-for-help-with-elastic-apm/202743",
    "title": "Looking for help with elastic apm",
    "category": [
      "APM"
    ],
    "author": "SAssociates",
    "date": "October 8, 2019, 11:40pm October 9, 2019, 7:24am October 30, 2019, 3:24am",
    "body": "Im looking for a contractor to help with open source APM landscape and be able to integrate Elastic APM for performance analysis of our distributed image processing framework of the tool. Im hoping someone in the group might be interested.",
    "website_area": "discuss"
  },
  {
    "id": "807fe284-1684-466f-ac6f-5268bd0812eb",
    "url": "https://discuss.elastic.co/t/post-custom-metrics-from-nodejs-apm-agent/201307",
    "title": "Post custom metrics from nodejs apm agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 26, 2019, 8:02pm September 27, 2019, 4:18am September 27, 2019, 7:00am October 16, 2019, 12:27pm October 28, 2019, 4:25pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "08317892-75be-4fff-b349-d9c614d4ca8a",
    "url": "https://discuss.elastic.co/t/apm-node-agent-wont-recognize-cloud-cert/201677",
    "title": "APM Node agent won't recognize cloud cert",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 30, 2019, 4:12pm October 7, 2019, 8:22pm October 28, 2019, 4:22pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b6bba262-ba04-448e-a69f-954c29d47cf2",
    "url": "https://discuss.elastic.co/t/no-lines-drawn-in-time-spent-by-span-time-although-data-is-available/199792",
    "title": "No lines drawn in \"Time spent by span time\", although data is available",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 17, 2019, 12:21pm September 23, 2019, 8:18am October 7, 2019, 1:13pm October 28, 2019, 9:13am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b5635e06-c825-42f7-9bf0-49de641d400f",
    "url": "https://discuss.elastic.co/t/vue-js-integration-for-spa-rum/202369",
    "title": "Vue.js Integration for SPA - RUM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 4, 2019, 5:06pm October 7, 2019, 2:52am October 7, 2019, 1:11pm October 28, 2019, 9:11am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "be8b117e-0294-4f2e-b987-46575acecc2c",
    "url": "https://discuss.elastic.co/t/apm-spring-jms-template/201796",
    "title": "APM Spring JMS Template",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 1, 2019, 2:58pm October 1, 2019, 2:59pm October 2, 2019, 6:25am October 2, 2019, 7:05am October 2, 2019, 8:19am October 2, 2019, 9:48am October 2, 2019, 11:19am October 2, 2019, 11:47am October 7, 2019, 8:20am October 28, 2019, 4:20am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "70de6917-1669-40c8-a137-b3341467495d",
    "url": "https://discuss.elastic.co/t/sending-apm-logs-without-using-apm-server/202110",
    "title": "Sending APM logs without using APM server",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 3, 2019, 8:28am October 4, 2019, 5:54am October 4, 2019, 9:03am October 4, 2019, 9:34am October 7, 2019, 3:54am October 27, 2019, 11:54pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0672a70b-dc96-4c6c-a3d9-45e7c3e8b6d0",
    "url": "https://discuss.elastic.co/t/apm-agent-detach-updates/202259",
    "title": "Apm agent detach updates",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 4, 2019, 6:17am October 6, 2019, 5:43am October 6, 2019, 3:14pm October 6, 2019, 3:33pm October 27, 2019, 11:40am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e39780e9-e660-4754-bd6d-54ddf2a8286c",
    "url": "https://discuss.elastic.co/t/valid-application-packages-values/202282",
    "title": "Valid application packages values",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 4, 2019, 5:44am October 6, 2019, 5:30am October 6, 2019, 4:29am October 27, 2019, 12:29am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "104a0184-34f1-452a-a78b-f740fe882c01",
    "url": "https://discuss.elastic.co/t/distributed-trace-parent-transaction-ending-before-child-distributed-span-comes-back/202426",
    "title": "Distributed trace, parent transaction ending before child distributed span comes back",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 5, 2019, 6:13pm October 5, 2019, 1:34pm October 26, 2019, 9:21am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "71a76b77-96c9-40ee-be9d-bb082dc65597",
    "url": "https://discuss.elastic.co/t/apm-agent-java-doubts-about-default-transactions-relation-with-custom-spans/202368",
    "title": "[APM AGENT][JAVA] Doubts about default transactions relation with custom spans",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 4, 2019, 4:46pm October 5, 2019, 6:41am October 26, 2019, 2:41am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "cb1a814c-4148-488b-b80d-21814babaa3e",
    "url": "https://discuss.elastic.co/t/try-to-start-apm-server-inside-elasticsearch-pod/201824",
    "title": "Try to start APM server inside elasticsearch pod",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 2, 2019, 7:50am October 3, 2019, 10:03am October 3, 2019, 10:52am October 4, 2019, 12:22pm October 4, 2019, 2:03pm October 25, 2019, 10:03am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "10c806e3-ad8a-4792-bd39-d9ce5d33a963",
    "url": "https://discuss.elastic.co/t/custom-context-merge-in-rum-js/202164",
    "title": "Custom context merge in RUM JS",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 3, 2019, 12:30pm October 4, 2019, 8:26am October 25, 2019, 4:27am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b97b5ecc-64ca-4381-be6d-b57341567c51",
    "url": "https://discuss.elastic.co/t/apm-server-behind-load-balancer/202086",
    "title": "APM server behind load balancer",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 3, 2019, 6:14am October 4, 2019, 5:26am October 4, 2019, 6:17am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "59f3d5a0-e911-45bd-b89a-ddcb868d2a0e",
    "url": "https://discuss.elastic.co/t/unknown-route-yet-url-is-populated/201303",
    "title": "Unknown route yet URL is populated",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 26, 2019, 7:35pm September 27, 2019, 1:26am September 27, 2019, 4:16am September 27, 2019, 6:58am October 1, 2019, 12:58pm October 2, 2019, 6:42am October 2, 2019, 9:02am October 3, 2019, 7:03pm October 24, 2019, 3:03pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "af089db0-b3ba-434f-9860-d35cf47d717b",
    "url": "https://discuss.elastic.co/t/net-agent-does-not-recognize-elasticsearch-net-calls-in-the-timeline/202058",
    "title": ".NET Agent does not recognize Elasticsearch.NET calls in the timeline",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 2, 2019, 9:28pm October 3, 2019, 1:18pm October 3, 2019, 1:16pm October 3, 2019, 2:46pm October 24, 2019, 10:46am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "50d2247a-482d-46ed-ae1e-b21c1a88542f",
    "url": "https://discuss.elastic.co/t/apm-7-4-error-with-default-pipeline-apm/201976",
    "title": "APM 7.4 error with default pipeline [apm]",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 3, 2019, 6:12am October 2, 2019, 6:53pm October 3, 2019, 10:35am October 3, 2019, 2:43pm October 3, 2019, 2:43pm October 24, 2019, 10:43am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d1a073e1-348a-4f5b-927b-604613d3e3f4",
    "url": "https://discuss.elastic.co/t/performance-impact-of-installing-apm-java-and-net/201094",
    "title": "Performance impact of installing APM (Java and .NET)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 26, 2019, 9:37pm September 25, 2019, 7:41pm September 26, 2019, 3:10am September 26, 2019, 7:54pm October 3, 2019, 2:24pm October 24, 2019, 10:34am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "48bee3c9-1e61-40c7-90fc-486f9d7c9ee5",
    "url": "https://discuss.elastic.co/t/autoinstrument-and-transaction-handling-logic/202140",
    "title": "Autoinstrument and transaction handling logic",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 3, 2019, 9:52am October 3, 2019, 10:44am October 24, 2019, 6:44am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "515567fb-98ac-4c3a-a864-9fbbd4da272d",
    "url": "https://discuss.elastic.co/t/serving-apm-server-both-with-and-without-ssl/201851",
    "title": "Serving APM server both with and without SSL",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 1, 2019, 7:17pm October 2, 2019, 6:13am October 2, 2019, 2:09pm October 23, 2019, 10:10am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c124b08e-fe04-42a2-ab55-43317182dc43",
    "url": "https://discuss.elastic.co/t/apm-wont-load-balance-amongst-kafka-partitions/201691",
    "title": "APM won't load balance amongst kafka partitions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 30, 2019, 6:41pm October 2, 2019, 11:18am October 23, 2019, 7:18am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a498ad8d-ed87-44c2-9035-3d3438504363",
    "url": "https://discuss.elastic.co/t/how-remove-apm-application-from-kibana-ui/201014",
    "title": "How remove APM application from Kibana UI",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 25, 2019, 2:16pm September 25, 2019, 2:52pm September 27, 2019, 8:12am October 2, 2019, 3:17pm October 30, 2019, 8:34am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "db7e5ee6-95c6-4367-95fc-28507468117d",
    "url": "https://discuss.elastic.co/t/node-js-agent-not-instrumenting-mongoose/201254",
    "title": "Node.js Agent not instrumenting Mongoose",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 26, 2019, 2:41pm September 26, 2019, 7:26pm September 26, 2019, 8:09pm October 1, 2019, 4:26pm October 22, 2019, 12:26pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8e29cafa-1c1e-44b6-b9d7-30d14ed44448",
    "url": "https://discuss.elastic.co/t/dynamically-expiring-secret-token-on-server-and-agents/201670",
    "title": "Dynamically expiring secret token on server and agents",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 30, 2019, 3:55pm October 1, 2019, 7:18am October 1, 2019, 3:49pm October 22, 2019, 11:49am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f6563932-8f48-4786-916c-cda80b258e81",
    "url": "https://discuss.elastic.co/t/labels-are-getting-added-to-only-transaction-event-not-to-span-event/201069",
    "title": "Labels are getting added to only transaction event not to span event",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 25, 2019, 3:00pm September 25, 2019, 3:00pm September 26, 2019, 4:56am September 27, 2019, 1:21pm September 27, 2019, 1:43pm September 27, 2019, 2:20pm September 30, 2019, 11:49am September 30, 2019, 1:26pm September 30, 2019, 1:30pm September 30, 2019, 2:58pm October 21, 2019, 11:03am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1defa99b-274c-4ee7-90b2-c41fb04749d9",
    "url": "https://discuss.elastic.co/t/apm-java-agent-capture-body-backend-request-apache-httpclient/201646",
    "title": "APM Java agent - capture body backend request - Apache HttpClient",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 30, 2019, 1:14pm September 30, 2019, 1:35pm September 30, 2019, 1:47pm October 21, 2019, 9:48am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9fd744e2-7b30-4495-8e1d-5df08c1e1ed2",
    "url": "https://discuss.elastic.co/t/how-to-add-application-metrics-to-elastic-apm-dashbords/201341",
    "title": "How to add application metrics to elastic APM dashbords",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 27, 2019, 8:32am September 27, 2019, 9:21am September 28, 2019, 7:13am September 28, 2019, 8:20am September 30, 2019, 12:06pm October 21, 2019, 8:06am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "3609eed4-ef68-42f9-88ee-a65e8555e3f5",
    "url": "https://discuss.elastic.co/t/distributed-tracing-missing-expansion-for-one-service/201329",
    "title": "Distributed tracing missing expansion for one service",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 27, 2019, 12:56am September 27, 2019, 12:57pm September 27, 2019, 7:31pm September 27, 2019, 7:44pm September 30, 2019, 12:29pm October 21, 2019, 3:37am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "88e67b20-10a6-4d66-a4b3-c596d5a249dd",
    "url": "https://discuss.elastic.co/t/java-apm-agent-configuration-via-external-configuration-management/201517",
    "title": "Java APM agent configuration via external configuration management",
    "category": [
      "APM"
    ],
    "author": "surenraju",
    "date": "September 29, 2019, 4:56am September 30, 2019, 7:09am October 21, 2019, 3:10am",
    "body": "Elastic Search Version: 7.3.1 APM Server Version: 7.3.1 APM Client Version: APM Java Agent: 1.x (current) APM Agent language and version: Java 1.8 We are evaluating elastic APM for our micro service stack. As part of that we are evaluating programmatic self-attach of APM agent with Spring Boot application. https://www.elastic.co/guide/en/apm/agent/java/current/setup-attach-api.html As part of that we are wondering if we can use external config solution(using spring-cloud-consul) to manage APM agents configuration. Please let us know if it is recommended to control elasticapm.properties via external config management solution like consul key value for a spring boot application. Our original intention for external configuration is to dynamically change properties like active transaction_sample_rate",
    "website_area": "discuss"
  },
  {
    "id": "72fd724a-a9f0-470a-922a-2054a4656344",
    "url": "https://discuss.elastic.co/t/secret-tokens-for-frontend-webapp-and-backend-applications/201504",
    "title": "Secret tokens for frontend webapp and backend applications",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 29, 2019, 8:32am October 2, 2019, 4:03am October 19, 2019, 7:21pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "77bd48e0-26cd-4e23-8c36-1b51544e228e",
    "url": "https://discuss.elastic.co/t/making-sure-no-logs-are-lost/200508",
    "title": "Making sure no logs are lost",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 21, 2019, 4:02pm September 23, 2019, 1:22am September 28, 2019, 6:53pm October 19, 2019, 2:53pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c0decbea-6c14-4810-b8d9-daeee89251ca",
    "url": "https://discuss.elastic.co/t/apm-time-complexity-questions/200744",
    "title": "APM Time Complexity Questions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 24, 2019, 1:54am September 25, 2019, 5:14am September 26, 2019, 7:45pm September 27, 2019, 5:10pm September 27, 2019, 6:50pm October 18, 2019, 2:50pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1eb572d3-82e5-4fb2-8e12-49a7b070061a",
    "url": "https://discuss.elastic.co/t/apm-self-instrumentation/201179",
    "title": "APM self instrumentation",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 26, 2019, 9:54am September 26, 2019, 10:42am September 26, 2019, 11:03am September 26, 2019, 12:41pm September 26, 2019, 12:38pm October 17, 2019, 8:38am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5c9ad092-41a3-40cf-93e0-457794e14fd7",
    "url": "https://discuss.elastic.co/t/logs-are-not-showing-from-apm/201049",
    "title": "Logs are not showing from APM",
    "category": [
      "APM"
    ],
    "author": "mahendra.garg",
    "date": "September 25, 2019, 2:28pm September 26, 2019, 10:35am September 26, 2019, 10:50am October 24, 2019, 10:50am",
    "body": "Hello Guys I have configured the Kibana, APM, elasticsearch, For APM I have created a domain https://example.com as APM Agent in flask application, When I run the application on development environment logs are getting showing inside APM tab, but in production environment I'm not seeing any logs. For Kibana server.host: \"0.0.0.0\" elasticsearch.hosts: http://IP:PORT elasticsearch.ssl.certificateAuthorities: [\"ca.pem\"] For APM Server host: \"0.0.0.0:8200\" output.elasticsearch: hosts: [\"http://IP:9200\"] ssl.certificate_authorities: [\"ca.pem\"] For Elasticsearch network.host: IP discovery.seed_hosts: [\"IP\"] For Flask APM Agent: current_app.config['ELASTIC_APM'] = { # Set required service name. Allowed characters: # a-z, A-Z, 0-9, -, _, and space 'SERVICE_NAME': app_name, # Use if APM Server requires a token 'SECRET_TOKEN': '', # Set custom APM Server URL (default: http://localhost:8200) 'SERVER_URL': \"https://example.com\", 'LOGGING': True, 'DEBUG': True, 'VERIFY_SERVER_CERT': False, } apm = ElasticAPM(current_app, logging=True) apm.init_app(current_app, service_name=app_name, secret_token='') Please let me know where I need to make correction to make it work?",
    "website_area": "discuss"
  },
  {
    "id": "c4a417f6-3ba3-4013-a2f2-96c1b33b8831",
    "url": "https://discuss.elastic.co/t/explore-how-to-associate-apm-tracking-with-logs/199951",
    "title": "Explore: How to associate APM tracking with logs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 18, 2019, 8:41am September 18, 2019, 8:47am September 25, 2019, 3:27am September 19, 2019, 2:10am September 19, 2019, 2:49am September 20, 2019, 3:29am September 20, 2019, 3:45am September 20, 2019, 4:00am September 25, 2019, 3:27am September 20, 2019, 6:54am September 20, 2019, 7:51am September 20, 2019, 7:53am September 23, 2019, 1:13pm September 23, 2019, 1:27pm September 25, 2019, 3:07am October 15, 2019, 11:17pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6e65d0f5-11e5-48a9-8ffc-bb159c473295",
    "url": "https://discuss.elastic.co/t/getting-and-setting-transaction-id/197947",
    "title": "Getting and Setting Transaction ID",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 3, 2019, 9:33pm September 3, 2019, 9:16pm September 3, 2019, 9:31pm September 3, 2019, 9:47pm September 3, 2019, 10:29pm September 5, 2019, 11:13pm September 4, 2019, 6:12pm September 4, 2019, 10:31pm September 4, 2019, 9:08pm September 5, 2019, 11:13pm September 17, 2019, 3:16pm September 24, 2019, 8:44pm September 25, 2019, 2:23am October 15, 2019, 5:31pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1658f3cb-2360-4c2b-a25c-0bed1a09b1f2",
    "url": "https://discuss.elastic.co/t/where-are-my-metrics/200943",
    "title": "Where are my metrics?",
    "category": [
      "APM"
    ],
    "author": "ethrbunny",
    "date": "September 25, 2019, 12:03am September 25, 2019, 10:44am October 15, 2019, 10:06pm",
    "body": "I have the 1.1.0 agent running as -javaagent with my java 1.8 app. I can see transactions in the apm UI (and in the apm-* index I can see \"transaction\" and \"span\" in the processor.index field).. but no metrics. What should I expect to see and why don't I?",
    "website_area": "discuss"
  },
  {
    "id": "4dd8d53a-7b39-44cd-b0e8-0006f66300d1",
    "url": "https://discuss.elastic.co/t/intake-api-memory-usage-metricset-does-not-recieve-data/200247",
    "title": "Intake API Memory Usage Metricset Does Not Recieve Data",
    "category": [
      "APM"
    ],
    "author": "LordMathis",
    "date": "September 19, 2019, 3:10pm September 23, 2019, 2:59pm September 23, 2019, 2:26pm September 23, 2019, 2:34pm September 23, 2019, 2:59pm September 24, 2019, 2:13am September 24, 2019, 12:05pm October 15, 2019, 8:05am",
    "body": "Kibana version: 7.3 Elasticsearch version: 7.3 APM Server version: 7.3 APM Agent language and version: Intake API Hello, I am trying to make an agent using the intake API, for the language that our company uses. I was able to successfully find a way to gather the memory usage in our language but when I send the data down to apm, the data does not show on Kibana. Here is a link to the list of variables I should be using: https://www.elastic.co/guide/en/apm/server/current/exported-fields-system.html And here is an example of the JSON I am sending: {\"metricset\": {\"samples\": {\"system.memory.total\": {\"value\": 2147483647}, \"system.memory.used.bytes\": {\"value\": 1116691496}}, \"timestamp\": 1568813856000000}} {\"metricset\": {\"samples\": {\"system.memory.total\": {\"value\": 2147483647}, \"system.memory.used.bytes\": {\"value\": 1116691496}}, \"timestamp\": 1568813856000000}} {\"metricset\": {\"samples\": {\"system.memory.total\": {\"value\": 2147483647}, \"system.memory.used.bytes\": {\"value\": 1116691496}}, \"timestamp\": 1568813857000000}} {\"metricset\": {\"samples\": {\"system.memory.total\": {\"value\": 2147483647}, \"system.memory.used.bytes\": {\"value\": 1116691496}}, \"timestamp\": 1568813857000000}} And here is an example of the CPU usage that I am successfully sending as a reference of a working send: {\"metricset\": {\"samples\": {\"system.process.cpu.total.norm.pct\": {\"value\": 0}}, \"timestamp\": 1568905657000000}} {\"metricset\": {\"samples\": {\"system.process.cpu.total.norm.pct\": {\"value\": 0}}, \"timestamp\": 1568905657000000}} {\"metricset\": {\"samples\": {\"system.process.cpu.total.norm.pct\": {\"value\": 0}}, \"timestamp\": 1568905658000000}} {\"metricset\": {\"samples\": {\"system.process.cpu.total.norm.pct\": {\"value\": 0}}, \"timestamp\": 1568905658000000}}",
    "website_area": "discuss"
  },
  {
    "id": "5f3f9238-c302-4f8d-a646-eedb19c9f8a6",
    "url": "https://discuss.elastic.co/t/elasticsearch-apm-usage-for-custom-java-applicatio-call-methods/200821",
    "title": "Elasticsearch APM usage for Custom java applicatio call methods",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 24, 2019, 8:51am September 24, 2019, 9:44am September 24, 2019, 10:25am October 15, 2019, 6:25am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a495363a-bd67-4550-be39-5eaa4f133ea1",
    "url": "https://discuss.elastic.co/t/searching-for-a-custom-made-header/200403",
    "title": "Searching for a custom made header",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 22, 2019, 9:06am September 24, 2019, 7:16am September 24, 2019, 8:38am October 15, 2019, 4:38am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "59e19efb-83cb-4a64-8df8-6376606aafd1",
    "url": "https://discuss.elastic.co/t/basic-c-apm-application-error-systemtotalcpuprovider-failed-instantiating-performancecounter/200467",
    "title": "Basic C# Apm Application Error: {SystemTotalCpuProvider} Failed instantiating PerformanceCounter",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 21, 2019, 7:05am September 21, 2019, 5:14am September 20, 2019, 7:09pm September 21, 2019, 7:21am October 4, 2019, 4:56am September 23, 2019, 2:00pm September 23, 2019, 2:01pm September 23, 2019, 3:04pm September 23, 2019, 3:20pm October 14, 2019, 11:20am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "2cb3d9aa-1fa7-4690-b59a-1ebe97b82659",
    "url": "https://discuss.elastic.co/t/how-to-group-spans-metadata-and-transactions-together-for-visualization/200227",
    "title": "How to Group Spans, Metadata and Transactions Together for Visualization",
    "category": [
      "APM"
    ],
    "author": "LordMathis",
    "date": "September 19, 2019, 2:25pm September 20, 2019, 7:09pm September 23, 2019, 2:04pm September 23, 2019, 2:59pm October 14, 2019, 10:07am",
    "body": "Kibana version: 7.3 Elasticsearch version: 7.3 APM Server version: 7.3 APM Agent language and version: Intake API (Building new agent) Hello, I am looking for a way to link spans, transactions and metadata all together in Kibana so I can properly use information from the metadata to group the spans. I'll give a specific example of what I'm trying to accomplish. I'm trying to show the average duration of a specific span on a line graph, where each line is a different version number. To do this I first filter for the span.name that I am wanting to track: Then I add average span duration to the y axis: Next I make the x axis a date histogram so that I can see this data over a period of time: Now here comes the problem. I need to split the lines by version number so I can see how long specific version numbers were taking at a given time but since the data is tracking the spans and not the metadata, the version tag in the metadata is not accessible to be sorted by. Even though the spans were all sent in with the metadata tag, there is no value that links these two events and even if there was I wouldn't know how to link them visually in Kibana. My hacky solution was to pass the version number as a label in the spans context for every single span. Then the data exists as a label for the spans so I can track it: image.png934419 19.2 KB This method however is not scalable. Lets say in the future I want to group spans by their parent transactions, transaction type. Even though there is a parent id relationship between a span and it's transaction, I have no way to visually link the two together in Kibana. Does anyone know of a way to handle this issue? Thank you, Matt",
    "website_area": "discuss"
  },
  {
    "id": "45e31767-8cad-42be-b6fe-a21c82f2621f",
    "url": "https://discuss.elastic.co/t/apm-server-publishing-data-to-single-kakfa-topic-partition/199113",
    "title": "Apm server publishing data to single kakfa topic partition",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 12, 2019, 4:53am September 11, 2019, 5:45pm September 12, 2019, 6:49am September 20, 2019, 3:03pm September 23, 2019, 7:40am September 23, 2019, 2:28pm October 14, 2019, 10:28am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "271693a6-ce15-4655-8672-8e3a4b65a1cf",
    "url": "https://discuss.elastic.co/t/understand-a-span-detail/200360",
    "title": "Understand a span detail",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 20, 2019, 7:59am September 23, 2019, 1:52pm October 14, 2019, 9:15am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6c05afe8-3c5b-41b0-ab4c-428df5465e1a",
    "url": "https://discuss.elastic.co/t/net-core-exception/200485",
    "title": ".NET core exception",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 20, 2019, 8:05pm September 20, 2019, 8:20pm October 11, 2019, 4:22pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "81120e66-8c56-45c7-b342-fd33c316e0c4",
    "url": "https://discuss.elastic.co/t/net-core-agent-not-showing-transactions/199129",
    "title": ".Net Core Agent not showing transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 11, 2019, 9:31pm September 11, 2019, 7:45pm September 11, 2019, 7:58pm September 11, 2019, 8:06pm September 11, 2019, 8:16pm September 11, 2019, 8:39pm September 11, 2019, 8:45pm September 11, 2019, 8:59pm September 20, 2019, 8:00pm September 20, 2019, 8:12pm October 11, 2019, 4:12pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "f712cbde-57c9-4018-a740-1aba4c73fc8d",
    "url": "https://discuss.elastic.co/t/how-to-unhide-redacted-values-using-apm/200438",
    "title": "How to unhide redacted values using apm",
    "category": [
      "APM"
    ],
    "author": "zozo6015",
    "date": "September 20, 2019, 2:08pm September 20, 2019, 3:13pm September 20, 2019, 2:24pm September 20, 2019, 2:43pm September 20, 2019, 3:13pm September 20, 2019, 3:08pm September 20, 2019, 3:13pm September 20, 2019, 3:14pm October 11, 2019, 11:14am",
    "body": "Kibana version: 7.3.0 Elasticsearch version: 7.3.0 APM Server version: 7.3.0 APM Agent language and version: java 1.8.0 Browser version: safari 13.0.1 Original install method (e.g. download page, yum, deb, from source, etc.) and version: from apt repository Fresh install or upgraded from other version? : fresh installation Is there anything special in your setup? no Description of the problem including expected versus actual behavior. Please include screenshots (if relevant): some of the headers are showing {REDACTED} as value while we would like to have the ability to show the real data from the header in order to be able to indentify the customer which is making the call while investigating the issue. We have tried to enable persona_data from apm-server configuration but that does not seam to fix hte problem. Is there anything else I can try? Steps to reproduce: 1. 2. 3. Errors in browser console (if relevant): Provide logs and/or server output (if relevant):",
    "website_area": "discuss"
  },
  {
    "id": "3a93004a-4ae5-4339-a3dd-cde61d564e70",
    "url": "https://discuss.elastic.co/t/apm-nodejs-agent-not-intercepting-mongodb-queries-no-spans/200338",
    "title": "APM Nodejs Agent not intercepting MongoDB queries (no spans)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 20, 2019, 4:44am September 20, 2019, 5:02am September 20, 2019, 5:47am September 20, 2019, 11:49am September 20, 2019, 12:15pm September 20, 2019, 12:45pm October 11, 2019, 8:45am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "477a400e-3008-4d56-8a01-cf6bf21a1fe9",
    "url": "https://discuss.elastic.co/t/duplicated-transaction-with-different-ip/197479",
    "title": "Duplicated transaction with different ip",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "August 30, 2019, 9:15am August 30, 2019, 11:06am September 2, 2019, 3:49am September 9, 2019, 2:21am September 9, 2019, 2:40pm September 10, 2019, 8:09am September 10, 2019, 1:30pm September 20, 2019, 7:49am September 20, 2019, 11:22am October 11, 2019, 7:23am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c11494aa-a50d-48ee-9b4a-8a3bbf772bef",
    "url": "https://discuss.elastic.co/t/apm-server-error-connection-refused/200109",
    "title": "APM Server error (Connection refused)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 19, 2019, 4:47am September 19, 2019, 8:28am September 19, 2019, 9:35am September 19, 2019, 10:55am September 19, 2019, 3:03pm September 20, 2019, 4:16am September 20, 2019, 4:25am September 21, 2019, 5:03am September 20, 2019, 10:38am October 11, 2019, 6:38am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "3d2922ae-95e0-421c-9124-deaacfa56b51",
    "url": "https://discuss.elastic.co/t/littleendian-vs-bigendian-in-go-agent/200232",
    "title": "LittleEndian vs BigEndian in go agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 19, 2019, 2:28pm September 20, 2019, 7:30am September 20, 2019, 7:29am October 11, 2019, 3:29am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "19001153-a45e-4f2c-a2d3-aa5a9ee40763",
    "url": "https://discuss.elastic.co/t/kibana-shows-post-to-intake-api-as-part-of-transaction/200149",
    "title": "Kibana shows POST to intake API as part of transaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 19, 2019, 8:32am September 19, 2019, 1:12pm September 19, 2019, 1:42pm October 10, 2019, 9:42am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a5fdf21c-022d-4728-983b-ffa8c5692f56",
    "url": "https://discuss.elastic.co/t/hide-or-discard-a-transaction-or-span/199989",
    "title": "Hide or discard a transaction or span",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 18, 2019, 12:18pm September 18, 2019, 12:21pm September 18, 2019, 12:43pm September 18, 2019, 1:02pm September 19, 2019, 8:55am October 10, 2019, 4:55am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "86eff9ce-133c-4c71-b38c-64f1fed612e0",
    "url": "https://discuss.elastic.co/t/apm-transaction-didnt-consider-the-delay-in-jms/199751",
    "title": "APM transaction didn't consider the delay in jms",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 18, 2019, 6:16am September 18, 2019, 6:27am September 18, 2019, 3:22pm September 18, 2019, 3:46pm September 18, 2019, 5:41pm October 16, 2019, 5:41pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1a52cb52-bbef-4207-a780-4891f8b4152d",
    "url": "https://discuss.elastic.co/t/server-side-rendering-and-distributed-tracing/199986",
    "title": "Server Side Rendering and distributed tracing",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 19, 2019, 5:47am September 18, 2019, 1:16pm October 9, 2019, 9:16am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "2be31175-978c-4068-9033-d39e4b728b09",
    "url": "https://discuss.elastic.co/t/can-apmzerolog-send-every-log-to-link-tracking/199802",
    "title": "Can apmzerolog send every log to link tracking?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 17, 2019, 10:24am September 17, 2019, 10:26am September 18, 2019, 1:58am September 18, 2019, 2:14am September 18, 2019, 2:30am September 18, 2019, 7:49am September 18, 2019, 8:15am October 9, 2019, 4:07am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c80c2de5-e16d-437f-ac37-79f34f215898",
    "url": "https://discuss.elastic.co/t/400-bad-request-when-loading-kibana-apm-dashboard/199515",
    "title": "400 Bad request when loading Kibana APM dashboard",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 16, 2019, 2:18pm September 16, 2019, 2:19pm September 16, 2019, 5:00pm September 17, 2019, 10:14am September 18, 2019, 1:43am October 16, 2019, 1:43am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "9e09c6b9-933c-41b1-b3a9-6d10da16db1d",
    "url": "https://discuss.elastic.co/t/when-we-edit-apm-server-yml-for-giving-ip-and-restart-service-wont-start/199668",
    "title": "When we edit apm-server.yml for giving ip and restart, service wont start",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 16, 2019, 1:17pm September 16, 2019, 2:15pm September 18, 2019, 8:29am September 17, 2019, 4:39am September 18, 2019, 8:29am October 8, 2019, 5:10am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8a026bc8-f7c0-4c92-9420-544012433b6e",
    "url": "https://discuss.elastic.co/t/apm-tracing-with-zuul-netflix-oss/198838",
    "title": "APM tracing with ZUUL (Netflix OSS)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 10, 2019, 7:55am September 10, 2019, 7:52am September 10, 2019, 8:21am September 11, 2019, 1:46pm September 11, 2019, 3:20pm September 12, 2019, 7:45am September 13, 2019, 7:20am September 13, 2019, 9:35am September 13, 2019, 12:57pm September 13, 2019, 11:58am September 16, 2019, 11:17am September 16, 2019, 11:48am October 7, 2019, 8:02am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "427b59eb-c2cf-4ddc-8ff5-cdd3c0140acd",
    "url": "https://discuss.elastic.co/t/is-apm-perfromance-like-this-because-of-java-version-1-7-at-the-application-side/199175",
    "title": "Is APM perfromance like this because of Java version 1.7 at the Application side",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 12, 2019, 6:02am September 12, 2019, 6:04am September 12, 2019, 6:30am September 12, 2019, 7:06am September 12, 2019, 7:25am September 16, 2019, 10:15am October 7, 2019, 6:15am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "00597263-3249-4e3b-ae71-17b0ebc6d32d",
    "url": "https://discuss.elastic.co/t/agent-logs-how-can-i-make-it-in-debug-mode/199564",
    "title": "Agent logs, how can I make it in debug mode",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 16, 2019, 4:07am September 16, 2019, 4:32am October 7, 2019, 12:32am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d7cd029b-c99d-46e1-872d-506363c9f1de",
    "url": "https://discuss.elastic.co/t/apm-to-make-it-work-with-python/199165",
    "title": "APM to make it work with Python",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 12, 2019, 3:38am September 12, 2019, 10:56am September 16, 2019, 4:02am October 7, 2019, 12:02am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "cc7ffb4d-13f7-4bb1-b12a-02bda6962cf2",
    "url": "https://discuss.elastic.co/t/apm-java-agent-crashes-with-springboot-2/197330",
    "title": "APM Java Agent crashes with Springboot 2",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "August 29, 2019, 12:20pm August 29, 2019, 2:39pm August 29, 2019, 2:41pm August 29, 2019, 2:47pm August 29, 2019, 3:33pm August 30, 2019, 6:19am August 30, 2019, 6:21am September 9, 2019, 7:09am September 9, 2019, 7:27am September 9, 2019, 11:05am September 12, 2019, 11:30am September 13, 2019, 11:05am October 4, 2019, 7:05am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "81bfa6d3-0bb4-41ec-9430-01659de5910c",
    "url": "https://discuss.elastic.co/t/elastic-apm-hides-error-message-in-log/199375",
    "title": "Elastic APM hides error message in log",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 13, 2019, 8:41am September 13, 2019, 8:58am October 4, 2019, 4:58am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4611a004-9267-403e-a795-2f43c10f595a",
    "url": "https://discuss.elastic.co/t/after-sometime-transaction-are-not-generating-only-spans-are-coming/199003",
    "title": "After sometime transaction are not generating only spans are coming",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 11, 2019, 5:32am September 11, 2019, 5:37am September 11, 2019, 7:05am September 11, 2019, 7:35am September 11, 2019, 11:58am September 12, 2019, 12:35pm September 12, 2019, 12:35pm October 3, 2019, 8:35am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "450b0a01-1a08-4e33-9e3d-28d72b72a43c",
    "url": "https://discuss.elastic.co/t/apm-net-agent-and-outsystems/198985",
    "title": "APM .NET agent and Outsystems",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 10, 2019, 11:37pm September 11, 2019, 4:06am September 12, 2019, 8:25am October 3, 2019, 4:25am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "37cb6526-f8e7-4add-bae2-0fafa8c912ee",
    "url": "https://discuss.elastic.co/t/my-concern-when-in-internet-public-access/199169",
    "title": "My Concern when in Internet public access",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 12, 2019, 4:30am September 12, 2019, 4:58am October 3, 2019, 12:58am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b6dd11b5-4453-47b9-bf1d-43b2a59eb490",
    "url": "https://discuss.elastic.co/t/tomcat-application-server-agent-status-show-no-data-has-been-received-from-agents-yet/198999",
    "title": "Tomcat application server -- agent status show --No data has been received from agents yet",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 11, 2019, 4:10am September 11, 2019, 5:31am September 11, 2019, 6:41am September 11, 2019, 12:02pm October 2, 2019, 8:02am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "50206975-20ea-444a-9b03-6450039ed504",
    "url": "https://discuss.elastic.co/t/has-anyone-been-able-to-use-elastic-apm-rum-angular-with-aot/198456",
    "title": "Has anyone been able to use @elastic/apm-rum-angular with aot?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 6, 2019, 5:30pm September 10, 2019, 5:00pm September 10, 2019, 1:03pm September 10, 2019, 4:59pm October 1, 2019, 12:59pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "254f623e-9f1a-42cf-b726-af42816e5358",
    "url": "https://discuss.elastic.co/t/apm-server-transport-error-econnrefused-connect-econnrefused-127-0-0-1-8200/198693",
    "title": "APM Server transport error (ECONNREFUSED): connect ECONNREFUSED 127.0.0.1:8200",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 9, 2019, 2:06pm September 10, 2019, 4:56am September 10, 2019, 4:56am September 10, 2019, 8:00am October 1, 2019, 4:00am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "5500dcf7-9d5f-467a-a89b-66950f797a2d",
    "url": "https://discuss.elastic.co/t/non-http-request-for-apm-server/198247",
    "title": "Non-http request for APM server",
    "category": [
      "APM"
    ],
    "author": "YvorL",
    "date": "September 5, 2019, 12:45pm September 6, 2019, 1:53am September 9, 2019, 11:52am September 10, 2019, 3:49am September 30, 2019, 11:49pm",
    "body": "Sorry for deleting the template but this is more of is there a way than a specific issue. I'm trying to set up a custom APM pipeline and my initial thoughts were that I can install the agents in various containers, have one APM server on a different node which will forward the data to Elasticsearch. But the agents have a big performance impact if I use an HTTP connection (not to mention other issues with this) so I thought I could simply write out the events and ship those to the APM server. Is it possible to set up APM server to receive the events which are shipped from a file?",
    "website_area": "discuss"
  },
  {
    "id": "caa148b0-4653-40a5-b53b-b4a5e2627167",
    "url": "https://discuss.elastic.co/t/trouble-configuring-agent-server/197970",
    "title": "Trouble configuring agent server",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 4, 2019, 3:54am September 4, 2019, 3:56am September 4, 2019, 3:58am September 4, 2019, 5:40am September 4, 2019, 5:42am September 4, 2019, 6:29am September 4, 2019, 6:31am September 4, 2019, 6:57am September 4, 2019, 7:00am September 4, 2019, 7:02am September 4, 2019, 8:29am September 9, 2019, 8:43am September 30, 2019, 4:43am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "09462360-2991-407a-878d-7c7cfaaedb7d",
    "url": "https://discuss.elastic.co/t/java-agent-websphere-not-starting-up-with-agent-installed/198547",
    "title": "JAVA Agent - Websphere not starting up with agent installed",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 8, 2019, 9:04am September 8, 2019, 10:21am September 29, 2019, 6:21am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "eb4756f9-d3d1-4d2c-8e4c-4d1412345477",
    "url": "https://discuss.elastic.co/t/failed-to-parse-field-labels-name-of-type-scaled-float/197139",
    "title": "Failed to parse field [labels.name] of type [scaled_float]",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 3, 2019, 6:00pm August 28, 2019, 3:29pm September 3, 2019, 1:44pm September 4, 2019, 8:06am September 5, 2019, 12:51pm September 5, 2019, 2:09pm September 6, 2019, 9:11am September 27, 2019, 5:18am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "25961632-ebef-4626-ada0-166428de9f25",
    "url": "https://discuss.elastic.co/t/capturing-and-visualizing-threaded-http-requests/196200",
    "title": "Capturing and Visualizing Threaded HTTP Requests",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "August 22, 2019, 5:16pm September 3, 2019, 8:31pm September 3, 2019, 8:31pm September 5, 2019, 12:29pm September 26, 2019, 8:29am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "568123db-6bdf-4934-a1f3-0470f302a7cf",
    "url": "https://discuss.elastic.co/t/django-agent-ignoring-spans-related-to-templates/198208",
    "title": "Django Agent - Ignoring spans related to templates",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 5, 2019, 9:51am September 5, 2019, 9:49am September 26, 2019, 5:49am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "8b603625-279c-4b69-b368-1d1c22ad794e",
    "url": "https://discuss.elastic.co/t/how-to-use-distributed-tracing-in-with-elastic-apm-in-non-net-core-application/197356",
    "title": "How to use Distributed tracing in with elastic apm in non .net core application?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "August 29, 2019, 2:10pm August 29, 2019, 2:35pm August 29, 2019, 2:34pm September 3, 2019, 9:33am September 3, 2019, 10:04pm September 4, 2019, 5:10pm September 5, 2019, 4:58am September 26, 2019, 12:58am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "66faadd9-2959-4007-a245-130f674b38dc",
    "url": "https://discuss.elastic.co/t/elastic-apm-property-elastic-apm-enable-log-correlation-does-not-work/198097",
    "title": "Elastic APM property elastic.apm.enable_log_correlation does not work",
    "category": [
      "APM"
    ],
    "author": "surenraju",
    "date": "September 4, 2019, 6:52pm September 5, 2019, 1:21am September 25, 2019, 9:21pm",
    "body": "Elastic Search Version: 7.3.1 APM Server Version: 7.3.1 APM Client Version: 1.9.0 I am running APM Client agent along with Spring Boot based microservices. APM Client is able to instrument and able to collect traces, metric errors. As part of the distributed tracing, i also wanted to trace.id and transaction.id in micro service logs. Our spring boot micro services use SLF4j and Logback. I am using the property elastic.apm.enable_log_correlation=true to enable logging. https://www.elastic.co/guide/en/apm/agent/java/current/config-logging.html Logback configuration as follows { \"trace.id\" : \"%X{trace.id}\", \"transaction.id\" : \"%X{transaction.id}\" } But these configuration does not print any values for trace.id and transaction.id in my logs. Is there dependency which i need to add to my spring boot project?",
    "website_area": "discuss"
  },
  {
    "id": "575d8634-940c-4bb5-a639-09c93ef72bd2",
    "url": "https://discuss.elastic.co/t/apm-index-failing-ilm-rollover/197823",
    "title": "APM index failing ILM rollover",
    "category": [
      "APM"
    ],
    "author": "ethranes",
    "date": "September 3, 2019, 10:40am September 3, 2019, 11:38am September 4, 2019, 10:29am September 4, 2019, 10:46am September 4, 2019, 11:51am September 4, 2019, 11:51am September 25, 2019, 7:51am",
    "body": "Hi, I have my APM setup, and I thought I setit up correctly for ILM, it rolled over once from 0000001 to 0000002 but now the index won't rollover, I get the following error illegal_argument_exception: setting [index.lifecycle.rollover_alias] for index [apm-6.7.1-span-000002] is empty or not defined Could anyone offer any help or advice on debugging and getting my rollovers working?",
    "website_area": "discuss"
  },
  {
    "id": "af7b2bdb-d2c7-4f1a-a87c-534a961b7772",
    "url": "https://discuss.elastic.co/t/can-i-monitor-my-logstash-containers-using-apm/197839",
    "title": "Can i monitor my Logstash containers using APM?",
    "category": [
      "APM"
    ],
    "author": "Maheswari",
    "date": "September 3, 2019, 11:42am September 3, 2019, 10:42pm September 24, 2019, 6:42pm",
    "body": "Hi, I have my elasticsearch , kibana and Logstash installed. I wanted to monitor my logstash containers(running pipelines) using APM. i have installed APM Server and linked to elasticsearch and kibana. But i am not getting which APM agent i should use to monitor docker containers and how to configure it. Kindly please help me in implementing this. Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "2597081f-79e3-4f64-bd62-0a2ec5b1a3a2",
    "url": "https://discuss.elastic.co/t/unable-to-disable-java-apm-agent-in-1-9-0/197669",
    "title": "Unable to disable Java APM agent in 1.9.0",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 2, 2019, 10:28am September 3, 2019, 7:59am September 3, 2019, 9:47am September 3, 2019, 1:29pm September 3, 2019, 1:29pm September 3, 2019, 2:39pm September 24, 2019, 10:43am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "fbe84d58-4d91-468d-bdeb-04c19d2f8739",
    "url": "https://discuss.elastic.co/t/apm-agent-failed-to-start-on-open-jdk-11/197702",
    "title": "Apm agent failed to start on open JDK 11",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 2, 2019, 2:17pm September 2, 2019, 2:35pm September 2, 2019, 3:01pm September 3, 2019, 4:29am September 3, 2019, 7:54am September 24, 2019, 2:46am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "82b88f4a-25c7-4ab6-9a57-fd6c7b456baa",
    "url": "https://discuss.elastic.co/t/net-core-2-1-no-database-statment-in-stacktrace/197457",
    "title": ".net core 2.1 no Database statment in Stacktrace",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "August 30, 2019, 7:22am August 30, 2019, 1:43pm August 30, 2019, 1:46pm September 2, 2019, 10:56am September 2, 2019, 12:47pm September 23, 2019, 8:47am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1f41b49e-200d-45da-a051-9fff81b8e336",
    "url": "https://discuss.elastic.co/t/authentication-of-rum-event-post-from-browser/197655",
    "title": "Authentication of RUM event post from browser",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "September 3, 2019, 7:55am September 2, 2019, 8:22am September 2, 2019, 9:00am September 2, 2019, 9:18am September 23, 2019, 5:21am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "73f00398-9387-42d3-a0a3-db1bb4bc9a7b",
    "url": "https://discuss.elastic.co/t/why-do-elastic-provide-crippleware-on-the-aws-marketplace/196791",
    "title": "Why do Elastic provide Crippleware on the AWS Marketplace",
    "category": [
      "APM"
    ],
    "author": "RoryBrowne",
    "date": "August 26, 2019, 2:21pm August 26, 2019, 3:46pm August 26, 2019, 4:08pm August 27, 2019, 5:51am August 27, 2019, 9:17am September 1, 2019, 8:18pm September 22, 2019, 4:24pm",
    "body": "OK - I can understand why Elastic would cripple services from other providers that Elastic aren't paid for, such as Amazon's hosted service ( that's managed by Amazon ). I can't understand however, why Elastic would limit features when they're being paid for, just because the payment is coming via the AWS Marketplace instead of directly to Elastic. Yes, it's slightly less crippled than the Amazon hosted version, but it's still crippled. What adds injury to insult here, is that where the option for APM is normally shown, it's simply missing. If there was some text there to say that you hate Amazon, and don't want Amazon customers to be able to pay for your features, I could at least have saved the hour I spent re-tracing my steps trying to figure out where I missed the step to enable APM, only to find out later in some obscure part of the manual that it's not available on AWS.",
    "website_area": "discuss"
  },
  {
    "id": "6f8290b2-23e9-4bf6-ba6b-7a05f40f449d",
    "url": "https://discuss.elastic.co/t/opentelemetry-and-elastic-apm-status/197189",
    "title": "OpenTelemetry and Elastic APM status",
    "category": [
      "APM"
    ],
    "author": "azhurbilo",
    "date": "August 28, 2019, 6:52pm August 30, 2019, 3:26am September 19, 2019, 11:26pm",
    "body": "UP topic OpenTelemetry Plans for Elastic APM one more time What does happen after 31st of May, any news about OpenTelemetry and APM?",
    "website_area": "discuss"
  },
  {
    "id": "76d05d80-3767-4589-b90a-42bad5344231",
    "url": "https://discuss.elastic.co/t/apm-server-monitoring-visualizations-in-kibana/196549",
    "title": "APM Server monitoring visualizations in Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "August 23, 2019, 2:52pm August 23, 2019, 8:14pm August 26, 2019, 12:35pm August 30, 2019, 1:07pm September 19, 2019, 3:45pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "96631e36-4137-4aeb-9674-451cbc262af5",
    "url": "https://discuss.elastic.co/t/apm-java-agent-implement-client-side-load-balancing/196961",
    "title": "APM Java agent - Implement client-side load balancing",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "August 29, 2019, 8:00am August 27, 2019, 2:32pm August 28, 2019, 7:09am August 28, 2019, 7:39am August 29, 2019, 7:38am August 29, 2019, 8:05am August 29, 2019, 8:20am August 29, 2019, 8:44am September 19, 2019, 4:44am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "b8a0ea6a-b364-4bc1-b9c7-cbfa5fc046e3",
    "url": "https://discuss.elastic.co/t/about-the-logs-category/156755",
    "title": "About the Logs category",
    "category": [
      "Logs"
    ],
    "author": "warkolm",
    "date": "November 14, 2018, 10:48pm",
    "body": "Everything related to the Logs app  setup with Filebeat, Filebeat modules, and using the Kibana Logs app.",
    "website_area": "discuss"
  },
  {
    "id": "86febb5f-1fe6-45ea-b0c0-a71ff62a22a9",
    "url": "https://discuss.elastic.co/t/sending-log4j-logs-in-xml-again/215739",
    "title": "Sending Log4J logs (in XML) again",
    "category": [
      "Logs"
    ],
    "author": "JY_DT",
    "date": "January 20, 2020, 1:15pm January 20, 2020, 3:40pm January 21, 2020, 6:54am January 21, 2020, 8:51am January 21, 2020, 12:27pm January 22, 2020, 1:13pm January 22, 2020, 3:41pm",
    "body": "Hi, This is a follow up to an earlier post on a similar topic. I'm trying to send Log4j logs in XML format to Elasticsearch using Logstash. My XML file is: <log4j:event logger=\"Common.Core.Sessions.SessionManager\" level=\"INFO\" timestamp=\"1567418641859\" thread=\"8\"> <log4j:message> Session fb9d3408-d370 created for user {9131559e-3b0b} at 127.0.0.1:4931</log4j:message> <log4j:properties> <log4j:data name=\"ConnectionId\" value=\"0HLPFHJFNA3PK\" /> <log4j:data name=\"RequestId\" value=\"0HLPFHJFNA3PK:00000007\" /> </log4j:properties> </log4j:event> and logstash.conf file is: input { beats { port => 5044 type => \"log\" } } filter { xml { source => \"message\" xpath => [ \"/log4j:event/log4j:message/text()\", \"messageMES\" ] store_xml => true target => \"doc\" } } output { elasticsearch { hosts => \"localhost:9200\" sniffing => true manage_template => false index => \"%{[@metadata][beat]}-%{+yyyy.ww}\" document_type => \"%{[@metadata][type]}\" } } My Filebeat config (partially) is: filebeat.inputs: - type: log enabled: true paths: - C:\\ProgramData\\LogTest\\*.xml #Multiline options multiline.pattern: '^<log4j:event' multiline.negate: true multiline.match: after The issue is that all the services start correctly and I do not see any errors in any log files, but the message that I've captured from the XML file ( \"Session fb9d3408-d370 created for user {9131559e-3b0b} at 127.0.0.1:4931\" ) does not show up in the Kibana logs. Is there anything wrong in the configs? Or is there something else ? Thanks in advance, Jy",
    "website_area": "discuss"
  },
  {
    "id": "8d0bf039-1f7b-4363-9aed-9ae43022c7b0",
    "url": "https://discuss.elastic.co/t/unable-to-output-elastalert-log/215477",
    "title": "Unable to output elastalert.log",
    "category": [
      "Logs"
    ],
    "author": "Sansao",
    "date": "January 17, 2020, 2:27pm",
    "body": "I need to remove elastalert information from /var/log/messages, and I'm having trouble creating a log file for elastalert. the default configuration in config.yaml doesn't work and i'm not getting it through the /etc/rsyslog.conf rules either any suggestion?",
    "website_area": "discuss"
  },
  {
    "id": "43f239a4-78ef-438e-9ff2-1833dd91feec",
    "url": "https://discuss.elastic.co/t/detecting-and-alerting-on-log-loss-from-logstash-or-beats/214482",
    "title": "Detecting and Alerting on Log Loss from Logstash or beats",
    "category": [
      "Logs"
    ],
    "author": "ciphee",
    "date": "January 9, 2020, 7:49pm January 9, 2020, 9:34pm January 10, 2020, 12:11am January 16, 2020, 5:04pm",
    "body": "Hello, I was trying to be able to detect/alert when either logstash or a beats product stops sendings logs. The problem in the past we have had is when a specific logstash server goes down, or is up but not sending any logs, we were unaware until we attempted to lookup logs coming from that specific server and did not have any new events coming in. Question: Is there any simple way to alert on a host when it stops sending logs to elastic after 24 hours or so?",
    "website_area": "discuss"
  },
  {
    "id": "e07aa483-7527-4434-9c92-a9338024159a",
    "url": "https://discuss.elastic.co/t/sending-log4j-logs-in-xml-format-to-elasticsearch/214387",
    "title": "Sending Log4j logs ( in XML format) to Elasticsearch",
    "category": [
      "Logs"
    ],
    "author": "JY_DT",
    "date": "January 9, 2020, 9:22am January 9, 2020, 10:21am January 9, 2020, 10:40am January 9, 2020, 10:43am January 16, 2020, 11:35am January 16, 2020, 1:38pm",
    "body": "Hello, I need to send log files generated using Log4j on client machines to Elasticsearch installed on a server. The logs are in XML format. Is there any other plugin, etc required, or can it just be done using Filebeat itself? As mentioned here: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-log4j.html \"This plugin is deprecated. It is recommended that you use filebeat to collect logs from log4j.\" So, how do we collect logs from log4j using Filebeat? Thanks, Jy",
    "website_area": "discuss"
  },
  {
    "id": "f11e34e1-c910-4707-a732-8844c0d02e31",
    "url": "https://discuss.elastic.co/t/message-failed-to-find-message-in-kibana-logs/210522",
    "title": "Message: \"failed to find message\" in Kibana Logs",
    "category": [
      "Logs"
    ],
    "author": "jmteba",
    "date": "December 10, 2019, 9:53am December 10, 2019, 11:03am December 10, 2019, 11:08am January 7, 2020, 11:08am",
    "body": "Hello, We have a problem when we try to see logs from Kibana \"Logs\", so we get this message: \"failed to find message\", failed_to_find_message|690x424 but in this path there are multiple log's files. failed_to_find_message_server|530x121 ! By the other hand, we see this logs in \"Discover\" failed_to_find_message_discover.png1045414 28.8 KB ! But it seems that there is some wrong in Kibana \"Logs\" I need to do something to achieve get my full logs here? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "3c2ae64c-d750-4804-9034-6d97157e3e19",
    "url": "https://discuss.elastic.co/t/how-to-see-the-surrounding-documents-in-kibana-v7-4-2-log-tab/208834",
    "title": "How to see the surrounding documents in kibana v7.4.2 log tab",
    "category": [
      "Logs"
    ],
    "author": "111244",
    "date": "November 21, 2019, 1:17pm November 21, 2019, 4:28pm November 25, 2019, 8:37am December 23, 2019, 8:43am",
    "body": "how to see the surrounding documents in kibana v7.4.2 log tab? i can't find this link image.png149492 17.3 KB",
    "website_area": "discuss"
  },
  {
    "id": "0dab9fce-7315-4b20-87d1-6c00e5aa0fa6",
    "url": "https://discuss.elastic.co/t/apache-nginx-logs-wrapped-in-syslog-format/207211",
    "title": "Apache/nginx logs wrapped in syslog format",
    "category": [
      "Logs"
    ],
    "author": "w_o_j_t_e_k",
    "date": "November 9, 2019, 12:04pm November 11, 2019, 8:57am November 11, 2019, 10:40am December 9, 2019, 10:40am",
    "body": "Hi, I've got a centralized log server that receives apache + nginx logs. I would like to use filebeat on this box to forward these logs into elastic. The logs are wrapped in the syslog format, e.q: 2019-11-09T11:50:56+00:00 foobar nginx_access: 202.18.3.162 - - [09/Nov/2019:11:50:56 +0000] \"GET / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\" so first columns are added by rsyslog therefore i can't use apache/nginx filebeat module Obviously I could change rsyslog format to exclude these 3 columns (ts, host, tag) but wondering if it's possible to pre-process on the filebeat level so I can use filebeat modules rather than writting my post processors Regards",
    "website_area": "discuss"
  },
  {
    "id": "7f531268-af77-4c4f-b2ca-87f1295417ad",
    "url": "https://discuss.elastic.co/t/logs-in-logstash-archive/205640",
    "title": "Logs in Logstash / archive",
    "category": [
      "Logs"
    ],
    "author": "juuuhuuu",
    "date": "October 29, 2019, 10:29am October 29, 2019, 11:04am October 30, 2019, 11:47am October 30, 2019, 12:24pm October 30, 2019, 12:24pm October 30, 2019, 12:24pm November 27, 2019, 12:24pm",
    "body": "Hello, Im new in elastic. I would like to know, for how long the logs are \"active \" in logstash. Can I archive them, if yes how? Thank you",
    "website_area": "discuss"
  },
  {
    "id": "78f98192-3117-4c81-849b-b0196afe7dda",
    "url": "https://discuss.elastic.co/t/logsui-in-kibana-7-4-0-getting-error-when-trying-to-look-at-the-logs-with-date-nanos/204335",
    "title": "LogsUI in kibana 7.4.0 getting error when trying to look at the logs with date_nanos",
    "category": [
      "Logs"
    ],
    "author": "Alex-St",
    "date": "October 20, 2019, 6:54am October 21, 2019, 11:33am October 23, 2019, 8:54pm October 23, 2019, 9:55pm November 20, 2019, 9:55pm",
    "body": "open logs UI - configure to use SourceTime which is in date_nanos instead of @timestamp field, logsUI tries to load data for several seconds, then displays graphics on the left side (timeline), and briefly shows the logs, but almost immediately causes the following error: Invariant Violation: Minified React error #185; visit https://reactjs.org/docs/error-decoder.html?invariant=185 for the full message or use the non-minified dev environment for full errors and additional helpful warnings. at ba (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:315) at x (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:584) at qf (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:96685) at Object.enqueueSetState (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:48105) at DatePicker.E.setState (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:192:1610) at DatePicker.componentDidUpdate (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:184:64661) at Vh (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:85102) at Zh (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:86620) at http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:99742 at Object.exports.unstable_runWithPriority (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:151:3314)",
    "website_area": "discuss"
  },
  {
    "id": "c1ed0f19-c549-46bc-9ae9-b3a6078f91be",
    "url": "https://discuss.elastic.co/t/unifying-log4j-2-layouts-and-ecslayout/197698",
    "title": "Unifying Log4j 2 layouts and EcsLayout",
    "category": [
      "Logs"
    ],
    "author": "vyazici",
    "date": "September 2, 2019, 2:00pm September 2, 2019, 3:46pm September 3, 2019, 9:10am September 3, 2019, 8:00pm September 6, 2019, 12:16pm September 9, 2019, 9:22am October 4, 2019, 12:56pm October 7, 2019, 6:41am October 10, 2019, 11:07am November 7, 2019, 11:21am",
    "body": "Dear fellow Elastic developers, I am the maintainer of the log4j2-logstash-layout project, the fastest and the only fully customizable JSON layout plugin for Log4j 2: LogstashLayout. A couple of days ago (2019-08-30) Felix Barnsteiner from Elastic announced a new project: java-ecs-logging. This is great news for the Java world since the development gap between the code and getting your logs stashed to Elasticsearch is closing even more. java-ecs-logging transforms Log4j1/Log4j2/Logback LogEvent's into JSON strings compatible with the Elastic Common Schema (ECS). That said, I find the re-invention of yet another Log4j 2 JSON layout (EcsLayout) partly sad and disappointing for a couple of reasons: LogstashLayout is already a battle-tested well-established solution used by hundreds of companies around the world. LogstashLayout has been enhanced with many feature requests/PRs provided by its users. Its design and existence does not emanate from hypothetical uses cases. LogstashLayout supports way more features and allows full schema customization where none is available in EcsLayout. One can perfectly support ECS in LogstashLayout by just changing the JSON schema, no plugin/software update is needed. Now there are 3 contenders in the market: JSONLayout, LogstashLayout, and EcsLayout. I have briefly benchmarked EcsLayout and LogstashLayout and observed that EcsLayout is ~0.5X faster for lite LogEvents and LogstashLayout is faster ~10X faster for full (stack trace, MDC, NDC, etc.) LogEvents. In each case LogstashLayout showed the lowest GC load. That said, do you really need something faster given LogstashLayout can render 793,597 LogEvent/sec on a single core? (I will share my findings once java-ecs-logging publishes an artifact to Maven Central.) EcsLayout doesn't have any dependencies. LogstashLayout depends on Jackson, though provides a fat JAR artifact along with every release. I would like to kindly ask you to consider a unifying approach rather than deprecating existing solutions with inferior new alternatives. Rather than coming up with a new Log4j 2 layout, IMHO, java-ecs-logging could have chosen (and can still chose!) one of the following paths: Contributing to LogstashLayout for the missing features, if there is any Using LogstashLayout within the project Relaying the Log4j 2 user base to LogstashLayout I would really appreciate a solution that would benefit both Elastic and its existing Log4j 2 users. I am open to any kind of collaboration along this direction and will be looking forward for your response. Best.",
    "website_area": "discuss"
  },
  {
    "id": "6a24a428-649b-40a5-b12c-dcdb69ad71d2",
    "url": "https://discuss.elastic.co/t/how-to-esclude-from-filebeat-this-log/202303",
    "title": "How to esclude from filebeat this log",
    "category": [
      "Logs"
    ],
    "author": "mirketto82",
    "date": "October 4, 2019, 8:31am October 6, 2019, 12:52pm October 7, 2019, 10:11am October 7, 2019, 11:08am November 4, 2019, 11:09am",
    "body": "hi all i want to exclude from filebeat this logs? at org.apache.camel.spring.spi.TransactionErrorHandler.process(TransactionErrorHandler.java:101) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.process(TransactionErrorHandler.java:114) Oct 4, 2019 @ 10:27:45 at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:542) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.processByErrorHandler(TransactionErrorHandler.java:220) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.doInTransactionTemplate(TransactionErrorHandler.java:176) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.processInTransaction(TransactionErrorHandler.java:136) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.process(TransactionErrorHandler.java:114) Oct 4, 2019 @ 10:27:45 at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:197) Oct 4, 2019 @ 10:27:45 at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:172) Oct 4, 2019 @ 10:27:45 at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:542) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.processByErrorHandler(TransactionErrorHandler.java:220) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler$1.doInTransactionWithoutResult(TransactionErrorHandler.java:183) Oct 4, 2019 @ 10:27:45 at org.springframework.transaction.support.TransactionCallbackWithoutResult.doInTransaction(TransactionCallbackWithoutResult.java:3 can you help me?",
    "website_area": "discuss"
  },
  {
    "id": "23f1c236-bb1e-4e8c-be79-b563d8115d60",
    "url": "https://discuss.elastic.co/t/java-ecs-logging/201266",
    "title": "Java-ecs-logging",
    "category": [
      "Logs"
    ],
    "author": "Rem",
    "date": "September 27, 2019, 8:30am September 26, 2019, 5:06pm September 27, 2019, 8:29am September 27, 2019, 3:49pm September 30, 2019, 6:49pm September 30, 2019, 7:02pm October 1, 2019, 10:21am October 29, 2019, 10:21am",
    "body": "Hi, Hope there is someone that can help me. Config.xml for Log4j2 : <Console name=\"LogToConsole\" target=\"SYSTEM_OUT\"> <Appenders> <Console name=\"LogToConsole\" target=\"SYSTEM_OUT\"> <EcsLayout> <KeyValuePair key=\"additionalField1\" value=\"constant value\"/> <KeyValuePair key=\"typeFromStructMsg\" value=\"${sd:msg}\"/> <KeyValuePair key=\"nameFromMapMsg\" value=\"${map:name}\"/> <KeyValuePair key=\"nameFromContext\" value=\"${ctx:test}\"/> <KeyValuePair key=\"mySysProperty\" value=\"${sys:mySysProperty}\"/> </EcsLayout> </Console> </Appenders> <Loggers> <Root> <AppenderRef ref=\"LogToConsole\" /> </Root> </Loggers> </Configuration> When I add value in a Map or in the ThreadContext, I always get \"labels.name\" or \"labels.test\" but I need the fields to be at the base of my Elastic Common Schema (ECS). How can I do it and is it thread safe? Thank you! Example of code : StringMapMessage mapMsg = new StringMapMessage(); mapMsg.put(\"name\", \"arun\"); logger.warn(mapMsg); or ThreadContext.put(\"test\", \"test\"); logger.info(\"any message\"); using this open source project : https://github.com/elastic/java-ecs-logging",
    "website_area": "discuss"
  },
  {
    "id": "c1514206-e040-46e9-bf63-8ec3f562b601",
    "url": "https://discuss.elastic.co/t/log-parsing-in-logstash/201313",
    "title": "Log parsing in logstash",
    "category": [
      "Logs"
    ],
    "author": "sk545",
    "date": "September 26, 2019, 9:17pm September 27, 2019, 11:07am September 27, 2019, 1:35pm September 27, 2019, 10:31pm October 25, 2019, 10:31pm",
    "body": "I am wondering , how can i parse below sample event into relevant fields in logstash ?currently i am not seeing any fields from this log in Kibana UI {\"COMPILATION_TIME\":40,\"DATABASE_ID\":19,\"DATABASE_NAME\":\"EEERD\",\"END_TIME\":\"2019-09-25 07:19:22.397 -0400\",\"EXECUTION_STATUS\":\"SUCCESS\",\"EXECUTION_TIME\":13,\"INBOUND_DATA_TRANSFER_BYTES\":0,\"OUTBOUND_DATA_TRANSFER_BYTES\":0,\"QUERY_ID\":\"018f1f27-030c-c5e0-0000-18a11974c28e\",\"QUERY_TAG\":\"\",\"QUERY_TEXT\":\"GRANT REFERENCES ON EEERD.WS_EDT_DATA_DEV.EDTCNTLPORTCYCBLZTMP3 to role EEER_DB_CATALOG;\",\"QUERY_TYPE\":\"GRANT\",\"QUEUED_OVERLOAD_TIME\":0,\"QUEUED_PROVISIONING_TIME\":0,\"QUEUED_REPAIR_TIME\":0,\"ROLE_NAME\":\"EEER_DB_DEPLOY\",\"SCHEMA_NAME\":\"INFORMATION_SCHEMA\",\"SESSION_ID\":27079812049690,\"START_TIME\":\"2019-09-25 07:19:22.314 -0400\",\"TOTAL_ELAPSED_TIME\":83,\"TRANSACTION_BLOCKED_TIME\":0,\"USER_NAME\":\"ABCZXGT\",\"WAREHOUSE_ID\":49,\"WAREHOUSE_NAME\":\"RE_TY_USER_RRR_WH\"}",
    "website_area": "discuss"
  },
  {
    "id": "40b5ea95-fd9b-414a-9cc3-6816b596d6c6",
    "url": "https://discuss.elastic.co/t/parse-syslog-ng-log-to-elasticsearch/197830",
    "title": "Parse syslog-ng log to elasticsearch",
    "category": [
      "Logs"
    ],
    "author": "ravipemmasani",
    "date": "September 3, 2019, 11:12am September 4, 2019, 6:27am September 4, 2019, 8:49am October 2, 2019, 8:49am",
    "body": "Hi All, Iam glad to join this group. Basically i'm in the midst of setting up POC for centralize log collection and analyse the log. Following is my setup on RHEL 7.6. Syslog-ng-collect log from remote clients Elasticsearch Kibana Need your help to provide sample config file to parse syslog-ng log to elasticsearch and create index pattern,really appreciate your help. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "d2ff26ed-a28c-4645-b815-fac0ebd3a661",
    "url": "https://discuss.elastic.co/t/filebeats-not-working-in-7-x-and-front-end-app-logging/197240",
    "title": "Filebeats not working in 7.x, and front end app logging",
    "category": [
      "Logs"
    ],
    "author": "ngg971",
    "date": "August 28, 2019, 11:42pm August 30, 2019, 9:22am September 27, 2019, 9:22am",
    "body": "Hi, Ive been using the ELK stack to capture our logs from our Kubernetes cluster. I had filebeats setup on the cluster and everything was working fine until I updated from 6.7 to 7.3 (I also updated the docker image that the filebeat DaemonSet was pulling from). When I initially made the update, all the logs ended up being pushed into one index, whereas before they were split by day and namespace. This is the configuration I have for the output to Elasticsearch and the index template: output.elasticsearch: hosts: ['<url>'] index: \"filebeat-%{[kubernetes.namespace]:default}-%{[agent.version]}-%{+yyyy.MM.dd}\" setup.template: name: \"filebeat-%{[kubernetes.namespace]:default}\" pattern: \"filebeat-%{[kubernetes.namespace]:default}-*\" However since then, the logs have stopped showing up all together, including in the newly created log index. I've searched and haven't found any other indices that match the index pattern filebeat-*. I cant see any errors in the Daemonset logs on Kubernetes so Im wondering where the output is getting dumped? As it is clearly not following the configuration above. On a separate note, what would be the easiest way to send logs from a React webapp into Elastic? Currently we are sending them to Sentry using POST requests. If we were to use filebeats, I assume the only way would be to store the logs locally, setup FB locally and have it read from them, or to send them over a UDP/TCP web socket and have FB read from there? I've seen other implementations that use Logstash as an intermediate but would rather not have to set it up for now. Thanks, Nathanael",
    "website_area": "discuss"
  },
  {
    "id": "71482de5-2e46-40b8-bfbe-0b84ae7745d5",
    "url": "https://discuss.elastic.co/t/how-i-can-send-logs-from-filebeat-to-elasticsearch-in-another-gke/193372",
    "title": "How I can send logs from filebeat to Elasticsearch in another GKE",
    "category": [
      "Logs"
    ],
    "author": "David_Oceans",
    "date": "August 1, 2019, 3:55pm August 8, 2019, 11:27am September 5, 2019, 11:27am",
    "body": "Hi! I have two GKE clusters in different GCP projects. GKE clusters: 1) GKE-ELASTIC (with kibana and elastic) 2) GKE-APPS (with my microservices) In the GKE-APPs I disabled the gke logging, because I want to redirect all logging to my \"external\"/dedicated gke-elastic. For that I installed the fluent-bit. Step: helm install --name fluentbit stable/fluent-bit I guest its the only thing I need to install in my GKE-Apps cluster right? but the point is find the right setup for the backend. backend: type: es es: host: 10.148.0.242 port: 9200 index: kubernetes_cluster type: flb_type logstash_prefix: kubernetes_cluster replace_dots: \"On\" time_key: \"@timestamp\" http_user: \"elastic\" http_passwd: \"vt8sv2zhrr2nbpkdgh422flc\" tls: \"off\" tls_verify: \"on\" tls_ca: \"\" tls_debug: 1 I try with the service name, with the clusterIP, but nothing... The error always is like that [2019/08/01 15:36:17] [ info] [filter_kube] API server connectivity OK [2019/08/01 15:36:17] [ info] [sp] stream processor started [2019/08/01 15:36:18] [error] [http_client] broken connection to 10.148.0.242:9200 ? [2019/08/01 15:36:18] [ warn] [out_es] http_do=-1 URI=/_bulk [2019/08/01 15:36:18] [error] [http_client] broken connection to 10.148.0.242:9200 ? [2019/08/01 15:36:18] [ warn] [out_es] http_do=-1 URI=/_bulk [2019/08/01 15:36:18] [error] [http_client] broken connection to 10.148.0.242:9200 ? Do you know what configuration should I try? Any suggestions will be appreciated! Thank you",
    "website_area": "discuss"
  },
  {
    "id": "e801c4b8-72d7-4566-b52f-2c155ca4181e",
    "url": "https://discuss.elastic.co/t/multiple-custom-grok-patterns-not-matching-but-they-successfully-match-alone/193175",
    "title": "Multiple custom grok patterns not matching, but they successfully match alone?",
    "category": [
      "Logs"
    ],
    "author": "kmiklas",
    "date": "July 31, 2019, 4:37pm August 5, 2019, 5:51pm August 6, 2019, 1:28pm September 3, 2019, 1:38pm",
    "body": "Grok matches single custom patterns, but does match when custom patterns are combined. Complete, working, an verifiable example Sample data: OK 05/20 20:12:10:067 ABC_02~~DE_02 FGH_IJK jsmith _A0011 Custom patterns: MMDD [0-1][0-9]/[0-3][0-9] THREAD _W\\w+ They work separately; specifically, this pattern works by itself: %{MMDD:mmdd} // Result { \"mmdd\": [ [ \"05/20\" ] ] } ... and this pattern works by itself: %{THREAD:thread} // Result { \"thread\": [ [ \"_A0011\" ] ] } ..but together, they fail: %{MMDD:mmdd} %{THREAD:keyword} No Matches Puzzling. Tyvm Keith :^) Note that I tried the solution presented in this post, but to no avail: Kibana Grok Debugger Multiple Custom Patterns Kibana Hi Lukas. Currently all I am trying to do is to put more than one custom grok pattern in the debugger. But for the purpose of resolving this issue I will provide you with a screenshot of what I have so far. [image] So as you can see above I have used a custom grok pattern to match the 'Host' section of the log. Below is what I would like to achieve, however I believe my syntax is incorrect in the 'Custom Grok Patterns' section. [image] cheers, G Also testing here: https://grokdebug.herokuapp.com/ Regex Resource: regex101.com Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript Regex101 allows you to create, debug, test and have your expressions explained for PHP, PCRE, Python, Golang and JavaScript. The website also features a community where you can share useful expressions.",
    "website_area": "discuss"
  },
  {
    "id": "8cf6d08f-02bd-4618-a46e-46f69b678be0",
    "url": "https://discuss.elastic.co/t/filebeat-docker-input/192657",
    "title": "Filebeat docker input",
    "category": [
      "Logs"
    ],
    "author": "mihaijulien",
    "date": "July 29, 2019, 9:27am July 29, 2019, 9:29am July 31, 2019, 9:15am August 1, 2019, 8:21am August 1, 2019, 8:21am August 29, 2019, 8:21am",
    "body": "Hello, I have the following filebeat.yml file: filebeat.config: modules: path: ${path.config}/modules.d/*.yml reload.enabled: false #================================ Logging ====================================== logging.level: debug logging.to_files: true logging.files: path: /var/log/filebeat name: filebeat keepfiles: 7 permissions: 0644 #----------------------------- Filebeat inputs -------------------------------- filebeat.inputs: - type: docker containers.ids: - \"*\" # - type: log # enabled: true # paths: # - '/var/lib/docker/containers/*/*.log' #----------------------------- Logstash output -------------------------------- output.logstash: hosts: [\"logstash:5044\"] There is nothing forwarded to logstash. A previous input that worked fine looked like this: filebeat.inputs: - type: log enabled: true paths: - /usr/share/filebeat/logs/server.log Nothing seems to work when is use type: docker in the input. The filebeat log looks like this: 2019-07-29T09:24:24.982Z INFO log/input.go:138 Configured paths: [/var/lib/docker/containers/*/*.log] 2019-07-29T09:24:24.982Z INFO input/input.go:114 Starting input of type: docker; ID: 8760737756938806393 2019-07-29T09:24:24.982Z DEBUG [cfgfile] cfgfile/reload.go:118 Checking module configs from: /usr/share/filebeat/modules.d/*.yml 2019-07-29T09:24:24.983Z DEBUG [input] log/input.go:174 Start next scan 2019-07-29T09:24:24.983Z DEBUG [cfgfile] cfgfile/reload.go:132 Number of module configs found: 0 2019-07-29T09:24:24.983Z INFO crawler/crawler.go:106 Loading and starting Inputs completed. Enabled inputs: 1 2019-07-29T09:24:24.983Z INFO cfgfile/reload.go:150 Config reloader started 2019-07-29T09:24:24.983Z DEBUG [input] log/input.go:195 input states cleaned up. Before: 0, After: 0, Pending: 0 2019-07-29T09:24:24.983Z DEBUG [cfgfile] cfgfile/reload.go:176 Scan for new config files 2019-07-29T09:24:24.983Z DEBUG [cfgfile] cfgfile/reload.go:195 Number of module configs found: 0 2019-07-29T09:24:24.983Z DEBUG [reload] cfgfile/list.go:62 Starting reload procedure, current runners: 0 2019-07-29T09:24:24.983Z DEBUG [reload] cfgfile/list.go:80 Start list: 0, Stop list: 0 2019-07-29T09:24:24.983Z INFO cfgfile/reload.go:205 Loading of config files completed. 2019-07-29T09:24:34.983Z DEBUG [input] input/input.go:152 Run input 2019-07-29T09:24:34.983Z DEBUG [input] log/input.go:174 Start next scan 2019-07-29T09:24:34.983Z DEBUG [input] log/input.go:195 input states cleaned up. Before: 0, After: 0, Pending: 0",
    "website_area": "discuss"
  },
  {
    "id": "e8124fea-d7f6-4fa2-a2be-7d607cb42532",
    "url": "https://discuss.elastic.co/t/log4net-implemenation-with-elastic-stack-7-2/191508",
    "title": "Log4net implemenation with elastic stack 7.2",
    "category": [
      "Logs"
    ],
    "author": "herzel",
    "date": "July 21, 2019, 8:42am July 22, 2019, 9:03am July 23, 2019, 11:10am August 20, 2019, 11:10am",
    "body": "Hi. Thanks a lot in advance for any reply. I have mission from my boss to apply elastic stack 7.2 with log4net. can not find any howto in the web. can anyone help with this issue. I need howto that will explain howto collect logs from log4net source pipeline through logstash over into redirecting the pipeline into elastic DB and there to be able to index the data. seems any help will be greatly appreciate.",
    "website_area": "discuss"
  },
  {
    "id": "045c3511-a37e-461f-9207-3bed6c14cf0e",
    "url": "https://discuss.elastic.co/t/grok-pattern-content-management/188162",
    "title": "Grok pattern / content management",
    "category": [
      "Logs"
    ],
    "author": "thedude",
    "date": "June 30, 2019, 1:32am June 30, 2019, 1:46am July 28, 2019, 1:47am",
    "body": "Is there any ability in the platform today to build out grok patterns within the UI and distribute them to filebeats agents or push these into updated logstash pipelines? If not, could this be a feature request? Would be neat to interactively build out grok patterns in the UI, and distribute to various collectors/processors with a single click.",
    "website_area": "discuss"
  },
  {
    "id": "28ee5a7f-e539-4392-a92b-c79da300247c",
    "url": "https://discuss.elastic.co/t/upload-my-logs-from-node-to-elasticsearch/187003",
    "title": "Upload my logs from node to ElasticSearch",
    "category": [
      "Logs"
    ],
    "author": "Shahar-Y",
    "date": "June 23, 2019, 11:43am June 23, 2019, 11:54am June 24, 2019, 10:25am June 25, 2019, 6:33am July 22, 2019, 5:56pm",
    "body": "Hi, I'm new to ElasticSearch. I want my application to upload its logs while it's running - to my ElasticSearch server. My application is using node (typescript), and I can't understand how to upload a JSON log I created to the server. Do I have to create my own log file and only then send it to the ES server? Please help",
    "website_area": "discuss"
  },
  {
    "id": "b9338f99-6067-4b82-9d33-0d906b68ff9d",
    "url": "https://discuss.elastic.co/t/support-multiple-sources/184117",
    "title": "Support multiple sources",
    "category": [
      "Logs"
    ],
    "author": "tomeri",
    "date": "June 4, 2019, 8:41am June 4, 2019, 9:00am July 2, 2019, 9:00am",
    "body": "Hi, I don't know if this already implemented, but I think it'll be helpful to add an option for configuring multiple sources then display them as a dropdown for quick navigation between the different sources without the need of re-editing the current source every time. I use Kibana 6.7.1 Thanks,",
    "website_area": "discuss"
  },
  {
    "id": "43228f82-6144-4c99-9b67-583a4c3fdd9a",
    "url": "https://discuss.elastic.co/t/kibana-logs-are-not-in-order-6-7-version/181887",
    "title": "Kibana logs are not in order 6.7 version",
    "category": [
      "Logs"
    ],
    "author": "Rampsrr",
    "date": "May 28, 2019, 1:41am May 28, 2019, 12:57pm June 25, 2019, 1:07pm",
    "body": "Filebeat-- logstash - - elastic search - - kibana Version all 6.7 It is not working both scenario #10005 ticket, since in real time i have more log lines of 10 to 15 have same @timestamp with respective to seconds but offset different, so i have created new number filed in logstash with combined @timestamp and offset together... After that In kibana discover tab i can able to see the new number field with unique number.. But in kibana logs ui, it stopped streaming if i try to order by newly created number field. Or Any possibility to combine @timestamp, offset in kibana logs UI tab.. Because i tried multiple combination but failed.",
    "website_area": "discuss"
  },
  {
    "id": "717edbe4-e311-4f15-8ca1-94ab7ac0288a",
    "url": "https://discuss.elastic.co/t/configure-multiple-servers-logs-into-single-kibana-dashboard/180931",
    "title": "Configure multiple servers logs into single kibana dashboard",
    "category": [
      "Logs"
    ],
    "author": "pavansai",
    "date": "May 14, 2019, 6:59am May 14, 2019, 7:32am May 14, 2019, 9:08am May 14, 2019, 10:59am May 14, 2019, 11:42am May 14, 2019, 12:10pm May 15, 2019, 5:46am May 15, 2019, 5:48am May 15, 2019, 10:28am May 15, 2019, 5:13pm May 15, 2019, 12:17pm May 15, 2019, 5:14pm May 16, 2019, 8:55am May 20, 2019, 6:24am May 20, 2019, 11:32am May 21, 2019, 8:23am June 18, 2019, 8:23am",
    "body": "Please assist how to configure multiple servers logs into single kibana dashboard",
    "website_area": "discuss"
  },
  {
    "id": "73f40992-650c-4ddf-8b7a-bd77b24aad61",
    "url": "https://discuss.elastic.co/t/logs-ui-along-with-spaces/177212",
    "title": "Logs UI along with spaces",
    "category": [
      "Logs"
    ],
    "author": "sl1729",
    "date": "April 17, 2019, 9:51am April 18, 2019, 3:10pm April 22, 2019, 7:31am April 23, 2019, 8:39am April 24, 2019, 3:31am May 8, 2019, 2:23pm June 5, 2019, 2:23pm",
    "body": "I created a space called 'dev' and then created objects like index-pattern, visualization and dashboards etc .. And then a role with read permission to that space and read access to the related indexes. Created a user 'dev-user' under that role. But now when I login as admin [navigate to the dev space], I am able to view the the logs in Logs UI. But when login as dev-user [navigate to the dev space], I am only seeing message \" Looks like you don't have any metrics indices.\". Even though I am able to view the dashboards without problem with all visualizations. How can I provide dev-user access to view the logs in Logs-UI ?.",
    "website_area": "discuss"
  },
  {
    "id": "23ec2807-f2ce-43c0-b6a4-c6ab0fb583fb",
    "url": "https://discuss.elastic.co/t/roles-related-to-infra-ui/176129",
    "title": "Roles related to infra UI",
    "category": [
      "Logs"
    ],
    "author": "sl1729",
    "date": "April 10, 2019, 2:58am April 14, 2019, 10:42pm April 15, 2019, 1:50am April 15, 2019, 2:40pm April 17, 2019, 7:21am May 15, 2019, 7:21am",
    "body": "We are experimenting infra UI and logs UI. Going good so far, except few known issues which are getting discussed in forum. But when providing access to the users we are having difficulty. We have a scenario where we need to provide users read access for dashboard and ( logs UI or infra UI ). How can we setup this role and provide user no access to other menus.",
    "website_area": "discuss"
  },
  {
    "id": "97e5d718-5d09-484f-a631-8890dcfdf588",
    "url": "https://discuss.elastic.co/t/stream-logs-to-elastic-search-from-fastly/176217",
    "title": "Stream logs to elastic search from Fastly",
    "category": [
      "Logs"
    ],
    "author": "soerenfrisk",
    "date": "April 10, 2019, 12:24pm April 11, 2019, 8:46am April 11, 2019, 10:49am April 12, 2019, 1:38pm May 10, 2019, 1:38pm",
    "body": "I'm trying to find a way to stream logs from fastly.com to an elastic cloud service. According to Fastly's docs you are only able to do this through Logstash (which is not included in the cloud service). It seems impractical to have and manage a logstash instance for receiving logs from one source. Anyone have tried to successfully stream logs from Fastly to Elastic search directly? It seems the only way to stream logs to elastic is through beats.",
    "website_area": "discuss"
  },
  {
    "id": "02815068-6103-4a57-b248-ba2de407feef",
    "url": "https://discuss.elastic.co/t/capturing-logs-from-a-browser-spa/175159",
    "title": "Capturing logs from a browser (SPA)",
    "category": [
      "Logs"
    ],
    "author": "Matt_Russell",
    "date": "April 3, 2019, 10:13am April 5, 2019, 8:58am May 3, 2019, 9:08am",
    "body": "I wondered if anyone had any good patterns or advice on collecting logs from a single-page application (SPA) running in user browsers and getting them into Elasticsearch? I guess one approach might be to post AJAX to a /logs endpoint on our backend.",
    "website_area": "discuss"
  },
  {
    "id": "08e173a2-d8ab-472c-bab5-0e8bd1da3bab",
    "url": "https://discuss.elastic.co/t/setting-up-logs-functionality-with-fluentd/169923",
    "title": "Setting up logs functionality with fluentd",
    "category": [
      "Logs"
    ],
    "author": "Erik_Tribou",
    "date": "February 26, 2019, 2:48am February 26, 2019, 12:53pm February 26, 2019, 11:13pm March 23, 2019, 9:23am April 3, 2019, 1:07pm May 1, 2019, 1:07pm",
    "body": "I'm using elasticsearch and kibana 6.5.1 and trying to get the logs functionality working with logs we are inserting via fluentd. Unfortunately all I'm getting is message that no logs can be found and to adjust my filter. I'm seeing some action on the timeline at the right hand of the screen, but nothing ever shows when I set logs to start streaming or do searches for messages I know are there. I can see new messages through the discover module of Kibana, but nothing ever shows up in the logs module. Any ideas on what could be wrong with the configuration? Relevant entries from kibana.yml: xpack.infra.sources.default.logAlias: 'fluentd-*' xpack.infra.sources.default.fields.message: ['msg', 'MESSAGE'] xpack.infra.sources.default.fields.host: '_HOSTNAME' xpack.infra.sources.default.fields.container: 'container_name' xpack.infra.sources.default.fields.pod: 'pod_name' xpack.infra.sources.default.fields.tiebreaker: '_SOURCE_REALTIME_TIMESTAMP' An example of the source field (json view in kibana) from one of our records (redacting a few fields): \"_index\": \"fluentd-2019.02.25\", \"_type\": \"fluentd\", \"_id\": \"7lARJ2kBLWoRN3y-8Wpv\", \"_version\": 1, \"_score\": null, \"_source\": { \"PRIORITY\": \"6\", \"_PID\": \"62984\", \"_UID\": \"0\", \"_GID\": \"0\", \"_COMM\": \"dockerd-current\", \"_EXE\": \"/usr/bin/dockerd-current\", \"_CMDLINE\": \"blah blah cmdline\", \"_CAP_EFFECTIVE\": \"1fffffffff\", \"_SYSTEMD_CGROUP\": \"/system.slice/docker.service\", \"_SYSTEMD_UNIT\": \"docker.service\", \"_SYSTEMD_SLICE\": \"system.slice\", \"_BOOT_ID\": \"a4a22bc8790b4ad1a666e4446f60cc06\", \"_MACHINE_ID\": \"955d8fb4b61d4db3ad878e45e9008bcc\", \"_HOSTNAME\": \"blahblah.com\", \"CONTAINER_ID\": \"fd1f537b429f\", \"CONTAINER_ID_FULL\": \"fd1f537b429fb51eca428b60334c93689029b7ed24bbf471f2999b62d013353f\", \"CONTAINER_NAME\": \"blah-blah-container-name\", \"CONTAINER_TAG\": \"\", \"_TRANSPORT\": \"journal\", \"_SELINUX_CONTEXT\": \"system_u:system_r:container_runtime_t:s0\", \"MESSAGE\": \"{\\\"@timestamp\\\":\\\"2019-02-25T15:50:43.046-08:00\\\",\\\"msg\":\\\"Clearing decision cache\\\",\\\"logger_name\\\":\\\"com.blahblah.fraud_analysis.cache.DecisionCacheHolder\\\",\\\"thread_name\\\":\\\"scheduling-1\\\",\\\"level\\\":\\\"INFO\\\",\\\"level_value\\\":20000,\\\"traceId\\\":\\\"2181d65dcf43ebbe\\\",\\\"spanId\\\":\\\"2181d65dcf43ebbe\\\",\\\"spanExportable\\\":\\\"true\\\",\\\"app\\\":\\\"siftscience\\\"}\", \"_SOURCE_REALTIME_TIMESTAMP\": \"1551138643047036\", \"name_prefix\": \"k8s\", \"container_name\": \"sift-science-30162-ce2b1acd\", \"pod_name\": \"sift-science-30162-ce2b1acd-1-z4g36\", \"namespace\": \"sift-science\", \"@timestamp\": \"2019-02-25T15:50:43.046-08:00\", \"msg\": \"Clearing decision cache\", \"logger_name\": \"com.blahblah.fraud_analysis.cache.DecisionCacheHolder\", \"thread_name\": \"scheduling-1\", \"level\": \"INFO\", \"level_value\": 20000, \"traceId\": \"2181d65dcf43ebbe\", \"spanId\": \"2181d65dcf43ebbe\", \"spanExportable\": \"true\", \"app\": \"siftscience\", \"syzygy\": \"syzygy44\" } Attaching what I see in the logs module: image.png17371324 166 KB",
    "website_area": "discuss"
  },
  {
    "id": "d4cde9f7-8654-4d3a-801a-00164d9184d9",
    "url": "https://discuss.elastic.co/t/sebp-elk-with-pfsense/173576",
    "title": "Sebp/ELK with PFSense",
    "category": [
      "Logs"
    ],
    "author": "stinkfly",
    "date": "March 23, 2019, 5:50am March 26, 2019, 11:09am March 27, 2019, 4:04am March 27, 2019, 11:11am April 2, 2019, 12:03am April 30, 2019, 12:03am",
    "body": "Hi team, I've setup sebp/ELK (https://elk-docker.readthedocs.io/), GitHub here https://hub.docker.com/r/sebp/elk/ with ELK 6.6.1 Winlogbeat and Metricbeat work ok sending from a Windows 2016 server Syslog from PFSense router does not receive any data. However, when I use a physical Ubuntu server with Logstash (with the same conf file) and Outputting to the Elasticsearch server running on the sebp/ELK it works fine The documentation on sebp site suggests to use Filebeat as a \"forwarding agent\" Q: Why does a physical server work and why does this image require a forwarding agent. Prefer not to install filebeat on PFsense if I don't have to - just use the GUI to point to IP:port Conf file looks like the following input { beats { port => 5044 } syslog { port => 5144 } } filter { if [type] == \"syslog\" { grok { match => { \"message\" => \"<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\\[%{POSINT:syslog_pid}\\])?: %{GREEDYDATA:syslog_message}\" } } date { match => [ \"syslog_timestamp\", \"MMM d HH:mm:ss\", \"MMM dd HH:mm:ss\" ] } } } output { elasticsearch { hosts => [\"localhost:9200\"] index => \"%{[@metadata][beat]}-%{+YYYY.MM.DD}\" document_type => \"%{[metadata][type]}\" } stdout { codec => rubydebug } }",
    "website_area": "discuss"
  },
  {
    "id": "d4ceae21-5dc6-4dc2-aae0-b436defca9f2",
    "url": "https://discuss.elastic.co/t/how-to-collecting-all-elasticsearch-clusters-monitoring-data-to-one-elasticsearch-using-logstash-or-metricbeat/174025",
    "title": "How to collecting all elasticsearch clusters monitoring data to one elasticsearch using logstash or metricBeat",
    "category": [
      "Logs"
    ],
    "author": "Kevins_Si",
    "date": "March 27, 2019, 1:11am March 27, 2019, 3:17pm March 28, 2019, 2:25pm April 25, 2019, 2:25pm",
    "body": "is there any info that collecting all elasticsearch clusters monitoring data to one elasticsearch using logstash or metricBeat?? we want to collecting many clusters monitoring log data to one. and keep as normal view as kibana. as metrics, index changing and so on.",
    "website_area": "discuss"
  },
  {
    "id": "033bc25a-a057-4cae-a54d-52e95a9ed2b4",
    "url": "https://discuss.elastic.co/t/kibana-maps-not-showing-locations/172331",
    "title": "Kibana maps not showing locations",
    "category": [
      "Logs"
    ],
    "author": "sc1",
    "date": "March 14, 2019, 12:02pm March 15, 2019, 10:08am March 15, 2019, 11:52am March 15, 2019, 3:52pm March 19, 2019, 10:24pm March 20, 2019, 8:27am March 20, 2019, 12:25pm March 20, 2019, 12:58pm March 20, 2019, 4:47pm March 26, 2019, 4:33pm March 26, 2019, 4:33pm April 23, 2019, 4:33pm",
    "body": "Hello, I have noticed that any map I have on a dashboard isn't showing the geolocations. For example, the sample dashboards from Filebeat showing attempted SSHs loads the dashboards, but it doesn't plot where any of the failed attempts are. I have no idea why this isn't working. I have reindexed the indexes as I saw on previous threads. Could someone direct me to what I need to send to get assistance ?",
    "website_area": "discuss"
  },
  {
    "id": "5664192a-049f-4d46-be22-f9b547248f48",
    "url": "https://discuss.elastic.co/t/need-help-with-parsing-json-fields/172686",
    "title": "Need help with parsing json fields",
    "category": [
      "Logs"
    ],
    "author": "Adrian_Hove",
    "date": "March 17, 2019, 5:29pm March 18, 2019, 4:39pm March 18, 2019, 4:38pm March 18, 2019, 4:47pm April 15, 2019, 4:47pm",
    "body": "I have a sweet log I am trying to parse into JSON. [2019-03-16 00:00:00] production.INFO {\"timestamp\":1552694400,\"execution_time\":0.0272369384765625} my default logstash config input { beats { port => 5044 } } filter { grok { match => { \"message\" => \"\\[%{TIMESTAMP_ISO8601:timestamp}\\] %{DATA:env}\\.%{DATA:severity}: %{GREEDYDATA:message}\"} } json { source => \"message\" } } output { elasticsearch { hosts => [ \"elasticsearch:9200\" ] } } Lastly I have this in filebeats yml processors: - decode_json_fields: fields: [\"message\"] process_array: false max_depth: 1 target: \"\" overwrite_keys: false When i reindex my logs, no json parsing is done. What am I missing.",
    "website_area": "discuss"
  },
  {
    "id": "6275ca00-ab80-40ec-9747-4441bb8cb4b6",
    "url": "https://discuss.elastic.co/t/create-filter/172282",
    "title": "Create filter",
    "category": [
      "Logs"
    ],
    "author": "nejmeddine_ammar",
    "date": "March 22, 2019, 10:50am March 15, 2019, 10:16am March 18, 2019, 9:21am March 18, 2019, 11:56am April 15, 2019, 11:56am",
    "body": "i 'am new in ELK , can you help me to create grok for the data",
    "website_area": "discuss"
  },
  {
    "id": "8e5e099a-8da7-4e89-a61f-d74224dfc383",
    "url": "https://discuss.elastic.co/t/filebeat-not-forwarding-my-iis-logs-to-elastic-search/170972",
    "title": "FileBeat not forwarding My IIS logs to elastic search",
    "category": [
      "Logs"
    ],
    "author": "syedsfayaz",
    "date": "March 11, 2019, 1:35pm March 5, 2019, 6:42pm March 6, 2019, 3:26pm March 6, 2019, 3:27pm March 6, 2019, 7:16pm March 6, 2019, 7:30pm March 7, 2019, 3:34pm April 4, 2019, 3:34pm",
    "body": "Hi Guys I am very new to Elastic stack. I am trying to setup a dashboard to monitor IIS logs. But the file beats is not working as expected. Here is my configuration. Installed Version:6.6.1 Kibana, Elastic Search, Filebeat with IIs module enabled on my local machine. The only different thing am doing here is I copied logs from my production server to my local machines and pointed file beat to that directory. But i am not seeing data on Kibana or elastic search. Please suggest me what Is wrong in my configuration file. Here is my configuration for filebeat.yml. filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: false # Paths that should be crawled and fetched. Glob based paths. paths: # - E:/Share/W3SVC2/*.log #- /var/log/*.log #- c:\\programdata\\elasticsearch\\logs\\* # matching any regular expression from the list. exclude_lines: ['^#'] #============================= Filebeat modules =============================== filebeat.config.modules: # Glob pattern for configuration loading path: \"C:/Program Files/filebeat/modules.d/*.yml\" # Set to true to enable config reloading reload.enabled: true # Period on which files under path should be checked for changes reload.period: 10s #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 3 #index.codec: best_compression #_source.enabled: false #============================== Kibana ===================================== setup.kibana: # Kibana Host host: \"localhost:5601\" # Kibana Space ID #space.id: #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"localhost:9200\"] # Enabled ilm (beta) to use index lifecycle management instead daily indices. #ilm.enabled: false # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ IIS module configuration - module: iis # Access logs access: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: [\"E:/Share/W3SVC2/*.log\"] # Error logs error: enabled: false # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. #var.paths:",
    "website_area": "discuss"
  },
  {
    "id": "4b0a2e7e-5c82-429f-973c-3c001c315c7a",
    "url": "https://discuss.elastic.co/t/error-on-the-logs-page-in-kibana-on-elastic-cloud-hosted-on-aws/169261",
    "title": "Error on the Logs page in Kibana on Elastic cloud hosted on AWS",
    "category": [
      "Logs"
    ],
    "author": "ajazam1",
    "date": "February 20, 2019, 6:08pm February 20, 2019, 6:17pm February 21, 2019, 9:26am March 21, 2019, 9:26am",
    "body": "We are using the Elastic Cloud hosted solution on AWS at version 6.6.1. We are using filebeats 6.6.0 and the IIS module to parse IIS 10 logs into Elastic Search. When we look at the logs page we are seeing many failed to format message from c:\\inetpub\\logs\\Logfiles\\W3SVC3\\u_ex190220.log lines. This error appears at https://github.com/elastic/kibana/blob/master/x-pack/plugins/infra/server/lib/domains/log_entries_domain/builtin_rules/index.ts in the source code. My typescript isn't good enough for me to trace the problem. Does anybody know if we are doing anything wrong or is there a bug in Kibana?",
    "website_area": "discuss"
  },
  {
    "id": "9fd3de27-9fb4-48fa-a3e2-7574190115d1",
    "url": "https://discuss.elastic.co/t/fluent-bit-filter-for-by-pods-calico-pod-in-kubernetes/168975",
    "title": "Fluent Bit Filter for by PODs (calico-pod) in kubernetes?",
    "category": [
      "Logs"
    ],
    "author": "JDev",
    "date": "February 19, 2019, 9:25am February 19, 2019, 9:32am February 19, 2019, 10:31am March 19, 2019, 10:31am",
    "body": "Hi, I installed fluentbit with default settings. I like that the fluent bit adds additional information (the name of the container). But he writes in the index everything. How do I do better? How to add a filter so that it takes all the logs, except for example the logs from the calico pod. Here is the default Filter. fluent-bit-filter.conf: [FILTER] Name kubernetes Match kube.* Kube_URL https://kubernetes.default.svc:443 Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token Merge_Log On K8S-Logging.Parser On K8S-Logging.Exclude On Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "cf53b3c5-613b-420e-a6bd-835fe9c157f1",
    "url": "https://discuss.elastic.co/t/automated-log-search-and-reporting/168447",
    "title": "Automated log search and reporting",
    "category": [
      "Logs"
    ],
    "author": "",
    "date": "February 14, 2019, 4:02pm February 14, 2019, 5:06pm February 14, 2019, 6:20pm March 14, 2019, 6:18pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1712766b-c864-4a4a-b711-f9ba1fab4323",
    "url": "https://discuss.elastic.co/t/does-logs-ui-infrastructure-ui-supported-under-cross-cluster-search/167749",
    "title": "Does Logs UI / Infrastructure UI supported under Cross Cluster Search?",
    "category": [
      "Logs"
    ],
    "author": "tomeri",
    "date": "February 10, 2019, 12:13pm February 11, 2019, 10:25am March 11, 2019, 10:25am",
    "body": "I tried to set xpack.infra.sources.default.logAlias within the Kibana configuration to something like MyClusterA:MyIndexAlias, in order to point the Logs UI to a remote index, but nothing showing under the Logs/Infrastructure UIs in Kibana. I use Elastic stack 6.6.0 Thanks,",
    "website_area": "discuss"
  },
  {
    "id": "08a91a3e-dd8e-4258-be33-3a45d93a6695",
    "url": "https://discuss.elastic.co/t/logs-module/167449",
    "title": "Logs module",
    "category": [
      "Logs"
    ],
    "author": "asp",
    "date": "February 8, 2019, 8:38pm February 8, 2019, 8:49pm March 8, 2019, 8:49pm",
    "body": "Hi all, We have a lot of long multiline logs which are outputting a lot of information like stacktraces, error hints, request body, response body, return codes, etc. So way to much to be really readable in a single message field. We are currently updating our source logfiles to be json structured, with many fields. The discovery panel is not comfortably usable for this, because the field payload may have 30 lines, a stacktrace can bekome 100+ lines, etc. Kibana is only showing the first few lines of an of a field, then it is truncating the rest. We want to use elastics stack to search the log data of many application hosts (big cluster). I have quite a good knowledge of building KPI aggregations and build useful visualizations for monitoring. But in my new project the main usecase will be combine logs from different application cluster nodes and search in them to track down errors in complex multi node, multi log environments. I updated my dev elastic stack to 6.5.4 and checked out the new logs-module. The view of timestamp and message field looks good, but I need to be able to show the content of one or more fields. And I need to change the fields on runtime via the GUI. Is there any way to do so? At indexing time I am not able to know, if I need to show the stack trace, the request or response body, or what field ever. So I need to change it on displaying time. Is there a way to do it or is it planed to do so? Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "9ffea2ec-266f-48a6-8fae-d08c393cc3db",
    "url": "https://discuss.elastic.co/t/can-we-add-syntax-color-to-logs-module/163753",
    "title": "Can we add syntax color to Logs module?",
    "category": [
      "Logs"
    ],
    "author": "Gael_RICHIER",
    "date": "January 10, 2019, 2:05pm January 10, 2019, 3:12pm January 10, 2019, 3:36pm January 10, 2019, 4:14pm February 1, 2019, 5:35am February 28, 2019, 6:22am",
    "body": "Hello everybody ! I'll like to know if we can set color syntax for log through logs module ? Example for syslog or auth log. (Error => RED / WARNING => ORANGE ....) Thanks",
    "website_area": "discuss"
  },
  {
    "id": "320f5b89-42ff-4783-bc68-d7741473ab6f",
    "url": "https://discuss.elastic.co/t/logs-ui-selectable-message-fields/166051",
    "title": "Logs UI - Selectable message fields?",
    "category": [
      "Logs"
    ],
    "author": "ceekay",
    "date": "January 29, 2019, 11:23am January 29, 2019, 11:20am January 29, 2019, 11:22pm February 26, 2019, 11:22pm",
    "body": "I'd like to use the Logs UI for a bit more than plain syslog (e.g., Apache all the logs) however I can't get it to display anything other than a plain message field. This is not ideal when displaying non-syslog logs for more than one host as it's impossible to tell the source of an entry in the Logs UI if it doesn't contain the hostname. Even if you do identify an interesting log entry, you can't do anything with it as there's no interaction in the UI, so you need to go and find it again in the Discover tab. I've tried playing with the setting xpack.infra.sources.default.fields.message: ['message', '@message'] by adding extra fields but it doesn't have any effect whatsoever on the interface. In fact, this setting appears to be hard-coded, as even completely replacing the default fields does nothing to the interface (unless I'm misunderstanding its purpose). So, is it possible to customise the output at all? Ideally I'd like to be able to set up a table of sorts, e.g., [timestamp], [hostname], [severity], [message] or variations on this depending on the log type. Perhaps a setting like xpack.infra.sources.default.fields.columns would allow people to format their own output by providing a list of event fields? Also, are there any plans to add interactivity to this interface, (i.e., clickable log entries to create filters, etc.) At the moment it just seems to be a big combined tail of everything.",
    "website_area": "discuss"
  },
  {
    "id": "6a3affe7-8965-42a3-b478-eb28c919deec",
    "url": "https://discuss.elastic.co/t/how-to-use-the-log-tail-feature-in-kibana-6-5-4/163658",
    "title": "How to use the log tail feature in Kibana 6.5.4",
    "category": [
      "Logs"
    ],
    "author": "akhisar",
    "date": "January 29, 2019, 7:24am January 10, 2019, 6:44am January 17, 2019, 7:16am January 29, 2019, 11:40am February 26, 2019, 11:40am",
    "body": "Hello All, I am using fluentd as my log shipper for kubernetes microservices. I have read that the new kibana version have the log tailing feature for viewing the changes in the logs. Can someone guide me how it can work with a fluentd shipper!! Currently I am using the logtrail pluggin for this purpose. Rgds, -Akhil",
    "website_area": "discuss"
  },
  {
    "id": "60f5c7fa-7d0e-4bb9-8a6c-f1901f419739",
    "url": "https://discuss.elastic.co/t/failed-to-format-message-from/165728",
    "title": "Failed to format message from *",
    "category": [
      "Logs"
    ],
    "author": "Code",
    "date": "January 25, 2019, 12:52pm January 25, 2019, 12:57pm January 28, 2019, 3:25am January 28, 2019, 3:26am January 28, 2019, 11:28am February 25, 2019, 11:28am",
    "body": "a.PNG1307602 126 KB I want to see the detail from my server's IIS logs , and I found that kibana cannot format the information of IIS logs. Pls tell what measure should i do ,so I can see the logs' detail. thank you",
    "website_area": "discuss"
  },
  {
    "id": "9db18fb7-6da0-426b-aa0a-03868818c2f7",
    "url": "https://discuss.elastic.co/t/filebeat-fails-to-process-kibana-json-logs-failed-to-format-message-from-json-log-in-a-kubernetes-enviroment/163558",
    "title": "Filebeat fails to process kibana json logs \"failed to format message from *json-.log \"in a kubernetes enviroment",
    "category": [
      "Logs"
    ],
    "author": "paltaa",
    "date": "January 9, 2019, 3:03pm January 9, 2019, 5:49pm January 9, 2019, 5:55pm January 9, 2019, 6:01pm January 9, 2019, 6:32pm January 9, 2019, 6:30pm January 9, 2019, 6:32pm January 9, 2019, 6:38pm January 9, 2019, 6:39pm January 9, 2019, 6:48pm January 9, 2019, 7:11pm January 9, 2019, 7:15pm January 9, 2019, 7:20pm January 9, 2019, 7:24pm January 9, 2019, 7:28pm January 9, 2019, 7:32pm January 24, 2019, 2:21pm February 21, 2019, 2:21pm",
    "body": "So ive mounted ELK stack with filebeat in a kubernetes enviroment, im parsing all the logs correctly, only problem is the kibana json-logs format that get error failed to format message from /var/lib/docker/containers/b685d94ec5e83c08cbe7728bcc9ebc3827cf2015c25490fb8d62e5c16c12b8ba/b685d94ec5e83c08cbe7728bcc9ebc3827cf2015c25490fb8d62e5c16c12b8ba-json.log So did a kubectl describe pods and realized that docker container was kibana. Version 6.5.2 Filebeat configuration: --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: kube-system labels: k8s-app: filebeat data: filebeat.yml: |- filebeat.config: inputs: path: ${path.config}/inputs.d/*.yml reload.enabled: false modules: path: ${path.config}/modules.d/*.yml reload.enabled: false processors: - add_cloud_metadata: - drop_fields: when: has_fields: ['kubernetes.labels.app'] fields: - 'kubernetes.labels.app' output.elasticsearch: hosts: ['http://elasticsearch.whitenfv.svc.cluster.local:9200'] --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-inputs namespace: kube-system labels: k8s-app: filebeat data: kubernetes.yml: |- - type: docker json.keys_under_root: false json.add_error_key: false json.ignore_decoding_error: true containers.ids: - \"*\" processors: - add_kubernetes_metadata: in_cluster: true --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: filebeat namespace: kube-system labels: k8s-app: filebeat spec: template: metadata: labels: k8s-app: filebeat spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 containers: - name: filebeat image: {{ filebeat_image_full }} args: [ \"-c\", \"/etc/filebeat.yml\", \"-e\", ] securityContext: runAsUser: 0 resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: inputs mountPath: /usr/share/filebeat/inputs.d readOnly: true - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: inputs configMap: defaultMode: 0600 name: filebeat-inputs - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: filebeat subjects: - kind: ServiceAccount name: filebeat namespace: kube-system roleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: filebeat labels: k8s-app: filebeat rules: - apiGroups: [\"\"] resources: - namespaces - pods verbs: - get - watch - list --- apiVersion: v1 kind: ServiceAccount metadata: name: filebeat namespace: kube-system labels: k8s-app: filebeat",
    "website_area": "discuss"
  },
  {
    "id": "f3a8b296-4da6-4f56-ba71-3e4ce7b6638d",
    "url": "https://discuss.elastic.co/t/how-to-charge-for-three-elk-software/165507",
    "title": "How to charge for three elk software",
    "category": [
      "Logs"
    ],
    "author": "wcfCode",
    "date": "January 24, 2019, 1:29am January 24, 2019, 1:30am January 24, 2019, 1:31am January 24, 2019, 1:34am January 24, 2019, 1:37am February 21, 2019, 1:38am",
    "body": "How to charge for three elk software",
    "website_area": "discuss"
  },
  {
    "id": "3cfc2b4b-a6c9-4058-b9e0-4d874b3e6d33",
    "url": "https://discuss.elastic.co/t/how-to-set-a-range-in-logs-ui/164244",
    "title": "How to set a range in Logs UI",
    "category": [
      "Logs"
    ],
    "author": "makefriend7",
    "date": "January 17, 2019, 1:51pm January 17, 2019, 3:11pm January 18, 2019, 5:14am January 18, 2019, 8:41am February 15, 2019, 8:41am",
    "body": "In es we can do this GET _search { \"_source\":[\"message\"], \"query\": { \"range\" : { \"my.time\": { \"gte\" : \"20190113-09:15:57\", \"lte\" : \"20190114-10:15:57\" } } } } Is it possible in Logs UI?",
    "website_area": "discuss"
  },
  {
    "id": "1e80ef61-791e-4574-be12-255b5ba11e6e",
    "url": "https://discuss.elastic.co/t/elk-6-5-4/163920",
    "title": "ELK 6.5.4",
    "category": [
      "Logs"
    ],
    "author": "Denisboud",
    "date": "January 11, 2019, 8:14pm January 14, 2019, 3:24pm January 16, 2019, 6:48pm February 13, 2019, 6:48pm",
    "body": "Hello everybody, My objective is to facilitate and customize the display of alerts (format date, recover the real PID of the log before transformation, etc...) Early January 2019, installation Docker ELK 6.5.4 (Wazuh, Logstash, Elasticsearch, Nginx HTTPS and Kibana) Agent wazuh installed on the monitored machines (Ubuntu, Windows...). Launch of supervisor by Docker-compose. Kibana is operational and remounted logs as well. Alerts are visible in the Kibana interface. From Discover, then from the filter, then clicking on the Start button of the log, 2 under Columns appears (Table and JSON). Is it possible to change one of these tables that will allow you to customize the view? It seems to me that from filter created and integrated with Logstash.YML, I can customize formats such as \"@timestamp \", etc.. What is the way to display alerts in the Kibana interface with the least manipulation possible and optimized for example. The same goes for the integrity logs. Thank you",
    "website_area": "discuss"
  },
  {
    "id": "3abe99a9-8531-46b6-86f8-36101d944177",
    "url": "https://discuss.elastic.co/t/stream-live-not-updating/163734",
    "title": "Stream live not updating",
    "category": [
      "Logs"
    ],
    "author": "aviator",
    "date": "January 10, 2019, 12:33pm January 10, 2019, 3:08pm January 10, 2019, 3:19pm January 10, 2019, 3:39pm January 10, 2019, 3:49pm January 10, 2019, 3:53pm February 7, 2019, 3:53pm",
    "body": "Hi When using the Stream Live feature the logs do not get updated in realtime, the logs are there because a page refresh displays them. I think the key is that each time I load up the Logs app the newest/latest entry is: 2019-12-31 23:59:53.000 INFO: xxxxx daemon running Everything behind that is up to date i.e. 2019-01-10 12:29:21.587 Jan 10 12:29:20 localhost So I guess something is stuck somewhere - is there a file that checkpoints and needs clearing or similar? Regards Ed",
    "website_area": "discuss"
  },
  {
    "id": "4d68873e-2522-4d06-9662-c225c0c17cec",
    "url": "https://discuss.elastic.co/t/log-ui-failed-to-format-message-from/163196",
    "title": "Log UI failed to format message from",
    "category": [
      "Logs"
    ],
    "author": "pjanzen",
    "date": "January 7, 2019, 12:04pm January 7, 2019, 12:21pm January 7, 2019, 6:30pm January 7, 2019, 4:31pm January 7, 2019, 6:30pm February 5, 2019, 4:18am",
    "body": "Hi, I was looking in to Logs UI and I have setup filebeat to send the logs over to ES. However all I see in the Logs UI is an error message saying Failed to format message from /opt/logstash/logs/logstash-plain.log Does anyone have any pointers on what to check? Thanks, Paul.",
    "website_area": "discuss"
  },
  {
    "id": "30350928-da5b-48bd-b60a-d3ad01aba096",
    "url": "https://discuss.elastic.co/t/differences-between-discover-and-logs/161184",
    "title": "Differences between Discover and Logs?",
    "category": [
      "Logs"
    ],
    "author": "phr0gz",
    "date": "December 17, 2018, 3:36pm December 21, 2018, 9:46am January 7, 2019, 4:04pm January 7, 2019, 4:15pm January 7, 2019, 4:15pm February 4, 2019, 4:15pm",
    "body": "Hello, The live stream feature seems really nice! But I'm a little bit confused about this functionality: It looks like the same as the \"Discover\" view...isn't it? We are already using the Elastic stack as a \"big\" farm to parse, store, and analyse the logs (10TB/month). Beat is not an option because it doesn't work on closed systems like appliances, network devices... so I really hope you will not focus only on beat",
    "website_area": "discuss"
  },
  {
    "id": "24893e43-5cc0-4f1c-84ee-936adad2f98c",
    "url": "https://discuss.elastic.co/t/logs-ui-disable-failed-to-find-message-line/162958",
    "title": "Logs UI disable \"failed to find message\" line",
    "category": [
      "Logs"
    ],
    "author": "Eilyre",
    "date": "January 4, 2019, 12:57pm January 4, 2019, 1:03pm January 4, 2019, 1:06pm February 1, 2019, 1:07pm",
    "body": "Hello! Loving the new Logs UI functionality. Makes checking the logs of distributed systems much quicker and more pleasant to use. While it works well, there's one pet peeve I have - when a document does not have the \"message\" field, it will show the line as \"failed to find message\" in the Logs UI. This is a problem because I have indexes sorted by topics, for an example Slurm information is in the \"slurm-*\" indexes. There's three different kinds of information: Slurm jobs information Slurm nodes information Slurm logs from all the nodes Now the first two do not need a message field, all the data is in different fields that are used to visualize and search on. And there's thousands of such documents in between some log lines. Having log lines separated by thousands of lines of \"failed to find message\" is not very useful. I wish I could either turn that line off, or somehow hide it and save the setting permanently. I can currently circumvent this by using a search bar query, but every time you go to the page you need to rewrite the query. Possibility to save and default the search would help as well. Hope my two cents are helpful.",
    "website_area": "discuss"
  },
  {
    "id": "b8868b63-7b0b-425b-ac0c-4800d9176f0e",
    "url": "https://discuss.elastic.co/t/logs-ui-beta-how-it-works/157430",
    "title": "Logs UI beta, how it works !?",
    "category": [
      "Logs"
    ],
    "author": "dimuskin",
    "date": "November 19, 2018, 8:32pm November 19, 2018, 8:39pm November 20, 2018, 6:18am December 17, 2018, 3:01pm December 17, 2018, 3:22pm December 17, 2018, 3:39pm December 18, 2018, 9:50am January 3, 2019, 10:22am January 31, 2019, 10:22am",
    "body": "Hello, I finally upgraded to ES 6.5 and was pleasantly surprised by the innovations. Really liked the feature Logs UI (watching logs in real time), but unfortunately I did not find a description of her work. Documentation describes step by step installation from Filebeat to Kibana, but I want to use this feature on my existing indices. Is I understand, they use specific fields from defined indices patterns. (like filebeat-*). Is this parameters configurable? Best Regards.",
    "website_area": "discuss"
  },
  {
    "id": "215c41c6-5ad0-482c-af4a-05c6b72981fc",
    "url": "https://discuss.elastic.co/t/multiple-fields-instead-of-message/161301",
    "title": "Multiple fields instead of message",
    "category": [
      "Logs"
    ],
    "author": "phr0gz",
    "date": "December 18, 2018, 10:45am December 21, 2018, 9:47am January 18, 2019, 9:48am",
    "body": "Hello, is there any plan to add multiple fields instead of @message/message ? Because in my case to avoid redundant fields we drop the message field when the message is parsed.",
    "website_area": "discuss"
  },
  {
    "id": "cb25a009-8cf6-42cf-a6b8-0329c5c6f107",
    "url": "https://discuss.elastic.co/t/show-hostname-or-source-path/158229",
    "title": "Show hostname or source path",
    "category": [
      "Logs"
    ],
    "author": "benpolzin",
    "date": "November 26, 2018, 6:03pm November 29, 2018, 2:29am December 26, 2018, 6:30pm",
    "body": "The new infinite scroll feature of the Logs UI is fantastic! Thanks! I'm finding that I really miss the context of my logs, though. In the Discover view I frequently add hostname or the source path as columns because I have 100s of servers sending data from a couple dozen log files each. This context (and the ability to quickly filter on these fields) is critical to making sense of my log stream. Is there a way to add these fields to the Logs UI view in the current version?",
    "website_area": "discuss"
  },
  {
    "id": "56c27cca-f4ab-49c6-9203-64ee0fb89a8d",
    "url": "https://discuss.elastic.co/t/which-indexes-does-the-new-infrastructure-logs-feature-use/157920",
    "title": "Which indexes does the new Infrastructure / Logs feature use?",
    "category": [
      "Logs"
    ],
    "author": "Matin_Nayob",
    "date": "November 23, 2018, 12:52pm November 23, 2018, 12:59pm November 28, 2018, 10:00am November 29, 2018, 10:47am December 26, 2018, 6:26pm",
    "body": "The new logs feature in Kibana 6.5.0 only seems to display live logs from a subset of my indexes. Is there a way to configure this yet? I've tried adding specific indexes to the filter, but it doesnt display any data. Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "1f79f22d-9b93-428d-aed5-c72389bd6520",
    "url": "https://discuss.elastic.co/t/catenate-two-fields-as-message/157243",
    "title": "Catenate two fields as message",
    "category": [
      "Logs"
    ],
    "author": "Anton1",
    "date": "November 18, 2018, 7:56pm November 19, 2018, 7:06am November 19, 2018, 7:28am November 19, 2018, 10:58am December 17, 2018, 10:57am",
    "body": "I got the logs ui working with my own index. However, message and loglevel are two different fields. Is it possible to catenate level and message so that both are shown in logs ui?",
    "website_area": "discuss"
  },
  {
    "id": "bcc4877b-17f9-4979-9ab2-44504cd9d5a8",
    "url": "https://discuss.elastic.co/t/does-the-same-filtering-that-works-for-discover-also-work-for-logs-so-far-its-not-for-me/157123",
    "title": "Does the same filtering that works for Discover also work for Logs? So far it's not for me",
    "category": [
      "Logs"
    ],
    "author": "dfinn",
    "date": "November 16, 2018, 7:06pm November 16, 2018, 10:19pm November 16, 2018, 10:24pm November 16, 2018, 10:26pm December 14, 2018, 10:27pm",
    "body": "I've got Logs working, I think it's going to be a really helpful feature. I do have one question though. Here's an example of a query that works in Discovery but if I try it in Logs it says \" There are no log messages to display.\" beat.name.keyword:/ps-test-app.*/ AND type:rails_json In Discover this returns quite a bit of logs from the past 15 minutes. Am I doing something wrong?",
    "website_area": "discuss"
  },
  {
    "id": "b85c8f52-8079-49fd-93f7-e2e6849745dd",
    "url": "https://discuss.elastic.co/t/looks-like-you-dont-have-any-logging-indices/156915",
    "title": "\"Looks like you don't have any logging indices\"",
    "category": [
      "Logs"
    ],
    "author": "trondhindenes",
    "date": "November 15, 2018, 6:56pm November 15, 2018, 6:56pm November 15, 2018, 7:44pm November 16, 2018, 7:33am November 16, 2018, 7:46am November 16, 2018, 10:05am November 16, 2018, 10:08am November 16, 2018, 10:18am December 14, 2018, 10:21am",
    "body": "I'm trying to figure out what constitutes a \"logging index\" - I'm getting the message \" Looks like you don't have any logging indices\" when testing the new \"Logs\" app in Kibana, but I can't find any documentation around what it considers a logging index. Is this feature \"locked\" to the builtin pattern (filebeat*, logstash*) etc?",
    "website_area": "discuss"
  },
  {
    "id": "e40eec1c-cd10-436c-880d-16af925c8ded",
    "url": "https://discuss.elastic.co/t/elastic-stack-6-5-logs-ui/156960",
    "title": "Elastic stack 6.5 Logs UI",
    "category": [
      "Logs"
    ],
    "author": "shradhatx",
    "date": "November 16, 2018, 3:18am November 16, 2018, 3:33am November 16, 2018, 9:49am December 14, 2018, 9:49am",
    "body": "Where can I get more information on it? Is it a consolidated dashboard of MetricBeat and APM?",
    "website_area": "discuss"
  },
  {
    "id": "91e806bd-cd27-4260-84eb-bb3903bfb284",
    "url": "https://discuss.elastic.co/t/about-the-infrastructure-category/156754",
    "title": "About the Infrastructure category",
    "category": [
      "Metrics"
    ],
    "author": "warkolm",
    "date": "November 21, 2019, 9:05am",
    "body": "Everything related to metrics - Metricbeat, integrations and modules, Kibana dashboards and the Metrics app.",
    "website_area": "discuss"
  },
  {
    "id": "af85d02b-94c6-4439-92ce-33fce610a2d6",
    "url": "https://discuss.elastic.co/t/how-is-the-system-network-in-dropped-calculated/212403",
    "title": "How is the system.network.in.dropped calculated?",
    "category": [
      "Metrics"
    ],
    "author": "javadevmtl",
    "date": "December 18, 2019, 9:24pm December 18, 2019, 10:54pm December 18, 2019, 11:17pm December 19, 2019, 3:40pm January 16, 2020, 3:40pm",
    "body": "Hi, I have noticed a machine has a \"higher\" rate of dropped packets vs other machines. This machine is about +1% packet loss vs other machines are way below 1% I.e: Machine 1: 14 dropped packets over 200 million Machine 2: 2 million over 200 Million. You see \"dropped\": 2750373. Is this number cumulative over the uptime of the machine? Or is that how many packets where dropped at that particular timestamp? I run this query: GET metricbeat-*/_search { \"size\": 100, \"_source\": [\"@timestamp\", \"system.network.in.dropped\", \"host.name\"], \"query\": { \"query_string\" : { \"query\" : \"metricset.name:network AND host.name:XXXXXX-0001\" } } } And I get... { \"took\": 99, \"timed_out\": false, \"_shards\": { \"total\": 7, \"successful\": 7, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 118220, \"max_score\": 5.0651484, \"hits\": [ { \"_index\": \"metricbeat-6.4.2-2019.12.16\", \"_type\": \"doc\", \"_id\": \"Km65DW8Bbyfak3QNTgOk\", \"_score\": 5.0651484, \"_source\": { \"@timestamp\": \"2019-12-16T08:00:44.724Z\", \"system\": { \"network\": { \"in\": { \"dropped\": 0 } } }, \"host\": { \"name\": \"XXXXXX-0001\" } } }, { \"_index\": \"metricbeat-6.4.2-2019.12.16\", \"_type\": \"doc\", \"_id\": \"K265DW8Bbyfak3QNTgOk\", \"_score\": 5.0651484, \"_source\": { \"@timestamp\": \"2019-12-16T08:00:44.724Z\", \"system\": { \"network\": { \"in\": { \"dropped\": 2750373 } } }, \"host\": { \"name\": \"XXXXXX-0001\" } } } } ] } }",
    "website_area": "discuss"
  },
  {
    "id": "54fc1315-e3d0-45c1-ac19-507633128538",
    "url": "https://discuss.elastic.co/t/change-indices-settings-in-metric-app/211769",
    "title": "Change indices settings in Metric app",
    "category": [
      "Metrics"
    ],
    "author": "norgro2601",
    "date": "December 13, 2019, 9:21am December 17, 2019, 2:50pm December 17, 2019, 1:06pm December 17, 2019, 2:50pm January 14, 2020, 2:50pm",
    "body": "Hi, we have a lot of servers due to several kubernets environments, that are monitored by the Elastic stack. Due to this fact, I often get an error \"Failed to load datasources\" (Response 504) when opening the metrics or logs app. I want to speed up and get the search running again by changing the indices settings for each space, cause each space only uses a subset of the available filebeat and metricbeat indices. Unfortunately the settings page is also not working after the error occurred, I get a blank screen. Is there an API that I can use to change the settings ? And wouldn't it make more sense to have the settings added to the \"Advanced Settings\" on the Management panel as it is for e.g. the SIEM app ? Thanks in advance. Regards, Norbert",
    "website_area": "discuss"
  },
  {
    "id": "4bc98f0c-21c9-4236-896f-416265850ba9",
    "url": "https://discuss.elastic.co/t/cannot-visualize-windows-server-counters/212168",
    "title": "Cannot visualize windows server counters",
    "category": [
      "Metrics"
    ],
    "author": "thomas7467",
    "date": "December 17, 2019, 1:37pm January 14, 2020, 1:37pm",
    "body": "Hi all, I've just installed 7.5.0 elastic and Kibana on an Bionic ubuntu server. I've installed 7.5.0 metricbeats on a windows server Dataflow seems to be OK on elastic side, docs count increase normally in Kibana index management, but I cannot see any data in the Kibana visualize Folder, did I miss something? Thank you Thomas",
    "website_area": "discuss"
  },
  {
    "id": "ad011c48-4684-4777-9109-f55c1b7131dd",
    "url": "https://discuss.elastic.co/t/cant-monitor-windows-perfmon-counter/210726",
    "title": "Cant monitor windows perfmon counter",
    "category": [
      "Metrics"
    ],
    "author": "Alexandros888",
    "date": "December 6, 2019, 11:57am December 16, 2019, 1:07pm December 16, 2019, 2:28pm January 13, 2020, 2:28pm",
    "body": "Hello, I have the following perfmon counter names (Bytes IN) and (Bytes IN/sec) that i want to monitor as the following image show: My modules configuration in my windows.yml file is the following: Module: windows Docs: https://www.elastic.co/guide/en/beats/metricbeat/7.3/metricbeat-module-windows.html module: windows metricsets: service period: 1m module: windows metricsets: perfmon period: 10s perfmon.counters: instance_label: ESEL.Bytes IN measurement_label: ESEL.Bytes IN query: '\\ESEL\\Bytes IN' instance_label: ESEL.Bytes IN/sec measurement_label: ESEL.Bytes IN/sec query: '\\ESEL\\Bytes IN/sec' image.png858368 16.2 KB when i just want to monitor the Bytes IN counter i can also see it in Kibana but when i also add the second counder (Bytes IN/sec) in the yaml file i can not see in Kibana neither of them. Any ideas how to correct this? Thank you",
    "website_area": "discuss"
  },
  {
    "id": "cf5734fe-6751-4f16-8982-6bc2bc4b5688",
    "url": "https://discuss.elastic.co/t/how-do-i-get-the-infrastructure-map-working-for-kubernetes-pods/207731",
    "title": "How do I get the Infrastructure map working for Kubernetes Pods?",
    "category": [
      "Metrics"
    ],
    "author": "Richard_Neely",
    "date": "November 18, 2019, 9:42pm November 18, 2019, 11:35pm November 19, 2019, 12:46am November 19, 2019, 8:11am November 19, 2019, 3:09pm December 17, 2019, 3:09pm",
    "body": "All of my pods show up but metrics report as zero. Kube state metrics is running metrics are getting indexed into the metricbeat index. You can also see them on the Discover tab but nothing here. Running 6.8.3 across the board. Screen Shot 2019-11-13 at 10.32.53 AM.png1706902 45.5 KB",
    "website_area": "discuss"
  },
  {
    "id": "ed31396a-11e6-4a9a-b5b7-2c8b7a08420c",
    "url": "https://discuss.elastic.co/t/getting-information-aruba-iap-207/206024",
    "title": "Getting information Aruba IAP-207",
    "category": [
      "Metrics"
    ],
    "author": "Joseph_John",
    "date": "October 31, 2019, 10:38am November 7, 2019, 2:56pm November 8, 2019, 9:19am November 14, 2019, 7:49am November 14, 2019, 10:04am November 14, 2019, 10:21am November 14, 2019, 11:08am November 14, 2019, 11:28am December 12, 2019, 11:28am",
    "body": "Hi All, Good afternoon After adding filebeat and auditbeat to my linux host, now I wanted to try out getting details from access point \"Aruba IAP-207\" . Guidance requested for the any documentation URL which talks about integrating accesspoints to elastic search Any one used \"Aruba IAP-207\" with elasticseach, requesting for their feedback also thanks Joseph John",
    "website_area": "discuss"
  },
  {
    "id": "cb2e8d67-7075-4ae3-b413-78be214044b0",
    "url": "https://discuss.elastic.co/t/following-steps-for-setting-up-infrastructure-monitoring-app-does-not-work/202914",
    "title": "Following steps for setting up Infrastructure monitoring app does not work",
    "category": [
      "Metrics"
    ],
    "author": "akalra",
    "date": "October 9, 2019, 9:35pm October 9, 2019, 11:06pm October 10, 2019, 3:32pm October 10, 2019, 6:21pm October 10, 2019, 7:07pm November 7, 2019, 7:08pm",
    "body": "I am setting up the infrastructure monitoring for a Kubernetes cluster on GCP. I am using a hosted Elastic cloud. I followed the steps for setting up Metricbeat on kubernetes as stated online but the Infrastructure app or the kibana dashboard does not populate with any data. Please advice on correct approach to set up Kubernetes monitoring. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "94910231-8949-4577-98f1-b5008b0c1683",
    "url": "https://discuss.elastic.co/t/docker-stats-metrics/201862",
    "title": "Docker Stats Metrics",
    "category": [
      "Metrics"
    ],
    "author": "",
    "date": "October 1, 2019, 8:20pm October 7, 2019, 9:51am October 7, 2019, 1:49pm October 8, 2019, 6:56pm October 9, 2019, 1:55pm October 9, 2019, 1:55pm November 6, 2019, 1:55pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1ed47807-ff1d-47cd-b45b-71ab3be8d4d0",
    "url": "https://discuss.elastic.co/t/monitoring-systemd-services-using-metricbeat/198284",
    "title": "Monitoring systemd services using Metricbeat",
    "category": [
      "Metrics"
    ],
    "author": "Mohammad_Etemad",
    "date": "September 5, 2019, 3:28pm September 5, 2019, 5:29pm September 5, 2019, 6:53pm September 6, 2019, 3:57pm October 4, 2019, 3:57pm",
    "body": "I was wondering if there is a way to monitor Centos services like kubelet, keepalived, even a custom service that I wrote, using Metricbeat or any other Elastic service. i.e. is there a way to have metricbeat run a command like: service myService status and see if the results is running. Thank you",
    "website_area": "discuss"
  },
  {
    "id": "515a7092-9dca-4c73-991f-5f395bfee629",
    "url": "https://discuss.elastic.co/t/infrastructure-ui-is-empty/197933",
    "title": "Infrastructure UI is empty",
    "category": [
      "Metrics"
    ],
    "author": "luksi1",
    "date": "September 3, 2019, 9:01pm September 3, 2019, 9:07pm September 4, 2019, 12:42am September 4, 2019, 8:36am September 4, 2019, 12:01pm September 4, 2019, 2:13pm September 4, 2019, 2:24pm September 4, 2019, 2:59pm September 4, 2019, 3:27pm September 4, 2019, 5:28pm September 4, 2019, 5:39pm September 4, 2019, 5:51pm September 4, 2019, 6:50pm September 4, 2019, 7:54pm September 4, 2019, 9:17pm October 2, 2019, 9:17pm",
    "body": "Hello, I'm trying to diagnose why my Infrastructure UI is empty. The waffle map is empty, but the metrics explorer is populated. I can search the metricbeat-* index without any problems. The problem arose when upgrading metricbeat to 7.x. Kibana + Elastissearch versions: 7.3.1 Everything goes straight to Elasticsearch. The Infrastructure UI configruration option is using \"host.name\" to identify hosts. And here is the mapping. GET /metricbeat-*/_mapping/field/host.name { \"metricbeat-7.3.1\" : { \"mappings\" : { \"host.name\" : { \"full_name\" : \"host.name\", \"mapping\" : { \"name\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } } } } }",
    "website_area": "discuss"
  },
  {
    "id": "6c64fbbb-f3b9-4a03-8c66-46abd9ba49d5",
    "url": "https://discuss.elastic.co/t/there-is-no-data-to-display-after-upgrading-from-7-3-to-7-3-1/197942",
    "title": "There is no data to display after upgrading from 7.3 to 7.3.1",
    "category": [
      "Metrics"
    ],
    "author": "Smoky",
    "date": "September 3, 2019, 8:20pm September 4, 2019, 8:39am September 4, 2019, 2:16pm September 4, 2019, 3:01pm September 4, 2019, 3:11pm September 4, 2019, 5:29pm September 4, 2019, 7:02pm October 2, 2019, 7:02pm",
    "body": "HI. After upgrading from 7.3 to 7.3.1, the infrastructure page comes up saying \"There is no data to display.\" The logs are still there when I go to Logs or Discover. Anyone have any idea why? Terry",
    "website_area": "discuss"
  },
  {
    "id": "c35253b9-245c-4ad1-8060-83d2b4eda1c4",
    "url": "https://discuss.elastic.co/t/there-is-no-data-to-display-beats-via-logstash/197913",
    "title": "There is no data to display. - beats via logstash",
    "category": [
      "Metrics"
    ],
    "author": "jlim0930",
    "date": "September 3, 2019, 4:43pm September 3, 2019, 5:19pm September 3, 2019, 5:35pm October 1, 2019, 5:49pm",
    "body": "I have metricbeats and filebeats installed on various machines all sending data into elastic via haproxy -> logstash -> elasticsearch. I do see the data but in the Infrastructure tab I only see \"There is no data to display\" and many of the dashboards and visualizations are broken. When installing the beats i did run setup --dashboards however did not run the setup --index-management is setup --index-management the missing step or is there something different that needs to be done when beats are sending information in via logstash?",
    "website_area": "discuss"
  },
  {
    "id": "4d1ef245-8c71-4892-92be-df4cd852e786",
    "url": "https://discuss.elastic.co/t/view-metrics-shows-graphql-error/196258",
    "title": "\"View Metrics\" shows GraphQL Error",
    "category": [
      "Metrics"
    ],
    "author": "pfa",
    "date": "August 22, 2019, 7:27am August 26, 2019, 3:27pm August 26, 2019, 8:16pm August 27, 2019, 1:02pm August 27, 2019, 3:05pm August 27, 2019, 3:46pm August 27, 2019, 4:20pm August 28, 2019, 10:28am September 2, 2019, 11:45am September 30, 2019, 11:45am",
    "body": "Hi! After migration from 7.1.1 to 7.3.0 the Infrastructure UI \"Show Metrics\" isn't working any more. The tiles in global view(hosts and kubernetes) show correct metrics, \"view logs\" is also working properly. Error Message: \"GraphQL Error [object Object]\" i enabled debug mode for kibana, but didn't find any hint about it. in 7.1.1 everything works as expected",
    "website_area": "discuss"
  },
  {
    "id": "ca47a72b-3dc0-44b9-86c8-87b7eae9aea5",
    "url": "https://discuss.elastic.co/t/ecs-kibana-issue-is-it-possible-to-combine-dynamic-filter-for-kubernetes-pod-name-or-kubernetes-container-module-pod-name/195449",
    "title": "ECS-Kibana issue: is it possible to combine dynamic filter for kubernetes.pod.name or kubernetes.container._module.pod.name",
    "category": [
      "Metrics"
    ],
    "author": "asp",
    "date": "August 16, 2019, 4:29pm August 19, 2019, 3:44pm August 20, 2019, 6:35am September 17, 2019, 6:41am",
    "body": "Hi, I am using metricbeat to gather information about my kubernetes cluster. I want to build my own status dashboard to see if my deployments / statefulsets in kubernetes are healthy and I want to be able to dig deeper and show performance values of containers and pods. Currently my dashboard is looking like this. (created with enhanced-table plugin) image.png18951061 191 KB First question: Is it possible to achieve the same in TSVB (except for the filter panel)? Doing grouping on multiple levels and coloring a text cell based on a status (calculation between two fields). Second question I want to add pod and container usage statistics. When I check the events in elasticsearch I have following to offer: My table on the bottom is gathering information from metricset.nae: state_pod: image.png753928 58.6 KB Overall pod metrics can be found in metricset.name: pod: image.png7981108 79.9 KB And detailed container metrics can be found in metricset.name: contianer image.png8201104 82.2 KB Marked in green are metrics / values I would like to show. I want to be able to filter for example for a logical pod name by clicking on a table entry or in an object of tsvb. By clicking there I want to see every event which has the pod name value in the fields: kubernetes.container._module.pod.nameor kubernetes.pod.name or at best kubernetes.*.pod_name. At very best I want to also show events where field kubernetes.*.pod_name is not existing. Can this somehow be achieved with kibana? In my opinion it would be much, much easier if same information is always stored in the same field. Store the kubernetes.pod.name always in this field, regardless my event is state_pod, container, pod or whatever. Same issue I am encountering if i filter for kubernetes.node.name. If I set this filter no pod or container will be shown, because the field (with same meaning) is named differently. Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "a0ab9d42-ab18-48d3-af41-35a0b7a42550",
    "url": "https://discuss.elastic.co/t/3-6-shards-failing-when-trying-to-visualize-on-kibana/191857",
    "title": "3/6 shards failing when trying to visualize on Kibana",
    "category": [
      "Metrics"
    ],
    "author": "sarahvo",
    "date": "July 23, 2019, 3:38pm July 23, 2019, 4:49pm July 23, 2019, 4:53pm July 23, 2019, 5:02pm July 23, 2019, 6:15pm July 24, 2019, 7:54am July 24, 2019, 7:56pm July 25, 2019, 9:13am July 25, 2019, 6:48pm July 25, 2019, 6:56pm July 26, 2019, 8:06am July 26, 2019, 6:03pm July 29, 2019, 6:35pm July 29, 2019, 6:36pm July 29, 2019, 6:41pm August 26, 2019, 6:41pm",
    "body": "Hi, I'm currently trying to do a simple POC for a potential monitoring system for my company using the Elastic stack. I have metricbeat & winlogbeat going through Logstash, which ships to Elasticsearch. I'm currently only using a single-node and would like to avoid clustering until we have a better idea of if we're going to be using the Elastic stack for our monitoring system. Right now, I have 4 of our servers set up for monitoring and it looks like data is being collected properly from each. However, when I try to graph the data, i.e. graph the number of hosts, it tells me 3/6 shards have failed and it shows 0 hosts. I can do queries in the discover section, but am unable to graph certain data. I'm also using the latest version of everything 7.2.0. Any help solving this problem is appreciated! Additional Info: Nodes: 1 Disk Available 34.47% 20.0 GB / 58.0 GB JVM Heap 43.04% 1.7 GB / 4.0 GB Indices: 511 Documents 17,510,180 Disk Usage 6.7 GB Primary Shards 511 Replica Shards 0",
    "website_area": "discuss"
  },
  {
    "id": "7162f36f-013a-4c50-b1ad-0f828d12563e",
    "url": "https://discuss.elastic.co/t/metricbeat-docker-shows-different-stats-for-logstash-in-container-than-logstash-monitoring/192015",
    "title": "Metricbeat/docker shows different stats for logstash in container than logstash monitoring",
    "category": [
      "Metrics"
    ],
    "author": "sgreszcz",
    "date": "July 24, 2019, 11:42am July 24, 2019, 1:05pm August 21, 2019, 1:05pm",
    "body": "Im getting weird perf stats on logstash-netflow. Seems like the docker API perf mon says it is using 1200% CPU, but the actual logstash monitoring shows 8% CPU. Also htop also show lots of CPU headroom. Screenshot 2019-07-24 at 12.39.28.png1770894 275 KB Screenshot 2019-07-24 at 12.36.41.png17621042 81 KB Screenshot 2019-07-24 at 12.35.34.png27442314 263 KB",
    "website_area": "discuss"
  },
  {
    "id": "9c02572b-ed29-4941-ab94-36196066679a",
    "url": "https://discuss.elastic.co/t/problem-on-infrastructure-and-metricbeats/188864",
    "title": "Problem on Infrastructure and Metricbeats",
    "category": [
      "Metrics"
    ],
    "author": "antonopo",
    "date": "July 5, 2019, 8:01am July 5, 2019, 2:49pm August 2, 2019, 3:01pm",
    "body": "Hi, I have problem on the Infrastructure tab on Kibana. I cannot see any data of the metrics of my hosts. In the past i could see all the metrics for all the hosts there. After upgrading ELK 7.1 to 7.2 and all the metricbeats from 6.X to 7.X i cannot see the data of the metrics on the infrastructure and on the Dashboard on the metrics. 1.PNG1168571 69.9 KB 2.PNG1475795 31.5 KB Also if you check the metricbeat indexes on my screenshots you will see that are not with the date. Do you have any idea? Best Regards, Thanos",
    "website_area": "discuss"
  },
  {
    "id": "a0dfbe4e-1712-4abb-a9c5-9006a5114a2f",
    "url": "https://discuss.elastic.co/t/infrastructure-dashboard-legends-not-clear/187924",
    "title": "Infrastructure dashboard legends not clear",
    "category": [
      "Metrics"
    ],
    "author": "",
    "date": "July 2, 2019, 9:23am June 28, 2019, 4:06pm July 2, 2019, 9:54pm July 30, 2019, 9:54pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "67c00bd1-ec70-47e5-8b5f-9088ee33a0e7",
    "url": "https://discuss.elastic.co/t/save-setting-groupby-servers-by-custom-field-in-infrastructure-dashboard/187686",
    "title": "Save setting Groupby servers by custom field in infrastructure Dashboard",
    "category": [
      "Metrics"
    ],
    "author": "syedsfayaz",
    "date": "June 26, 2019, 10:18pm July 1, 2019, 11:06am July 1, 2019, 3:43pm July 29, 2019, 3:43pm",
    "body": "Hi I am able to categorize the servers based on the Group by : \"version\". But every time I load kibana or any new user logs in, they need to explicitly go and add that filter. Is there a way this filter can be saved for every users who uses the dashboard. ELK V 7.2 image.png1175258 18.3 KB",
    "website_area": "discuss"
  },
  {
    "id": "7ead3cb8-0293-4760-87b0-1d3c96be54a1",
    "url": "https://discuss.elastic.co/t/there-is-no-data-to-display-on-metric-page/185361",
    "title": "\"There is no data to display\" on metric page",
    "category": [
      "Metrics"
    ],
    "author": "aqiank",
    "date": "June 12, 2019, 8:48am June 12, 2019, 9:09am June 12, 2019, 9:22am June 12, 2019, 12:36pm June 12, 2019, 1:08pm June 12, 2019, 2:00pm June 12, 2019, 2:47pm June 13, 2019, 9:32am June 13, 2019, 10:55am June 13, 2019, 11:06am June 13, 2019, 3:06pm June 13, 2019, 5:03pm June 14, 2019, 2:49am June 14, 2019, 9:00am June 14, 2019, 9:03am June 14, 2019, 12:20pm June 14, 2019, 12:52pm June 15, 2019, 4:20am July 13, 2019, 4:20am",
    "body": "I found this message in one of the HTTP requests when loading the metrics page. [illegal_argument_exception] Fielddata is disabled on text fields by default. Set fielddata=true on [host.name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. But host.name in my case is already a keyword field so I don't know what to do next. Does anyone know how to solve this problem? I have the index mapping here if it helps.",
    "website_area": "discuss"
  },
  {
    "id": "cd3d4f60-cce3-4d18-9985-15c6b61ae86d",
    "url": "https://discuss.elastic.co/t/kibana-infrastructure-ui-shows-no-metrics-for-hosts-or-kubernetes-pods/184216",
    "title": "Kibana Infrastructure UI shows no metrics for hosts or kubernetes pods",
    "category": [
      "Metrics"
    ],
    "author": "zx10r",
    "date": "June 4, 2019, 6:53pm June 4, 2019, 6:56pm June 4, 2019, 7:14pm June 7, 2019, 8:32am June 7, 2019, 9:33am June 10, 2019, 3:22am June 10, 2019, 4:27am July 8, 2019, 4:27am",
    "body": "This happens when selecting something from the main Infrastructure UI when drilling into a pod or host and trying to go to metrics it loads empty while all data data is there in indexes. The error that i see logged is bellow: \"Caused by: java.lang.IllegalArgumentException: Fielddata is disabled on text fields by default. Set fielddata=true on [kubernetes.pod.name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\", metricbeat is outputting directly to elasticsearch and the entire stack is on 7.1.1 The other thing to add is that inspecting the browser shows this error when the infrastructure metrics view is loaded: Refused to execute inline script because it violates the following Content Security Policy directive: \"script-src 'unsafe-eval' 'nonce-r+WyUN70WSO1Kp04'\". Either the 'unsafe-inline' keyword, a hash ('sha256-SHHSeLc0bp6xt4BoVVyUy+3IbVqp3ujLaR+s+kSP5UI='), or a nonce ('nonce-...') is required to enable inline execution.",
    "website_area": "discuss"
  },
  {
    "id": "e8fb2b0d-f940-4bcf-9eef-4f41e30a553c",
    "url": "https://discuss.elastic.co/t/kibana-infrastructure-ui-does-not-show-metrics/181660",
    "title": "Kibana infrastructure UI does not show metrics",
    "category": [
      "Metrics"
    ],
    "author": "abhijith444",
    "date": "May 28, 2019, 1:39am June 3, 2019, 5:52pm July 1, 2019, 5:52pm",
    "body": "I have a setup of Beats->Logstash->Kafka<-Logstash->Elasticsearch The Infrastructure UI lists a bunch of hosts but when I click trough to \"View metrics\" I don't see any data. I am able to see the data if I write directly to Elasticsearch from metricbeat though. I am trying to figure out what are the filters used by the host page to draw the visualizations. The index pattern for the direct writes and writes through the pipeline is the same. The pipeline adds a couple of additional fields to indicate the environment of the node. Other than that the event is un altered. I found a similar issue in the below question but this doesn't talk about the pipeline. There is no data to display Infrastructure Hello World! I'm trying out Infrastructure (infra) Kibana' app, yet getting following message: There is no data to display. metricbeat-* exists with some data in it (mostly from system module) Please advise.",
    "website_area": "discuss"
  },
  {
    "id": "2ad3ae2e-2fa5-4643-bab2-c57f10adfeeb",
    "url": "https://discuss.elastic.co/t/kibana-infrastructure-data-source/180493",
    "title": "Kibana Infrastructure data source",
    "category": [
      "Metrics"
    ],
    "author": "Jehutywong",
    "date": "May 10, 2019, 6:20am May 10, 2019, 1:50pm May 10, 2019, 2:39pm May 10, 2019, 2:44pm May 14, 2019, 3:27am May 28, 2019, 6:53am May 28, 2019, 1:50pm June 3, 2019, 5:11pm July 1, 2019, 5:13pm",
    "body": "I'd wonder, if Kibana Infrastructure app can support data source other than metricbeat. I am now collecting system metrics by Collectd. I assume Infrastructure app is looking for pre-defined field names. Can I modify field names in Collectd by Logstash, so that Infrastructure app can recognize those data.",
    "website_area": "discuss"
  },
  {
    "id": "353e6289-c396-4b5b-8c13-3229048814c6",
    "url": "https://discuss.elastic.co/t/kubernetes-docker-data-through-non-metricbeat-index/182761",
    "title": "Kubernetes / docker data through non-metricbeat index?",
    "category": [
      "Metrics"
    ],
    "author": "ethrbunny",
    "date": "May 25, 2019, 2:52pm May 25, 2019, 5:27pm May 25, 2019, 10:22pm May 25, 2019, 11:05pm June 22, 2019, 11:05pm",
    "body": "Any chance of supporting this? Metricbeat indexes were getting too cumbersome so I split out k8 and docker (among other things) to separate indexes. I'd like to be able to use these in the infrastructure panel (in addition to the 'hosts' display which seems to be working ok)",
    "website_area": "discuss"
  },
  {
    "id": "62f33f76-f59f-45a9-bfe6-d2985e79f957",
    "url": "https://discuss.elastic.co/t/infrastructure-not-fully-working-for-users-with-limited-xpack-privileges/181761",
    "title": "Infrastructure not fully working for users with limited xpack privileges",
    "category": [
      "Metrics"
    ],
    "author": "willemdh",
    "date": "May 20, 2019, 8:06am May 20, 2019, 11:08am May 20, 2019, 11:52am May 20, 2019, 4:28pm June 17, 2019, 4:28pm",
    "body": "Hello, We have multiple Kibana Spaces and each Space has a number of users with limited privileges. All users of a Kibana Space have access to hosts where a certain field is set, for example: Read and view_index_metadata on metricbeat and metricbeat-* where {\"term\": {\"digipolis.subcel\":\"crm\"}} So infrastructure works fine for users such as myself who have access to all Kibana Spaces. Users with limited privilges seem to have access to the Infrastructure waffle view correctly. Only the hosts to which they have access are shown. But the moment they click a host and select \"View Metric\", the get \"There is no data to display\" message... In discovery they can see the metricbeat data needed. Grtz Willem",
    "website_area": "discuss"
  },
  {
    "id": "59c6e6a2-f0a1-4dc0-8a6a-463ac0dde0c6",
    "url": "https://discuss.elastic.co/t/issue-seeing-the-whole-infrastructure-page-when-to-many-groups-using-group-by/180869",
    "title": "Issue seeing the whole infrastructure page when to many groups using group by",
    "category": [
      "Metrics"
    ],
    "author": "DaveRoss",
    "date": "May 13, 2019, 7:17pm May 13, 2019, 7:48pm May 13, 2019, 10:57pm May 14, 2019, 1:17pm May 14, 2019, 1:56pm June 11, 2019, 1:22pm",
    "body": "I have a UI issue when trying to see pods by namespace using Kubernetes view in Infrastructure. Some group start before the window and some end after the window end. Screen Shot 2019-05-13 at 3.13.14 PM.png17401031 98.4 KB Anyone has the same issue ?",
    "website_area": "discuss"
  },
  {
    "id": "ff78f0e0-a3ff-4756-bb06-6fcc604c3142",
    "url": "https://discuss.elastic.co/t/kubernetes-pods-found-but-all-metrics-0/180061",
    "title": "Kubernetes pods found, but all metrics 0%",
    "category": [
      "Metrics"
    ],
    "author": "willemdh",
    "date": "May 7, 2019, 8:48pm May 7, 2019, 9:31pm May 8, 2019, 6:41am May 8, 2019, 8:43am May 8, 2019, 1:26pm May 8, 2019, 1:59pm May 8, 2019, 4:03pm June 5, 2019, 4:03pm",
    "body": "Hello, Did some tests on the new Infrastructure ui in Kibana 6.7.1. Hosts metrics are working fine, but Kubernetes metrics seem to be missing some data. Although the pod names, nodes and namespaces are recognised, all metrics report 0%, both for cpu, memory, the inbound / outbound traffic all show 0bit/s. Metricbeat data is generated on 1 master Openshift node with following module config: - module: kubernetes metricsets: - node - system - pod - container - volume period: 10s in_cluster: false add_metadata: true kube_config: ../.kube/config host: \"node1\" hosts: [\"https://node1:10250\",\"https://node2:10250\",.......] ssl.certificate_authorities: [\"/etc/pki/ca-trust/source/anchors/openshift-ca.crt\"] ssl.certificate: \"crt\" ssl.key: \"key\" image.png1753290 6.49 KB All kubernetes metricbeat data is in metricbeat-*. CHecking the data in discovery, I can find related metrics: As you can see, we are not running a daemonset (as this caused us too many headaches in Openshift), could it be related to in_cluster: false? Is there something else which can cause the kubernetes metrics in the Infrastructure ui to be '0'? Thanks. Willem",
    "website_area": "discuss"
  },
  {
    "id": "d1fb2629-62f1-4d0b-ae7f-291cab87ca7d",
    "url": "https://discuss.elastic.co/t/there-is-no-data-to-display/167664",
    "title": "There is no data to display",
    "category": [
      "Metrics"
    ],
    "author": "alexus",
    "date": "February 8, 2019, 4:15pm February 8, 2019, 4:33pm February 8, 2019, 5:26pm March 8, 2019, 3:15pm February 8, 2019, 7:54pm February 8, 2019, 7:58pm February 8, 2019, 10:01pm March 8, 2019, 3:04pm March 28, 2019, 9:51am April 17, 2019, 8:36am April 18, 2019, 2:59pm April 19, 2019, 4:33am April 19, 2019, 2:41pm April 20, 2019, 5:07pm May 3, 2019, 7:56pm May 3, 2019, 9:49pm May 3, 2019, 9:53pm May 6, 2019, 8:48am June 3, 2019, 8:48am",
    "body": "Hello World! I'm trying out Infrastructure (infra) Kibana' app, yet getting following message: There is no data to display. metricbeat-* exists with some data in it (mostly from system module) Please advise.",
    "website_area": "discuss"
  },
  {
    "id": "5f1aeaa5-844c-40cf-89b2-9c5230499a3e",
    "url": "https://discuss.elastic.co/t/see-infrastructure-dashboard-in-full-screen/175946",
    "title": "See infrastructure dashboard in full screen",
    "category": [
      "Metrics"
    ],
    "author": "syedsfayaz",
    "date": "April 9, 2019, 3:01pm April 9, 2019, 5:40pm April 9, 2019, 5:55pm April 9, 2019, 7:46pm April 9, 2019, 8:18pm April 9, 2019, 8:30pm May 7, 2019, 8:31pm",
    "body": "Hi I have upgraded my kibana to 6.7 and able to see infra dashboard. Is there a way we can close the top selection and see the dashboard in full screen . image.png1693752 107 KB",
    "website_area": "discuss"
  },
  {
    "id": "f1f9a4d6-b3f2-42fa-bf6d-e84a047b2d1e",
    "url": "https://discuss.elastic.co/t/ui-detailed-metrics-captured-data-has-wrong-alignment-firefox-only/175549",
    "title": "UI detailed metrics: captured data has wrong alignment (firefox only)",
    "category": [
      "Metrics"
    ],
    "author": "kaem2111",
    "date": "April 5, 2019, 9:07am April 9, 2019, 8:29pm May 7, 2019, 8:31pm",
    "body": "I am using Version 6.7.0 with firefox As shown in the picture Infra.PNG709213 14.4 KB the cursor is at position 9:40, the captured data is from 9:41, the inbound value at 9:40 in the chart is definitely below -200 while value the inbound value of 9:41 matches. The root cause, as shown in the picture is, that the origin of the marker (here at point zero) is not correctly aligned to the zero point of the graphic. Unfortunaly this only happens on firefox 60.5.2esr, on Internet Explorer it works correctly. Do you know any -moz option to fix this issue in css?",
    "website_area": "discuss"
  },
  {
    "id": "3c64121d-ff18-4442-8a6d-ea312b14effb",
    "url": "https://discuss.elastic.co/t/groupby-servers-by-custom-field-in-infrastructure-dashboard/174663",
    "title": "Groupby servers by custom field in infrastructure Dashboard",
    "category": [
      "Metrics"
    ],
    "author": "syedsfayaz",
    "date": "March 31, 2019, 9:05am April 1, 2019, 8:54am April 1, 2019, 9:17pm April 1, 2019, 9:30pm April 1, 2019, 9:32pm April 8, 2019, 6:35pm April 8, 2019, 6:40pm April 8, 2019, 6:51pm April 8, 2019, 10:26pm April 8, 2019, 11:16pm April 9, 2019, 9:28am April 9, 2019, 4:39pm May 7, 2019, 4:39pm",
    "body": "I am able to see servers in Infrastructure dashboard. But I want to categorize(group by) them by a custom field like env name. Is there any configuration in Kibana or Metricbeats configuration where I can configure this.",
    "website_area": "discuss"
  },
  {
    "id": "df331728-b61e-4650-a23f-8c56ec8199f5",
    "url": "https://discuss.elastic.co/t/metricbeat-data-missing-from-infrastructure-ui-when-using-logstash/175295",
    "title": "Metricbeat data missing from Infrastructure UI - when using Logstash",
    "category": [
      "Metrics"
    ],
    "author": "Wayne_Taylor",
    "date": "April 4, 2019, 1:09am April 5, 2019, 8:44am May 3, 2019, 8:44am",
    "body": "Hi All, previously we were using metricbeat to ship directly to ES - but recently added logstash listener in middle. This is beats 6.3.2 with Elastic 6.7 After doing so the filebeat data is loading with no issues and shows in the logs UI - but metricbeat indexes are created, values are accurate - but nothing in the UI. I verified the index patterns are named metricbeat-* The logstash config is just a simple input and output. No filtering etc. Any ideas? Thanks Wayne",
    "website_area": "discuss"
  },
  {
    "id": "584e6c2c-c2cd-4a0e-815b-fe9e6d5ff443",
    "url": "https://discuss.elastic.co/t/logs-time-6-hours-behind-local-time/169679",
    "title": "Logs time 6 hours behind local time",
    "category": [
      "Metrics"
    ],
    "author": "rugenl",
    "date": "February 23, 2019, 7:05pm February 25, 2019, 10:22am February 25, 2019, 1:28pm February 25, 2019, 2:56pm February 25, 2019, 7:46pm February 26, 2019, 3:18pm February 27, 2019, 11:11pm March 4, 2019, 3:01pm March 4, 2019, 7:12pm March 4, 2019, 7:20pm March 4, 2019, 7:28pm March 4, 2019, 7:41pm March 4, 2019, 7:59pm March 5, 2019, 1:15am March 5, 2019, 12:50pm March 5, 2019, 2:25pm March 5, 2019, 2:32pm March 5, 2019, 3:15pm March 5, 2019, 3:39pm March 5, 2019, 4:26pm",
    "body": "I just started investigating logs. I setup filebeat on my elastic cluster and enabled the elasticsearch and logstash modules. The data is there, but the log time is -6 hours from local. For example @timestamp is February 23rd 2019, 05:41:30.460, event.created is February 23rd 2019, 11:41:31.059 and the log record starts with [2019-02-23T11:41:30,460]. Not ironically, I'm US/Central time zone, we we are -6 offset now. Is this a bug or a configuration issue? If configuration, what component? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "b3c3566e-ed17-4513-b822-11960e5df32a",
    "url": "https://discuss.elastic.co/t/how-to-get-infrastructure-ui-working/167484",
    "title": "How to get infrastructure UI working?",
    "category": [
      "Metrics"
    ],
    "author": "asp",
    "date": "February 8, 2019, 8:37pm February 8, 2019, 8:45pm March 8, 2019, 8:58pm",
    "body": "Hi, I've setup metricbeat not to directly write into elasticsearch, but I configured it to write to a logfile. I like to get the metrics buffered in filesystem for the case that metricbeat or sth in the complete pipeline breaks, so that I don't lose events. This metricbeat probe logfile I ship via filebeat to redis. Logstash is fetching the entires and processes them by this filter: input { redis { data_type => \"list\" db => \"0\" host => \"${REDIS_HOST}\" key => \"metricbeat\" port => \"${REDIS_PORT}\" } } filter { json { id => \"json\" source => \"message\" } # delete message if no _jsonparsefailure if (\"_jsonparsefailure\" not in [tags]) { mutate { remove_field => ['message'] } } } output { elasticsearch { hosts => [\"${ES_HOST}:${ES_PORT}\"] #index => \"%{[logType]}-%{+YYYY.MM.dd}\" index => \"%{[logType]}-%{+YYYY.ww}\" } } In my eyes the result looks good in discover module: image.png1405923 51.6 KB But if I go for the infrastructure button, then no results are shown: image.png1723709 28.6 KB Could you help me and point at the error what I missed? Yes, I removed the message field after successful parsing, but the json output of metricbeat log does not contain a field called message. So I doubt it has sth. to do with this. I did not create the metricbeat dashboards and visualizations which are shipped metricbeat. Is that the problem? Next week i have to present kibana to some people and I would like to get this module running until then. Thanks a lot, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "7c8200ab-785b-46de-88ef-39ef2ce2e8fd",
    "url": "https://discuss.elastic.co/t/infrastructure-host-name-metric-are-not-displaying-properly/167340",
    "title": "Infrastructure - host.name & metric are not displaying (properly)",
    "category": [
      "Metrics"
    ],
    "author": "alexus",
    "date": "February 6, 2019, 5:39pm February 6, 2019, 5:50pm February 6, 2019, 5:52pm February 6, 2019, 5:54pm February 6, 2019, 5:58pm February 6, 2019, 7:57pm February 6, 2019, 8:02pm February 6, 2019, 8:08pm February 6, 2019, 8:12pm March 6, 2019, 8:24pm",
    "body": "Hello World! I'm using Elastic stack 6.6.0 now and while at Infrastructure Kibana's app (on smaller screen), host.name and metric are not displaying correctly (missing) unless user hover over mouse over blank box and then user sees host and metric. I didn't had this issue with previous release (on same screen size). Please advise.",
    "website_area": "discuss"
  },
  {
    "id": "22377789-0512-4604-8b9f-7f306b0d9c4a",
    "url": "https://discuss.elastic.co/t/infrastructure-ui-with-windows-and-unix-hosts/163799",
    "title": "Infrastructure UI with Windows and Unix hosts",
    "category": [
      "Metrics"
    ],
    "author": "Marc-Antoine_J",
    "date": "January 10, 2019, 7:45pm January 11, 2019, 11:22am January 11, 2019, 5:37pm January 16, 2019, 10:05am January 16, 2019, 5:53pm January 17, 2019, 11:30am January 30, 2019, 11:13am February 27, 2019, 11:25am",
    "body": "I'm discovering the new Infrastructure UI and I like it so far. But I had a few issue especially with the year change. I was configuring my beats to use the metricbeat-{+YYYY.MM} index naming pattern. Since I set it up in december it only created one index (metricbeat-2018.12). Then I have set my elastic host (unix) to log metricbeats directly to elasticsearch. After I have set many Windows host to do the same. Everything worked fine so far. Then the month (and year) changed... The new index created with no templates. The infrastructure stopped showing the hosts info. I went to look for the template mapping and it was set for metricbeat-6.5.4 index names only. I changed it for metricbeat-*. It worked for my unix host then I noticed that I had set 2 different template mapping for windows hosts and unix host. I had set Windows hosts to metricbeat-local. Don't bother asking for the confusion it was before the holiday leave. My head was already gone. Then I noticed that the template were different. If it was created by the unix host it started with the docker settings and maybe things are just not in the same order I thought. But then fixing both configs (Windows and Unix) I noticed that both did not get their logs correctly in the templates. So now I have configured metricbeat-unix-{+YYYY.MM.dd} (added day for rollover) for unix and metricbeat-windows-{+YYYY.MM.dd} for Windows hosts. I thought that infrastructure was taking it's docs from metricbeat-* but now I can see I have both indexes, they both have tons of documents but infrastructure UI shows only Windows hosts. I must be missing something? Thanks,",
    "website_area": "discuss"
  },
  {
    "id": "86ba263f-67c6-46fa-9c29-c77908f3877b",
    "url": "https://discuss.elastic.co/t/infrastructure-logs-ui-change-index/163428",
    "title": "Infrastructure Logs UI change index?",
    "category": [
      "Metrics"
    ],
    "author": "krainboltgreene",
    "date": "January 9, 2019, 12:31am January 22, 2019, 7:25pm January 9, 2019, 6:03pm February 6, 2019, 6:03pm",
    "body": "Hi, the logs UI is incredible, I love it, but I would much rather be able to define which indices I'm watching. Is that feasible?",
    "website_area": "discuss"
  },
  {
    "id": "1c5fface-00c9-4e2d-a01d-a7ac72ea28dd",
    "url": "https://discuss.elastic.co/t/group-servers-in-one-square-box/159867",
    "title": "Group servers in one square box",
    "category": [
      "Metrics"
    ],
    "author": "mladen",
    "date": "December 7, 2018, 7:28am December 7, 2018, 10:28am January 4, 2019, 10:28am",
    "body": "Hello, Your new feature Infrastructure UI is awesome. For some time, me and my colleague building something similar in Canvas. I have one question about grouping server. Is it possible to get the result of tag query in one square box which will represent all servers tags belongs to? Servers will be expanded (drill down) when click on square box. For example, to display one square box for all SAP servers that may be drill down for details per server as it is now. The idea is if one the server exceeds the threshold the square box will change color to display warning in server group. BR, Mladen",
    "website_area": "discuss"
  },
  {
    "id": "e0015d08-9b29-46b3-96fc-b9f341629efe",
    "url": "https://discuss.elastic.co/t/multiple-failed-to-format-messages-in-log-stream/159579",
    "title": "Multiple 'failed to format' messages in log stream",
    "category": [
      "Metrics"
    ],
    "author": "kenbergquist",
    "date": "December 5, 2018, 5:32pm December 5, 2018, 6:08pm December 5, 2018, 6:22pm December 6, 2018, 12:36pm January 3, 2019, 12:36pm",
    "body": "I'm seeing the message 'failed to format message from /var/log/audit/audit.log' repeatedly in the Infrastructure app log stream. Disabling the auditd module stops the messages, as you'd expect. Any insight would be sincerely appreciated.",
    "website_area": "discuss"
  },
  {
    "id": "54f99d05-674c-48d2-b945-dd028fb3e037",
    "url": "https://discuss.elastic.co/t/where-is-my-infrastructure-data/157039",
    "title": "Where is my infrastructure data?",
    "category": [
      "Metrics"
    ],
    "author": "ethrbunny",
    "date": "November 16, 2018, 11:36am November 16, 2018, 12:30pm November 16, 2018, 6:22pm November 16, 2018, 7:00pm November 17, 2018, 1:43pm November 18, 2018, 11:29pm November 19, 2018, 1:33pm November 19, 2018, 6:40pm November 20, 2018, 2:54am November 23, 2018, 7:41am November 23, 2018, 12:57pm November 23, 2018, 1:45pm November 23, 2018, 2:43pm November 23, 2018, 3:02pm November 23, 2018, 3:11pm November 23, 2018, 3:14pm November 23, 2018, 11:15pm November 26, 2018, 12:47pm November 27, 2018, 12:45pm November 28, 2018, 6:24pm",
    "body": "Just discovered the new \"infrastructure\" menu item. Unfortunately, despite having many, many GB of data there doesn't seem to be anything to display here. I have plenty of dashboard data and am gathering from a slew of different *beat providers. I've tried a variety of options in the \"search for infrastructure data\" field. So - what am I doing wrong?",
    "website_area": "discuss"
  },
  {
    "id": "2bb38643-091d-47fe-aa47-7966cc42d683",
    "url": "https://discuss.elastic.co/t/infrastructure-ui-colouring-feedback/159152",
    "title": "Infrastructure UI Colouring Feedback",
    "category": [
      "Metrics"
    ],
    "author": "adaisley",
    "date": "December 3, 2018, 11:21am December 5, 2018, 9:33am December 31, 2018, 12:51pm",
    "body": "Hi, Overall, the Infrastructure UI is absolutely fantastic! I had recently started looking into metricbeat to log our hosts and our Docker containers, and then v6.5 released and so we instantly upgraded to get this awesome new feature. My main question, and I suppose feedback at the moment is that the gradient for the metricset usage is not changeable. A member of our team is red/green colourblind and he said the shading for the grey-blue part of the gradient is not at all helpful and that he's unable to tell the difference. It would be nice if we could make our own gradient, so every member of our team doesn't have difficulty using the software. I know it's in Beta currently and this might already be a future feature. Something to bear in mind I guess cheers",
    "website_area": "discuss"
  },
  {
    "id": "7fca48a6-c856-40af-aa6b-29688e72f655",
    "url": "https://discuss.elastic.co/t/dark-mode/157958",
    "title": "Dark mode",
    "category": [
      "Metrics"
    ],
    "author": "alexus",
    "date": "November 23, 2018, 4:33am November 23, 2018, 2:44pm December 21, 2018, 10:45am",
    "body": "Is there a way to enable \"dark mode\"? Kibana in general definitely should be more \"dark mode\" friendly for sure...",
    "website_area": "discuss"
  },
  {
    "id": "3dc0e52f-edc1-4d5b-8f08-d9733e158b08",
    "url": "https://discuss.elastic.co/t/about-the-uptime-category/167978",
    "title": "About the Uptime category",
    "category": [
      "Uptime"
    ],
    "author": "warkolm",
    "date": "February 17, 2019, 12:45am March 27, 2019, 2:07pm March 27, 2019, 9:25pm March 27, 2019, 9:25pm",
    "body": "Welcome to the place to discuss all things Uptime. This is the perfect place to discuss the Uptime Kibana App. If you have questions, think you may have found a bug, or would like to discuss the Uptime app in any other way, post here!",
    "website_area": "discuss"
  },
  {
    "id": "b357b600-12cb-4761-863c-3fe522b2e1b6",
    "url": "https://discuss.elastic.co/t/missing-info-in-dashboard/215129",
    "title": "Missing info in dashboard",
    "category": [
      "Uptime"
    ],
    "author": "Heatzone87",
    "date": "January 15, 2020, 12:12pm",
    "body": "Hey, I have Heartbeat running. It is registering data correctly however in Kibana i cant see any monitors? The graph itself shows that it is registering data. image1810528 42.8 KB Best Pim",
    "website_area": "discuss"
  },
  {
    "id": "81b31f55-a1b4-4c3e-a6ab-76d2a354366b",
    "url": "https://discuss.elastic.co/t/heartbeat-checking-multiple-http-response-codes/214959",
    "title": "[Heartbeat] Checking multiple http response codes",
    "category": [
      "Uptime"
    ],
    "author": "seblg",
    "date": "January 14, 2020, 10:12am",
    "body": "Hi, We've started using heartbeat agents (6.8.2) and uptime monitoring for monitoring api endpoints. We'd like to attribute all http reponse codes which are not 500 or 503 as being \"up\", going through documentation this doesn't seem possible. Either we do not set, in the monitor yml, check.response: status: x and by default 4xx and 5xx are considered down, either we are only able to set one value in check.response status which will be labeled as up. Is there a workaround or a solution for passing a list of response codes we expect to mean up or ones we want to set to down ? Thanks,",
    "website_area": "discuss"
  },
  {
    "id": "6284f72e-2059-4bed-8ac0-f5589bac22bb",
    "url": "https://discuss.elastic.co/t/7-5-there-was-an-error-retrieving-the-index-pattern/210742",
    "title": "[7.5] There was an error retrieving the index pattern",
    "category": [
      "Uptime"
    ],
    "author": "robrotheram",
    "date": "December 5, 2019, 4:24pm December 10, 2019, 3:05am December 10, 2019, 10:31am December 11, 2019, 2:39pm December 11, 2019, 11:41pm December 13, 2019, 2:53pm December 26, 2019, 10:14am January 2, 2020, 10:21am January 2, 2020, 1:47pm January 2, 2020, 3:23pm January 7, 2020, 9:37pm",
    "body": "I just updated to 7.4.0 -> 7.5.0 and in uptime I now see below the query bar the warning There was an error retrieving the index pattern But I can see all 100+ monitors and can filter on location. But the query bar is grey out and is disabled. I have made sure that the 2 heartbeats are updated to 7.5 and rerun the setup just to make sure. I also tried to delete all kibana indexes and restart kibana no such luck Of note is that this is running in a air gaped environment and so I disabled newsfeed and telemetry",
    "website_area": "discuss"
  },
  {
    "id": "0e6b2a5c-01f2-49e9-a7e1-56b5a693d1ba",
    "url": "https://discuss.elastic.co/t/uptime-indices-setting-available/212311",
    "title": "Uptime indices setting available?",
    "category": [
      "Uptime"
    ],
    "author": "norgro2601",
    "date": "December 18, 2019, 11:58am January 6, 2020, 10:07am",
    "body": "Hi, in the spaces I've created, I limit access to certain indices, so space A has access to heartbeat-a-* indices and space B has access to heartbeat-b-* indices only. I've created role A and role B to achieve this. In the Metrics app or SIEM app, there are space related settings available, which I can use to set the used indices for that app in that space, for the Uptime app, I don't have this. This is a problem, if a user has both roles, then he sees the heartbeat monitors for both roles in the Uptime app, no matter, which space he is using. Are there any hidden settings I can use eg. with the saved objects APIs to set an indices value ? Regards, Norbert",
    "website_area": "discuss"
  },
  {
    "id": "f06af737-8261-42da-a03c-a5de62bbf915",
    "url": "https://discuss.elastic.co/t/uptime-availability-solution/212071",
    "title": "Uptime Availability Solution",
    "category": [
      "Uptime"
    ],
    "author": "Mauricio_Borges",
    "date": "December 16, 2019, 10:59pm December 17, 2019, 8:49pm January 10, 2020, 8:49pm",
    "body": "Heartbeat and Uptime are great tools to probe ICMP, TCP Ports and Webpages. However going further I'd like to know if exist or there plans to provide Reports and Visualizations/Dashboards. Such as: System and Services history be exportable ( ie: pdf, office files ), so we can save a Status to provide reports internally to customer. For example, suppose we have an Agreement with a customer to keep 95% of availability for specific Service, so it will be required provide some Monthly Reports to check availability and possible outages. Ability to create dynamic visualizations ( or maybe a canvas ) to show up a current status for specific Service or System. Or maybe some templates to easy integrate with Uptime. Easy way to create an Watch Alert on UpTime page. IE: If Service or System fail for X times send Alert to .... Thanks, Mauricio",
    "website_area": "discuss"
  },
  {
    "id": "6706a14a-e359-4187-bf5e-75f2f876ee66",
    "url": "https://discuss.elastic.co/t/uptime-7-5-overview-pagination-settings/210259",
    "title": "Uptime 7.5: Overview pagination settings",
    "category": [
      "Uptime"
    ],
    "author": "Slavik_Fursov",
    "date": "December 3, 2019, 12:18am December 5, 2019, 6:51pm December 29, 2019, 6:51pm",
    "body": "Upgraded to 7.5 I see, that Uptime app now has pagination. ok. However, it looks like I can't change number of rows per page. Is it true? Looks to be major omission. The app (only 10 rows) now looks strange on my 32\" 4k monitor. Submitted issue: github.com/elastic/uptime Overview Pagination: add ability to specify N of rows/page opened 11:16PM - 02 Dec 19 UTC SlavikCA 7.5.0 added Pagination to the overview page. However, it always 10 rows per page. Doesn't look good on my 32\" 4k monitor. Previously (without...",
    "website_area": "discuss"
  },
  {
    "id": "096d2b5c-95b5-44d5-ae8a-88b0c035d106",
    "url": "https://discuss.elastic.co/t/links-in-uptime-do-not-work-7-4/206983",
    "title": "Links in uptime do not work [7.4]",
    "category": [
      "Uptime"
    ],
    "author": "robrotheram",
    "date": "November 11, 2019, 1:48pm November 14, 2019, 8:48pm December 10, 2019, 10:33pm",
    "body": "I have upgraded to kibana 7.4 from 6.7 and the links in uptime to view in infra UI or Logui do not work. I running kibana through nginx and I have a basepath set to /kibana . I have set up a rewrite rule in nginx rewrite /kibana/(.*)$ /$1 The I have set the base-path in kibana to /kibana and set rewriteBasePath to false. if I click on one of the links in Uptime for example \"show host metrics\" I go to \"https://kibana/app/infra...\" which does not exist. It seem to be stripping out the server url. I am unsure what has gone wrong",
    "website_area": "discuss"
  },
  {
    "id": "612958ab-6f19-4c57-8734-c7cd615d2877",
    "url": "https://discuss.elastic.co/t/allow-monitoring-of-metricbeat-uptime-data-in-kibana-uptime-app/205395",
    "title": "Allow monitoring of metricbeat uptime data in Kibana Uptime app",
    "category": [
      "Uptime"
    ],
    "author": "bruce289",
    "date": "October 27, 2019, 11:32pm November 6, 2019, 9:09pm November 7, 2019, 9:32pm December 1, 2019, 9:32pm",
    "body": "Hi, The metricbeat system module is able to ship uptime metrics to elasticsearch: https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-metricset-system-uptime.html It would be great if these were made available in the Kibana Uptime view. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "112f33bc-42bb-42a2-8f8f-6ddcdca0d4e8",
    "url": "https://discuss.elastic.co/t/monitor-the-json-payload-from-heartbeat-yml/199740",
    "title": "Monitor the JSON payload from heartbeat.yml",
    "category": [
      "Uptime"
    ],
    "author": "mfang329",
    "date": "September 17, 2019, 12:24am October 9, 2019, 12:52pm November 2, 2019, 12:52pm",
    "body": "I'm new to the uptime / heartbeat service Version 6.8.3 that I'm trying to config brunch of urls which sends the healthbeat result to Uptime Kibana. The basic JSON payload body should return the following output, EG: curl -i https://identityserver-p-xx-ue1.abc.com/api/v1/health/details {\"CanConnectToIdentityDB\":\"Ok At: 09/17/2019 00:05:13 +00:00\",\"CanConnectToUserProfileService\":\"Ok At: 09/17/2019 00:05:13 +00:00\"} curl -i \"https://openidapi-d-xx-ue1.bluebeam-dev.com/api/v1/health/details\" {\"canConnectToIdentityDB\":\"Ok At: 09/17/2019 00:05:22 +00:00\",\"canConnectToUserProfileService\":\"Ok At: 09/17/2019 00:05:22 +00:00\"} What I'm trying to capture in the heartbeat.yaml is to detect the JSON response has a validate object such that Key can be any service strings and Value is \"OK\" without the timestamp string. {\"CanConnectToIdentityDB\":\"Ok*\", \"CanConnectToUserProfileService\":\"OK*\"} or {\"CanConnectToIdentityDB\":\"Ok*\"} From what I read here, https://www.elastic.co/guide/en/beats/heartbeat/6.8/configuration-heartbeat-options.html, it seems possible with JSON condition and mix multiline regex clauses. There is not much example in this area or any answer on the web, if you can provide an example on how to go about it then it would be wonderful.",
    "website_area": "discuss"
  },
  {
    "id": "832e0f29-93d9-49cb-af01-57da602937a2",
    "url": "https://discuss.elastic.co/t/missing-data-when-using-using-fields-and-tags-in-uptime-7-3-0/199460",
    "title": "Missing data when using using fields and tags in uptime 7.3.0",
    "category": [
      "Uptime"
    ],
    "author": "Dannyb",
    "date": "September 13, 2019, 4:21pm September 19, 2019, 1:54pm September 19, 2019, 4:18pm September 19, 2019, 6:13pm September 23, 2019, 2:25pm September 23, 2019, 6:42pm October 17, 2019, 6:42pm",
    "body": "Ive added some tags to my heartbeat monitor: fields: Environment: test SiteName: mysite fields_under_root: true ( although this doesn't make any different to the issue) Soon as I do this on the uptime board, the monitor status grid doesnt show anything.. it just shows the status going from left to right. the pings over time works fine. The search text box just has loading,. In it. I can find my documents in discovery searching against the new fields If remove the fields, delete the heart indexes, fresh the kibana indexes, pray a little it eventually sorts itself Also, if I add tags the search doesnt work in the uptime search bar like the discovery tab does.. Works: tags:api doesnt work tags:api AND tags:dev Im I using it wrong? Is there any reason guys didnt just create normal dashboards like the rest of us have to instead of creating your own custom ones? Ive just got the continue frustration and disappointment when trying to do anything in kibana dashboards. I love the stack, but just things always fall short. thanks",
    "website_area": "discuss"
  },
  {
    "id": "128976a2-b5ff-4550-823c-cd7846355683",
    "url": "https://discuss.elastic.co/t/monitors-not-showing-in-uptime/198243",
    "title": "Monitors not showing in Uptime",
    "category": [
      "Uptime"
    ],
    "author": "norgro2601",
    "date": "September 5, 2019, 12:18pm September 13, 2019, 12:28pm September 13, 2019, 12:28pm October 7, 2019, 12:28pm",
    "body": "I have an index with name \"heartbeat-something\". In the considerations for uptime it's said, that uptime requires an index pattern called \"heartbeat-7*\". https://www.elastic.co/guide/en/uptime/current/install-heartbeat.html I've created an alias \"heartbeat-7-something\" as the workaround, but still uptime is not showing any data. What have I missed ? Regards, Norbert",
    "website_area": "discuss"
  },
  {
    "id": "836d5320-9906-4729-a124-1fcefa0b82ae",
    "url": "https://discuss.elastic.co/t/monitor-status-table/196921",
    "title": "Monitor Status Table",
    "category": [
      "Uptime"
    ],
    "author": "Surya369",
    "date": "August 27, 2019, 10:09am August 29, 2019, 3:28pm September 22, 2019, 3:39pm",
    "body": "There has been an issue that the monitor status tab is only able to show 50 entries whereas I have setup this system for 54 entries. The endpoint status is displaying that all the 54 servers are up but the monitor status is only able to display 50 entries. Is this a limitation of the monitor status tab?? My elastic version is 6.8.1",
    "website_area": "discuss"
  },
  {
    "id": "fda0450a-fe49-4579-84c1-21433bc1c6c8",
    "url": "https://discuss.elastic.co/t/add-id-as-one-of-the-columns-index-page/196371",
    "title": "Add ID as one of the columns (index page)",
    "category": [
      "Uptime"
    ],
    "author": "shanekm",
    "date": "August 22, 2019, 4:52pm August 22, 2019, 5:06pm August 22, 2019, 6:43pm August 22, 2019, 7:38pm September 15, 2019, 7:38pm",
    "body": "I've set up everything and it's working. However my urls for heartbeat are pretty long (under ID drop down) ex: http://myservertocheck.uat.net:19081/QA/Configuration.Service/Status.Api/api/application/status/QA.Configuration -Is there any way to have ID field show up on the main screen as one of the columns? -Better yet, since ID in my case is long can it only display the last string after / ie \"QA.Configuration\" Example: image.png1892733 57.1 KB",
    "website_area": "discuss"
  },
  {
    "id": "f288b4f7-2aa9-48e3-9b02-46dbdb1aecf4",
    "url": "https://discuss.elastic.co/t/how-to-use-fields-and-attributes-in-uptime/185203",
    "title": "How to use fields and attributes in Uptime?",
    "category": [
      "Uptime"
    ],
    "author": "Johntdyer",
    "date": "June 11, 2019, 1:50pm July 5, 2019, 1:58pm August 10, 2019, 2:35am",
    "body": "I have configured Heartbeat to send fields via config but I am not sure how I would actually make use of those fields in the \"Uptime\" panel in Elasticsearch ? Are there any examples of how to make use of these attributes? I would love to for example search all endpoints that have field:foo and a response time over xxx seconds...",
    "website_area": "discuss"
  },
  {
    "id": "ce32f021-dbb8-4bd2-8868-745853482244",
    "url": "https://discuss.elastic.co/t/uptime-just-spins/193900",
    "title": "Uptime Just Spins",
    "category": [
      "Uptime"
    ],
    "author": "richard_N",
    "date": "August 6, 2019, 12:24am August 6, 2019, 11:43am August 10, 2019, 2:34am September 3, 2019, 2:34am",
    "body": "Just setup a fresh 7.3 cluster across the board. Everything was working but I deleted an old heartbeat index and now my Uptime dashboard just spins. The index recreated correctly and I can view all heartbeats on the Discover dashboard but nothing on Uptime. I just get the loading screen and it never stops. Everything is set to defaults. Index pattern is default 7.3 index pattern. Template is there.",
    "website_area": "discuss"
  },
  {
    "id": "0eb9a10c-a91a-414f-be61-da6505219c45",
    "url": "https://discuss.elastic.co/t/using-a-different-index-other-than-heartbeat-7-0-0/193134",
    "title": "Using a different Index other than Heartbeat-7.0.0",
    "category": [
      "Uptime"
    ],
    "author": "PeterPain",
    "date": "July 31, 2019, 2:09pm August 1, 2019, 8:17am August 2, 2019, 4:11am August 5, 2019, 10:30am August 5, 2019, 10:30am August 29, 2019, 10:30am",
    "body": "Hey dear Elastic-Team, The documentation gives following specification about how the index should be named for uptime App. Kibana 7.0 requires an index of heartbeat-7* (containing documents from Heartbeat 7.0). And im using Heartbeat but have a different name for my index so im not able to use the Uptime App. Is it possible to use a different index name than Heartbeat-7* for the Uptime-App ? Or is this currently not possible ? Thanks for any response",
    "website_area": "discuss"
  },
  {
    "id": "27bcf85d-2ffb-413e-a11a-d68029937a42",
    "url": "https://discuss.elastic.co/t/heatbeat-with-snmp/185398",
    "title": "Heatbeat with SNMP",
    "category": [
      "Uptime"
    ],
    "author": "Michel99_7",
    "date": "June 12, 2019, 11:35am July 6, 2019, 11:35am July 10, 2019, 10:19am",
    "body": "It would be nice to use such a tool to request a device via a snmp request...",
    "website_area": "discuss"
  },
  {
    "id": "7c930594-6642-40c8-ad3e-dd14c7a1fd95",
    "url": "https://discuss.elastic.co/t/uptime-for-iot-devices-without-heartbeat-tool/180014",
    "title": "Uptime for IoT devices without heartbeat tool",
    "category": [
      "Uptime"
    ],
    "author": "michaelsogos",
    "date": "May 7, 2019, 3:35pm May 9, 2019, 8:29pm May 10, 2019, 7:59am May 10, 2019, 12:07pm May 10, 2019, 12:42pm May 10, 2019, 4:22pm May 10, 2019, 5:01pm June 3, 2019, 5:09pm July 9, 2019, 5:39am",
    "body": "We have some IoT devices installed on an external area (think of a retail chain which for each shop has installed our IoT devices). And our main priority is to get UPTIME status for each of them. We investigate about Heartbeat, but seems that Heartbeat is responsible to collect information via HTTP, TCP or ICMP. Unfortunately no one of the above cases fit our needs because the IoT device is behind an firewall and not accessible from outside, then we need to SEND HEARTBEAT SIGNAL from IoT device to Elastic cluster (the opposite how heartbeat works). How we can do that? Also we saw and investigate about APM (for nodejs in our case) and we found a lot of usefull information about LOGS and METRICS but nothing about UPTIME; do you think that APM should also care about UPTIME metric?",
    "website_area": "discuss"
  },
  {
    "id": "dcbc4751-684d-49ed-8686-6bf65bb9d3cf",
    "url": "https://discuss.elastic.co/t/how-to-upgrade-uptime-from-6-8-to-7-1-easily/186790",
    "title": "How to upgrade Uptime from 6.8 to 7.1 easily",
    "category": [
      "Uptime"
    ],
    "author": "skyluke.1987",
    "date": "June 21, 2019, 3:31am June 21, 2019, 3:45am July 15, 2019, 3:45am",
    "body": "Dear all, recently I updated the Kibana from 6.7 to 7.1.1. When I launch the new kibana and found out that the data of uptime is coming in, but the visualization can't be loaded. Thinking to upgrade straightaway, but can't really find steps to do it directly. Anyone has came across any guide ?",
    "website_area": "discuss"
  },
  {
    "id": "cb741b1b-21af-4ac0-bf62-fa8129ef2b0d",
    "url": "https://discuss.elastic.co/t/query-from-uptime-app-causes-outofmemory-exception-in-elasticsearch/186206",
    "title": "Query from Uptime app causes OutOfMemory exception in Elasticsearch",
    "category": [
      "Uptime"
    ],
    "author": "Mattias_Arbin",
    "date": "June 18, 2019, 9:19am June 18, 2019, 9:04am June 18, 2019, 9:05am June 18, 2019, 11:22am June 18, 2019, 11:50am June 19, 2019, 8:24am July 13, 2019, 8:24am",
    "body": "I have a three node ELK cluster with 500M documents in 900 indices. The cluster has been running without any memory-related issues for over a year. I recently started using heartbeat and the Uptime app. Version is 6.8.0. I suddenly started getting OutOfMemory-exceptions that crashed one of the elasticsearch nodes. After some investigations, it is clear that this happens when you click on a link in the Error list table. See screenshot. I get a crash everytime I hit the top row in the table. I have heartbeat data for only 15 days. 2880 heartbeats per day in daily indices. 1 primary and one replica per index.",
    "website_area": "discuss"
  },
  {
    "id": "dddcb1cb-880d-4ae5-ba13-13b7385127b1",
    "url": "https://discuss.elastic.co/t/uptime-tab-says-no-data-available-when-heartbeat-today-index-keeps-incrementing/179160",
    "title": "Uptime Tab says \"No Data Available\" when heartbeat-today index keeps incrementing",
    "category": [
      "Uptime"
    ],
    "author": "phillhocking",
    "date": "April 30, 2019, 10:54pm May 1, 2019, 6:04am May 1, 2019, 7:14am May 1, 2019, 3:31pm May 1, 2019, 5:11pm May 16, 2019, 5:10am June 9, 2019, 5:10am",
    "body": "Hi Elastic friends! My Uptime app tab says \"No uptime data available - Configure Heartbeat to start logging uptime data\" but I have verified that heartbeat is shipping data at least according to _cat/indices: image.png1922782 69.8 KB It does appear like the uptime data is getting populated and logs on the machine that heartbeat is running on suggests that the data is being shipped, but Uptime doesn't seem to be getting the memo. What am I missing?",
    "website_area": "discuss"
  },
  {
    "id": "534f8462-424e-4cfa-a775-db35edfbd529",
    "url": "https://discuss.elastic.co/t/uptime-error-graphql-error-too-many-buckets-exception-trying-to-create-too-many-buckets-must-be-less-than-or-equal-to-10000-but-was-10001/179432",
    "title": "Uptime: `Error GraphQL error: [too_many_buckets_exception] Trying to create too many buckets. Must be less than or equal to: [10000] but was [10001]`",
    "category": [
      "Uptime"
    ],
    "author": "phillhocking",
    "date": "May 2, 2019, 8:00pm May 2, 2019, 8:22pm May 2, 2019, 8:31pm May 3, 2019, 6:58am May 3, 2019, 10:58am May 3, 2019, 8:29pm May 3, 2019, 9:29pm May 3, 2019, 11:27pm May 3, 2019, 11:48pm May 27, 2019, 11:48pm",
    "body": "Hey Elastic friends! I recently set up my Elastic cluster with the new 7.0.0 for the purpose of using the Uptime visualizations. I got everything working well, but now when I look at the Uptime tab I receive this error underneath the uptime summary: Error GraphQL error: [too_many_buckets_exception] Trying to create too many buckets. Must be less than or equal to: [10000] but was [10001]. This limit can be set by changing the [search.max_buckets] cluster level setting., with { max_buckets=10000 } image.png1884848 85 KB I came across this thread Requesting background info on `search.max_buckets` change that says this value is configurable, however, I am not sure where to configure this or what effect changing this value might have on the remainder of my cluster. Additionally, aside from having configured ILM, everything is a default setting - so perhaps the defaults should be changed? I look forward to hearing suggestions for how I can remediate this issue; thanks!",
    "website_area": "discuss"
  },
  {
    "id": "d094162d-00ce-491a-8383-dd96b362ed1a",
    "url": "https://discuss.elastic.co/t/uptime-version-7-0-graphql-error/176661",
    "title": "Uptime version 7.0 Graphql error",
    "category": [
      "Uptime"
    ],
    "author": "Luis_Pereira1",
    "date": "April 12, 2019, 3:28pm April 12, 2019, 7:12pm April 15, 2019, 8:52am April 15, 2019, 9:09am April 22, 2019, 5:52pm April 24, 2019, 1:53pm April 25, 2019, 9:30pm April 30, 2019, 5:10pm May 24, 2019, 5:10pm",
    "body": "After the update to version 7.0 the uptime is giving me this error imagem.png1856128 91.5 KB Any help is appreciated...",
    "website_area": "discuss"
  },
  {
    "id": "e35506fd-a355-4d22-b04f-8ec9856ba8d2",
    "url": "https://discuss.elastic.co/t/graphql-errors-uptime-heartbeat/178473",
    "title": "GraphQL errors uptime heartbeat",
    "category": [
      "Uptime"
    ],
    "author": "amergan",
    "date": "April 25, 2019, 1:30pm April 25, 2019, 9:31pm April 26, 2019, 4:21am April 29, 2019, 2:39pm April 29, 2019, 3:27pm April 30, 2019, 5:10pm May 24, 2019, 5:10pm",
    "body": "After configuring heartbeat, i get these errors in uptime. Error GraphQL error: [search_context_exception] unknown type for collapse field monitor.id, only keywords and numbers are acceptedError GraphQL error: [illegal_argument_exception] Fielddata is disabled on text fields by default. Set fielddata=true on [monitor.id] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. Error GraphQL error: [illegal_argument_exception] Fielddata is disabled on text fields by default. Set fielddata=true on [monitor.id] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. Error GraphQL error: [illegal_argument_exception] Fielddata is disabled on text fields by default. Set fielddata=true on [monitor.id] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.",
    "website_area": "discuss"
  },
  {
    "id": "13dbe043-f681-4dd8-bcf9-691fe622c682",
    "url": "https://discuss.elastic.co/t/kibana-uptime-built-in-dashboard/177644",
    "title": "Kibana Uptime (built-in dashboard)",
    "category": [
      "Uptime"
    ],
    "author": "Ryan_Clark",
    "date": "April 19, 2019, 9:34pm April 19, 2019, 10:05pm April 19, 2019, 10:17pm April 19, 2019, 10:23pm April 24, 2019, 1:05am April 25, 2019, 4:23pm April 26, 2019, 1:27pm May 24, 2019, 1:28pm",
    "body": "Setup: ELK with heartbeat v6.7.1 Heartbeat configured to ping 300+ hosts Problem: In the uptime section in Kibana, it shows me my total hosts count, up count, and down count (with accurate numbers) but in the monitoring status visualization, it only shows me a maximum of 50 hosts. Why is it not showing me the status of all 300+ hosts? I tried searching to see if anyone had the same issue and I'm not finding anything.",
    "website_area": "discuss"
  },
  {
    "id": "5c3bf1a4-679b-495d-b737-db8b6c478645",
    "url": "https://discuss.elastic.co/t/how-to-change-the-unique-id-in-the-id-field-es-7-0/177887",
    "title": "How to change the unique ID in the ID Field? (ES 7.0)",
    "category": [
      "Uptime"
    ],
    "author": "Matt_Vasquez",
    "date": "April 22, 2019, 5:59pm April 23, 2019, 7:55pm May 17, 2019, 7:55pm",
    "body": "How do I change the value in the ID Field in Up Time? I'd like to replace this with a value such as ID NGINX Reverse Proxy Prod Application Portal Report Application Portal Prod Database Report Database etc.",
    "website_area": "discuss"
  },
  {
    "id": "4435c24a-e31a-428a-9480-cd43bbc81fd3",
    "url": "https://discuss.elastic.co/t/autocomplete-for-search-ui-uptime/177162",
    "title": "Autocomplete for Search UI Uptime",
    "category": [
      "Uptime"
    ],
    "author": "Deyvis_Valladares",
    "date": "April 16, 2019, 10:31pm April 17, 2019, 6:06am May 11, 2019, 6:06am",
    "body": "Hi Team, I have a suggest for UI Uptime. I believe is necessary in filter search set autocomplete as Discovery UI filter.",
    "website_area": "discuss"
  },
  {
    "id": "e71b4644-6f70-476e-abe1-5924dff4b262",
    "url": "https://discuss.elastic.co/t/gui-suggestion/176692",
    "title": "GUI suggestion",
    "category": [
      "Uptime"
    ],
    "author": "syedsfayaz",
    "date": "April 12, 2019, 6:53pm April 15, 2019, 8:59am April 15, 2019, 5:19pm May 9, 2019, 5:19pm",
    "body": "It would be nice we can see the same GUI as an infrastructure. where I can monitor multiple sites as cubes. I am only monitoring status of multiple site inside my environment. I am monitoring 500 server urls http status. My requirement is only to see what the last status of my http health check is. It will be very convenient to categorize them with a custom field and monitor. The infrastructure dashboard GUI is very appealing. Env status we can show green/red color. If we need more details we can drill down to see whats going on.",
    "website_area": "discuss"
  },
  {
    "id": "e30b8b88-7baa-478f-87c4-220a901a691c",
    "url": "https://discuss.elastic.co/t/gui-suggestions/176456",
    "title": "GUI Suggestions",
    "category": [
      "Uptime"
    ],
    "author": "Ben_Frost",
    "date": "April 11, 2019, 4:34pm April 15, 2019, 8:58am May 9, 2019, 8:58am",
    "body": "It would be nice to be able to get the status page to fit on a single page versus scrolling or be able to list the errors before the monitor status. It would also be nice to be able to customize the columns to include fields such as tags or more details on what is being monitored. For example, if you have 5 http checks on a host, the monitor status shows just the hostname 5 times (with the port) but no URL information so it's not clear which one(s) are having an issue.",
    "website_area": "discuss"
  },
  {
    "id": "75675e1d-3d54-4bcd-a386-12cfd4728650",
    "url": "https://discuss.elastic.co/t/parse-the-field-host/176069",
    "title": "Parse the field host",
    "category": [
      "Uptime"
    ],
    "author": "Luis_Pereira1",
    "date": "April 9, 2019, 4:46pm April 9, 2019, 5:27pm April 10, 2019, 9:11am April 10, 2019, 12:44pm April 10, 2019, 5:11pm May 4, 2019, 5:19pm",
    "body": "Hello can you guys tell me how can i parse this ... I want to change the field host instead of saying icmp-icmp, i want a description or a name of the device. What options are available to do this ?",
    "website_area": "discuss"
  },
  {
    "id": "facc7c46-44b8-44e1-8c94-dd50635bf8af",
    "url": "https://discuss.elastic.co/t/get-uptime-percentage/176182",
    "title": "Get uptime percentage",
    "category": [
      "Uptime"
    ],
    "author": "OrangeDog",
    "date": "April 10, 2019, 9:29am April 10, 2019, 12:01pm April 10, 2019, 2:32pm May 4, 2019, 2:32pm",
    "body": "I'm not seeing a per-service uptime percentage anywhere in the UI? What's the best way to get it?",
    "website_area": "discuss"
  },
  {
    "id": "812d01f1-1ff0-447a-bd24-8289c04b4092",
    "url": "https://discuss.elastic.co/t/custom-heartbeat-shipper/176165",
    "title": "Custom heartbeat shipper",
    "category": [
      "Uptime"
    ],
    "author": "suikast42",
    "date": "April 10, 2019, 8:32am April 10, 2019, 12:04pm April 10, 2019, 12:04pm May 4, 2019, 12:04pm",
    "body": "I want track the health state of an application that exposes it's health over mocriprofile heatlhcheck api. So I have multible states in one healthcheck. I can't find in heartbeat a monitor that can handle this. So I end up with a custom solution. I look at the exported fields of heartbeat and try to create a hearbeat entry with that json fields jsonMap.put(\"@timestamp\", zonedDateTime.toString()); jsonMap.put(\"@metadata.beat\", \"heartbeat\"); jsonMap.put(\"@metadata.type\", \"doc\"); jsonMap.put(\"@metadata.version\", \"6.7.0\"); jsonMap.put(\"event.dataset\", \"uptime\"); jsonMap.put(\"beat.name\", \"heartbeat\"); jsonMap.put(\"beat.hostname\", \"heartbeat\"); jsonMap.put(\"beat.version\", \"6.7.0\"); jsonMap.put(\"host.name\", \"heartbeat\"); jsonMap.put(\"tcp.port\", 9090); jsonMap.put(\"monitor.id\", check.getName()); jsonMap.put(\"monitor.status\", check.getState().name().toLowerCase()); jsonMap.put(\"monitor.name\", \"http\"); jsonMap.put(\"monitor.type\", \"http\"); jsonMap.put(\"monitor.scheme\", \"http\"); jsonMap.put(\"monitor.ip\", \"172.17.8.101\"); jsonMap.put(\"http.url\", \"http://172.17.8.1:9990/health\"); jsonMap.put(\"tcp.rtt.connect.us\",50); jsonMap.put(\"http.rtt.response_header.us\",50); jsonMap.put(\"http.rtt.validate.us\",50); jsonMap.put(\"http.rtt.content.us\",50); jsonMap.put(\"http.rtt.write_request.us\",50); jsonMap.put(\"http.rtt.total.us\",5000); if (check.getState() == Entry.State.UP) { jsonMap.put(\"http.response.status_code\", 200); } else { if (check.getData() != null) { jsonMap.put(\"error.type\", \"validate\"); jsonMap.put(\"error.message\", check.getData().getErrorMessage()); } jsonMap.put(\"http.response.status_code\", _outcome.getStatusCode().value()); } With that approach I can see my entries in monitor status but have the follwing problems: Status is invalid Type and IP not shown image.png1636466 29.3 KB",
    "website_area": "discuss"
  },
  {
    "id": "14678ee7-c7f7-4175-8582-33564d53e37d",
    "url": "https://discuss.elastic.co/t/any-alert-integration-can-be-setup/175671",
    "title": "Any alert integration can be setup?",
    "category": [
      "Uptime"
    ],
    "author": "",
    "date": "April 6, 2019, 9:56am April 6, 2019, 11:59am April 8, 2019, 2:56pm April 8, 2019, 3:59pm April 8, 2019, 8:47pm April 9, 2019, 8:38am May 3, 2019, 8:38am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "d0d2201f-a1b9-4c1b-a619-dcd412b3cb43",
    "url": "https://discuss.elastic.co/t/6-6-upgrade-to-6-7-on-elastic-cloud-uptime-data-not-showing/175004",
    "title": "6.6 upgrade to 6.7 on Elastic Cloud Uptime data not showing",
    "category": [
      "Uptime"
    ],
    "author": "frankfoti",
    "date": "April 2, 2019, 1:29pm April 2, 2019, 2:45pm April 2, 2019, 9:45pm April 3, 2019, 6:36pm April 3, 2019, 8:47pm April 3, 2019, 8:48pm April 4, 2019, 11:24am April 28, 2019, 11:24am",
    "body": "We upgraded from 6.6 to 6.7 on Elastic Cloud Elasticsearch and Kibana. Then upgraded heartbeat on our internal vm's shipping to logstash. We also re-created the heartbeat template from the heartbeat 6.7 home. The Uptime page shows in kibana but no data is viewable. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "afc17f9e-78d2-4304-a9b1-9bd27b0038c7",
    "url": "https://discuss.elastic.co/t/downtime-for-sla-reporting/174146",
    "title": "Downtime for SLA-Reporting",
    "category": [
      "Uptime"
    ],
    "author": "icefish-creativ",
    "date": "March 27, 2019, 9:18pm March 27, 2019, 9:57pm March 27, 2019, 10:49pm April 20, 2019, 10:49pm",
    "body": "Hi Uptime is a really great feature.Is it planned in the next Versions build in \"Downtime\" ? i think it is useful for everybody or rather essential for SLA-Reporting. it would be nice you could set a SLA-Value , so you could also put a monitor on it and see if the service level are violated. Can i expect anything in that direction? ... then i could finally throw Nagios/Icinga out the window cheers Tim",
    "website_area": "discuss"
  },
  {
    "id": "9b7fac79-732d-4701-b03e-c9eb5db517ea",
    "url": "https://discuss.elastic.co/t/about-the-endpoint-security-category/204977",
    "title": "About the Endpoint Security category",
    "category": [
      "Endpoint Security"
    ],
    "author": "warkolm",
    "date": "October 24, 2019, 12:11am October 24, 2019, 1:08pm",
    "body": "As simple as antivirus, but way more powerful. Prevent, detect, hunt for, and respond to malware and adversaries.",
    "website_area": "discuss"
  },
  {
    "id": "77c322b2-fecd-437e-a586-9a04c8ba13ad",
    "url": "https://discuss.elastic.co/t/blog-series-on-macos-system-extensions-and-endpointsecurity-framework/214134",
    "title": "Blog series on macOS system extensions and EndpointSecurity framework",
    "category": [
      "Endpoint Security"
    ],
    "author": "tonymeehan",
    "date": "January 7, 2020, 9:20pm",
    "body": "Be sure to check out our new blog series on macOS system extension changes, the new EndpointSecurity and NetworkExtension frameworks, and tips and tricks for tacking advantage of these new systems to effectively protect macOS systems. Elastic Blog  7 Jan 20 Mac system extensions for threat detection: Part 1 In part 1, well go over some of the frameworks accessible by Mac kernel extensions that provide information about file system, process, and network events.",
    "website_area": "discuss"
  },
  {
    "id": "d014fd16-e619-4bfb-833f-4547f6b071e3",
    "url": "https://discuss.elastic.co/t/endgame/213227",
    "title": "Endgame",
    "category": [
      "Endpoint Security"
    ],
    "author": "odweik",
    "date": "December 27, 2019, 7:28pm January 7, 2020, 9:17pm",
    "body": "Hello, How I can learn Endgame endpoint security",
    "website_area": "discuss"
  },
  {
    "id": "dff7aa72-6441-418f-90eb-37359c6241de",
    "url": "https://discuss.elastic.co/t/endpoint-security-on-elastic-stack-community-slack/212696",
    "title": "Endpoint Security on Elastic Stack Community Slack",
    "category": [
      "Endpoint Security"
    ],
    "author": "tonymeehan",
    "date": "December 20, 2019, 6:36pm January 17, 2020, 6:36pm",
    "body": "In case you missed the announcement, we've got a community Slack setup for discussing endpoint security. See https://twitter.com/elastic/status/1207631666362998784 for more details and join us in #endpoint-security! -Tony",
    "website_area": "discuss"
  },
  {
    "id": "2695ff2f-9e51-43bc-8a6d-2c277a51cb4a",
    "url": "https://discuss.elastic.co/t/false-positive/208846",
    "title": "False positive",
    "category": [
      "Endpoint Security"
    ],
    "author": "vnovomeska",
    "date": "November 21, 2019, 9:43am December 6, 2019, 10:04pm January 3, 2020, 10:04pm",
    "body": "Hello, I encountered a false positive: https://www.virustotal.com/gui/file/9fdd641a90f64de570e08202dc0ffe9db8a94264bf969dcf16037413db49da1e/detection Endgame reports \"Malicious (moderate Confidence)\" but the file is safe and all other security vendors report \"Undetected\". Is it possible to fix this false positive please? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "4f1b9208-d4d5-4041-b3cc-b19422033d02",
    "url": "https://discuss.elastic.co/t/pricing-et-al/205410",
    "title": "Pricing et al",
    "category": [
      "Endpoint Security"
    ],
    "author": "hilt86",
    "date": "October 28, 2019, 6:39am October 28, 2019, 7:39pm November 21, 2019, 12:09am November 21, 2019, 12:54am November 21, 2019, 1:31am December 6, 2019, 10:03pm January 3, 2020, 10:03pm",
    "body": "How does endpoint security pricing work? After releasing such an exciting product you've made it very difficult to find / evaluate and buy Endpoint security..! H",
    "website_area": "discuss"
  },
  {
    "id": "84355c5d-2408-43e3-9faa-a3eeae93babe",
    "url": "https://discuss.elastic.co/t/deal-with-false-positives/208950",
    "title": "Deal with false positives",
    "category": [
      "Endpoint Security"
    ],
    "author": "p3464782",
    "date": "November 21, 2019, 5:48pm December 6, 2019, 10:00pm January 3, 2020, 10:01pm",
    "body": "Can one overrule a triggered detection and response rule via e.g. the API, for example if a false positive is identified by my company? Or does the Kibana SIEM allows an analist to revert any automatic responses?",
    "website_area": "discuss"
  },
  {
    "id": "7cb08156-6789-43f0-8a5f-74448dcd774b",
    "url": "https://discuss.elastic.co/t/edr-in-parallel-with-av/208946",
    "title": "EDR in parallel with AV",
    "category": [
      "Endpoint Security"
    ],
    "author": "p3464782",
    "date": "November 21, 2019, 5:44pm November 21, 2019, 7:16pm December 19, 2019, 7:16pm",
    "body": "Is it technically possible to run an Endgame or Elastic Endpoint Security agent in parallel with an existing AV/EPP solution like ESET, or does the agent conflicts with any AV running on the same system?",
    "website_area": "discuss"
  },
  {
    "id": "929eb565-747d-4f82-a44c-e50e37450356",
    "url": "https://discuss.elastic.co/t/agent-deployments-multi-tenancy/208948",
    "title": "Agent deployments multi tenancy",
    "category": [
      "Endpoint Security"
    ],
    "author": "p3464782",
    "date": "November 21, 2019, 5:46pm December 19, 2019, 5:46pm",
    "body": "Will the Elastic Endpoint Security cloud environment have multi-tenancy capabilities to group agents within multiple organizations/networks, for example within Kibana?",
    "website_area": "discuss"
  },
  {
    "id": "4fffdcae-0960-4a6a-881f-485367aa1421",
    "url": "https://discuss.elastic.co/t/false-postive-submission/205449",
    "title": "False Postive submission",
    "category": [
      "Endpoint Security"
    ],
    "author": "kman",
    "date": "October 28, 2019, 11:26am October 28, 2019, 7:31pm October 29, 2019, 7:08am November 26, 2019, 7:18am",
    "body": "Hi, our software is being detected by Endgame engine. we would like to remove the detection, could you guide me what needs to be done? thanks",
    "website_area": "discuss"
  },
  {
    "id": "e4e23577-b8ed-4d5e-9805-33e17c8c96f1",
    "url": "https://discuss.elastic.co/t/endpoint-introductions/205093",
    "title": "Endpoint Introductions",
    "category": [
      "Endpoint Security"
    ],
    "author": "Wadson",
    "date": "October 24, 2019, 2:36pm November 21, 2019, 2:36pm",
    "body": "Welcome to Elastic Endpoint! I'm Wadson and I am on the Endpoint Implementation team here at Elastic on the East Coast. If you're new to Elastic Endpoint or have already purchased Endpoint please introduce yourself here!",
    "website_area": "discuss"
  },
  {
    "id": "5487b169-2213-49ed-9063-c4f89e9f90e1",
    "url": "https://discuss.elastic.co/t/about-the-siem-category/181817",
    "title": "About the SIEM category",
    "category": [
      "SIEM"
    ],
    "author": "dadoonet",
    "date": "June 4, 2019, 10:00pm",
    "body": "Discussion about the Elastic SIEM app, supporting security information and event management use cases",
    "website_area": "discuss"
  },
  {
    "id": "73048a4c-8118-4231-96e1-f62a3ecf92d4",
    "url": "https://discuss.elastic.co/t/elastic-integration-with-zscaler-nss-service/211858",
    "title": "Elastic Integration with Zscaler NSS service",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "December 16, 2019, 1:37pm January 11, 2020, 3:32am January 18, 2020, 4:38am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4d244c45-5306-4d88-9f09-545b42ffeeef",
    "url": "https://discuss.elastic.co/t/siem-anomaly-detection-prebuild-jobs/213416",
    "title": "Siem anomaly detection prebuild jobs",
    "category": [
      "SIEM"
    ],
    "author": "jittinan",
    "date": "December 31, 2019, 7:28am January 2, 2020, 4:07pm",
    "body": "I am using trial version.I have installed filebeat,auditbeat,winlogbeat agent on target systems. The prebuilt jobs which i can use are siem-api-rare_process_linux_ecs siem-api-rare_process_windows_ecs 3.siem-api-suspicious_login_activity_ecs but from elasticsearch reference I should use all of these https://www.elastic.co/guide/en/siem/guide/7.x/prebuilt-ml-jobs.html why can I use only these 3 jobs? Does it because license type?",
    "website_area": "discuss"
  },
  {
    "id": "2bbb3ae4-8b2b-4aeb-961f-adf1aed43ba6",
    "url": "https://discuss.elastic.co/t/unable-to-start-auditbeat-for-siem/213431",
    "title": "Unable to start auditbeat for siem",
    "category": [
      "SIEM"
    ],
    "author": "Vamsi_Vutukuri",
    "date": "December 31, 2019, 9:46am",
    "body": "HI, im using elk stack of version 7.5.1 with x-pack installed and i cant able to start auditbeat for siem. Please help me solve it: Exiting: 2 errors: 1 error: failed to create audit client: failed to get audit status: operation not permitted; 1 error: unable to create DNS sniffer: failed creating af_packet sniffer: operation not permitted",
    "website_area": "discuss"
  },
  {
    "id": "ba604240-a4c6-472b-afb8-e318ace314ba",
    "url": "https://discuss.elastic.co/t/howto-change-indices-in-def-ml-jobs/211620",
    "title": "Howto change indices in def. ML jobs",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "December 12, 2019, 11:57am December 20, 2019, 12:29pm December 20, 2019, 10:06am January 17, 2020, 10:12am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4436eb29-307b-4f7a-a886-ce1ba95d4def",
    "url": "https://discuss.elastic.co/t/our-ml-job-stops-execution-with-an-exception-emptydatacountexception-null/212480",
    "title": "Our ML job stops execution with an exception: EmptyDataCountException: null",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "December 19, 2019, 12:06pm December 19, 2019, 2:29pm December 19, 2019, 2:59pm January 16, 2020, 2:58pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "35787f2c-1df6-4888-90df-be8601a3c6c3",
    "url": "https://discuss.elastic.co/t/siem-timeline-data-persistence-and-retention/212344",
    "title": "SIEM Timeline data persistence and retention",
    "category": [
      "SIEM"
    ],
    "author": "stefws",
    "date": "December 18, 2019, 7:04pm December 19, 2019, 7:15am December 19, 2019, 7:15am January 16, 2020, 7:15am",
    "body": "Playing a bit with SIEM App and wondering where are Timeline stored and how to possibly control their retention? Don't seem to find any godd system index candidates for this...",
    "website_area": "discuss"
  },
  {
    "id": "b1393f10-2b67-4e1d-a658-0e8fa156374a",
    "url": "https://discuss.elastic.co/t/elastic-siem-adding-more-data/212154",
    "title": "Elastic SIEM - Adding more data",
    "category": [
      "SIEM"
    ],
    "author": "Raj_Kumar",
    "date": "December 17, 2019, 11:56am December 17, 2019, 7:44pm January 14, 2020, 7:44pm",
    "body": "HI There, Iam Elastic's SIEM for security analysis, but I would like to know if can add other datas to SIEM app, for example there are many logs sources sends syslog to my logstash .Can I have those logs as well in my Elastic SIEM. Because if i click on add data on the SIEM app it gives me only few options. Any help would be really appreciable. Thanks, Raj",
    "website_area": "discuss"
  },
  {
    "id": "b9521cef-ad01-4763-8eec-b3e3405bffc4",
    "url": "https://discuss.elastic.co/t/auditbeat-fileintegrity-module-cannot-detect-file-update-from-vi/211920",
    "title": "Auditbeat fileintegrity module cannot detect file update from vi",
    "category": [
      "SIEM"
    ],
    "author": "jittinan",
    "date": "December 15, 2019, 7:00pm January 12, 2020, 7:00pm",
    "body": "from auditbeat documents,file integrity module detect changes by using inotify events from os.it works correctly with created,deleted,moved events by sending a message for each event to elasticserch but when the monitored file is modified by vim editor,file integrity module catch many events.After I debug how vim editor cause these events by using inotifywait.I notice that vim create/update/delete many temporary files (.swp,.swx,4913, etc.) so it should be reasonable if file integrity module catch these events .However I cannot find any message from file integrity module which notify about file updated event. After I turn on debug log on auditbeat,I notice that file integrity module can detect write event but it does not notify file updated event. please see my log snippets.At time 2019-12-14T22:34:52.557+0700 is what I mention to. 1.open file with #vi test.conf 2019-12-14T22:16:12.266+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/.test.conf.swp\", \"event_flags\": \"CREATE\"} 2019-12-14T22:16:12.266+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/.test.conf.swx\", \"event_flags\": \"REMOVE\"} 2019-12-14T22:16:12.266+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/.test.conf.swp\", \"event_flags\": \"REMOVE\"} 2019-12-14T22:16:12.266+0700 DEBUG [file_integrity] file_integrity/metricset.go:252 File changed since it was last seen {\"file_path\": \"/root/test/.test.conf.swp\", \"took\": 191670, \"event\": {\"old\": null, \"new\": {\"timestamp\":\"2019-12-14T15:16:12.26638284Z\",\"path\":\"/root/test/.test.conf.swp\",\"info\":{\"inode\":100974252,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"root\",\"group\":\"root\",\"size\":4096,\"mtime\":\"2019-12-14T15:16:12.264689909Z\",\"ctime\":\"2019-12-14T15:16:12.264689909Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"fsnotify\",\"action\":\"created\",\"hash\":{\"sha1\":\"2fd9f6ff74f92b1802066124df9f146b5dc52610\"}}}} 2019-12-14T22:16:12.267+0700 DEBUG [file_integrity] file_integrity/metricset.go:252 File changed since it was last seen {\"file_path\": \"/root/test/.test.conf.swx\", \"took\": 6160, \"event\": {\"old\": null, \"new\": {\"timestamp\":\"2019-12-14T15:16:12.266606262Z\",\"path\":\"/root/test/.test.conf.swx\",\"info\":null,\"source\":\"fsnotify\",\"action\":\"deleted\"}}} 2019-12-14T22:16:12.268+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/.test.conf.swp\", \"event_flags\": \"CREATE\"} 2019-12-14T22:16:12.269+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/.test.conf.swp\", \"event_flags\": \"WRITE\"} 2019-12-14T22:16:12.270+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/.test.conf.swp\", \"event_flags\": \"CHMOD\"} 2019-12-14T22:16:16.269+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/.test.conf.swp\", \"event_flags\": \"WRITE\"} 2019-12-14T22:16:16.270+0700 DEBUG [file_integrity] file_integrity/metricset.go:252 File changed since it was last seen {\"file_path\": \"/root/test/.test.conf.swp\", \"took\": 297524, \"event\": {\"old\": {\"timestamp\":\"2019-12-14T15:16:12.270272121Z\",\"path\":\"/root/test/.test.conf.swp\",\"info\":{\"inode\":100974252,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"\",\"group\":\"\",\"size\":4096,\"mtime\":\"2019-12-14T15:16:12.264689909Z\",\"ctime\":\"2019-12-14T15:16:12.264689909Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"fsnotify\",\"action\":\"attributes_modified\",\"hash\":{\"sha1\":\"2fd9f6ff74f92b1802066124df9f146b5dc52610\"}}, \"new\": {\"timestamp\":\"2019-12-14T15:16:16.270041359Z\",\"path\":\"/root/test/.test.conf.swp\",\"info\":{\"inode\":100974252,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"root\",\"group\":\"root\",\"size\":12288,\"mtime\":\"2019-12-14T15:16:16.2686964Z\",\"ctime\":\"2019-12-14T15:16:16.2686964Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"fsnotify\",\"action\":\"updated\",\"hash\":{\"sha1\":\"730042f3e0de417f03908dc0307565a75e24e7b7\"}}}} 2.write fille with :w 2019-12-14T22:34:52.551+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/4913\", \"event_flags\": \"REMOVE\"} 2019-12-14T22:34:52.551+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/test.conf\", \"event_flags\": \"RENAME\"} 2019-12-14T22:34:52.552+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/test.conf~\", \"event_flags\": \"CREATE\"} 2019-12-14T22:34:52.552+0700 DEBUG [file_integrity] file_integrity/metricset.go:252 File changed since it was last seen {\"file_path\": \"/root/test/4913\", \"took\": 6230, \"event\": {\"old\": null, \"new\": {\"timestamp\":\"2019-12-14T15:34:52.551963912Z\",\"path\":\"/root/test/4913\",\"info\":null,\"source\":\"fsnotify\",\"action\":\"deleted\"}}} 2019-12-14T22:34:52.553+0700 DEBUG [file_integrity] file_integrity/metricset.go:252 File changed since it was last seen {\"file_path\": \"/root/test/test.conf\", \"took\": 157659, \"event\": {\"old\": {\"timestamp\":\"2019-12-14T15:34:28.493429017Z\",\"path\":\"/root/test/test.conf\",\"info\":{\"inode\":100974249,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"\",\"group\":\"\",\"size\":0,\"mtime\":\"2019-12-14T15:03:43.629492538Z\",\"ctime\":\"2019-12-14T15:03:43.629492538Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"scan\",\"action\":\"none\",\"hash\":{\"sha1\":\"da39a3ee5e6b4b0d3255bfef95601890afd80709\"}}, \"new\": {\"timestamp\":\"2019-12-14T15:34:52.552019167Z\",\"path\":\"/root/test/test.conf\",\"info\":{\"inode\":100974253,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"root\",\"group\":\"root\",\"size\":2,\"mtime\":\"2019-12-14T15:34:52.550520431Z\",\"ctime\":\"2019-12-14T15:34:52.550520431Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"fsnotify\",\"action\":\"moved\",\"hash\":{\"sha1\":\"e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e\"}}}} 2019-12-14T22:34:52.554+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/test.conf\", \"event_flags\": \"CREATE\"} 2019-12-14T22:34:52.554+0700 DEBUG [file_integrity] file_integrity/metricset.go:252 File changed since it was last seen {\"file_path\": \"/root/test/test.conf~\", \"took\": 84476, \"event\": {\"old\": null, \"new\": {\"timestamp\":\"2019-12-14T15:34:52.552237997Z\",\"path\":\"/root/test/test.conf~\",\"info\":{\"inode\":100974249,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"root\",\"group\":\"root\",\"size\":0,\"mtime\":\"2019-12-14T15:03:43.629492538Z\",\"ctime\":\"2019-12-14T15:34:52.550520431Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"fsnotify\",\"action\":\"created\",\"hash\":{\"sha1\":\"da39a3ee5e6b4b0d3255bfef95601890afd80709\"}}}} 2019-12-14T22:34:52.555+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/test.conf\", \"event_flags\": \"WRITE\"} 2019-12-14T22:34:52.555+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/test.conf\", \"event_flags\": \"CHMOD\"} 2019-12-14T22:34:52.556+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/.test.conf.swp\", \"event_flags\": \"WRITE\"} 2019-12-14T22:34:52.557+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/test.conf~\", \"event_flags\": \"REMOVE\"} 2019-12-14T22:34:52.557+0700 DEBUG [file_integrity] file_integrity/metricset.go:252 File changed since it was last seen {\"file_path\": \"/root/test/.test.conf.swp\", \"took\": 131000, \"event\": {\"old\": {\"timestamp\":\"2019-12-14T15:34:28.493169456Z\",\"path\":\"/root/test/.test.conf.swp\",\"info\":{\"inode\":100974252,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"\",\"group\":\"\",\"size\":12288,\"mtime\":\"2019-12-14T15:30:47.314117427Z\",\"ctime\":\"2019-12-14T15:30:47.314117427Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"scan\",\"action\":\"none\",\"hash\":{\"sha1\":\"da611b007320981c61dbb7f642c79f5ff020da55\"}}, \"new\": {\"timestamp\":\"2019-12-14T15:34:52.556603673Z\",\"path\":\"/root/test/.test.conf.swp\",\"info\":{\"inode\":100974252,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"root\",\"group\":\"root\",\"size\":12288,\"mtime\":\"2019-12-14T15:34:52.552520435Z\",\"ctime\":\"2019-12-14T15:34:52.552520435Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"fsnotify\",\"action\":\"updated\",\"hash\":{\"sha1\":\"530808091273a1887bfeeb6dba788a4087ff8f1b\"}}}} 2019-12-14T22:34:52.558+0700 DEBUG [file_integrity] file_integrity/metricset.go:252 File changed since it was last seen {\"file_path\": \"/root/test/test.conf~\", \"took\": 7128, \"event\": {\"old\": {\"timestamp\":\"2019-12-14T15:34:52.552237997Z\",\"path\":\"/root/test/test.conf~\",\"info\":{\"inode\":100974249,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"\",\"group\":\"\",\"size\":0,\"mtime\":\"2019-12-14T15:03:43.629492538Z\",\"ctime\":\"2019-12-14T15:34:52.550520431Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"fsnotify\",\"action\":\"created\",\"hash\":{\"sha1\":\"da39a3ee5e6b4b0d3255bfef95601890afd80709\"}}, \"new\": {\"timestamp\":\"2019-12-14T15:34:52.557249178Z\",\"path\":\"/root/test/test.conf~\",\"info\":null,\"source\":\"fsnotify\",\"action\":\"deleted\"}}} 2019-12-14T22:34:56.558+0700 DEBUG [file_integrity] file_integrity/eventreader_fsnotify.go:136 Received fsnotify event {\"file_path\": \"/root/test/.test.conf.swp\", \"event_flags\": \"WRITE\"} 2019-12-14T22:34:56.558+0700 DEBUG [file_integrity] file_integrity/metricset.go:252 File changed since it was last seen {\"file_path\": \"/root/test/.test.conf.swp\", \"took\": 258151, \"event\": {\"old\": {\"timestamp\":\"2019-12-14T15:34:52.556603673Z\",\"path\":\"/root/test/.test.conf.swp\",\"info\":{\"inode\":100974252,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"\",\"group\":\"\",\"size\":12288,\"mtime\":\"2019-12-14T15:34:52.552520435Z\",\"ctime\":\"2019-12-14T15:34:52.552520435Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"fsnotify\",\"action\":\"updated\",\"hash\":{\"sha1\":\"530808091273a1887bfeeb6dba788a4087ff8f1b\"}}, \"new\": {\"timestamp\":\"2019-12-14T15:34:56.55847786Z\",\"path\":\"/root/test/.test.conf.swp\",\"info\":{\"inode\":100974252,\"uid\":0,\"gid\":0,\"sid\":\"\",\"owner\":\"root\",\"group\":\"root\",\"size\":12288,\"mtime\":\"2019-12-14T15:34:56.557527027Z\",\"ctime\":\"2019-12-14T15:34:56.557527027Z\",\"type\":\"file\",\"mode\":420,\"setuid\":false,\"setgid\":false,\"origin\":null},\"source\":\"fsnotify\",\"action\":\"updated\",\"hash\":{\"sha1\":\"c4eee1d7c73f8f04be5010ae50db5a8a89f1e993\"}}}}",
    "website_area": "discuss"
  },
  {
    "id": "f69cfb90-9579-49c9-b09d-5239337d359f",
    "url": "https://discuss.elastic.co/t/i-want-to-access-the-siem-app-without-clicking-the-siem-app/211122",
    "title": "I want to access the SIEM app without clicking the SIEM app",
    "category": [
      "SIEM"
    ],
    "author": "Vishnu_mk",
    "date": "December 9, 2019, 1:29pm December 9, 2019, 1:29pm December 12, 2019, 12:04pm January 9, 2020, 12:04pm",
    "body": "I have created a space , which has privileges for only SIEM app. Can I access the SIEM app directly without clicking the SIEM . I mean the SIEM app should automatically open",
    "website_area": "discuss"
  },
  {
    "id": "a7e0aa84-09de-4221-bbde-84ba7b80e486",
    "url": "https://discuss.elastic.co/t/anomaly-detection-statuscode-404/211142",
    "title": "Anomaly detection Statuscode 404",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "December 10, 2019, 2:34pm December 9, 2019, 4:55pm December 11, 2019, 8:44am December 12, 2019, 9:20am December 12, 2019, 9:20am January 9, 2020, 9:20am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e4706657-d868-4325-98fc-44e6ca666383",
    "url": "https://discuss.elastic.co/t/i-want-to-enable-the-map-which-is-present-in-siem-app/211054",
    "title": "I want to enable the map which is present in SIEM app",
    "category": [
      "SIEM"
    ],
    "author": "Vishnu_mk",
    "date": "December 9, 2019, 1:46pm January 6, 2020, 6:06am",
    "body": "Hi , I am having Elasticsearch version 7.4, and in the SIEM app network section I want to enable the map present. I want to enable it through my own index , so i am doing ECS mapping of my index field but when i convert my client_ip to geoip field using geoip filter, field corresponding to geoip gets created . I want my client_ip filed to be coverted to 'source.geo.location' field since the 'MAP' in SIEM APP gets generated through this field.",
    "website_area": "discuss"
  },
  {
    "id": "494987d2-1bf5-4eee-833f-c7bca4b6fe6f",
    "url": "https://discuss.elastic.co/t/error-receiving-audit-reply-no-buffer-space-available/210382",
    "title": "Error receiving audit reply: no buffer space available",
    "category": [
      "SIEM"
    ],
    "author": "vaclav",
    "date": "December 5, 2019, 3:07pm December 9, 2019, 9:28am December 30, 2019, 9:28am",
    "body": "Hello, I am having issue with auditbeat service running, but there is no output and some commands are not working. Found this error in syslog: auditbeat[23346]: 2019-12-03T11:48:38.116Z#011ERROR#011[auditd]#011auditd/audit_linux.go:155#011Failure receiving audit events#011{\"error\": \"failed to set audit PID (current audit PID 0): error receiving audit reply: no buffer space available\", \"errorVerbose\": \"no buffer space available\\nerror receiving audit reply\\ngithub.com/elastic/beats/vendor/github.com/elastic/go-libaudit.(*AuditClient).getReply\\n\\t/home/builder/go/src/github.com/elastic/beats/vendor/github.com/elastic/go-libaudit/audit.go:474\\ngithub.com/elastic/beats/vendor/github.com/elastic/go-libaudit.(*AuditClient).set\\n\\t/home/builder/go/src/github.com/elastic/beats/vendor/github.com/elastic/go-libaudit/audit.go:513\\ngithub.com/elastic/beats/vendor/github.com/elastic/go-libaudit.(*AuditClient).SetPID\\n\\t/home/builder/go/src/github.com/elastic/beats/vendor/github.com/elastic/go-libaudit/audit.go:318\\ngithub.com/elastic/beats/auditbeat/module/auditd.(*MetricSet).initClient\\n\\t/home/builder/go/src/github.com/elastic/beats/auditbeat/module/auditd/audit_linux.go:344\\ngithub.com/elastic/beats/auditbeat/module/auditd.(*MetricSet).receiveEvents\\n\\t/home/builder/go/src/github.com/elastic/beats/auditbeat/module/auditd/audit_linux.go:372\\ngithub.com/elastic/beats/auditbeat/module/auditd.(*MetricSet).Run\\n\\t/home/builder/go/src/github.com/elastic/beats/auditbeat/module/auditd/audit_linux.go:152\\ngithub.com/elastic/beats/metricbeat/mb/module.(*metricSetWrapper).run\\n\\t/home/builder/go/src/github.com/elastic/beats/metricbeat/mb/module/wrapper.go:196\\ngithub.com/elastic/beats/metricbeat/mb/module.(*Wrapper).Start.func1\\n\\t/home/builder/go/src/github.com/elastic/beats/metricbeat/mb/module/wrapper.go:140\\nruntime.goexit\\n\\t/home/builder/agent/_work/_tool/go/1.12.4/x64/src/runtime/asm_amd64.s:1337\\nfailed to set audit PID (current audit PID 0)\\ngithub.com/elastic/beats/auditbeat/module/auditd.(*MetricSet).initClient\\n\\t/home/builder/go/src/github.com/elastic/beats/auditbeat/module/auditd/audit_linux.go:348\\ngithub.com/elastic/beats/auditbeat/module/auditd.(*MetricSet).receiveEvents\\n\\t/home/builder/g should I set rate_limit and backpressure_strategy ? or what are recommended values ? thank you",
    "website_area": "discuss"
  },
  {
    "id": "fd5ccd68-4e4b-4e69-bf65-c4df11bf7c08",
    "url": "https://discuss.elastic.co/t/authentication-fields-used-by-siem-vs-ecs/210882",
    "title": "Authentication fields used by SIEM vs ECS",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "December 6, 2019, 11:54am December 6, 2019, 4:55pm December 6, 2019, 4:58pm December 6, 2019, 5:04pm January 3, 2020, 5:18pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ea5c2bbe-b0f2-4221-94ab-9a23b269916f",
    "url": "https://discuss.elastic.co/t/siem-any-overlap-between-filbeat-ingesting-syslog-auditlog-authlog-and-auditbeat-with-auditd-system-and-fi-modules/210235",
    "title": "SIEM - Any overlap between filbeat ingesting syslog, auditlog, authlog and auditbeat (with auditd, system and FI modules)?",
    "category": [
      "SIEM"
    ],
    "author": "vsubrama",
    "date": "December 5, 2019, 3:05pm December 4, 2019, 9:20pm December 5, 2019, 10:26pm December 26, 2019, 10:26pm",
    "body": "We are currently ingesting syslog, auditlog, authlog from our ubuntu systems via filebeat to Elasticsearch 7.4.2 as a part of build our SIEM. We are planning to add auditbeat with auditd, system and file integrity modules. By doing this are we double ingesting something now (such as auditlog) or are these two independent things?",
    "website_area": "discuss"
  },
  {
    "id": "41d6cc87-429f-4cc7-b43a-5286cc5f47b0",
    "url": "https://discuss.elastic.co/t/unable-to-start-audit-beat/209594",
    "title": "Unable to start audit beat",
    "category": [
      "SIEM"
    ],
    "author": "Sudeep_Khare",
    "date": "December 5, 2019, 3:12pm December 25, 2019, 3:38am",
    "body": "I am relatively new to ELK and I am trying to send logs from audit beat to Kibana. I have followed the steps from ELK documentation to install audit-beats on Linux . But at the last step when I run the audit beat I am getting the following error : Cannot continue: audit configuration is locked in the kernel (enabled=2) which prevents using unicast sockets. Multicast audit subscriptions are not available in this kernel. Disable locking the audit configuration to use auditbeat Is there any way I can do it without a restart ?",
    "website_area": "discuss"
  },
  {
    "id": "97554892-2dd1-4064-be43-7c2574137d97",
    "url": "https://discuss.elastic.co/t/auditbeat-will-not-start/210005",
    "title": "AuditBeat Will Not Start",
    "category": [
      "SIEM"
    ],
    "author": "nuffskillz",
    "date": "December 5, 2019, 3:07pm December 20, 2019, 9:58pm",
    "body": "Hi A fresh install of auditbeat will not start returning the following below. Exiting: 1 error: 1 error: system/socket dataset setup failed: unable to guess one or more required parameters: guess_sk_buff_proto failed: failed to monitor probe: PERF_EVENT_IOC_ID: inappropriate ioctl for device cat /etc/redhat-release Red Hat Enterprise Linux Server release 6.5 (Santiago) uname -a Linux REMOVED -HOSTNAME 2.6.32-431.el6.x86_64 #1 SMP Sun Nov 10 22:19:54 EST 2013 x86_64 x86_64 x86_64 GNU/Linux rpm -qa |grep -i auditbeat auditbeat-7.4.2-1.x86_64 auditbeat -e -d \"*\" 2019-11-29T16:09:36.918-0500 INFO instance/beat.go:607 Home path: [/usr/share/auditbeat] Config path: [/etc/auditbeat] Data path: [/var/lib/auditbeat] Logs path: [/var/log/auditbeat] 2019-11-29T16:09:36.918-0500 DEBUG [beat] instance/beat.go:659 Beat metadata path: /var/lib/auditbeat/meta.json 2019-11-29T16:09:36.918-0500 INFO instance/beat.go:615 Beat ID: 2e50be9a-9e4b-485b-9538-0d430fd41edb 2019-11-29T16:09:36.921-0500 DEBUG [filters] add_cloud_metadata/providers.go:126 add_cloud_metadata: starting to fetch metadata, timeout=3s 2019-11-29T16:09:39.921-0500 DEBUG [filters] add_cloud_metadata/providers.go:169 add_cloud_metadata: timed-out waiting for all responses 2019-11-29T16:09:39.921-0500 DEBUG [filters] add_cloud_metadata/providers.go:129 add_cloud_metadata: fetchMetadata ran for 3.00034529s 2019-11-29T16:09:39.921-0500 INFO add_cloud_metadata/add_cloud_metadata.go:87 add_cloud_metadata: hosting provider type not detected. 2019-11-29T16:09:39.921-0500 DEBUG [processors] processors/processor.go:101 Generated new processors: add_host_metadata=[netinfo.enabled=[false], cache.ttl=[5m0s]], add_cloud_metadata=null 2019-11-29T16:09:39.921-0500 INFO [seccomp] seccomp/seccomp.go:101 Syscall filter could not be installed because the kernel does not support seccomp 2019-11-29T16:09:39.921-0500 INFO [beat] instance/beat.go:903 Beat info {\"system_info\": {\"beat\": {\"path\": {\"config\": \"/etc/auditbeat\", \"data\": \"/var/lib/auditbeat\", \"home\": \"/usr/share/auditbeat\", \"logs\": \"/var/log/auditbeat\"}, \"type\": \"auditbeat\", \"uuid\": \"2e50be9a-9e4b-485b-9538-0d430fd41edb\"}}} 2019-11-29T16:09:39.921-0500 INFO [beat] instance/beat.go:912 Build info {\"system_info\": {\"build\": {\"commit\": \"15075156388b44390301f070960fd8aeac1c9712\", \"libbeat\": \"7.4.2\", \"time\": \"2019-10-28T19:43:11.000Z\", \"version\": \"7.4.2\"}}} 2019-11-29T16:09:39.921-0500 INFO [beat] instance/beat.go:915 Go runtime info {\"system_info\": {\"go\": {\"os\":\"linux\",\"arch\":\"amd64\",\"max_procs\":40,\"version\":\"go1.12.9\"}}} 2019-11-29T16:09:39.923-0500 INFO [beat] instance/beat.go:919 Host info {\"system_info\": {\"host\": {\"architecture\":\"x86_64\",\"boot_time\":\"2016-09-28T12:20:09-04:00\",\"containerized\":false,\"name\":\"serverhost.someserver.com\",\"ip\":[\"127.0.0.1/8\",\"10.248.1.6/25\",\"10.248.1.132/25\"],\"kernel_version\":\"2.6.32-431.el6.x86_64\",\"mac\":[\"fc:5b:39:2d:5a:76\",\"fc:5b:39:2d:5a:77\"],\"os\":{\"family\":\"redhat\",\"platform\":\"redhat\",\"name\":\"Red\",\"version\":\"6.5 (Santiago)\",\"major\":6,\"minor\":5,\"patch\":0,\"codename\":\"Santiago\"},\"timezone\":\"EST\",\"timezone_offset_sec\":-18000,\"id\":\"bdf26306c00c44ac223a2bd80000000d\"}}} 2019-11-29T16:09:39.923-0500 INFO [beat] instance/beat.go:948 Process info {\"system_info\": {\"process\": {\"capabilities\": {\"inheritable\":null,\"permitted\":[\"chown\",\"dac_override\",\"dac_read_search\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"linux_immutable\",\"net_bind_service\",\"net_broadcast\",\"net_admin\",\"net_raw\",\"ipc_lock\",\"ipc_owner\",\"sys_module\",\"sys_rawio\",\"sys_chroot\",\"sys_ptrace\",\"sys_pacct\",\"sys_admin\",\"sys_boot\",\"sys_nice\",\"sys_resource\",\"sys_time\",\"sys_tty_config\",\"mknod\",\"lease\",\"audit_write\",\"audit_control\",\"setfcap\",\"mac_override\",\"mac_admin\",\"syslog\",\"wake_alarm\",\"block_suspend\",\"audit_read\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\"],\"effective\":[\"chown\",\"dac_override\",\"dac_read_search\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"linux_immutable\",\"net_bind_service\",\"net_broadcast\",\"net_admin\",\"net_raw\",\"ipc_lock\",\"ipc_owner\",\"sys_module\",\"sys_rawio\",\"sys_chroot\",\"sys_ptrace\",\"sys_pacct\",\"sys_admin\",\"sys_boot\",\"sys_nice\",\"sys_resource\",\"sys_time\",\"sys_tty_config\",\"mknod\",\"lease\",\"audit_write\",\"audit_control\",\"setfcap\",\"mac_override\",\"mac_admin\",\"syslog\",\"wake_alarm\",\"block_suspend\",\"audit_read\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\"],\"bounding\":[\"chown\",\"dac_override\",\"dac_read_search\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"linux_immutable\",\"net_bind_service\",\"net_broadcast\",\"net_admin\",\"net_raw\",\"ipc_lock\",\"ipc_owner\",\"sys_module\",\"sys_rawio\",\"sys_chroot\",\"sys_ptrace\",\"sys_pacct\",\"sys_admin\",\"sys_boot\",\"sys_nice\",\"sys_resource\",\"sys_time\",\"sys_tty_config\",\"mknod\",\"lease\",\"audit_write\",\"audit_control\",\"setfcap\",\"mac_override\",\"mac_admin\",\"syslog\",\"wake_alarm\",\"block_suspend\",\"audit_read\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\"],\"ambient\":null}, \"cwd\": \"/var/log\", \"exe\": \"/usr/share/auditbeat/bin/auditbeat\", \"name\": \"auditbeat\", \"pid\": 28895, \"ppid\": 27852, \"seccomp\": {\"mode\":\"\"}, \"start_time\": \"2019-11-29T16:09:36.050-0500\"}}} 2019-11-29T16:09:39.923-0500 INFO instance/beat.go:292 Setup Beat: auditbeat; Version: 7.4.2 2019-11-29T16:09:39.923-0500 DEBUG [beat] instance/beat.go:318 Initializing output plugins 2019-11-29T16:09:39.923-0500 INFO [index-management] idxmgmt/std.go:178 Set output.elasticsearch.index to 'auditbeat-7.4.2' as ILM is enabled. 2019-11-29T16:09:39.924-0500 INFO elasticsearch/client.go:170 Elasticsearch url: http://localhost:9200 2019-11-29T16:09:39.924-0500 DEBUG [publisher] pipeline/consumer.go:137 start pipeline event consumer 2019-11-29T16:09:39.924-0500 INFO [publisher] pipeline/module.go:97 Beat name: serverhost.someserver.com 2019-11-29T16:09:39.924-0500 DEBUG [modules] beater/metricbeat.go:121 Available modules and metricsets: Register [ModuleFactory:[system], MetricSetFactory:[auditd/auditd, file_integrity/file, system/host, system/login, system/package, system/process, system/socket, system/user]] 2019-11-29T16:09:39.924-0500 INFO [auditd] auditd/audit_linux.go:106 auditd module is running as euid=0 on kernel=2.6.32-431.el6.x86_64 2019-11-29T16:09:39.924-0500 INFO [auditd] auditd/audit_linux.go:133 socket_type=unicast will be used. 2019-11-29T16:09:39.925-0500 DEBUG [file_integrity] file_integrity/metricset.go:97 Initialized the file event reader. Running as euid=0 2019-11-29T16:09:39.926-0500 WARN [cfgwarn] host/host.go:167 BETA: The system/host dataset is beta 2019-11-29T16:09:39.926-0500 DEBUG [system] host/host.go:450 No last host information found on disk. 2019-11-29T16:09:39.926-0500 WARN [cfgwarn] login/login.go:95 BETA: The system/login dataset is beta 2019-11-29T16:09:39.927-0500 DEBUG [login] login/utmp.go:539 Restored 0 UTMP file records from disk 2019-11-29T16:09:39.927-0500 DEBUG [login] login/utmp.go:571 Restored 0 open login sessions from disk 2019-11-29T16:09:39.927-0500 WARN [cfgwarn] package/package.go:170 BETA: The system/package dataset is beta 2019-11-29T16:09:39.927-0500 DEBUG [package] package/package.go:203 No state timestamp found 2019-11-29T16:09:39.927-0500 DEBUG [package] package/package.go:211 Restored 0 packages from disk 2019-11-29T16:09:39.927-0500 WARN [cfgwarn] process/process.go:131 BETA: The system/process dataset is beta 2019-11-29T16:09:39.927-0500 DEBUG [process] process/process.go:170 No state timestamp found 2019-11-29T16:09:39.927-0500 WARN [cfgwarn] socket/socket_linux.go:81 BETA: The system/socket dataset is beta. 2019-11-29T16:09:39.927-0500 INFO [socket] socket/socket_linux.go:197 Setting up system/socket for kernel 2.6.32-431.el6.x86_64 2019-11-29T16:09:39.928-0500 DEBUG [socket] socket/socket_linux.go:245 IPv6 supported: false 2019-11-29T16:09:39.928-0500 DEBUG [socket] socket/socket_linux.go:252 IPv6 enabled: false 2019-11-29T16:09:39.981-0500 DEBUG [socket] socket/socket_linux.go:305 Selected kernel function sys_newuname for SYS_UNAME 2019-11-29T16:09:39.981-0500 DEBUG [socket] socket/socket_linux.go:305 Selected kernel function ip_local_out for IP_LOCAL_OUT 2019-11-29T16:09:39.981-0500 DEBUG [socket] socket/socket_linux.go:305 Selected kernel function __skb_recv_datagram for RECV_UDP_DATAGRAM 2019-11-29T16:09:39.981-0500 DEBUG [socket] socket/socket_linux.go:305 Selected kernel function sys_execve for SYS_EXECVE 2019-11-29T16:09:39.981-0500 DEBUG [socket] socket/socket_linux.go:305 Selected kernel function sys_gettimeofday for SYS_GETTIMEOFDAY 2019-11-29T16:09:39.984-0500 INFO [socket] guess/guess.go:258 Running 16 guesses ... 2019-11-29T16:09:39.984-0500 DEBUG [socket] guess/guess.go:270 Guess guess_inet_sock_ipv6 skipped. 2019-11-29T16:09:39.991-0500 WARN [cfgwarn] user/user.go:205 BETA: The system/user dataset is beta 2019-11-29T16:09:39.992-0500 DEBUG [user] user/user.go:247 No state timestamp found 2019-11-29T16:09:39.992-0500 DEBUG [user] user/user.go:255 Restored 0 users from disk 2019-11-29T16:09:39.992-0500 INFO instance/beat.go:385 auditbeat stopped. 2019-11-29T16:09:39.992-0500 ERROR instance/beat.go:878 Exiting: 1 error: 1 error: system/socket dataset setup failed: unable to guess one or more required parameters: guess_sk_buff_proto failed: failed to monitor probe: PERF_EVENT_IOC_ID: inappropriate ioctl for device Exiting: 1 error: 1 error: system/socket dataset setup failed: unable to guess one or more required parameters: guess_sk_buff_proto failed: failed to monitor probe: PERF_EVENT_IOC_ID: inappropriate ioctl for device",
    "website_area": "discuss"
  },
  {
    "id": "91b21ef6-82cc-4b43-bc2e-489b7192ef44",
    "url": "https://discuss.elastic.co/t/an-ecs-compliant-kibana-index-pattern-must-be-configured-to-view-event-data-on-the-map/209969",
    "title": "An ECS compliant Kibana index pattern must be configured to view event data on the map",
    "category": [
      "SIEM"
    ],
    "author": "MarcusCaepio",
    "date": "November 29, 2019, 1:05pm November 29, 2019, 2:14pm November 29, 2019, 3:23pm December 4, 2019, 7:23pm December 5, 2019, 2:02pm January 2, 2020, 2:02pm",
    "body": "Hi all, I am using Filebeat module cisco to get logs. I am not storing this logs in an index called filebeat-* but cisco-*. To get the correct mapping for this pattern, I exported the filebeat template and imported it for the pattern cisco-*. Anyway, the SIEM Network Map tells me that An ECS compliant Kibana index pattern must be configured to view event data on the map. When using beats, you can run the following setup commands to create the required Kibana index patterns, otherwise you can configure them manually within Kibana settings. I already added cisco-* to the default SIEM index search in management. But this doesn't seem to be solution. How can I add the cisco-* to the map? Cheers, Marcus",
    "website_area": "discuss"
  },
  {
    "id": "fb5a214e-681f-40b8-9ac7-b77426a6f9f7",
    "url": "https://discuss.elastic.co/t/what-field-are-used-to-populate-the-entire-siem-app/209263",
    "title": "What field are used to populate the entire SIEM APP",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "November 29, 2019, 1:56pm November 29, 2019, 1:57pm December 3, 2019, 9:42am December 31, 2019, 9:42am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e2aac437-8fad-45bd-9fd9-20ff870a7ebf",
    "url": "https://discuss.elastic.co/t/thehive-integration-for-siem-case-management/209129",
    "title": "TheHIVE integration for SIEM Case Management",
    "category": [
      "SIEM"
    ],
    "author": "ch3no2",
    "date": "November 23, 2019, 12:49am November 29, 2019, 2:55pm December 27, 2019, 2:55pm",
    "body": "In the last Elastic SIEM Presentation (11/21/2019), the presentation showed the integration with MISP... Is that the same MISP that is part of TheHive ? AND is there is there any consideration of having hooks in the SIEM to integrate with TheHive to handle case management ? Thanks, -dt",
    "website_area": "discuss"
  },
  {
    "id": "c2443137-f32a-433a-9674-f4f3655c0bb5",
    "url": "https://discuss.elastic.co/t/fielddata-is-disabled/206816",
    "title": "Fielddata is disabled",
    "category": [
      "SIEM"
    ],
    "author": "Manoel",
    "date": "November 6, 2019, 3:26pm November 7, 2019, 11:54am November 7, 2019, 3:10pm November 7, 2019, 5:22pm November 22, 2019, 2:44pm November 25, 2019, 11:52am November 28, 2019, 5:51pm December 26, 2019, 5:51pm",
    "body": "Hello. I upgraded from Elastic Stack version 6.8 to 7.4. I installed AudioBeat on all my servers, with the configuration below. auditbeat.modules: - module: auditd audit_rule_files: [ '${path.config}/audit.rules.d/*.conf' ] audit_rules: | - module: file_integrity paths: - /bin - /usr/bin - /sbin - /usr/sbin - /etc - /opt - module: system datasets: - host # General host information, e.g. uptime, IPs - login # User logins, logouts, and system boots. - package # Installed, updated, and removed packages - process # Started and stopped processes - socket # Opened and closed sockets - user # User information user.detect_password_changes: true login.wtmp_file_pattern: /var/log/wtmp* login.btmp_file_pattern: /var/log/btmp* setup.template.settings: index.number_of_shards: 1 setup.kibana: host: \"http://yspp0051.ymdb.com.br:80\" output.elasticsearch: hosts: [\"yspp0053.ymdb.com.br:9200\"] processors: - add_host_metadata: ~ - add_cloud_metadata: ~ I set up SIEM on Elastic Stack and everything was normal until yesterday. Today is presenting the following error: uncommon_process.png1528782 57.3 KB process.png1578727 40 KB My mapping is: https://justpaste.it/3hl4g Can you help me solve? I'm new to Elastic Stack and I don't know much.",
    "website_area": "discuss"
  },
  {
    "id": "a07e6f1d-d1b3-4834-b8f3-62e0eb739282",
    "url": "https://discuss.elastic.co/t/auditbeat-docker-7-4-2-starts-and-then-terminates-with-no-error/209348",
    "title": "Auditbeat docker (7.4.2) starts and then terminates with no error",
    "category": [
      "SIEM"
    ],
    "author": "yoshugo",
    "date": "December 5, 2019, 3:12pm November 26, 2019, 5:01pm December 17, 2019, 5:01pm",
    "body": "Hi, i'm new to auditbeat. i've configured a test machine in my lab and configured all ELK stack as dockers. the Auditbeat starts and load dashboards and then stops. auditbeat.GIF1408439 52.3 KB",
    "website_area": "discuss"
  },
  {
    "id": "f63c51b0-3ac3-4624-bfa3-fbf004f5b384",
    "url": "https://discuss.elastic.co/t/can-someone-help-me-configure-suricata-filebeat-on-elastic-cloud/208962",
    "title": "Can Someone Help me Configure Suricata Filebeat on elastic cloud?",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "November 21, 2019, 6:29pm November 21, 2019, 6:56pm December 19, 2019, 6:56pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "0e495fc3-89ec-450a-9b21-bfbb44a8bb4d",
    "url": "https://discuss.elastic.co/t/gcp-vpc-flows-in-siem/208259",
    "title": "GCP VPC Flows in SIEM",
    "category": [
      "SIEM"
    ],
    "author": "numbersix",
    "date": "November 18, 2019, 6:13am November 19, 2019, 1:00am November 19, 2019, 2:16pm December 17, 2019, 2:16pm",
    "body": "I have GCP VPC Flows in Elasticsearch but what is the easiest way to integrate them into the SIEM? I see net flow integrations available for other sources but not GCP. I'm using the Google Cloud Module for Filebeat currently. My cluster is self-managed. Even if someone could steer me in a direction that would be helpful. Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "c3e1f3ff-8e66-4d40-9440-703597ef5931",
    "url": "https://discuss.elastic.co/t/siem-not-ingesting-forwarded-windows-logs/207311",
    "title": "SIEM not ingesting Forwarded Windows logs",
    "category": [
      "SIEM"
    ],
    "author": "mustdiee",
    "date": "November 11, 2019, 9:26am November 12, 2019, 12:19pm November 12, 2019, 2:49pm November 14, 2019, 6:29am November 14, 2019, 5:53pm November 14, 2019, 5:54pm December 12, 2019, 5:54pm",
    "body": "Hello! I'am collecting logs from all Windows PCs in my infrastructure with EventForwarding. Only one server has a winlogbeat installed, which is configured on other workstations as a subscription server (this is the easier way to collect than deploying winlogbeat around all domain PCs). When I upgraded to 7.x, I noticed that SIEM does not ingest forwarded events. Is there any solution that will help SIEM ingest such data?",
    "website_area": "discuss"
  },
  {
    "id": "2ddc80a0-0ef0-4f70-946b-01be45694512",
    "url": "https://discuss.elastic.co/t/in-ubuntu-18-04-auditbeat-logs-goes-to-syslog-than-var-log-auditbeat/206269",
    "title": "In Ubuntu 18.04 auditbeat logs goes to syslog than /var/log/auditbeat",
    "category": [
      "SIEM"
    ],
    "author": "Joseph_John",
    "date": "November 3, 2019, 5:26am November 3, 2019, 7:57am November 13, 2019, 10:39am November 13, 2019, 9:03pm December 11, 2019, 9:03pm",
    "body": "Hi All, Good morning I am using Ubuntu 18.04 I have auditbeat installed and running on my system, in the auditbeat.yml files I have given the following logging.level: info path.logs: /var/log/auditbeat When I check for the logs, the logs file are not created in \"/var/log/auditbeat\" instead I see them on /var/log/syslog . Doubting permission issues I have given chmod 777 to /var/log/auditbeat [ not a good idea, but for troubleshooting] Guidance requested to send the logs to /var/log/auditbeat than to syslog thanks Joseph John Kibana version: 7.4.1 Elasticsearch version: 7.4.1 APM Server version: 7.4.1 **filebeat version ** 7.4.1 APM Agent language and version: NA Logstash version 7.4.1-1",
    "website_area": "discuss"
  },
  {
    "id": "dac1178a-d4f7-452d-9563-58801f6c91dd",
    "url": "https://discuss.elastic.co/t/metricbeat-c-etc-metricbeat-yml-logs-goes-to-the-path-specified-when-stating-with-systemctl-it-does-not/206273",
    "title": "Metricbeat -c /etc/metricbeat.yml logs goes to the path specified , when stating with systemctl it does not",
    "category": [
      "SIEM"
    ],
    "author": "Joseph_John",
    "date": "November 3, 2019, 7:51am November 4, 2019, 11:35am November 13, 2019, 10:41am November 13, 2019, 10:51am November 13, 2019, 10:54am December 11, 2019, 10:54am",
    "body": "Hi All, Good morning I am experiencing a strange observation here with logs when I run metricbeat -c /etc/metricbeat/metricbeat.yml the logs goes to the specified location else not going, ie when started as service \"systemctl start metricbeat\" the log goes to the syslog the contents of metricbeat.yml is given by logging.level: debug logging.to_files: true logging.files: path: /var/log/metricbeat My package details are ElasticSearch 7.4.2, Kibana 7.4.2 , Auditbeat 7.4.2 , FileBeat 7.4.2, LogStash 7.4.2-1 , Metricbeat 7.4.2",
    "website_area": "discuss"
  },
  {
    "id": "899a52c3-0b9c-4e37-a676-1697b3d8da82",
    "url": "https://discuss.elastic.co/t/kibana-displaying-of-hosts-takes-a-lot-of-time-i-have-only-few-hosts-6-max/206004",
    "title": "Kibana , displaying of hosts takes a lot of time [ I have only few hosts 6 max]",
    "category": [
      "SIEM"
    ],
    "author": "Joseph_John",
    "date": "November 1, 2019, 5:58pm November 13, 2019, 10:45am December 11, 2019, 10:46am",
    "body": "Hi All, My kibana interaces displays the status of the machines very very slow, I do not have much hosts, 6 hosts for tesing purpose and it is taking too much time to display, I have added the screen shot for reference Also adding the hardware details root@apmserver:~# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 4 On-line CPU(s) list: 0-3 RAM I have 16GB ram, with only 3GB used free -m total used free shared buff/cache available Mem: 16040 3097 10394 1 2549 12632 Swap: 4095 0 4095 Why it is taking this much time to display the list of hosts info , guidance requested thanks Kibana version: 7.4.1 Elasticsearch version: 7.4.1 APM Server version: 7.4.1 **filebeat version ** 7.4.1 APM Agent language and version: NA Logstash version 7.4.1-1 Browser version: Firefox 70 KibanaTakingLongtimeToShow.png1736938 30.1 KB",
    "website_area": "discuss"
  },
  {
    "id": "3409fdfe-52b3-4be4-9b31-0874df13f96c",
    "url": "https://discuss.elastic.co/t/add-another-reputation-link-into-kibana-siem/205072",
    "title": "Add Another Reputation Link into Kibana SIEM",
    "category": [
      "SIEM"
    ],
    "author": "joshuasmith",
    "date": "October 24, 2019, 1:12pm November 13, 2019, 10:42am December 11, 2019, 10:43am",
    "body": "Hello, We are starting to use the SIEM app some more and wanted to add a few additional links for Threat Intel and context. What is the process for getting another customizable link (similar to the virustotal or talosintelligence links) into the SIEM networking app? I have read through the SIEM documentation but didn't see any mention on how to customize items. Here is where I am trying to add an additional link: siem_reputation.png26771058 153 KB Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "4c317cf6-a348-49b8-ae33-66c6171ed0e8",
    "url": "https://discuss.elastic.co/t/zeek-dns-logs-show-only-as-zeek-notice-leaving-dns-fields-empty/207633",
    "title": "Zeek dns logs show only as zeek.notice leaving dns fields empty",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "November 13, 2019, 6:58am December 11, 2019, 6:34am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "1b9c4a0c-5f3b-472e-ab9a-3bb6e8b5a2de",
    "url": "https://discuss.elastic.co/t/autonomous-system-number-asn-not-displaying/206195",
    "title": "Autonomous System Number (ASN) not displaying",
    "category": [
      "SIEM"
    ],
    "author": "Justin_Doles",
    "date": "November 1, 2019, 4:53pm November 1, 2019, 7:58pm November 1, 2019, 5:44pm November 1, 2019, 7:58pm November 29, 2019, 7:58pm",
    "body": "I have an index that I've created that follows the ECS standard. Within the index I have the source.as.* and destination.as.* fields that are populated using the GeoLite2ASN database. The fields are being populated in the index, but the network section of SIEM doesn't display them. The main page nor the drilled down view of the IP shows this information. I verified with the \"inspect\" feature that I am populating the right fields. This is on version 7.4.0 if that matters. Here's a sample that doesn't show the ASN in SIEM, but does show in searches. Any ideas what I might be missing/doing incorrectly? { \"source\": { \"ip\": \"192.168.1.1\", \"interface\": \"X16\", \"mac\": \"00:00:00:00:00:00\", \"bytes\": 119311, \"port\": \"62190\" }, \"agent\": { \"type\": \"logstash\", \"name\": \"vDEVLOG1\" }, \"observer\": { \"type\": \"firewall\", \"ip\": \"1.1.1.1\", \"serial_number\": \"NA\", \"vendor\": \"unknown\" }, \"message\": \"%{[message][1]}\", \"destination\": { \"port\": \"443\", \"as\": { \"asn\": 46489, \"as_org\": \"Twitch Interactive Inc.\", \"ip\": \"52.223.226.232\" }, \"interface\": \"X5\", \"ip\": \"52.223.226.232\", \"geo\": { \"location\": { \"lat\": 47.6348, \"lon\": -122.3451 }, \"longitude\": -122.3451, \"country_code3\": \"US\", \"postal_code\": \"98109\", \"region_code\": \"WA\", \"continent_code\": \"NA\", \"city_name\": \"Seattle\", \"ip\": \"52.223.226.232\", \"latitude\": 47.6348, \"dma_code\": 819, \"region_name\": \"Washington\", \"country_code2\": \"US\", \"timezone\": \"America/Los_Angeles\", \"country_name\": \"United States\" }, \"bytes\": 17384244, \"mac\": \"40:a6:e8:5f:ba:d6\" }, \"network\": { \"protocol\": \"https\", \"transport\": \"tcp\", \"bytes\": 17503555 }, \"host\": { \"ip\": \"192.168.1.1\" }, \"url\": { \"domain\": \"video-edge-eddcc8.ord02.abs.hls.ttvnw.net\", \"path\": \"/v1/\" }, \"@version\": \"1\", \"id\": \"a-fw\", \"tags\": [ \"syslog\", \"unknown\", \"privateip_source\" ], \"@timestamp\": \"2019-10-31T23:00:02.000Z\", \"_index\": \"syslog-1.0-000028\", \"_type\": \"_doc\", \"_id\": \"aKUMJG4B3alxQ_lAFsNy\", \"_score\": 1 }",
    "website_area": "discuss"
  },
  {
    "id": "3a4c7e19-b9e2-4151-b42d-09130bbd3187",
    "url": "https://discuss.elastic.co/t/viewing-pinned-timeline-events/205132",
    "title": "Viewing Pinned Timeline Events",
    "category": [
      "SIEM"
    ],
    "author": "MrTrav",
    "date": "October 24, 2019, 6:43pm October 25, 2019, 6:57am November 22, 2019, 6:57am",
    "body": "Is it possible to only view events which have been pinned (marked as persisted with the timeline)?",
    "website_area": "discuss"
  },
  {
    "id": "1665915f-e3b8-4fa2-b9b8-baad37e8027a",
    "url": "https://discuss.elastic.co/t/bulk-ingest-of-netflow-and-zeek-logs-into-elastic-siem/204746",
    "title": "Bulk ingest of netflow and zeek logs into Elastic SIEM",
    "category": [
      "SIEM"
    ],
    "author": "cks",
    "date": "October 23, 2019, 1:21am October 24, 2019, 3:07pm November 21, 2019, 3:07pm",
    "body": "What would be best way to bulk ingest netflow and zeek logs into Elasticsearch, which I would like to access in SIEM in Kibana? I am looking to ingest several TBs of logs. I plan to bulk ingest other pcap, auth logs and dns traffic. Can I bypass logstash and ingest directly into ElasticSearch?",
    "website_area": "discuss"
  },
  {
    "id": "1f23ea5d-e46c-479b-9d19-8eb8bf642645",
    "url": "https://discuss.elastic.co/t/new-siem-infrastructure-with-elasticsearch/204559",
    "title": "New SIEM infrastructure with Elasticsearch",
    "category": [
      "SIEM"
    ],
    "author": "elk2",
    "date": "October 21, 2019, 9:58pm October 22, 2019, 1:21am October 22, 2019, 8:20am October 22, 2019, 11:51pm November 19, 2019, 11:52pm",
    "body": "I want to implement a new SIEM infrastructure for threat detection and incident response. I would host the systems in our datacenter where all others servers are hosted. I want to use beat to send logs from servers to SIEM app and with machine learning detect the network anomalies. It' possible to use only one elasticsearch instance for machine learning and SIEM purpose or it's better to create a cluster with minimum 3 nodes even with a small infrastructure?",
    "website_area": "discuss"
  },
  {
    "id": "977d2fac-d270-4ef5-a4ec-f99acb1eed16",
    "url": "https://discuss.elastic.co/t/problem-with-siem/204593",
    "title": "Problem with SIEM",
    "category": [
      "SIEM"
    ],
    "author": "Aleix_Abrie_Prat",
    "date": "October 22, 2019, 7:24am October 22, 2019, 4:52pm October 22, 2019, 10:40am October 22, 2019, 11:56am October 22, 2019, 12:27pm October 22, 2019, 12:37pm October 22, 2019, 4:14pm October 22, 2019, 4:52pm November 19, 2019, 4:52pm",
    "body": "Hello everyone, I have a problem once i have configured SIEM and auditbeat on the \"client\" machine. I configured one machine only with auditbeat and the coniguration standars. I will post below the configuration. The problem is that when I enter Kibana SIEM and fix the fielddata (is there any way to make it more comfortable?), it appears as if I had 4 hosts configured. Attached image. hosts.PNG839727 37.5 KB Here i post the configuration of auditbeat: ###################### Auditbeat Configuration Example ####################### auditbeat.modules: - module: auditd # Load audit rules from separate files. Same format as audit.rules(7). audit_rule_files: [ '${path.config}/audit.rules.d/*.conf' ] #audit_rules: | ## Define audit rules here. ## Create file watches (-w) or syscall audits (-a or -A). Uncomment these ## examples or add your own rules. ## If you are on a 64 bit platform, everything should be running ## in 64 bit mode. This rule will detect any use of the 32 bit syscalls ## because this might be a sign of someone exploiting a hole in the 32 ## bit API. #-a always,exit -F arch=b32 -S all -F key=32bit-abi ## Executions. #-a always,exit -F arch=b64 -S execve,execveat -k exec ## External access (warning: these can be expensive to audit). #-a always,exit -F arch=b64 -S accept,bind,connect -F key=external-access ## Identity changes. #-w /etc/group -p wa -k identity #-w /etc/passwd -p wa -k identity #-w /etc/gshadow -p wa -k identity ## Unauthorized access attempts. #-a always,exit -F arch=b64 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit=-EACCES -k access #-a always,exit -F arch=b64 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit=-EPERM -k access - module: file_integrity paths: - /bin - /usr/bin - /sbin - /usr/sbin - /etc - module: system datasets: - host # General host information, e.g. uptime, IPs - login # User logins, logouts, and system boots. - package # Installed, updated, and removed packages - user # User information period: 1m - module: system datasets: - process #Started and stopped processes - socket #Opened and closed sockets period: 1s socket.enable_ipv6: false user.detect_password_changes: true login.wtmp_file_pattern: /var/log/wtmp* login.btmp_file_pattern: /var/log/btmp* #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. #name: # The tags of the shipper are included in their own field with each # transaction published. #tags: [\"service-X\", \"web-tier\"] # Optional fields that you can specify to add additional information to the # output. #fields: # env: staging #================================ Outputs ===================================== #----------------------------- Logstash output -------------------------------- output.logstash: # The Logstash hosts hosts: [\"10.10.12.114:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" #================================ Processors ===================================== # Configure processors to enhance or manipulate events generated by the beat. #processors: #- add_host_metadata: ~ #- add_cloud_metadata: ~ #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] #============================== X-Pack Monitoring =============================== # auditbeat can export internal metrics to a central Elasticsearch monitoring # cluster. This requires xpack monitoring to be enabled in Elasticsearch. The # reporting is disabled by default. # Set to true to enable the monitoring reporter. #monitoring.enabled: false # Sets the UUID of the Elasticsearch cluster under which monitoring data for this # Auditbeat instance will appear in the Stack Monitoring UI. If output.elasticsearch # is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch. #monitoring.cluster_uuid: # Uncomment to send the metrics to Elasticsearch. Most settings from the # Elasticsearch output are accepted here as well. # Note that the settings should point to your Elasticsearch *monitoring* cluster. # Any setting that is not set is automatically inherited from the Elasticsearch # output configuration, so if you have the Elasticsearch output configured such # that it is pointing to your Elasticsearch monitoring cluster, you can simply # uncomment the following line. #monitoring.elasticsearch: #================================= Migration ================================== # This allows to enable 6.7 migration aliases #migration.6_to_7.enabled: false Anyone can help me? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "f06b1e81-0350-42b9-9572-88efb71835ae",
    "url": "https://discuss.elastic.co/t/for-example-i-have-machine-a-running-as-a-server-and-i-would-like-to-manage-other-clients-such-as-machine-b-c-d-etc-so-how-to-do-that-how-to-get-many-hosts/201330",
    "title": "For example, I have machine A running as a Server and I would like to manage other clients such as machine B, C, D,...etc So, how to do that? How to get many hosts?",
    "category": [
      "SIEM"
    ],
    "author": "pbona",
    "date": "September 27, 2019, 2:28am October 2, 2019, 11:46am October 5, 2019, 1:54am October 9, 2019, 6:28am October 9, 2019, 7:25am October 9, 2019, 8:57am October 9, 2019, 8:58am October 9, 2019, 8:49am October 9, 2019, 8:52am October 9, 2019, 8:55am October 9, 2019, 8:56am October 9, 2019, 8:54am October 9, 2019, 9:07am October 9, 2019, 9:09am October 9, 2019, 9:12am October 9, 2019, 9:14am October 9, 2019, 9:21am October 9, 2019, 9:24am October 9, 2019, 9:29am October 9, 2019, 9:40am",
    "body": "image.png1812424 28.5 KB",
    "website_area": "discuss"
  },
  {
    "id": "d61c2023-756e-4fc7-9ed4-066a258dee0d",
    "url": "https://discuss.elastic.co/t/how-to-change-query-in-siem/204113",
    "title": "How to change query in SIEM",
    "category": [
      "SIEM"
    ],
    "author": "tatdat",
    "date": "October 17, 2019, 6:21pm October 18, 2019, 11:19am October 21, 2019, 2:32am November 18, 2019, 2:32am",
    "body": "Im using Elastic stack 7.4 and stup auditbeat, filebeat, packetbeat ready. Im focus on SIEM -> network, in Top DNS domains panel, i saw , default get top root domain not real domain. ( dns.question.registered_domain instead of dns.question.name) For example, server query domain abc.xyz.com. In Top DNS domains panel will show xyz.com. It not good, When i use timeline, drag and drop xyz.com, get many result, many event is unrelated. It can be list down aaa.xyz.com, bbb.xyz.com. How to change query in SIEM panel ? Thank { \"aggregations\": { \"dns_count\": { \"cardinality\": { \"field\": \"dns.question.registered_domain\" } }, \"dns_name_query_count\": { \"terms\": { \"field\": \"dns.question.registered_domain\", \"size\": 10, \"order\": { \"unique_domains\": \"desc\" } }, \"aggs\": { \"unique_domains\": { \"cardinality\": { \"field\": \"dns.question.name\" } }, \"dns_bytes_in\": { \"sum\": { \"field\": \"source.bytes\" } }, \"dns_bytes_out\": { \"sum\": { \"field\": \"destination.bytes\" } } } } }, \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"@timestamp\": { \"gte\": 1571249363494, \"lte\": 1571335763494 } } } ], \"must_not\": [ { \"term\": { \"dns.question.type\": { \"value\": \"PTR\" } } } ] } } }",
    "website_area": "discuss"
  },
  {
    "id": "e7879041-f38b-4026-bb6e-e1efb3cef188",
    "url": "https://discuss.elastic.co/t/siem-not-detecting-asa-success-failure-logins/203754",
    "title": "SIEM not detecting ASA success failure logins",
    "category": [
      "SIEM"
    ],
    "author": "mancharagopan",
    "date": "October 16, 2019, 6:16am October 17, 2019, 1:46pm October 18, 2019, 3:46am October 18, 2019, 4:24pm October 19, 2019, 5:24am October 19, 2019, 5:31am November 16, 2019, 5:32am",
    "body": "SIEM not detecting success and failure logins from ASA syslog messages. and also it detect filebeat hostname as the host in SIEM app. What can i do? ASA Syslog message parsing could be better from filebeat 7.4 to populate observer hostname, ip and event type, category. I use following grok filters in Logstash 6.6 to parse ASA firewall syslog messages, grok { match => { \"message\" => \"<%{INT:recordId}>%{DATA:[hostname]} \\%%{DATA:[event][dataset]}-%{INT:[severity]}-%{INT:[cisco][asa][message_id]}:\" } } if \"%ASA-5-111008\" in [message] { grok { match => {\"message\" => \"111008: User '%{DATA:[cisco][asa][source_username]}' %{GREEDYDATA:[event][action]}\"} add_field => { \"[log][original]\" => \"Command Executed\" \"[event][outcome]\" => \"success\" } } } if \"ASA-6-113008\" in [message] { grok { match => {\"message\" => \"113008: %{GREEDYDATA:[event][action]} : user = %{WORD:[cisco][asa][source_username]}\"} add_field => { \"[log][original]\" => \"AAA transaction status ACCEPT\" \"[event][outcome]\" => \"success\" } } } if \"ASA-6-113012\" in [message] { grok { match => {\"message\" => \"113012: %{GREEDYDATA:[event][action]} : local database : user = %{WORD:[cisco][asa][source_username]}\"} add_field => { \"[log][original]\" => \"Successfull AAA Authentication\" \"[event][outcome]\" => \"success\" } } } if \"ASA-6-113015\" in [message] { grok { match => {\"message\" => \"113015: %{GREEDYDATA:[event][action]} : reason = (?<event.reason>\\w+ \\w+) : local database : user = %{WORD:[cisco][asa][source_username]}\"} add_field => { \"[log][original]\" => \"Failed AAA Authentication - %{event.reason}\" \"[event][outcome]\" => \"failure\" } } } if \"ASA-6-113013\" in [message] { grok { match => {\"message\" => \"113013: %{GREEDYDATA:[event][action]} : reason = (?<event.reason>\\w+ \\w+) : local database : user = %{WORD:[cisco][asa][source_username]}\"} add_field => { \"[log][original]\" => \"Failed AAA Transaction - %{[event][reason]}\" \"[event][outcome]\" => \"failure\" } } } if \"ASA-6-113019\" in [message] { grok { match => {\"message\" => \"113019: %{GREEDYDATA:[event][action]} : reason = (?<event.reason>\\w+ \\w+) : local database : user = %{WORD:[cisco][asa][source_username]}\"} add_field => { \"[log][original]\" => \"Session Timeout - %{[event][reason]}\" \"[event][outcome]\" => \"failure\" } } } }",
    "website_area": "discuss"
  },
  {
    "id": "c5dca938-c5ef-44ff-9d90-a9f733a37cf5",
    "url": "https://discuss.elastic.co/t/active-directory-logs-and-mapping-to-ecs-i-am-stumped/203321",
    "title": "Active Directory logs and mapping to ECS (I am stumped)",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "October 14, 2019, 12:50pm October 12, 2019, 6:17pm October 14, 2019, 1:25am October 14, 2019, 12:29pm October 14, 2019, 2:08pm October 14, 2019, 3:15pm October 14, 2019, 5:18pm November 11, 2019, 5:18pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "fdf95c2e-bc0d-4fd1-a300-ffdd5798e0c8",
    "url": "https://discuss.elastic.co/t/siem-command-line-auditing-4688-4689/202656",
    "title": "SIEM Command Line Auditing 4688 - 4689",
    "category": [
      "SIEM"
    ],
    "author": "willemdh",
    "date": "October 8, 2019, 11:49am October 11, 2019, 8:26am October 14, 2019, 10:41am October 14, 2019, 12:21pm October 14, 2019, 12:35pm October 14, 2019, 12:39pm October 14, 2019, 12:46pm October 14, 2019, 12:45pm October 14, 2019, 12:51pm October 14, 2019, 12:54pm October 14, 2019, 1:12pm November 11, 2019, 1:01pm",
    "body": "Hello, 2 of the most interesting event id's in siem are 4688 and 4689, which can be enabled with a gpo and enable us to monitor every command used in your network. Interesting fields are: winlog.event_data.NewProcessName winlog.event_data.ParentProcessName winlog.event_data.ProcessName Unfortunately some of the processes audited are located in %USERPROFILE% folder, which means it looks like this: C:\\Users\\username\\AppData\\Local\\GitHubDesktop\\app-2.2.0\\resources\\app\\git\\mingw64\\bin\\git.exe It would be nice if the security processor of Winlogbeat could get expanded and the executable itself could be extracted to process.name That would enable us to do all kinds of siem related interesting analytics on them. Is this already on a to do? Should I make a GitHub issue? Grtz Willem",
    "website_area": "discuss"
  },
  {
    "id": "0d3a2ea8-b98e-4a60-86a1-51eceefd6e36",
    "url": "https://discuss.elastic.co/t/how-to-get-more-hosts-in-siem-auditbeat/201904",
    "title": "How to get more hosts in SIEM (Auditbeat)",
    "category": [
      "SIEM"
    ],
    "author": "pbona",
    "date": "October 2, 2019, 7:45am October 2, 2019, 11:45am October 30, 2019, 11:45am",
    "body": "image.png1174799 40.6 KB",
    "website_area": "discuss"
  },
  {
    "id": "4343f280-aad4-4ee7-9e21-8d2c2646f45c",
    "url": "https://discuss.elastic.co/t/defenxor-dsiem-for-event-correlation-with-logstash/201649",
    "title": "Defenxor DSIEM for Event Correlation with Logstash",
    "category": [
      "SIEM"
    ],
    "author": "gargantua",
    "date": "September 30, 2019, 1:47pm October 28, 2019, 1:47pm",
    "body": "Hello, I want to correlate my events with defenxor/dsiem. Is there anyone who uses dsiem for event correlation? Is it enough for correlation? What are advantages and disadvantages of dsiem?",
    "website_area": "discuss"
  },
  {
    "id": "22ee87c2-a04e-46b0-a46b-310c8222db1b",
    "url": "https://discuss.elastic.co/t/siem-infrastructure-design/201529",
    "title": "SIEM Infrastructure design",
    "category": [
      "SIEM"
    ],
    "author": "lokmanopt",
    "date": "September 29, 2019, 11:20am September 30, 2019, 10:35am October 28, 2019, 10:35am",
    "body": "Dear Concern, I want to implement SIEM solution for production. I need to ELK expert's help Please share Design for production environment. I have more then 300 system and 300 router as well as 6 Cisco firewall. So please give me appropriate solution for production scenario. Regard's Lokman Hakim",
    "website_area": "discuss"
  },
  {
    "id": "5ab0af55-c148-4f68-a943-7a510e9fc764",
    "url": "https://discuss.elastic.co/t/filter-uncommon-host-processes/197874",
    "title": "Filter Uncommon Host Processes",
    "category": [
      "SIEM"
    ],
    "author": "willemdh",
    "date": "September 3, 2019, 1:38pm September 26, 2019, 10:08am September 27, 2019, 11:05am October 25, 2019, 11:05am",
    "body": "Hello, Read here (Uncommon Processes) that the Uncommon Processes query is an aggregation on process.name sorted by host cardinality first (cardinality of host.name where this process name occurs) and number of documents second. The result is that the list if full of processes from our Rundeck server, which generates a unique id for each job running. Is there any way to prevent these processes from showing up in SIEM? They look like: 548-129241-loca 549-129241-loca 550-129241-loca Grtz Willem",
    "website_area": "discuss"
  },
  {
    "id": "b14359d5-3cfd-4092-a6a1-f520a1d05bf1",
    "url": "https://discuss.elastic.co/t/hash-used-in-elastic/201326",
    "title": "Hash used in Elastic?",
    "category": [
      "SIEM"
    ],
    "author": "zeno",
    "date": "September 26, 2019, 11:42pm September 27, 2019, 10:27am September 27, 2019, 10:49am October 25, 2019, 11:00am",
    "body": "Hi so i am studying and using Kibana open source and wanted to know whether kibana open source by default creates hashes of files ? The document of elastic says that it does use SHA256, SHA-1 & MD5 but as per the auditbeat logs there were no hashes found. I have tried searching it but got no luck. would really appreciate if someone can explain it. thank you in advance",
    "website_area": "discuss"
  },
  {
    "id": "85b46da8-bae8-4d53-a5e5-ce124357a3e5",
    "url": "https://discuss.elastic.co/t/siem-ecs-descriptions-taking-huge-amount-of-unneccesary-space-in-siem/200972",
    "title": "SIEM ECS descriptions taking huge amount of unneccesary space in SIEM",
    "category": [
      "SIEM"
    ],
    "author": "willemdh",
    "date": "October 23, 2019, 8:39pm September 27, 2019, 10:27am October 25, 2019, 10:28am",
    "body": "Hello, Just some constructive feedback. When data is added to a SIEM timeline and the results are analyzed, it seems every field has a description column, which imho takes a huge amount of space, which could be used for more interesting things.. Maybe the ECS field description could only be shown when hovering over a little information icon instead of being shown by default for every field? image.png1085545 72 KB Grtz Willem",
    "website_area": "discuss"
  },
  {
    "id": "6f3b88a8-d33e-44bc-916e-e6e4b7926c2b",
    "url": "https://discuss.elastic.co/t/how-many-swap-files-are-created-when-you-update-a-text-file/197625",
    "title": "How many swap files are created when you update a text file",
    "category": [
      "SIEM"
    ],
    "author": "zeno",
    "date": "September 1, 2019, 5:30pm September 1, 2019, 7:44pm September 2, 2019, 8:33pm September 3, 2019, 11:41am September 4, 2019, 9:26pm September 13, 2019, 8:17am September 14, 2019, 9:56pm September 25, 2019, 11:14pm September 26, 2019, 7:16am October 24, 2019, 7:16am",
    "body": "I want to know how many swap files are created in kibana for Ubuntu server when you create a text file using nano. i see 4 swap files deleted when i deleted the text file i have created in Ubuntu. Similarly the no of swap files deleted data shown on kibana is more than no of file created E.g if abc.txt file is created. there are more than 1 swap files shown as created in kibana. I will really appreciate any help as i can't find the answer in google for this",
    "website_area": "discuss"
  },
  {
    "id": "4f81ee6e-ee49-4e46-9b8c-7525cfe6d406",
    "url": "https://discuss.elastic.co/t/add-additional-data-source-to-siem-dashboard/199712",
    "title": "Add additional data source to SIEM dashboard",
    "category": [
      "SIEM"
    ],
    "author": "Justin_Doles",
    "date": "September 16, 2019, 6:47pm September 17, 2019, 12:33pm September 17, 2019, 12:34pm September 18, 2019, 8:46am October 16, 2019, 8:46am",
    "body": "Is there a way to add additional data sources to the SIEM dashboard? I have an index, syslog-, that collects data from our network switches & firewalls. It does use the ECS conventions. The dashboard seems to only use auditbeat-, filebeat-, packetbeat-, winlogbeat-*. Data is currently collected via a Logstash UDP input plugin and then parsed & enriched before being sent to ES. I know filebeat can do syslog, but I haven't played with creating my own module or using processors/filters.",
    "website_area": "discuss"
  },
  {
    "id": "604c59f7-d318-4137-a3e3-7f119a142bea",
    "url": "https://discuss.elastic.co/t/hosts-tab-in-siem-and-wef/190162",
    "title": "Hosts tab in SIEM and WEF",
    "category": [
      "SIEM"
    ],
    "author": "smerzlyakov",
    "date": "July 12, 2019, 8:07am August 2, 2019, 3:37pm August 5, 2019, 1:18pm August 13, 2019, 11:09am August 14, 2019, 12:01pm August 15, 2019, 11:13am August 15, 2019, 12:17pm August 15, 2019, 12:19pm August 15, 2019, 1:36pm August 15, 2019, 1:46pm August 15, 2019, 2:40pm August 19, 2019, 12:24pm September 11, 2019, 5:05pm September 12, 2019, 9:31am September 13, 2019, 10:07am September 16, 2019, 1:57pm September 19, 2019, 12:30pm October 14, 2019, 2:26pm",
    "body": "There is Hosts tab in SIEM. I think nobody in Enterprise uses Winlogbeat on every Windows hosts. It is standard to use collector for logs and send Logs using Windows Event Forwarding on it. So, in field Host it will be name of collector. Or just collect logs from AD DC. But in SIEM tab \"Hosts\" i defenetly want to see hostname of Users Windows stations. So, how to fix it?I do not want to rename fields, because it breakes existing Scheme. Maybe in future you will change the logic of this page? What is the best solution to see real hosts on this SIEM tab?",
    "website_area": "discuss"
  },
  {
    "id": "aea3a49b-59fa-48ab-9893-b0d824d84fc6",
    "url": "https://discuss.elastic.co/t/sonicwall-firewall-and-siem-or-snmp/199546",
    "title": "SonicWall Firewall and SIEM or SNMP",
    "category": [
      "SIEM"
    ],
    "author": "jabalo1327",
    "date": "September 15, 2019, 5:13pm September 15, 2019, 5:49pm October 13, 2019, 5:49pm",
    "body": "How Do I configure my sonicwall firewall to send all the logs to the cloud for this product and what product Do I actually need? Is there any approximate price range for month? I will like to see all the data that my sonicwall gets send to the cloud. Please and thank you",
    "website_area": "discuss"
  },
  {
    "id": "9d4ddf3d-2f67-484f-9654-dacf11377359",
    "url": "https://discuss.elastic.co/t/envoyproxy/195755",
    "title": "Envoyproxy",
    "category": [
      "SIEM"
    ],
    "author": "mcapua",
    "date": "August 19, 2019, 4:28pm August 30, 2019, 3:17pm September 7, 2019, 7:21am October 5, 2019, 7:31am",
    "body": "Hello, I'm trying to send envoyproxy logs to SIEM but I'm receiving a WARN message. ISTIO was configured with stdout access logs and running on Kubernetes > Finished:false, Fileinfo:(*os.fileStat)(0xc0035c2ea0), Source:\"/var/lib/docker/containers/4476e4a217a2b90ccceb9d79e4885a046e26b13a84ae0b9d58129fbdacb6ca1a/4476e4a217a2b90ccceb9d79e4885a046e26b13a84ae0b9d58129fbdacb6ca1a-json.log\", Offset:1549114, Timestamp:time.Time{wall:0xbf4ec7b36643d493, ext:5433491068, loc:(*time.Location)(0x30d3480)}, TTL:-1, Type:\"docker\", Meta:map[string]string(nil), FileStateOS:file.StateOS{Inode:0x1653503, Device:0xca01}}, TimeSeries:false}, Flags:0x1} (status=400): {\"type\":\"mapper_parsing_exception\",\"reason\":\"failed to parse field [http.response.body.bytes] of type [long] in document with id 'Bj0eqmwBx02RG54jRxkG'. Preview of field's value: '\\\"-\\\"'\",\"caused_by\":{\"type\":\"illegal_argument_exception\",\"reason\":\"For input string: \\\"\\\"-\\\"\\\"\"}} Here's the configuration file: data: filebeat.yml: |- filebeat.autodiscover: providers: - type: kubernetes host: ${NODE_NAME} hints.enabled: true hints.default_config: type: container paths: - /var/log/containers/*${data.kubernetes.container.id}.log templates: - condition: equals: kubernetes.container.name: \"istio-proxy\" config: - module: envoyproxy log: input: type: docker #containers.stream: stdout containers.ids: - \"${data.kubernetes.container.id}\" Istio format log: \"format\": \"[%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE% %RESPONSE_FLAGS% \\\"%DYNAMIC_METADATA(istio.mixer:status)%\\\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(X-FORWARDED-FOR)%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME%\\n\" Has anyone set up this module?",
    "website_area": "discuss"
  },
  {
    "id": "ae0ef409-306a-40c5-9b88-450efb1b3d6d",
    "url": "https://discuss.elastic.co/t/fielddata-error-preventing-authentications-tab-populating/197337",
    "title": "Fielddata error preventing Authentications tab populating",
    "category": [
      "SIEM"
    ],
    "author": "hilt86",
    "date": "August 29, 2019, 12:38pm August 29, 2019, 4:11pm August 29, 2019, 9:48pm September 4, 2019, 3:41am October 2, 2019, 3:41am",
    "body": "Hi all, I'm using Filebeat 7.3.1 with Elastic Cloud 7.3.0 and the Authentications pane (in SIEM app) is empty (despite being able to view them in Discover / filebeat index. When I inspect the response in the SIEM app I get : type\": \"illegal_argument_exception\", \"reason\": \"Fielddata is disabled on text fields by default The index template is loaded so not sure why this is happening? Same is happening on Auditbeat",
    "website_area": "discuss"
  },
  {
    "id": "3cba1eca-7309-491c-a942-e285f63b2e21",
    "url": "https://discuss.elastic.co/t/poc-use-elk-to-aggregate-multiple-loginsight-systems-into-one-soc/196615",
    "title": "PoC - Use ELK to aggregate multiple LogInsight Systems into one SOC",
    "category": [
      "SIEM"
    ],
    "author": "stefws",
    "date": "August 24, 2019, 11:58am August 30, 2019, 9:49am September 3, 2019, 11:45am October 1, 2019, 11:45am",
    "body": "I've personally in the last 3-4 years been using Grafana+ELK for Log&Metric monitoring in running a large application platform, but have now changed internally to a position in Security & Operation Automation and are here trying to push for ELK usage across the board of hosting multi tenant hybrid IT Operations of OnPrem VMware-HyperV, Azure, GCP, AWS, Kubernetes hosts+services as a replacement/enhancement of various monitoring platforms like SCOM, Nagios, CheckMK, As a start I'm told to run a PoC for using ELK to possible aggregate SOC data across multiple LogInsight Systems of each their different area of interests (production islands) to feature search&analysis across all the LogRythm Systems. We do also run a Graylog SIEM for selected customers, which further in time possible could be moved to an ELK based system. Would appreciate any hints to assist in building a successfully PoC, TIA",
    "website_area": "discuss"
  },
  {
    "id": "71deb346-12e3-460d-b6f0-fbd7e641916c",
    "url": "https://discuss.elastic.co/t/siem-on-logstash-and-filebeat/196523",
    "title": "Siem on logstash and filebeat",
    "category": [
      "SIEM"
    ],
    "author": "Vikash_Singh1",
    "date": "August 23, 2019, 12:54pm August 30, 2019, 12:51pm September 27, 2019, 12:49pm",
    "body": "Hi..I am sending netflow data to my server via filebeat and the indices are successfully created as well as its implemented on siem too. But there are many information which are unavailable like geo location, protocols name, pam (port application mapping), etc. So now I am thinking to send the logs to filebeat from filebeat I will configure the output to logstash and from there to elasticsearch. This is the typical layout which I am planning Netflow logs------>filebeat------->logstash------->elasticsearch. If I create a index using this mechanism then will I be able to use SIEM feature?? or I won't be able to implement?? Previously using logstash output I wasn't able to use SIEM feature as it didn't support.",
    "website_area": "discuss"
  },
  {
    "id": "d2615c03-0976-4dad-b24e-2e7eb5079a34",
    "url": "https://discuss.elastic.co/t/event-correlation-on-elk/194362",
    "title": "Event Correlation on ELK",
    "category": [
      "SIEM"
    ],
    "author": "alperensoydan",
    "date": "August 8, 2019, 1:59pm August 14, 2019, 12:59pm August 26, 2019, 12:07pm September 23, 2019, 12:07pm",
    "body": "Hello, I installed ELK as a SIEM and It works nicely. There is only one problem is that correlation of different events and it does not come default within ELK. According to my researches, Logstash filters work for this job but there is no decent document for it. My question is that: Is there a simple way for this job like an engine or plug-in? If it is not, how can I find a documentation about correlation on ELK? (with Logstash or another way) Also, logs can come from differebt sources like firewall and server. P.S. My structure: BEAT -> Logstash -> Elastic -> Kibana",
    "website_area": "discuss"
  },
  {
    "id": "85718924-0e76-4c6e-b26d-d8283f5679a1",
    "url": "https://discuss.elastic.co/t/graphql-internal-error/195405",
    "title": "GraphQL internal error",
    "category": [
      "SIEM"
    ],
    "author": "j13029",
    "date": "August 15, 2019, 11:05pm August 19, 2019, 10:24am September 16, 2019, 10:27am",
    "body": "I am currently getting some error messages under SIEM menus. \"Unable to parse JSON < at line 52\" Checking the network logs, this call seems to be failing along. It would sometimes go through and sometimes fail. What could be the issue here? Thanks! Request URL: https://34.20.139.21/api/siem/graphql Request Method: POST Status Code: 502 {operationName: \"GetKpiHostsQuery\",}",
    "website_area": "discuss"
  },
  {
    "id": "31b141c1-df3d-4b23-bc57-2f1bab83b3b0",
    "url": "https://discuss.elastic.co/t/difference-between-source-destination-and-server-client/195075",
    "title": "Difference between source/destination and server/client",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "August 14, 2019, 1:45pm August 16, 2019, 1:48pm September 13, 2019, 1:56pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "abfd7888-1650-462a-9889-414520127b13",
    "url": "https://discuss.elastic.co/t/im-not-seeing-any-geoip-data-from-my-zeek-logs-in-my-siem-map/194677",
    "title": "I'm not seeing any geoip data from my zeek logs in my SIEM map",
    "category": [
      "SIEM"
    ],
    "author": "dpangallo",
    "date": "August 9, 2019, 10:42pm August 12, 2019, 8:05am August 12, 2019, 5:09pm September 9, 2019, 5:09pm",
    "body": "I'm using Filebeats 7.3.0 to ingest Zeek network logs into Elasticsearch 7.3.0, but I'm not seeing any geoip fields created in my indexes. The Zeek ingest module automatically created pipelines for each of the different log file types; http, ssl, dns, etc.. Here is my filebeat yml: #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: hosts: \"http://[private natted ip]:9200\" pipeline: filebeat-7.3.0-zeek-http-pipeline Here is my pipeline for filebeat-7.3.0-zeek-http-pipeline: { \"filebeat-7.3.0-zeek-http-pipeline\" : { \"description\" : \"Pipeline for normalizing Zeek http.log\", \"processors\" : [ { \"script\" : { \"source\" : \"ctx.event.created = ctx['@timestamp']; ctx['@timestamp'] = (long)ctx['zeek']['http']['ts'] * 1000; ctx.zeek.http.remove('ts');\", \"lang\" : \"painless\" } }, { \"set\" : { \"field\" : \"event.id\", \"value\" : \"{{zeek.session_id}}\", \"if\" : \"ctx.zeek.session_id != null\" } }, { \"set\" : { \"field\" : \"source.ip\", \"value\" : \"{{source.address}}\" } }, { \"set\" : { \"field\" : \"destination.ip\", \"value\" : \"{{destination.address}}\" } }, { \"set\" : { \"field\" : \"url.port\", \"value\" : \"{{destination.port}}\" } }, { \"geoip\" : { \"field\" : \"destination.ip\", \"target_field\" : \"destination.geo\" } }, { \"geoip\" : { \"field\" : \"source.ip\", \"target_field\" : \"source.geo\" } }, { \"user_agent\" : { \"field\" : \"user_agent.original\", \"ignore_missing\" : true } } ], \"on_failure\" : [ { \"set\" : { \"field\" : \"error.message\", \"value\" : \"{{ _ingest.on_failure_message }}\" } } ] } } Here is a sample result of doc with missing geoip fields: { \"_index\" : \"filebeat-7.3.0-2019.08.09-000001\", \"_type\" : \"_doc\", \"_id\" : \"FTYveGwBUFrAUZ51xToN\", \"_version\" : 1, \"_seq_no\" : 122365, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"agent\" : { \"hostname\" : \"zeek-server\", \"id\" : \"d9b26eb5-e10c-48c5-919a-254ffed87cb9\", \"type\" : \"filebeat\", \"ephemeral_id\" : \"67c89f50-8910-4366-997c-4387da92508a\", \"version\" : \"7.3.0\" }, \"log\" : { \"file\" : { \"path\" : \"/usr/local/bro/spool/logger/http.log\" }, \"offset\" : 523009 }, \"destination\" : { \"address\" : \"35.171.236.97\", \"port\" : 80 }, \"zeek\" : { \"session_id\" : \"C9V9Al4BAB3hboznb8\", \"http\" : { \"resp_mime_types\" : [ \"text/json\" ], \"trans_depth\" : 1, \"status_msg\" : \"OK\", \"ts\" : \"2019-08-09T21:01:30.272945Z\", \"resp_fuids\" : [ \"FUEzx03onGBuCa9xQk\" ], \"tags\" : } }, \"source\" : { \"address\" : \"[internal natted ip redacted]\", \"port\" : 63052 }, \"fileset\" : { \"name\" : \"http\" }, \"error\" : { \"message\" : \"cannot explicitly cast def [java.lang.String] to long\" }, \"network\" : { \"community_id\" : \"1:h8xEg+e7aCg3NNU48d2V7rublBk=\", \"transport\" : \"tcp\" }, \"tags\" : [ \"zeek.http\" ], \"input\" : { \"type\" : \"log\" }, \"@timestamp\" : \"2019-08-09T21:01:31.210Z\", \"ecs\" : { \"version\" : \"1.0.1\" }, \"service\" : { \"type\" : \"zeek\" }, \"host\" : { \"name\" : \"vhsbns\" }, \"http\" : { \"request\" : { \"body\" : { \"bytes\" : 0 } }, \"response\" : { \"status_code\" : 200, \"body\" : { \"bytes\" : 33 } }, \"version\" : \"1.1\" }, \"event\" : { \"created\" : \"2019-08-09T21:01:31.210Z\", \"module\" : \"zeek\", \"dataset\" : \"zeek.http\" } } } What am i doing wrong?",
    "website_area": "discuss"
  },
  {
    "id": "b8dc8e0d-3078-487c-aa41-8aee3d3d9be7",
    "url": "https://discuss.elastic.co/t/filebeat-for-sophos-xg-firewall/192952",
    "title": "Filebeat for Sophos XG Firewall",
    "category": [
      "SIEM"
    ],
    "author": "kal1s",
    "date": "July 30, 2019, 4:53pm July 31, 2019, 6:26am July 31, 2019, 5:08pm August 1, 2019, 7:12am August 5, 2019, 12:46pm August 5, 2019, 12:24pm August 6, 2019, 3:33pm August 6, 2019, 6:03pm August 7, 2019, 4:15pm September 4, 2019, 4:15pm",
    "body": "Hi, Will be possible to have Filebeat for Sophos XG FIrewall and integrated with Elastic SIEM like Filebeat Cisco? Best Regards, Ricardo Calimanis",
    "website_area": "discuss"
  },
  {
    "id": "4657e94a-5920-4062-a70f-e09f5661cea4",
    "url": "https://discuss.elastic.co/t/why-dont-sudo-events-from-auth-log-have-an-event-category-event-action/193911",
    "title": "Why don't sudo events from auth.log have an event.category/event.action?",
    "category": [
      "SIEM"
    ],
    "author": "agx",
    "date": "August 6, 2019, 3:13am August 7, 2019, 11:10am September 4, 2019, 11:14am",
    "body": "Hello, I've been exploring the new SIEM features in 7.3 and am pretty excited about promoting ECS at work. I noticed that when ingesting my auth logs from Ubuntu 18.04 using the system module included with filebeat, that sudo entries from the auth.log just show up blank in the timeline explorer. Was this a design decision? Shouldn't a sudo event have event fields populated too? elastic_siem.png896269 25.5 KB",
    "website_area": "discuss"
  },
  {
    "id": "1e7fabbd-a958-4afb-a8dd-480e98ffb402",
    "url": "https://discuss.elastic.co/t/about-the-app-search-category/159239",
    "title": "About the App Search category",
    "category": [
      "App Search"
    ],
    "author": "warkolm",
    "date": "December 4, 2018, 12:16am",
    "body": "App Search is a search solution that simplifies the building of rich search experiences for software applications of every kind  from ecommerce websites, to SaaS applications, to mobile apps.",
    "website_area": "discuss"
  },
  {
    "id": "3e0d257e-a467-4444-bd6d-04bc3a946408",
    "url": "https://discuss.elastic.co/t/self-hosted-elastic-app-search-add-user-via-invitation-not-working/214601",
    "title": "Self hosted Elastic App Search add user via invitation not working",
    "category": [
      "App Search"
    ],
    "author": "abhishek.haith",
    "date": "January 13, 2020, 6:07pm January 10, 2020, 4:50pm January 13, 2020, 12:27pm January 13, 2020, 2:11pm January 16, 2020, 11:05am January 22, 2020, 7:18am January 22, 2020, 7:18am",
    "body": "I have set up Elastic App Search in standard security mode. Configuration picked from below document - Swiftype App Search, Self Managed, Security and User Management | Swiftype Documentation Learn how to get the most out of Swiftype Also configured mailer so I am able to get invitation mail. After submitting the create account form getting Authentication has failed error. Elastic App Search user registration failed402676 32.1 KB Logs on console - appsearch_1 | [2020-01-10T12:49:24.958+00:00][13][2284][app-server][INFO]: [81d2d6bf-cd7e-4d29-956b-55c9783dd068] Started POST \"/authenticate/accept_invitation\" for 172.23.0.1 at 2020-01-10 12:49:24 +0000 appsearch_1 | [2020-01-10T12:49:24.990+00:00][13][2284][action_controller][INFO]: [81d2d6bf-cd7e-4d29-956b-55c9783dd068] Processing by AuthenticateController#accept_invitation as */* appsearch_1 | [2020-01-10T12:49:24.995+00:00][13][2284][action_controller][INFO]: [81d2d6bf-cd7e-4d29-956b-55c9783dd068] Parameters: {\"auth_strategy\"=>\"elasticsearch_standard\", \"user\"=>{\"password\"=>\"[FILTERED]\"}, \"lm_invitation_code\"=>\"hCYBOH9KuGvbgVQmOgUaQA\", \"host\"=>\"localhost:3002\", \"protocol\"=>\"http\"} appsearch_1 | [2020-01-10T12:49:25.063+00:00][13][2284][action_view][INFO]: [81d2d6bf-cd7e-4d29-956b-55c9783dd068] Rendered text template (0.2ms) appsearch_1 | [2020-01-10T12:49:25.066+00:00][13][2284][action_controller][INFO]: [81d2d6bf-cd7e-4d29-956b-55c9783dd068] Completed 403 Forbidden in 69ms (Views: 10.3ms) Please guide to make this working. Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "5d38f450-3d3c-416a-b653-8419ac0bba54",
    "url": "https://discuss.elastic.co/t/signed-search-key-via-api/215457",
    "title": "Signed Search Key via API",
    "category": [
      "App Search"
    ],
    "author": "malik_aditya",
    "date": "January 21, 2020, 4:20pm January 17, 2020, 12:52pm January 20, 2020, 5:08am January 21, 2020, 12:20pm",
    "body": "Hi, I was trying to use SignedSearchKey approach via API but the search and result settings are applied to the whole engine instead for that specifc key. Is there any workaround for that? Like creating keys with predefined search and result settings. So you can just use different keys to search on different specific fields and get differnt result sets.",
    "website_area": "discuss"
  },
  {
    "id": "466f2a15-7f5b-4dbc-948f-470dcb317954",
    "url": "https://discuss.elastic.co/t/self-hosted-app-search-limited-to-one-es-node/215211",
    "title": "Self-hosted App Search limited to one ES Node?",
    "category": [
      "App Search"
    ],
    "author": "fred_wang",
    "date": "January 15, 2020, 8:59pm January 16, 2020, 4:18am January 20, 2020, 10:21am January 17, 2020, 4:03pm January 20, 2020, 10:33am",
    "body": "I tried to configure a multi-node elastic search (3 nodes) and an app search engine using Docker compose. I finally got what I think is a working docker-compose.yml for this (The documentation on using Docker compose with App search only includes a single node instance) However, in running it, I get the following error message: appsearch-engine | [2020-01-15T20:36:12.852+00:00][14][2002][app-server][ERROR]: appsearch-engine | -------------------------------------------------------------------------------- appsearch-engine | appsearch-engine | Elasticsearch cluster must be licensed. OSS versions of Elasticsearch do not contain a supported license. Please download and run an Elasticsearch binary from https://elastic.co/downloads/elasticsearch to acquire a free, Basic license. appsearch-engine | appsearch-engine | -------------------------------------------------------------------------------- Does this mean we can not run multi-node elastic search clusters backing app search if we self-host? If this is is not the right interpretation, how do we apply a basic license to the docker-compose.yml during the startup?",
    "website_area": "discuss"
  },
  {
    "id": "b952c7cf-0d04-4f85-afba-c98c50d5e674",
    "url": "https://discuss.elastic.co/t/work-with-geolocation/215242",
    "title": "Work with GeoLocation",
    "category": [
      "App Search"
    ],
    "author": "malik_aditya",
    "date": "January 16, 2020, 11:32am January 16, 2020, 11:36am January 16, 2020, 11:36am January 17, 2020, 1:19pm January 17, 2020, 1:20pm",
    "body": "Hi. I'm trying to implement the near me feature using geolocation type field. However i don't see any documentation regarding quering using geolocation, so if you can please help me or just point to the right documentation, that'll be great.",
    "website_area": "discuss"
  },
  {
    "id": "556e038c-c2a9-4cb6-aa32-ff383793b336",
    "url": "https://discuss.elastic.co/t/how-to-implement-near-me/215304",
    "title": "How to implement Near Me",
    "category": [
      "App Search"
    ],
    "author": "JasonStoltz",
    "date": "January 16, 2020, 11:32am",
    "body": "A user asked this question and then withdrew their post. Re-posting the answer here because I thought it was a valid question that others may have in the future: Near me can be implemented with a Geo Filter: https://swiftype.com/documentation/app-search/api/search/filters#geo-filters. \"filters\": { \"location\": { \"center\": \"37.386483, -122.083842\", \"distance\": 300, \"unit\": \"km\" } } Pass the user's location in as \"center\" and then adjust the \"distance\" and \"unit\" to whatever you would consider \"near\".",
    "website_area": "discuss"
  },
  {
    "id": "762e8f80-d841-456f-ba43-24ebf5ec3501",
    "url": "https://discuss.elastic.co/t/appsearch-auto-sync-with-database/214887",
    "title": "AppSearch Auto Sync with database",
    "category": [
      "App Search"
    ],
    "author": "Jonathan_Gautier",
    "date": "January 13, 2020, 6:14pm January 14, 2020, 10:28am",
    "body": "Hi ! I want to know if some software or script like Monstache (MongoDB -> Elasticsearch), exist for AppSearch to auto sync one database with MongoDB or another database to AppSearch. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "9aedb9f8-4e82-4bb4-896e-f229a2e1bd4e",
    "url": "https://discuss.elastic.co/t/signed-search-keys-rate-limit/214885",
    "title": "Signed Search Keys Rate Limit",
    "category": [
      "App Search"
    ],
    "author": "Jonathan_Gautier",
    "date": "January 13, 2020, 6:09pm January 13, 2020, 6:17pm January 13, 2020, 7:03pm",
    "body": "Hi ! I want to know if we can make ratelimiting with Signed Search Keys ? Or another ways to limit on key usage ? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "a7f513cd-1a0a-4e8b-84c4-9d16b7876ce7",
    "url": "https://discuss.elastic.co/t/app-search-parent-child-relationships/214618",
    "title": "App search parent child relationships",
    "category": [
      "App Search"
    ],
    "author": "arjunyel",
    "date": "January 10, 2020, 3:11pm January 13, 2020, 10:39am January 13, 2020, 10:46am",
    "body": "Hello, we are evaluating app search vs regular elasticsearch, does app search support parent/child relationships? I can't find any mention of it in the docs. Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "d9e6506c-a388-448d-972b-0f9ed582473e",
    "url": "https://discuss.elastic.co/t/self-hosted-appsearch-and-eck-can-not-login-to-appsearch-invalid-credentials/214683",
    "title": "Self hosted AppSearch and ECK - can not login to AppSearch - invalid credentials",
    "category": [
      "App Search"
    ],
    "author": "byteandbit",
    "date": "January 10, 2020, 10:56pm",
    "body": "Does anyone successfully configured the latest AppSearch Docker image (http://docker.elastic.co/app-search/app-search:7.5.1) with https://www.elastic.co/products/elastic-cloud-kubernetes (also 7.5.1) ? I have set up Elastic App Search in standard security mode. I have used default username and password. I also have reset the password in AppSearch via: bin/app-search --reset-auth and also in ElasticSearch via: POST /_security/user/app_search/_password Unfortunately nothing is working and can not login to AppSearch (keep getting Completed 403 Forbidden in AppSearch logs). Is there anything specific set in ECK that is not mentioned in AppSearch documentation (https://swiftype.com/documentation/app-search/self-managed/security) ?",
    "website_area": "discuss"
  },
  {
    "id": "e4ae9b57-a22c-4466-a073-207ec8254cc2",
    "url": "https://discuss.elastic.co/t/no-results-while-updating-re-indexing-schema/214472",
    "title": "No results while Updating/Re-indexing schema",
    "category": [
      "App Search"
    ],
    "author": "Harry_Knowles",
    "date": "January 9, 2020, 4:55pm January 10, 2020, 2:05pm",
    "body": "Hi, I've just noticed that when we run the \"update schema\" function, results no longer show at all when using the search until the index is complete. I can see why this happens, I'm just wondering if there is a way to avoid it, or serve some older/cached results while the schema is updating? The issue I'm having is every now and again, I am getting the \"Unable to save document\" error. When this happens, I've been instructed by your support team to simply run a schema update/re-index which fixed the issue. This would be fine if it wasn't for the schema update taking around an hour to complete, whilst results are unavailable. Is there a work around for this? Or another way other than re-indexing to fix the \"Unable to save document\" error? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "ad3f3954-02bf-47e9-827d-8bb9cc75d525",
    "url": "https://discuss.elastic.co/t/missing-user-roles-on-self-hosted-app-search/214370",
    "title": "Missing User Roles on Self Hosted App Search",
    "category": [
      "App Search"
    ],
    "author": "abhishek.haith",
    "date": "January 9, 2020, 8:34am January 10, 2020, 5:03am January 10, 2020, 5:03am",
    "body": "Currently I am trying to configure App Search with Role Based Access Control. I followed the documentation given in the link below - Swiftype App Search, Self Managed, Security and User Management | Swiftype Documentation Learn how to get the most out of Swiftype I have tried Standard and Elasticsearch Native Realm security mode but found only Owner and Admin roles in the App Search User section. As documented on the below link, other roles like Dev, Editor etc are not showing. Swiftype Role Based Access Control Guide | Swiftype Documentation Learn how to get the most out of Swiftype How to create/enable those roles? Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "b08c7759-3ff8-4d0c-a16a-6c612292aba2",
    "url": "https://discuss.elastic.co/t/docker-app-search-how-to-find-default-password/213235",
    "title": "Docker - app_search - how to find default password",
    "category": [
      "App Search"
    ],
    "author": "",
    "date": "December 27, 2019, 11:01pm December 30, 2019, 8:35am December 30, 2019, 8:37am December 30, 2019, 10:26am December 30, 2019, 10:29am January 1, 2020, 6:44am January 6, 2020, 12:18pm January 6, 2020, 11:01pm January 8, 2020, 10:42pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "3fa5e553-99a8-408e-9366-0e0269a6d249",
    "url": "https://discuss.elastic.co/t/document-deletion-is-painfully-slow-with-self-hosted-app-search/212914",
    "title": "Document deletion is painfully slow with self-hosted app-search",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "December 24, 2019, 5:09am December 30, 2019, 9:50am January 8, 2020, 6:53am",
    "body": "I'm currently testing app-search locally and have setup ES + App Search latest version (7.5.1). I'm also making use of the official PHP client: https://github.com/elastic/app-search-php to test. I added about 2500 records (documents) in an engine and later decided to delete all of them. Since deleting the engine won't delete the documents, I decided loop over each document and delete them using IDs. Turns out that deleting each document is painfully slow - taking about ~2 seconds for each. I also tried feeding array of 100 Ids to delete; but it still takes about the same time. Is this the expected behavior? Is there any way to speed this up? PS: Also want to know if the self-hosted app-search is limited to indexing 100,000 documents?",
    "website_area": "discuss"
  },
  {
    "id": "92ac8c72-7abd-41b8-930c-afe0e179220b",
    "url": "https://discuss.elastic.co/t/how-to-import-10-millions-of-documents-about-5g-into-app-search/211716",
    "title": "How to import 10 millions of documents (about 5G) into app search?",
    "category": [
      "App Search"
    ],
    "author": "viphuangwei",
    "date": "December 12, 2019, 7:58pm December 13, 2019, 11:55am December 13, 2019, 6:40pm December 13, 2019, 8:21pm December 14, 2019, 4:20pm December 29, 2019, 9:00pm January 2, 2020, 12:23pm January 3, 2020, 3:32am January 3, 2020, 1:52pm January 3, 2020, 2:05pm January 3, 2020, 6:02pm January 6, 2020, 1:11am January 6, 2020, 11:52am January 8, 2020, 3:51am",
    "body": "Hi There, How to import 10 million of documents (about 5G) into the App search (self-managed)? What's the best practice to do it? Is there any limitation to do this. Thanks, Wei.",
    "website_area": "discuss"
  },
  {
    "id": "a2008202-7302-4939-a799-f8337c77c067",
    "url": "https://discuss.elastic.co/t/maximum-character-limit-in-a-query/213740",
    "title": "Maximum character limit in a Query",
    "category": [
      "App Search"
    ],
    "author": "joao_Dubas",
    "date": "January 3, 2020, 6:49pm January 6, 2020, 11:51am January 6, 2020, 12:12pm January 6, 2020, 12:38pm",
    "body": "Hi! I'm new to the elk, and i have installed the self-managed appsearch. And i need to make querys with more then 128 characters, how do i raise that limit?",
    "website_area": "discuss"
  },
  {
    "id": "c8e395ed-85ad-440c-b686-02a4e4680784",
    "url": "https://discuss.elastic.co/t/how-to-make-search-ui-show-results-in-a-new-tab/212577",
    "title": "How to make Search UI show results in a new tab?",
    "category": [
      "App Search"
    ],
    "author": "kasibiel",
    "date": "December 20, 2019, 12:25am December 20, 2019, 11:24am December 20, 2019, 9:37pm January 2, 2020, 12:18pm",
    "body": "Hello! I am new to React, so please excuse me if I am asking a lame question. I would like to set up the Reference UI to show search results (upon pressing the \"Search\" button) in a new tab or on a new page. How can I achieve that? After spending nearly a day on this I wasn't able to figure out by myself. Basically, I would like to configure my website so that it shows the search box on the home page, but the search results are displayed on a separate page. I would appreaciate your assistance. Thank you! Kasia",
    "website_area": "discuss"
  },
  {
    "id": "0a150edf-b86e-4267-b022-e6acc3795126",
    "url": "https://discuss.elastic.co/t/about-indexing-documents/213378",
    "title": "About Indexing Documents",
    "category": [
      "App Search"
    ],
    "author": "viphuangwei",
    "date": "December 30, 2019, 7:21pm December 30, 2019, 7:26pm December 30, 2019, 7:52pm",
    "body": "Hi There, Is that any way to support the uppercase name of the indexing document, It seems that it only supports the lowercase name of the document. e.g. we would like to support {'orderID': 100001}, it only support {'orderid': 100001} now. Because our existing name of JSON for both Frond and Back End are all camel style. There would be a lot of places need to change. Is that any good suggestion?",
    "website_area": "discuss"
  },
  {
    "id": "0d0c83f4-2d1c-42d1-80a4-e5cbe55fd44d",
    "url": "https://discuss.elastic.co/t/docker-and-kubernetes-problem-with-connecting-to-elasticsearch/213129",
    "title": "Docker and Kubernetes problem with connecting to Elasticsearch",
    "category": [
      "App Search"
    ],
    "author": "",
    "date": "December 27, 2019, 5:09am December 27, 2019, 8:55pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "429ea8cb-44fe-4aaa-bed8-454ec881089e",
    "url": "https://discuss.elastic.co/t/not-seeing-clicks-being-reported/212766",
    "title": "Not seeing Clicks being reported",
    "category": [
      "App Search"
    ],
    "author": "CKennedy",
    "date": "December 22, 2019, 7:26am December 23, 2019, 8:22am December 23, 2019, 10:47am January 20, 2020, 10:47am",
    "body": "Hi We have recently implemented Appsearch and everything is working well. Only thing is that we are not seeing any click reporting in the UI and preety confident that there have been some volume. Is there an additional integration required to get this to roll up? Many thanks",
    "website_area": "discuss"
  },
  {
    "id": "47ce39ed-f0c2-4d08-979f-53fa07b06dae",
    "url": "https://discuss.elastic.co/t/how-to-do-date-range-filter-for-documents-in-swiftype/212069",
    "title": "How to do Date range filter for documents in Swiftype?",
    "category": [
      "App Search"
    ],
    "author": "viphuangwei",
    "date": "December 16, 2019, 10:37pm December 20, 2019, 12:43pm December 18, 2019, 5:31pm December 20, 2019, 12:43pm",
    "body": "Hi There, Is that any sample to do the date range filter for documents in Swiftype, Do we support this now? e.g. I would like to filter out the \"invoice date\" between '2019-01-01' and '2019-12-31' Thanks, Wei.",
    "website_area": "discuss"
  },
  {
    "id": "0c6f40e2-669b-434b-a98d-84ce87b83728",
    "url": "https://discuss.elastic.co/t/receiving-unable-to-save-document-when-submitting-documents-through-api/211468",
    "title": "Receiving \"Unable to save document\" when submitting documents through API",
    "category": [
      "App Search"
    ],
    "author": "Harry_Knowles",
    "date": "December 11, 2019, 1:19pm December 12, 2019, 11:55am December 12, 2019, 1:50pm December 13, 2019, 12:00pm January 10, 2020, 12:00pm",
    "body": "Hi, can't find this error anywhere in the docs. I'm wondering when this error is supposed to be thrown? I'm using a Shopify webhook to send product data to the search engine when product information updates and I seem to be getting this error a lot more than it being successful. Is it possible that this is thrown when a documents contents hasn't changed? Thanks, Harry.",
    "website_area": "discuss"
  },
  {
    "id": "bc8eab08-8148-4c5a-8347-31feda2af9e6",
    "url": "https://discuss.elastic.co/t/filter-search-by-score/208322",
    "title": "Filter search by score",
    "category": [
      "App Search"
    ],
    "author": "AlejandroNextChance",
    "date": "November 19, 2019, 1:17pm November 18, 2019, 10:11pm November 18, 2019, 10:12pm November 19, 2019, 9:25am November 19, 2019, 12:40pm November 19, 2019, 2:09pm November 22, 2019, 12:16pm November 22, 2019, 6:02pm November 25, 2019, 12:56pm November 25, 2019, 3:33pm November 25, 2019, 5:24pm November 26, 2019, 3:37pm November 30, 2019, 7:11am December 2, 2019, 10:32pm December 5, 2019, 4:34am December 12, 2019, 11:58am December 13, 2019, 6:49am January 10, 2020, 6:49am",
    "body": "Id like to perform a search, but only get back search results that have a score greater-than 1 for instance. Is this possible?",
    "website_area": "discuss"
  },
  {
    "id": "e1ababaa-805d-4ebd-8749-60efde61b058",
    "url": "https://discuss.elastic.co/t/appsearch-swiftype-duplicatekeyerror-self-managed/208215",
    "title": "AppSearch Swiftype::Documents::DuplicateKeyError Self-Managed",
    "category": [
      "App Search"
    ],
    "author": "Harry_Knowles",
    "date": "November 17, 2019, 10:30am November 17, 2019, 4:19pm November 19, 2019, 1:48pm November 19, 2019, 1:48pm November 19, 2019, 10:42am December 4, 2019, 11:01am December 9, 2019, 1:40pm January 6, 2020, 1:40pm",
    "body": "Hi we're trying to install a self managed version of App Search on our server, however when trying to connect to the API to index documents it's returning me the following error: [\"Internal server error. Please email support@swiftype.com if this error persists.\"] In the server logs, it looks like there is some sort of duplicate key error, but I can't find anything about this online. It does let 1 or 2 API requests through before throwing this error. You can see the server logs here: http://134.209.185.32/appsearcherror.txt Anyone have any ideas on how we could fix this? Thanks, Harry.",
    "website_area": "discuss"
  },
  {
    "id": "92280b0d-8d9e-4c26-9f7e-2c50af232bcb",
    "url": "https://discuss.elastic.co/t/response-total-results-doesnt-match-results/210274",
    "title": "Response total results doesn't match results",
    "category": [
      "App Search"
    ],
    "author": "abohannon",
    "date": "December 3, 2019, 3:44am December 3, 2019, 9:51am December 3, 2019, 6:17pm December 4, 2019, 9:17am January 1, 2020, 9:17am",
    "body": "I'm attempting to remove duplicate entries in our index by deleting all documents before re-uploading, but after I delete all documents 60 still remain and I'm unable to delete them. And when I query for the list of documents, I get total_results: 60 but an empty array for the actual results. Is this a bug or am I overlooking something? Screenshot of query response: Screenshot of trying to access a document in the web app (this document, despite being deleted, is still somehow being indexed on our site): Screen Shot 2019-12-02 at 7.43.09 PM.png2046296 40.5 KB",
    "website_area": "discuss"
  },
  {
    "id": "29ad80b6-34a1-4c2f-87a3-ba3bb3c56e81",
    "url": "https://discuss.elastic.co/t/elastic-app-search-self-managed-and-bonsai-io-integration/209580",
    "title": "Elastic app search(Self-managed) and bonsai.io integration",
    "category": [
      "App Search"
    ],
    "author": "111249",
    "date": "November 26, 2019, 8:45pm November 27, 2019, 10:27pm December 24, 2019, 9:43pm",
    "body": "I have an Elastic cluster on bonsai.io, I want to use it to work with elastic app search locally on my computer. In app-search.yaml I added elasticsearch.host, elasticsearch.username and elasticsearch.password, but when I run bin / app-search I get an error: \"Elasticsearch cluster must be licensed. OSS versions of Elasticsearch do not contain a supported license. Please download and run an Elasticsearch binary from https://elastic.co/downloads/elasticsearch to acquire a free, Basic license.\" Has anyone encountered a similar error or knows how to fix it",
    "website_area": "discuss"
  },
  {
    "id": "4d4ee412-3ab4-4d45-bf5a-41b40b486c69",
    "url": "https://discuss.elastic.co/t/swiftype-app-search-not-showing-indexed-documents-using-python-api/209330",
    "title": "Swiftype App Search Not Showing Indexed Documents using Python API",
    "category": [
      "App Search"
    ],
    "author": "jh.94",
    "date": "November 25, 2019, 2:50pm November 25, 2019, 3:20pm December 23, 2019, 3:20pm",
    "body": "Hi There, I am currently refering to this tutorial on using python API to index document to the elastic app search engine. PyPI elastic-app-search An API client for Elastic App Search The code works fine but after going to the Swiftype App Search dashboard at localhost:3002 , it is not showing the index documents. image.png1085671 41.5 KB Any suggestions as to why it is not updating? Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "86c33a5d-6d15-4f71-bdff-5816bd395667",
    "url": "https://discuss.elastic.co/t/how-to-backup-and-restore-your-engines/207438",
    "title": "How to backup and restore your engines?",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "November 12, 2019, 5:02am November 15, 2019, 12:10pm November 25, 2019, 3:19am December 23, 2019, 3:19am",
    "body": "Dear Team, I'm using App Search self-managed. So how to backup and restore your engines? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "f72bb7d4-112c-4bd6-8cc3-dce6b88cfa0d",
    "url": "https://discuss.elastic.co/t/how-to-process-the-nest-object-from-es-response-in-search-ui/208844",
    "title": "How to process the nest object (from ES response) in search-ui",
    "category": [
      "App Search"
    ],
    "author": "Nee_Defeng",
    "date": "November 21, 2019, 9:33am November 21, 2019, 12:36pm November 21, 2019, 7:52pm November 22, 2019, 2:09am November 22, 2019, 11:43am December 20, 2019, 11:43am",
    "body": "I now this is for App Search, but not sure where to post this search-ui for elasticsearch question. In a hard way that I learned that search-ui is expecting the 'id' field in the elasticsearch index hence the response. The problem is that the index is created by another program which does not have 'id', though it has the '_id' field. In the buildState.js file, I am able to convert '_id' to 'id', but for this reason, I have to use the 'record' instead of 'record._source'. With this, the result set returns the _source as this: \"_source\": [object Object] while 'id' is normal: \"id\": cee86628eaebffb36d3378be7a94651 So, how can I get the field inside the '_source' object? Here is the code from buildState.js: const res = hits.map(record => { return Object.entries(record) .map(([fieldName, fieldValue]) => [ fieldName == \"_id\" ? \"id\" : fieldName, toObject(fieldValue, getHighlight(record, fieldName)) ]) .reduce(addEachKeyValueToObject, {}); }); Here is the state returned by buildState.js, and you can see it is an object.: { \"id\": { \"raw\": \"8239827cd9e538e88a03f71d2c8ae14\" }, \"_index\": { \"raw\": \"myfirstfsjob\" }, \"_score\": { \"raw\": 9.116072 }, \"_source\": { \"raw\": { \"content\": \"XXX\", \"file\": { \"filename\": \"dt4_8.pdf\", \"url\": \"c:\\\\dt4_8.pdf\" } } } }",
    "website_area": "discuss"
  },
  {
    "id": "4698a20a-1851-40d9-8143-8fb3c88ffc0d",
    "url": "https://discuss.elastic.co/t/in-app-search-api-how-to-check-if-the-field-exists-or-not-in-a-document-during-search/208535",
    "title": "In App Search API how to check if the field exists or not in a document during search",
    "category": [
      "App Search"
    ],
    "author": "jayrraval",
    "date": "November 19, 2019, 3:10pm November 19, 2019, 9:18pm November 20, 2019, 3:59am November 20, 2019, 6:30pm November 21, 2019, 9:06am December 19, 2019, 9:11am",
    "body": "In App Search API, how i can in query for documents that does not contain a particular field. In elastic search we have \"exists\" keyword in while in App Search API there does not seems to be \"exists\" keyword to check if a field exists in a particular document.",
    "website_area": "discuss"
  },
  {
    "id": "361b504f-2d39-42fd-9ea3-140698db67da",
    "url": "https://discuss.elastic.co/t/in-app-search-api-how-to-check-if-the-field-exists-or-not-in-a-document-during-search/208535",
    "title": "In App Search API how to check if the field exists or not in a document during search",
    "category": [
      "App Search"
    ],
    "author": "jayrraval",
    "date": "November 19, 2019, 3:10pm November 19, 2019, 9:18pm November 20, 2019, 3:59am November 20, 2019, 6:30pm November 21, 2019, 9:06am December 19, 2019, 9:11am",
    "body": "In App Search API, how i can in query for documents that does not contain a particular field. In elastic search we have \"exists\" keyword in while in App Search API there does not seems to be \"exists\" keyword to check if a field exists in a particular document.",
    "website_area": "discuss"
  },
  {
    "id": "67f9f17c-a9c7-450e-a356-1a75236e3232",
    "url": "https://discuss.elastic.co/t/typeerror-cannot-read-property-suggestion-of-null/208665",
    "title": "TypeError: Cannot read property 'suggestion' of null",
    "category": [
      "App Search"
    ],
    "author": "Swapnil_Ghorpade",
    "date": "November 20, 2019, 9:50am November 20, 2019, 3:53pm November 21, 2019, 7:30am December 19, 2019, 7:30am",
    "body": "Search ui is throwing error on pressing esc button. Steps to reproduce : Enter text for search and select search text from suggestions. 2.Press esc key to clear input it will throw 'Cannot read property 'suggestion' of null'",
    "website_area": "discuss"
  },
  {
    "id": "2571417d-c6c3-4622-8586-782fc8555f8f",
    "url": "https://discuss.elastic.co/t/swiftype-hosted-api-error-failed-to-fetch-9-39am-cors-issue/208473",
    "title": "Swiftype Hosted API - error Failed to fetch 9:39am (cors issue)",
    "category": [
      "App Search"
    ],
    "author": "Harry_Knowles",
    "date": "November 19, 2019, 9:41am November 19, 2019, 10:44am November 19, 2019, 10:59am November 19, 2019, 11:11am November 19, 2019, 11:16am November 19, 2019, 1:45pm December 17, 2019, 1:45pm",
    "body": "Hi, I'm having an issue with the API currently. Website: https://www.rydale.com/ We're recieving a CORS error: Access to fetch at 'https://host-64fz8w.api.swiftype.com/api/as/v1/engines/rydale/search.json' from origin 'https://www.rydale.com' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: The value of the 'Access-Control-Allow-Credentials' header in the response is '' which must be 'true' when the request's credentials mode is 'include'. Does anyone know what is going on? Our entire search is currently offline during our black friday sale! Thanks, Harry.",
    "website_area": "discuss"
  },
  {
    "id": "84083e3c-a939-47b5-a1df-c8e6409c8cb7",
    "url": "https://discuss.elastic.co/t/security-how-to-avoid-exposing-app-search-keys-searchkey-host-identifier-enginename-in-search-ui/207993",
    "title": "Security - how to avoid exposing app search keys (searchKey,host identifier,engineName) in search ui",
    "category": [
      "App Search"
    ],
    "author": "Swapnil_Ghorpade",
    "date": "November 15, 2019, 6:03am November 15, 2019, 11:31am November 18, 2019, 7:10am November 18, 2019, 12:44pm November 18, 2019, 1:12pm November 18, 2019, 1:37pm November 18, 2019, 1:42pm November 19, 2019, 6:28am November 19, 2019, 12:23pm December 17, 2019, 12:23pm",
    "body": "I'm using app search with search ui https://github.com/elastic/search-ui Everything looks great about search ui except security. All keys Credentials (searchKey,host identifier,engineName) are visible in api call. I thought to move Connectors key logic to backend. So the flow will be like 1.search ui call backend api. 2 backend will call app search-api and return response as it is to show result on ui. is the right way to go? I havent found any way to give own api call from search-ui. How can I do this?",
    "website_area": "discuss"
  },
  {
    "id": "36c84492-8e2c-46ce-a31a-59c9a6549a07",
    "url": "https://discuss.elastic.co/t/relevance-tuning-settings-bad-request-on-save/207227",
    "title": "Relevance Tuning Settings Bad Request on SAVE",
    "category": [
      "App Search"
    ],
    "author": "Jonathan_Gautier",
    "date": "November 10, 2019, 2:14am November 15, 2019, 12:13pm November 15, 2019, 2:27pm November 15, 2019, 3:13pm December 13, 2019, 3:13pm",
    "body": "Hello, I got problem with AppSearch 7.4.2 ( Docker Version ). When i want to update Relevance Tuning Settings by click on SAVE button, i got 400 Bad Request. If somebody have an idea what is happening or Elastic Staff ? Thanks. URL Call: https://*****/as/engines/****/search_setting METHOD: PUT Content: {\"search_fields\":{\"description\":{\"weight\":1},\"title\":{\"weight\":1},\"picture\":{\"weight\":1},\"id\":{\"weight\":1}},\"result_fields\":{\"date\":{\"raw\":{}},\"price\":{\"raw\":{}},\"description\":{\"raw\":{}},\"id\":{\"raw\":{}},\"currency\":{\"raw\":{}},\"title\":{\"raw\":{}},\"picture\":{\"raw\":{}}},\"boosts\":{}}",
    "website_area": "discuss"
  },
  {
    "id": "967d14eb-782f-4be7-b68b-79c367d99bca",
    "url": "https://discuss.elastic.co/t/analytics-on-self-hosted-instance/205641",
    "title": "Analytics on self-hosted instance",
    "category": [
      "App Search"
    ],
    "author": "p_ban",
    "date": "October 29, 2019, 10:37am October 29, 2019, 10:43am October 29, 2019, 11:05am October 29, 2019, 11:19am October 29, 2019, 11:22am November 6, 2019, 1:35pm November 7, 2019, 1:06pm November 9, 2019, 6:59am November 12, 2019, 7:50am December 10, 2019, 7:50am",
    "body": "Hello, How can I start using Analytics with self-hosted App Search instance (the tab has no data, everything is 0)? I am performing search requests using php client with private-key. Do I need to add something to my requests?",
    "website_area": "discuss"
  },
  {
    "id": "63798d26-6c47-4fac-aea0-9ce168c634b5",
    "url": "https://discuss.elastic.co/t/api-logs-view-request-details-very-slow/207427",
    "title": "[API Logs] View Request details very slow",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "November 12, 2019, 1:42am December 10, 2019, 1:42am",
    "body": "Dear Team, Current i'm using App Search self-managed 7.4.2. I am having problems viewing logs details : View Request details very slow Please check for me thanks",
    "website_area": "discuss"
  },
  {
    "id": "8eccbd4f-831e-423f-abbc-077b4dc5183a",
    "url": "https://discuss.elastic.co/t/credential-search-limiting/206971",
    "title": "Credential Search Limiting",
    "category": [
      "App Search"
    ],
    "author": "Jonathan_Gautier",
    "date": "November 7, 2019, 2:23pm November 7, 2019, 3:32pm November 7, 2019, 3:32pm December 5, 2019, 3:32pm",
    "body": "Hello, I want to know if is possible to create now or in the future credential for searching, where we can limit value user can search and retrieve. Example, I got a Document with this schema: { title: string, description: string, country: string } I want user can just search title and description and retrieve it. And another user with another credential search can search and retrieve all values. Tell me if this can be possible. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "65c63c51-609d-4c98-9378-095d98d794b2",
    "url": "https://discuss.elastic.co/t/app-search-trying-to-authenticate-using-admin-cookie-even-though-a-search-token-is-provided/206755",
    "title": "App search trying to authenticate using admin cookie even though a search token is provided",
    "category": [
      "App Search"
    ],
    "author": "Max_Akn",
    "date": "November 6, 2019, 9:03am November 6, 2019, 12:12pm November 6, 2019, 1:16pm November 6, 2019, 1:21pm December 4, 2019, 1:21pm",
    "body": "When accessing both the admin and the application using app search within the same browser, app search always tries to authenticate a request via the admin cookie, even when the search token is provided. So when the cookie is expired, you see an http auth dialog popping when doing a search. This issue was already discussed here Failed authentication after period of time",
    "website_area": "discuss"
  },
  {
    "id": "fcb139ca-658b-4a9e-97c0-edffd8b86156",
    "url": "https://discuss.elastic.co/t/auto-refetch-refresh-result-on-clearing-search-box-text/206233",
    "title": "Auto Refetch/refresh result on clearing search box text",
    "category": [
      "App Search"
    ],
    "author": "Swapnil_Ghorpade",
    "date": "November 2, 2019, 10:02am November 4, 2019, 7:09pm November 5, 2019, 6:30am November 5, 2019, 1:56pm November 6, 2019, 11:02am December 4, 2019, 11:02am",
    "body": "Result is not get automatically refresh or fetched on clearing entered text from search box ? Its showing old result. Need to hit search button or enter.",
    "website_area": "discuss"
  },
  {
    "id": "f95d3798-50ed-47a5-b966-4ff2a188de25",
    "url": "https://discuss.elastic.co/t/bug-app-search-dashboard-fails-to-load-after-restarting-the-server/206237",
    "title": "[Bug?] App Search Dashboard Fails To Load After Restarting The Server",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "November 2, 2019, 1:26pm November 2, 2019, 5:10pm November 5, 2019, 6:23pm December 3, 2019, 6:24pm",
    "body": "I'm trying to install app search on ubuntu 18.04 and was able to successfully setup. Here's an overview of my setup: Installed Elasticsearch and App Search 7.4.2 via apt Added xpack.security.enabled: true to elasticsearch.yml Generated automatic passwords by running bin/elasticsearch-setup-passwords auto and noted down the passwords. Added the following to app-search.yml: allow_es_settings_modification: true app_search.auth.source: standard elasticsearch.username: elastic elasticsearch.password: <password generated in step #3> Ran app-search by running bin/app-search I then noted down the username and default password generated username: app-search@example.com password: hnrxxam9khgtq38u I was able to login to the app-search console. I however then stopped the app-search by pressing CTRL+C and tried to restart with by running bin/app-search. The server did start; but now I could not get to the dashboard again. The console output was as follows, while the chrome got into endless waiting for page state - **app-server.1 |** [2019-11-02T13:20:08.019+00:00][6800][2268][app-server][INFO]: [c1d9ba72-f40e-4925-b5f0-56c1e9744054] Started GET \"/\" for 49.36.27.83 at 2019-11-02 13:20:08 +0000 **app-server.1 |** [2019-11-02T13:20:08.044+00:00][6800][2268][action_controller][INFO]: [c1d9ba72-f40e-4925-b5f0-56c1e9744054] Processing by LocoTogo::HomeController#index as HTML **app-server.1 |** [2019-11-02T13:20:08.047+00:00][6800][2268][action_controller][INFO]: [c1d9ba72-f40e-4925-b5f0-56c1e9744054] Parameters: {\"host\"=>\"139.59.11.80:3002\", \"protocol\"=>\"http\"} **app-server.1 |** [2019-11-02T13:20:08.164+00:00][6800][2268][action_controller][INFO]: [c1d9ba72-f40e-4925-b5f0-56c1e9744054] Redirected to http://139.59.11.80:3002/login **app-server.1 |** [2019-11-02T13:20:08.180+00:00][6800][2268][action_controller][INFO]: [c1d9ba72-f40e-4925-b5f0-56c1e9744054] Completed 302 Found in 130ms **app-server.1 |** [2019-11-02T13:20:08.234+00:00][6800][2250][app-server][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Started GET \"/login\" for 49.36.27.83 at 2019-11-02 13:20:08 +0000 **app-server.1 |** [2019-11-02T13:20:08.245+00:00][6800][2250][action_controller][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Processing by LocoTogo::SessionsController#login as HTML **app-server.1 |** [2019-11-02T13:20:08.248+00:00][6800][2250][action_controller][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Parameters: {\"host\"=>\"139.59.11.80:3002\", \"protocol\"=>\"http\"} **app-server.1 |** [2019-11-02T13:20:08.295+00:00][6800][2250][action_view][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Rendered eui_icons/_close.html (0.4ms) **app-server.1 |** [2019-11-02T13:20:08.306+00:00][6800][2250][action_view][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Rendered eui_icons/_lock.html (0.9ms) **app-server.1 |** [2019-11-02T13:20:08.317+00:00][6800][2250][action_view][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Rendered loco_togo/app/views/loco_togo/sessions/login.html.rb (47.4ms) **app-server.1 |** [2019-11-02T13:20:08.320+00:00][6800][2250][action_controller][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Completed 200 OK in 71ms (Views: 52.2ms) I can confirm that I've repeatedly reproduced this issue. Can someone from Elastic team confirm that this is an issue? Or is there something wrong with my configuration?",
    "website_area": "discuss"
  },
  {
    "id": "17ad41e5-e0f6-44bb-9a34-42147c948088",
    "url": "https://discuss.elastic.co/t/please-support-app-search-api-client-c/203946",
    "title": "Please support App Search Api Client (C#)",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "October 17, 2019, 12:21am October 22, 2019, 7:22pm November 3, 2019, 12:43pm December 1, 2019, 12:43pm",
    "body": "Dear Team, Please support App Search Api Client for C# (dotnet FW or .Net Core). Thanks",
    "website_area": "discuss"
  },
  {
    "id": "2e39aafd-61b9-45aa-9212-2c30e2efe07c",
    "url": "https://discuss.elastic.co/t/please-support-result-settings-for-appsearch-self-managed/206239",
    "title": "Please support Result Settings for AppSearch self-managed",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "November 2, 2019, 2:57pm November 30, 2019, 2:41pm",
    "body": "Dear Team, Current, i'm using AppSearch self-managed version 7.4.2. I don't see the Search Settings=>Result Settings feature of this version. 2019-11-02_21-50-32.jpg38401806 552 KB 2019-11-02_21-54-58.jpg38401806 339 KB Please support Result Settings for next version. Thanks Team",
    "website_area": "discuss"
  },
  {
    "id": "d5aff3c7-150b-4c2c-a5ae-90641f1ee1a2",
    "url": "https://discuss.elastic.co/t/set-appsearch-to-use-an-existing-index/205924",
    "title": "Set AppSearch to use an existing index",
    "category": [
      "App Search"
    ],
    "author": "Brian_B",
    "date": "October 31, 2019, 12:54pm November 1, 2019, 10:05am November 2, 2019, 1:16pm November 2, 2019, 1:16pm",
    "body": "We have an existing ES index and now want to self-host AppSearch to manage it, but I can't find a way to tell AppSearch to use our existing index. It just creates a new index using a random Guid in the name and forces us to work with that index. According to another post here, someone suggested this isn't possible. So AppSearch requires that you dump your existing index and create a new one from scratch?? How does that make any sense? I can't believe that is the reality. It makes self-hosted AppSearch more of a learning tool than something you could actually use in production. Here is the other post: Elastic App Search using Elastic Search Index App Search Hi Is there any way possible to map my current elastic search documents to App search instead of writing to App search documents ? Details: We have existing elastic search index with lots of documents in it. I would like to use this index to build Elastic App Search. Please advise. Thanks in Advance Srini.",
    "website_area": "discuss"
  },
  {
    "id": "f2e8d7c9-523c-4a75-87a9-15b7bd3c1dd2",
    "url": "https://discuss.elastic.co/t/where-exactly-is-the-app-search-password-is-printed/206200",
    "title": "Where exactly is the app-search password is printed?",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "November 1, 2019, 5:58pm November 2, 2019, 10:52am November 30, 2019, 10:52am",
    "body": "The documentation says that it's printed in the console. Is it the \"console\" in Chrome's inspector or some other place? Can someone help me locate it, please?",
    "website_area": "discuss"
  },
  {
    "id": "ff91587e-51b0-4e7d-ac24-df8e9e52a216",
    "url": "https://discuss.elastic.co/t/how-to-exclude-facets-in-filters-or-get-conditional-filters-conditionalfacets/206044",
    "title": "How to exclude facets in filters or get conditional filters/conditionalFacets",
    "category": [
      "App Search"
    ],
    "author": "Swapnil_Ghorpade",
    "date": "October 31, 2019, 1:37pm October 31, 2019, 5:05pm November 1, 2019, 6:01am November 1, 2019, 10:35am November 1, 2019, 11:14am November 1, 2019, 11:30am November 1, 2019, 11:49am November 1, 2019, 12:15pm November 1, 2019, 12:43pm November 1, 2019, 1:30pm November 1, 2019, 1:36pm November 2, 2019, 9:41am November 30, 2019, 9:41am",
    "body": "App structure Engine : national-parks-example schema: [title, id, state, description]. Facet shows filters based on state field values. I have following scenario There are different users who can access documents from engine. User's will be able to access documents excluding some states (excluding restricted states for each type of users) . Show respected state facet/filters to users Example : Consider there are 2 users User-1 : restricted states ['California'] User-2 : restricted states ['Texas', 'Florida'] * If User-1 logged in he can see all states documents except 'California' * Facets will show all states in filters except 'California' * For User-2 : show documents excluding state 'Texas' and 'Florida' * Facets will show all states in filters except 'Texas', 'Florida'",
    "website_area": "discuss"
  },
  {
    "id": "a31b1f25-d692-4ec9-a762-99f62e99e09d",
    "url": "https://discuss.elastic.co/t/did-i-configure-app-search-yml-right/206138",
    "title": "Did I configure app-search.yml right?",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "November 1, 2019, 12:00pm November 29, 2019, 9:56am",
    "body": "I've successfully installed app-search (self-managed) on Ubuntu 18.04. I'm however not sure if I've configured it right. Here are the entries I've modified in my app-search.yml file: - allow_es_settings_modification: true app_search.external_url: http://<MY-SERVER-IP>:3002 app_search.listen_host: <MY-SERVER-IP> I can now access app-search from the http://<MY-SERVER-IP>:3002 and it seems to be working as expected. I'm however wondering if above settings are correct. Should the app_search.listen_host` be set to 127.0.0.1 OR my server IP? Is there any other setting I should pay attention to when going live with production server? Thank you in advance. Addendum: Upon restarting the app-search server; I'm unable to load pages. App Search throws - Completed 401 Unauthorized in 68ms I'm running with default settings and have not set any credentials to access app-search. What exactly is going wrong?",
    "website_area": "discuss"
  },
  {
    "id": "7b215c3e-0ca1-4132-9954-29691ae664b9",
    "url": "https://discuss.elastic.co/t/how-to-display-json-keys-in-search-ui-query-response/205772",
    "title": "How to display json keys in Search UI query response",
    "category": [
      "App Search"
    ],
    "author": "cerro",
    "date": "October 30, 2019, 12:33pm October 31, 2019, 5:17pm November 1, 2019, 11:14am November 29, 2019, 11:17am",
    "body": "I have json docs that contain one field \"content\" (and ID) with many nested json key:value objects. In addition, there can be many of the same keys int the document. An example would look something like this: { \"content\": [ {\"url\": \"https://www.xxx.com\" {\"title\": \"doc title\"}, {\"person\": \"person1\"}, {\"city\": \"city1\" {\"person\": \"person2\"}, {\"person\": \"person3\"} ]} I can index and search the documents. However, I want to display the URL, Title, etc., as the query response. I see how to do this if they are fields in a simpler structure, but unclear how to display these based on the example. Currently, I get the \"content\" with the key:value pair based on the search term. i.e. \"content\": {\"title\":\"doc title\"} \"id\": \"doc-12233444\" or \"content\": {\"url\":\"https://www.xxx.com\"} \"id\": \"doc-12233444\" This is how the field in the above example is used in Kibana: {\"field\": \"content.title.keyword\"} I'm a nube at Elastic App Search. Any help would be greatly appreciated. Thanks in advance!",
    "website_area": "discuss"
  },
  {
    "id": "6509950f-1dac-405c-b05e-9442609d8baf",
    "url": "https://discuss.elastic.co/t/building-a-website-search-engine-with-appsearch/205962",
    "title": "Building a website search engine with Appsearch",
    "category": [
      "App Search"
    ],
    "author": "bpamiri",
    "date": "October 31, 2019, 12:22am October 31, 2019, 5:21pm November 28, 2019, 5:21pm",
    "body": "Any thought of open sourcing the crawler from site search. Id like to create a website search engine for a vertical market and see being able to use app search for the indexing and searching but need a solution for the crawling.",
    "website_area": "discuss"
  },
  {
    "id": "c8ae26c6-51a7-4849-9cc3-d382d3a9e41c",
    "url": "https://discuss.elastic.co/t/open-source-versus-basic-plan/205949",
    "title": "Open Source versus Basic Plan?",
    "category": [
      "App Search"
    ],
    "author": "Brian_B",
    "date": "October 30, 2019, 9:19pm October 30, 2019, 9:45pm October 30, 2019, 9:56pm October 30, 2019, 11:41pm November 27, 2019, 11:28pm",
    "body": "According to the subscriptions page, there are two different free downloads: Open Source and Basic. The Basic version appears to have more features. But when I click on the links for the two options they both redirect you to the same download page, thus making it seem that they are actually identical. Are there two different downloads are are they both the same product? Here is the page: https://www.elastic.co/subscriptions",
    "website_area": "discuss"
  },
  {
    "id": "12b72231-85f8-49da-adde-73a793c94ec2",
    "url": "https://discuss.elastic.co/t/specify-the-index-for-app-search-to-use/205927",
    "title": "Specify the index for App Search to use",
    "category": [
      "App Search"
    ],
    "author": "Brian_B",
    "date": "October 30, 2019, 6:02pm October 30, 2019, 9:52pm November 27, 2019, 9:45pm",
    "body": "We have an existing ES index and now want to self-host AppSearch to manage it, but I can't find a way to tell AppSearch to use our existing index. It just creates a new index using a random Guid in the name and forces us to work with that index. According to another post here, someone suggested this isn't possible. So AppSearch requires that you dump your existing index and create a new one from scratch?? How does that make any sense? I can't believe that is the reality. It makes self-hosted AppSearch more of a learning tool than something you could actually use in production. Here is the other post: Elastic App Search using Elastic Search Index App Search Hi Is there any way possible to map my current elastic search documents to App search instead of writing to App search documents ? Details: We have existing elastic search index with lots of documents in it. I would like to use this index to build Elastic App Search. Please advise. Thanks in Advance Srini.",
    "website_area": "discuss"
  },
  {
    "id": "dca23c46-8595-467b-b946-47a81100b9ac",
    "url": "https://discuss.elastic.co/t/search-for-2-key-words-in-a-single-query/205679",
    "title": "Search for 2 key words in a single query",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 29, 2019, 1:58pm October 30, 2019, 3:58pm November 27, 2019, 3:58pm",
    "body": "Is there any way to search for 2 key words in a single query. Eg: [ { id:1 name:Jim, description:cool guy, employer:Delta Dental, address: Okemos }, { id:2, name:Jim, description: grumpy guy, employer: FedEx, address:Memphis }, { id:3, name:Jim, description: funny guy, employer: Delta Dental, address: Okemos }, id:4, name:Jim, description: funny guy, employer: Delta Dental, address: Minnesota } ] I wanna search for user Jim, who is a cool guy, who works for DeltaDental and lives in Okemos?? I am thinking the query should be Query:\"Jim\" \"filters\": { \"all\": [ { description:\"cool guy\" }, { employer: Delta Dental }, { address: Okemos } ] } Is the above query correct?? or is there any other efficient way ? sometimes the query could be search for user who is a cool guy, who works for DeltaDental and lives in Okemos?? In this case there is not user name specified. so what do I do now? Is there a way for the below query?? Query:\"Jim\",\"cool guy\" \"filters\": { \"all\": [ { employer: Delta Dental }, { address: Okemos } ] }",
    "website_area": "discuss"
  },
  {
    "id": "37debb74-4ff3-4e62-936b-06dd1e6617ce",
    "url": "https://discuss.elastic.co/t/wildcard-in-query-for-app-search/205697",
    "title": "Wildcard in query for app-search?",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 29, 2019, 2:59pm October 30, 2019, 3:53pm November 27, 2019, 11:42am",
    "body": "Is there a way to put wildcard in query field of app-search?? Like \"query\": \"[%]\"",
    "website_area": "discuss"
  },
  {
    "id": "50d56d3e-f8c9-432c-ab3f-50e6f75ae009",
    "url": "https://discuss.elastic.co/t/how-to-implement-proximity-search-in-app-search/203966",
    "title": "How to implement proximity search in app search",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "October 17, 2019, 4:31am October 22, 2019, 6:52am October 22, 2019, 11:36am October 22, 2019, 11:40am October 28, 2019, 8:41am October 28, 2019, 10:28am October 28, 2019, 10:30am October 28, 2019, 10:32am October 28, 2019, 10:36am October 28, 2019, 10:39am October 28, 2019, 11:12am November 25, 2019, 11:12am",
    "body": "How to implement proximity search in app search?",
    "website_area": "discuss"
  },
  {
    "id": "f47ee51d-fe97-4d3c-8f16-fd00280d341b",
    "url": "https://discuss.elastic.co/t/bulkindexing/205325",
    "title": "BulkIndexing",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 25, 2019, 8:36pm October 25, 2019, 8:41pm November 22, 2019, 8:42pm",
    "body": "Is there a way for bulk indexing in app-search similar to Elastic search??",
    "website_area": "discuss"
  },
  {
    "id": "c71694f0-83db-48c9-8a3d-8625eaae6713",
    "url": "https://discuss.elastic.co/t/good-visualisation-tools-to-manage-searches-in-website/204430",
    "title": "Good visualisation tools to manage searches in website",
    "category": [
      "App Search"
    ],
    "author": "jamartins",
    "date": "October 22, 2019, 10:39pm October 22, 2019, 10:38pm October 25, 2019, 11:49am November 22, 2019, 11:50am",
    "body": "Hi all! I've been looking for made tools, or ideas to develop our own, to help me visualize the searches our clients do in our website, their evolution over time and their rate of success (for example, successive queries with high similarity are probably not a good sign), but haven't found anything that works at that business intelligence level, only visualisations for infrastructural information like health of the nodes and such. I'd really appreciate any help or suggestions on interfaces that can help us in managing the distribution and discovery of our products through our search bar. I've checked Kibana, dejavu, elasticHQ, but they all seem to be made with the technical maintainer of the infrastructure in mind, and not to verify how well are people using the search bar. Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "8db571b0-a23c-4c3c-9947-bd5536c6fc06",
    "url": "https://discuss.elastic.co/t/synonym-set-limit/204050",
    "title": "Synonym Set Limit?",
    "category": [
      "App Search"
    ],
    "author": "jgrams",
    "date": "October 17, 2019, 12:22pm October 22, 2019, 7:47pm October 22, 2019, 7:48pm October 23, 2019, 11:31am November 20, 2019, 11:31am",
    "body": "Hello, I work for an e-commerce company, and we are looking to migrate our product search to App Search. In our current product search application, we have over 700 synonym sets. I began copying these synonym sets to App Search, but once I reached 256 sets, I could not add another one. Trying to add another set results in the message: Engine has the maximum number of synonyms, no more can be added Is there any way to get past this? A configurable setting or some other workaround? Thank you for your help",
    "website_area": "discuss"
  },
  {
    "id": "1b973e5e-0489-4e66-b0c6-8cb619a17d99",
    "url": "https://discuss.elastic.co/t/dictionary-vs-map-datastructure-in-app-search/204141",
    "title": "Dictionary vs Map datastructure in app-search?",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 17, 2019, 11:30pm October 22, 2019, 7:43pm November 19, 2019, 7:43pm",
    "body": "{ \"address_id\":1 \"residential_address\": { \"current\": \"USA\", \"previous\": \"India\" }, \"office_address\": { \"current\": \"Okemos\", \"previous\": \"Bangalore\" }, \"id\": 1 } In the above mentioned data structure I have Map of address which internally holds map of current and previous address. What is the efficient way to index this data structure in app-search?? can I use dictionary to efficiently save the structure(if so can anyone tell me how)?",
    "website_area": "discuss"
  },
  {
    "id": "68ceb402-14bc-4abd-adac-f13f7791c447",
    "url": "https://discuss.elastic.co/t/how-to-get-the-intersection-of-the-search-results-of-a-multi-search-in-app-search/204154",
    "title": "How to get the intersection of the search results of a multi-search in App-search?",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 18, 2019, 2:29am October 22, 2019, 11:44am November 19, 2019, 11:57am",
    "body": "Query: \"queries\": [ { \"search_fields\": { \"fruit_name\": {} }, \"query\": \"apples\" }, { \"search_fields\": { \"type\": {} }, \"query\": \"gala\" } ] Result: [ { \"results\": [ { \"fruit_id\": { \"raw\": \"9\" }, \"fruit_name\": { \"raw\": \"apple\" }, \"type\": { \"raw\": \"gala\" } \"id\": { \"raw\": \"9\" } }, { \"fruit_id\": { \"raw\": \"1\" }, \"fruit_name\": { \"raw\": \"apple\" }, \"type\": { \"raw\": \"red\" } \"id\": { \"raw\": \"1\" } } ] }, { \"results\": [ { \"fruit_id\": { \"raw\": \"9\" }, \"fruit_name\": { \"raw\": \"apple\" }, \"type\": { \"raw\": \"gala\" } \"id\": { \"raw\": \"9\" } }, { \"fruit_id\": { \"raw\": \"10\" }, \"festival_name\": { \"raw\": \"festival\" }, \"type\": { \"raw\": \"gala\" } \"id\": { \"raw\": \"10\" } } ] } ] Is there any way to get the common result from both the result sets??",
    "website_area": "discuss"
  },
  {
    "id": "8f6b7fba-29b3-4bb2-aadc-08b57874dfc4",
    "url": "https://discuss.elastic.co/t/cant-connect-to-aws-elasticsearch-cluster/204296",
    "title": "Can't connect to aws elasticsearch cluster",
    "category": [
      "App Search"
    ],
    "author": "Ardy_Febriansyah",
    "date": "October 19, 2019, 1:51pm October 21, 2019, 8:37am October 21, 2019, 1:36pm October 21, 2019, 2:55pm November 18, 2019, 2:57pm",
    "body": "I tried to install self managed app search with my existing aws elasticsearch cluster. but i got problem on installation process. > $ docker run -ti \\ > -e elasticsearch.host=https://search-xxx.xxxx.ap-southeast-1.es.amazonaws.com \\ > -e secret_session_key=wkwk \\ > docker.elastic.co/app-search/app-search:7.3.0 i got error message {\"message\":\"Authorization header requires 'Credential' parameter. Authorization header requires 'Signature' parameter. Authorization header requires 'SignedHeaders' parameter. Authorization header requires existence of either a 'X-Amz-Date' or a 'Date' header. Authorization=Basic ZWxhc3RpYzpjaGFuZ2VtZQ==\"} i think this issue same as Prevent inclusion of Authorization header. App search send authorization header to aws while if authorization header present, aws need another header to process request. How to prevent app search include authorization header ?",
    "website_area": "discuss"
  },
  {
    "id": "ec264fe2-2ced-4ded-902c-8365118a72d4",
    "url": "https://discuss.elastic.co/t/where-app-search-data-stored/204320",
    "title": "Where app search data stored?",
    "category": [
      "App Search"
    ],
    "author": "Ardy_Febriansyah",
    "date": "October 19, 2019, 6:46pm October 21, 2019, 2:27pm October 21, 2019, 12:24pm November 18, 2019, 12:24pm",
    "body": "Where app search data stored ? and is safe run self managed app search on stateless deployment on kubernetes ?",
    "website_area": "discuss"
  },
  {
    "id": "6394f690-35fc-4fa9-b080-5f030be53867",
    "url": "https://discuss.elastic.co/t/unable-to-access-app-search-on-remote-server/204310",
    "title": "Unable to access app search on remote server",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "October 19, 2019, 12:36pm October 21, 2019, 8:42am October 21, 2019, 8:53am October 21, 2019, 9:01am November 18, 2019, 9:15am",
    "body": "I've successfully installed the app-search on Ubuntu 18.04 and tweaked the app-search.yml to include: app_search.external_url: http://my-domain.com/search-dashboard The page however is captured by the framework running on the front end; and not by app-search. I'm running out of ideas on how to make it work. Would really appreciate your help in fixing this issue.",
    "website_area": "discuss"
  },
  {
    "id": "3a1e38dc-333c-4aff-9ff1-3ab96cb10be7",
    "url": "https://discuss.elastic.co/t/delete-the-schema-fileds-from-app-search/203954",
    "title": "Delete the schema fileds from app-search",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 17, 2019, 2:20am October 17, 2019, 9:15am November 14, 2019, 9:15am",
    "body": "Hi , I am a new user to app-search while I was playing with app-search I updated some documents and later I have decided to change the fields from the documents, but I have realized that the old fields still exist in the schema. Is there any way to delete the old fields from the schema?",
    "website_area": "discuss"
  },
  {
    "id": "076ff29b-a90e-4dc7-875f-444963fee5f2",
    "url": "https://discuss.elastic.co/t/app-search-create-new-engine-problem/202855",
    "title": "App search: create new engine problem",
    "category": [
      "App Search"
    ],
    "author": "JyL",
    "date": "October 9, 2019, 2:43pm October 9, 2019, 2:46pm October 9, 2019, 2:49pm October 9, 2019, 2:50pm October 9, 2019, 2:52pm October 9, 2019, 3:37pm October 10, 2019, 12:50pm November 7, 2019, 12:50pm",
    "body": "Hey guys, need help with a create new engine problem on app search. There is no engine with the same name that I am willing to create. But I did have an engine with the same name previous, so now when I create new engine with this name, it shows an error \" Name cannot be reused: xxxxxxx\". Could anyone have any idea of that?",
    "website_area": "discuss"
  },
  {
    "id": "a06b80f7-ab81-48f7-ae1c-3c21d41cc4eb",
    "url": "https://discuss.elastic.co/t/combine-join-search-results/202373",
    "title": "Combine/join search results",
    "category": [
      "App Search"
    ],
    "author": "JyL",
    "date": "October 4, 2019, 6:37pm October 9, 2019, 3:38pm October 9, 2019, 6:50pm November 6, 2019, 6:51pm",
    "body": "Need help: want to find a way to join search results we get from multi_search api. I want to have multiple queries, each of them related with a different search field, want to join the result. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "9273a076-4d75-44b7-8e74-00bce279116e",
    "url": "https://discuss.elastic.co/t/deleting-app-search-engines/202848",
    "title": "Deleting app-search engines",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 9, 2019, 2:30pm October 9, 2019, 2:44pm October 9, 2019, 2:47pm October 9, 2019, 2:53pm October 9, 2019, 3:37pm",
    "body": "Hey All, I am trying to delete an app-search engine and trying to create another engine with the same name as the deleted engine but I am getting the error as 'name cannot be reused'. Can anyone tell me why is this occurring and what is the solution.",
    "website_area": "discuss"
  },
  {
    "id": "0b8e1ab2-5e85-4655-bc80-411d11a623a7",
    "url": "https://discuss.elastic.co/t/tips-to-increase-query-performance/202304",
    "title": "Tips to increase query performance",
    "category": [
      "App Search"
    ],
    "author": "jarnor",
    "date": "October 4, 2019, 8:35am October 4, 2019, 3:26pm October 7, 2019, 8:08am November 4, 2019, 8:08am",
    "body": "Hi! I'm wondering if you have any tips to increase the query performance of App Search. We've an index with ~10 million documents which have quite poor query performance. Simple queries can take between 4-5 seconds and paging between the documents in the App Search dashboard (/documents) takes >2 seconds. This can be compared to ~300 ms (for paging in the dashboard) for another index which only have ~300k documents, but identical schema/data. The query performance barley changed when moving from a 8 GB RAM to a 58 GB RAM cluster in Elastic Cloud. I'm quite green on elasticsearch in general, but noticed that the index .app-search-engine-xxxxxx (the one powering the queries I assume) only is divided between two shards? Could this be related to the poor performance? Is there any possibility to increase the shard count? Best, Johan",
    "website_area": "discuss"
  },
  {
    "id": "fcd8aba9-f8e3-4fb8-9407-d3b4aba81446",
    "url": "https://discuss.elastic.co/t/appsearch-language-support-by-es-plugins/202389",
    "title": "Appsearch language support by ES plugins",
    "category": [
      "App Search"
    ],
    "author": "mrcallocu",
    "date": "October 4, 2019, 8:52pm October 4, 2019, 9:00pm October 4, 2019, 8:56pm November 1, 2019, 8:56pm",
    "body": "Hello, My language is not supported by default by appserach, however I would like to use one of the analysis plugins available for Elasticserach (https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-stempel.html). How can I use it with appsearch? Is it possible to use custom analyzers, tokenizers, token filters for appsearch index in elasticsearch",
    "website_area": "discuss"
  },
  {
    "id": "12b050d3-404b-4872-8ffb-bd2783e2bfcb",
    "url": "https://discuss.elastic.co/t/searching-by-ranked-information/201422",
    "title": "Searching by Ranked information",
    "category": [
      "App Search"
    ],
    "author": "David_Williams1",
    "date": "September 27, 2019, 3:05pm October 2, 2019, 4:28pm October 2, 2019, 6:50pm October 4, 2019, 11:45am November 1, 2019, 11:45am",
    "body": "Elastic has the ability to do a ranked search but I cannot see that I can store the information in the document and then return the documents based on the rank. This an example of how I would store the information in a field (but I am open to anything) but I'm not sure if/how to query the engine to sort based on Topic A's rank. [{\"Topic A\":.5}, (\"Topic B\":.855}, {\"Topic C\":.255}, {\"Topic D\":.641}, {\"Topic E\":.500}, {\"Topic F\":.541}, {\"Topic G\":.523}] Has anyone done this or something similar? https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-rank-feature-query.html",
    "website_area": "discuss"
  },
  {
    "id": "eab3c739-f598-4639-bbe0-c2f75975d6e7",
    "url": "https://discuss.elastic.co/t/self-managed-app-search-docker-7-4-0-e-worker-threads-do-not-work/201941",
    "title": "Self-managed app search docker 7.4.0, -e worker.threads do not work",
    "category": [
      "App Search"
    ],
    "author": "axelhildingson",
    "date": "October 2, 2019, 11:08am October 2, 2019, 3:15pm October 30, 2019, 3:15pm",
    "body": "I am running the app search docker and trying to change the worker pool but it nothing happens when I set -e worker.threads=8. When I look at port 3003 is the worker count still 4. docker run -ti -p 3003:3003 -p 3002:3002 -e elasticsearch.host=<host> -e elasticsearch.password=<password> -e elasticsearch.username=<username> -e worker.threads=8 docker.elastic.co/app-search/app-search:7.4.0",
    "website_area": "discuss"
  },
  {
    "id": "537d5f95-bbd5-4ddc-bb5a-d6086537ba8d",
    "url": "https://discuss.elastic.co/t/is-there-any-way-to-implement-document-level-security-in-app-search/201154",
    "title": "Is there any way to implement document level security in app search?",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "September 26, 2019, 6:37am September 27, 2019, 12:33pm September 29, 2019, 5:27am September 30, 2019, 11:59am October 28, 2019, 11:59am",
    "body": "Hi Team, We need to implement security trimming on search result. Presently we are using App search and Search UI to show result. Can we implement document level security in app search ?",
    "website_area": "discuss"
  },
  {
    "id": "4795472d-5f3f-4cbc-b86f-69302371459d",
    "url": "https://discuss.elastic.co/t/reference-ui-how-to-modify-whats-displayed-react-newbie/199471",
    "title": "Reference UI - how to modify what's displayed (React newbie)",
    "category": [
      "App Search"
    ],
    "author": "Per_Burstrom",
    "date": "September 13, 2019, 6:24pm September 13, 2019, 7:14pm September 13, 2019, 8:11pm September 17, 2019, 9:26am September 17, 2019, 1:28pm September 17, 2019, 2:17pm September 17, 2019, 3:29pm September 18, 2019, 2:30am September 18, 2019, 12:49pm September 18, 2019, 1:16pm September 18, 2019, 3:20pm September 21, 2019, 3:29pm September 23, 2019, 2:12pm September 28, 2019, 5:50pm October 26, 2019, 5:50pm",
    "body": "Hi, I'm currently evaluating Elastic App Search. I have written a script that inserts that data, and then I generated and downloaded the Reference UI React code to query the data from my site. It works great, but I would like to get control over the output. For example one of the fields outputted in plain text is an image link I would like to use to actually show the image. I have spent the whole day trying to figure out how to modify the App.js code to be able to do that, but I have now given up. Thing is that I'm totally new to React as well. bodyContent={ } So I would like to insert an tag which retrieve the url from one of the fields in the data I got from Elastic Search App. Any ideas? Thanks! /Per",
    "website_area": "discuss"
  },
  {
    "id": "5da0d9b2-18d4-46a6-b99b-c7695525010b",
    "url": "https://discuss.elastic.co/t/document-size-limit/198289",
    "title": "Document size limit",
    "category": [
      "App Search"
    ],
    "author": "KBuev",
    "date": "September 5, 2019, 4:07pm September 5, 2019, 4:14pm September 27, 2019, 9:54am October 25, 2019, 9:54am",
    "body": "Hi, all. Is there any way to get around the 102400 bytes document size limit in self-managed App Search? We're trying to use App Search to build a search solution for an internal knowledge base and the default limit is just way too low.",
    "website_area": "discuss"
  },
  {
    "id": "5349a950-86be-44c2-a466-3d0317fd56cf",
    "url": "https://discuss.elastic.co/t/pass-additional-filters-with-search-text-from-search-ui-to-app-search/199647",
    "title": "Pass additional filters with search text from Search UI to App search",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "September 16, 2019, 11:26am September 16, 2019, 11:46am September 16, 2019, 12:11pm September 17, 2019, 11:56am September 17, 2019, 4:28pm September 18, 2019, 5:57am September 18, 2019, 12:52pm September 18, 2019, 1:00pm September 18, 2019, 1:11pm September 18, 2019, 2:35pm September 19, 2019, 11:56am September 26, 2019, 6:38am September 26, 2019, 6:39am October 24, 2019, 6:40am",
    "body": "Is there any way to pass additional filters with search text from search UI to app search? We have advance filter screen, there user can select the several filters and hit the submit button for search result.",
    "website_area": "discuss"
  },
  {
    "id": "0faf5af3-2388-4fec-ac4a-cecb03a50b1b",
    "url": "https://discuss.elastic.co/t/deleting-all-documents-inside-an-engine/200634",
    "title": "Deleting all documents inside an engine",
    "category": [
      "App Search"
    ],
    "author": "rajithsam",
    "date": "September 23, 2019, 7:19am September 23, 2019, 3:35pm September 26, 2019, 5:55am September 26, 2019, 5:55am October 24, 2019, 5:55am",
    "body": "Is it possible to delete all the documents inside an engine at once?",
    "website_area": "discuss"
  },
  {
    "id": "82c980f2-d852-4157-9063-25b3d5b217fe",
    "url": "https://discuss.elastic.co/t/elastic-app-search-search-against-multiple-engines/200633",
    "title": "Elastic App Search - Search against multiple engines",
    "category": [
      "App Search"
    ],
    "author": "rajithsam",
    "date": "September 23, 2019, 7:18am September 26, 2019, 5:55am September 26, 2019, 5:55am October 24, 2019, 5:55am",
    "body": "Hi! We are having 3 engines inside app-search. We want to do a search against these 3 engines & show the results. How to achieve this?",
    "website_area": "discuss"
  },
  {
    "id": "0afcce40-5120-4b74-a500-534435d724c9",
    "url": "https://discuss.elastic.co/t/deploying-reference-ui-for-app-search/200729",
    "title": "Deploying reference UI for App Search",
    "category": [
      "App Search"
    ],
    "author": "mc1392",
    "date": "September 23, 2019, 4:51pm September 23, 2019, 4:57pm September 23, 2019, 7:20pm September 23, 2019, 7:39pm September 24, 2019, 2:26pm September 24, 2019, 1:18pm September 24, 2019, 2:27pm October 22, 2019, 2:27pm",
    "body": "Has anyone had success deploying the reference search UI for app search? I am a bit confused. Do I need to install Apache or can I just spin up a Ubuntu and deploy?",
    "website_area": "discuss"
  },
  {
    "id": "16da59db-c91a-49c2-b1f4-2104fdc48061",
    "url": "https://discuss.elastic.co/t/unable-to-index-documents/200442",
    "title": "Unable to index documents",
    "category": [
      "App Search"
    ],
    "author": "mc1392",
    "date": "September 20, 2019, 2:28pm September 20, 2019, 2:28pm September 20, 2019, 2:44pm October 18, 2019, 2:44pm",
    "body": "I'm using Java to push data into Swiftype App Search. The logs have 200 has the response, but when I investigate the error message is: Here is the request body: [ { \"fileExtension\": \"jpg\", \"id\": 21 } ] Here is the response in the logs: [ { \"id\": \"21\", \"errors\": [ \"Fields can only contain lowercase letters, numbers, and underscores: fileExtension.\" ] } ] I decided to send less fields, to help minimize the potential culprits. Just sending 2 fields, id and fileExtension, that data is all text, not funny characters or anything. Why am I getting this error when I am indexing simple text? This code here, I create Maps of data to send to Swiftype: public static <T> List<Map<String, Object>> buildData(List<T> items) { List<Map<String, Object>> data = new ArrayList<Map<String, Object>>(); for (Iterator<T> it = items.iterator(); it.hasNext();) { HashMap<String, Object> fieldMap = new HashMap<String, Object>(); Object o = it.next(); Field[] f = o.getClass().getDeclaredFields(); for (int i = 0; i < f.length; i++) { f[i].setAccessible(true); String name = f[i].getName(); System.out.println(name); Object value = null; try { value = f[i].get(o); } catch (IllegalArgumentException | IllegalAccessException e) { e.printStackTrace(); } ieldMap.put(name, value); } data.add(fieldMap); } return data; } And this code here, is how I actually send the data to Swiftype: Client client = new Client(host, privateKey); try { client.indexDocuments(engineName, data); } catch (ClientException e) { e.printStackTrace(); }",
    "website_area": "discuss"
  },
  {
    "id": "c6b8d60b-e999-4a77-b3f7-59fe48d7cd8d",
    "url": "https://discuss.elastic.co/t/inbuilt-connectors-available-to-interact-or-push-data-in-app-search/200390",
    "title": "Inbuilt connectors available to interact or push data in app search",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "September 20, 2019, 9:36am September 20, 2019, 12:46pm October 18, 2019, 12:58pm",
    "body": "Is there any inbuilt connectors available to interact or push data in app search using .net/.net core. In documentation, python,php and nodejs are supported.",
    "website_area": "discuss"
  },
  {
    "id": "cb3a6684-fa96-4129-8289-7cb30b2cef7f",
    "url": "https://discuss.elastic.co/t/indexation-of-many-documents-in-appsearch/200039",
    "title": "Indexation of many documents in Appsearch",
    "category": [
      "App Search"
    ],
    "author": "misnard",
    "date": "September 18, 2019, 2:53pm October 16, 2019, 2:51pm",
    "body": "Hello, I currently encounter problems when indexing documents in self hosted AppSearch, I have 30 million entries in my source and as the documentation indicates we can only insert documents in packets of 100, for each packet of 100 it takes me 3087ms via the api node.js Appsearch instead when I use elasticsearch directly without going through appsearch and I index documents it is much faster (I do bulk indexing by packet of 10 000). Do you have solutions to increase the speed of document indexing in appsearch? Thank you, misnard.",
    "website_area": "discuss"
  },
  {
    "id": "5e910ce8-d866-47e2-a73c-7d6afe102c0b",
    "url": "https://discuss.elastic.co/t/elastic-seach-is-not-accessiblle-on-host-machine/199932",
    "title": "Elastic seach is not accessiblle on host machine",
    "category": [
      "App Search"
    ],
    "author": "",
    "date": "September 18, 2019, 7:27am October 16, 2019, 7:27am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "4caa192c-61f3-45eb-8811-aa797acbd9ae",
    "url": "https://discuss.elastic.co/t/ingest-meta-data-with-attachment-in-app-search/199656",
    "title": "Ingest meta data with attachment in App search",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "September 16, 2019, 12:14pm September 16, 2019, 5:09pm September 18, 2019, 7:07am October 16, 2019, 7:07am",
    "body": "hi Team In the elastic, we usually ingest the attachment in elasticsearch using attachment plugin(https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest-attachment.html) so we dont have to worry about the types of file we are trying to ingest. Typically, we need different types of library to read the content of different types of file(i.e work, excel,pdf,pptx). By using this plugin, we do not need any types of library to read the content. Can we use the same with app search?",
    "website_area": "discuss"
  },
  {
    "id": "0fb187b8-5988-4e5d-ba26-874c9732b49b",
    "url": "https://discuss.elastic.co/t/apply-fuzzy-search-in-app-search/199648",
    "title": "Apply fuzzy search in app search",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "September 16, 2019, 11:28am September 16, 2019, 11:37am September 16, 2019, 11:39am September 16, 2019, 11:46am October 14, 2019, 11:46am",
    "body": "Is there any way to enable and disable fuzzy search in app search?",
    "website_area": "discuss"
  },
  {
    "id": "6d89dfd7-3fb8-490a-a4c8-47ab606133b1",
    "url": "https://discuss.elastic.co/t/nameerror-uninitialized-constant-swiftype-loops/198253",
    "title": "NameError: uninitialized constant Swiftype::Loops",
    "category": [
      "App Search"
    ],
    "author": "kikil592",
    "date": "September 5, 2019, 1:02pm September 6, 2019, 12:08pm September 9, 2019, 7:31am September 10, 2019, 1:11pm September 12, 2019, 6:18pm October 10, 2019, 6:18pm",
    "body": "Hi there, I'm trying to index some documents with Elastic App Search. It is well connected with ElasticSearch but get the error: worker.1 | [2019-09-05T12:48:48.076+00:00][12003][2318][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [4a7070b0d56d1860a4ebec1c2551ce74f165041c] Performing Work::Engine::IndexAdder from EsqueuesMe(index_adder) with arguments: \"5d70d5accd7b75b751cd89bc\", [\"5d70d5c2cd7b75b751cd89c0\", \"5d70d5c2cd7b75b751cd89c1\"] worker.1 | [2019-09-05T12:48:48.149+00:00][12003][2314][rails][WARN]: Failed to claim job 4a7070b0d56d1860a4ebec1c2551ce74f165041c, claim conflict occurred worker.1 | [2019-09-05T12:48:48.300+00:00][12003][2318][rails][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [4a7070b0d56d1860a4ebec1c2551ce74f165041c] Adding document 5d70d5c2cd7b75b751cd89c0 to index for engine 5d70d5accd7b75b751cd89bc worker.1 | [2019-09-05T12:48:48.398+00:00][12003][2318][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [4a7070b0d56d1860a4ebec1c2551ce74f165041c] Performed Work::Engine::IndexAdder from EsqueuesMe(index_adder) in 315.18ms worker.1 | [2019-09-05T12:48:48.401+00:00][12003][2318][rails][ERROR]: Retrying Work::Engine::IndexAdder in 300 seconds, due to a StandardError. The original exception was #<NameError: uninitialized constant Swiftype::Loops>. worker.1 | [2019-09-05T12:48:48.406+00:00][12003][2318][rails][INFO]: [ActiveJob] Enqueueing a job into the '.app-search-esqueues-me_queue_v1_index_adder' index. {\"job_type\"=>\"ActiveJob::QueueAdapters::EsqueuesMeAdapter::JobWrapper\", \"payload\"=>{\"args\"=>[{\"job_class\"=>\"Work::Engine::IndexAdder\", \"job_id\"=>\"4a7070b0d56d1860a4ebec1c2551ce74f165041c\", \"queue_name\"=>\"index_adder\", \"arguments\"=>[\"5d70d5accd7b75b751cd89bc\", [\"5d70d5c2cd7b75b751cd89c0\", \"5d70d5c2cd7b75b751cd89c1\"]], \"locale\"=>:en, \"executions\"=>2}]}, \"status\"=>\"pending\", \"created_at\"=>1567687728405, \"perform_at\"=>1567688028403, \"attempts\"=>0} worker.1 | [2019-09-05T12:48:48.425+00:00][12003][2318][rails][INFO]: [ActiveJob] Ignoring duplicate job class=Work::Engine::IndexAdder, id=4a7070b0d56d1860a4ebec1c2551ce74f165041c, args=[\"5d70d5accd7b75b751cd89bc\", [\"5d70d5c2cd7b75b751cd89c0\", \"5d70d5c2cd7b75b751cd89c1\"]] worker.1 | [2019-09-05T12:48:48.428+00:00][12003][2318][active_job][INFO]: [ActiveJob] [2019-09-05 12:48:48 UTC] enqueued Work::Engine::IndexAdder job (4a7070b0d56d1860a4ebec1c2551ce74f165041c) on index_adder worker.1 | [2019-09-05T12:48:48.440+00:00][12003][2318][active_job][INFO]: [ActiveJob] Enqueued Work::Engine::IndexAdder (Job ID: 4a7070b0d56d1860a4ebec1c2551ce74f165041c) to EsqueuesMe(index_adder) at 2019-09-05 12:53:48 UTC with arguments: \"5d70d5accd7b75b751cd89bc\", [\"5d70d5c2cd7b75b751cd89c0\", \"5d70d5c2cd7b75b751cd89c1\"] worker.1 | [2019-09-05T12:48:48.444+00:00][12003][2318][rails][INFO]: Deleting: {:index=>\".app-search-esqueues-me_queue_v1_index_adder\", :type=>nil, :id=>\"4a7070b0d56d1860a4ebec1c2551ce74f165041c\"} Relevant line is: [rails][ERROR]: Retrying Work::Engine::IndexAdder in 300 seconds, due to a StandardError. The original exception was #<NameError: uninitialized constant Swiftype::Loops>. I've tried the 3 methods (json write, json load, curl) but same result What happen? ElasticSearch: 7.3.1 ElasticApp Search: 7.3.1 Java8 Ubuntu",
    "website_area": "discuss"
  },
  {
    "id": "fcb51db2-81ca-45c2-a371-e76ef95f3de7",
    "url": "https://discuss.elastic.co/t/question-multiple-fields-search-separate-searches-in-single-query/198730",
    "title": "Question: multiple fields search, separate searches in single query",
    "category": [
      "App Search"
    ],
    "author": "JyL",
    "date": "September 9, 2019, 3:33pm September 9, 2019, 4:14pm September 9, 2019, 5:46pm September 9, 2019, 6:00pm September 9, 2019, 6:19pm September 12, 2019, 6:18pm October 10, 2019, 6:18pm",
    "body": "Hey guys, I am pretty new to elastic app search. I am facing problem with how to do an advanced search in different field within a query. So for example, I have an object Ojb, and it has multiple fields that I want to do individual searches on. Ojb(field1, field2, field3, field4). I want to have searchFields has all of the 4 fields that I would like to search, and in the query string, I want to have for example \"f1query, f2query, f3query, f4query\", and each of the query in this string will do search in corresponding fields, e.g. f1query search in field1. Those f1query / f2query/... could be null if I don't want to search anything on that field. For example the query might look like \"f1query,,,f4query\" which means I only want to search on the 1st and 4th fields. Could you guys give me some suggestion on how to do this? I really appreciate your help!!",
    "website_area": "discuss"
  },
  {
    "id": "cc6abd54-2be2-4d26-a1b3-f4420568ecea",
    "url": "https://discuss.elastic.co/t/elastic-app-search-using-elastic-search-index/196631",
    "title": "Elastic App Search using Elastic Search Index",
    "category": [
      "App Search"
    ],
    "author": "Srini12",
    "date": "August 24, 2019, 5:40pm August 24, 2019, 6:20pm August 29, 2019, 7:14pm August 29, 2019, 7:16pm September 9, 2019, 6:59pm October 7, 2019, 7:07pm",
    "body": "Hi Is there any way possible to map my current elastic search documents to App search instead of writing to App search documents ? Details: We have existing elastic search index with lots of documents in it. I would like to use this index to build Elastic App Search. Please advise. Thanks in Advance Srini.",
    "website_area": "discuss"
  },
  {
    "id": "4cdbaa6e-530b-4b38-9cef-83e5e549c5aa",
    "url": "https://discuss.elastic.co/t/app-search-limitations/196601",
    "title": "App Search Limitations",
    "category": [
      "App Search"
    ],
    "author": "j-rewerts",
    "date": "August 23, 2019, 10:39pm September 9, 2019, 4:21pm October 7, 2019, 4:37pm",
    "body": "Hi there! I've been evaluating App Search for use in indexing >10 Million documents. I've got a fairly basic self-hosted ES cluster, along with self-hosted app search. I've written several hundred thousand documents using App Search's API, but it only shows ~12k on the dashboard. Is it possible to self-host App Search at this scale? What limitations are there on App Search?",
    "website_area": "discuss"
  },
  {
    "id": "3a6267b8-f3c1-4a96-b68a-42f61ca155f5",
    "url": "https://discuss.elastic.co/t/issue-running-app-search/198075",
    "title": "Issue running App Search",
    "category": [
      "App Search"
    ],
    "author": "fcsfaria",
    "date": "September 4, 2019, 4:14pm September 9, 2019, 7:34am October 7, 2019, 7:34am",
    "body": "Hi, I can run App Search with success in port 80. However when configured another port in parameter \"app_search.listen_port\" I receive an connection error in browser. It does not look as a \"network/firewall\" issue because App server console show \"http get\" message. Should the redirect URL to \"/login\" to produce the problem ? app-server.1 | [2019-09-04T15:53:10.561+00:00][29448][2368][rails][INFO]: [b3a9c185-dd2c-446c-9357-97f40c82b160] Started GET \"/\" for yyy.yy.yyy.yyy at 2019-09-04 15:53:10 +0000 app-server.1 | [2019-09-04T15:53:10.584+00:00][29448][2368][action_controller][INFO]: [b3a9c185-dd2c-446c-9357-97f40c82b160] Processing by LocoTogo::HomeController#index as HTML app-server.1 | [2019-09-04T15:53:10.586+00:00][29448][2368][action_controller][INFO]: [b3a9c185-dd2c-446c-9357-97f40c82b160] Parameters: {\"host\"=>\"hostname\", \"protocol\"=>\"http\"} app-server.1 | [2019-09-04T15:53:10.643+00:00][29448][2368][action_controller][INFO]: [b3a9c185-dd2c-446c-9357-97f40c82b160] Redirected to http://hostname/login",
    "website_area": "discuss"
  },
  {
    "id": "718f28cb-2e2f-4191-b2ff-b31ba45d8627",
    "url": "https://discuss.elastic.co/t/support-for-openid-connect/197127",
    "title": "Support for OpenId Connect?",
    "category": [
      "App Search"
    ],
    "author": "Dave_Russell",
    "date": "August 28, 2019, 1:40pm August 28, 2019, 4:17pm August 29, 2019, 8:12am August 29, 2019, 5:30pm September 26, 2019, 5:30pm",
    "body": "ElasticSearch supports OpenId Connect as an authentication source. I believe this requires a GOLD licence, or a paid-for security plugin (Search Guard) ? If my ES instance is using OpenID Connect, can AppSearch use the same users ? Or does it require AppSearch specific logins to be created? There is a mention of SAML support, but no mention of Active Directory / OpenID etc, or if ES plugin-provided security would be supported.",
    "website_area": "discuss"
  },
  {
    "id": "4ee143da-879e-4e4e-9fe6-25c7577ad78c",
    "url": "https://discuss.elastic.co/t/openapi-swagger-api-docs/197071",
    "title": "OpenAPI / Swagger API Docs?",
    "category": [
      "App Search"
    ],
    "author": "Dave_Russell",
    "date": "August 28, 2019, 8:48am August 28, 2019, 12:43pm August 28, 2019, 1:35pm August 28, 2019, 1:54pm September 25, 2019, 1:48pm",
    "body": "I'd like to dig into AppSearch as a potential integration with our SaaS offering, but there is no \"back end\" documentation available? No clear list of available APIs No Open API / Swagger document. No public sourcecode repo Do you have plans to make the App Search API details available publically, so we can create / generate our own clients in our language of choice ?",
    "website_area": "discuss"
  },
  {
    "id": "4c09d622-7ded-4760-b797-a9023c686b4c",
    "url": "https://discuss.elastic.co/t/facets-broken-in-national-parks-demo/197078",
    "title": "Facets broken in national parks demo?",
    "category": [
      "App Search"
    ],
    "author": "Dave_Russell",
    "date": "August 28, 2019, 9:46am August 28, 2019, 1:36pm September 25, 2019, 12:35pm",
    "body": "https://parks.swiftype.info/ I don't get any facets returned - just json documents with clickable headings.",
    "website_area": "discuss"
  },
  {
    "id": "51c912c8-13ea-4162-96fc-503f9eb06c52",
    "url": "https://discuss.elastic.co/t/query-string-filter-get-all-record-using-elastic-search-version-1-7/193107",
    "title": "Query_string filter get all record using elastic search version 1.7",
    "category": [
      "App Search"
    ],
    "author": "yasir_abbas",
    "date": "July 31, 2019, 11:36am August 20, 2019, 6:35pm September 17, 2019, 6:35pm",
    "body": "I m trying to find user by nationality using query_string but it query return other nationality records. Can anyone help me out how query_string is working. curl -X GET 'http:/127.0.0.1:9200/user_alias/_search?size=1000&pretty' -d '{\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"query\":\"nationality:\"\"AF\"\"\"}}}},\"filter\":{\"and\":[{\"terms\":{\"room_id\":[\"591\"]}},{\"bool\":{\"must\":[{\"exists\":{\"field\":\"nationality\"}}]}},{\"bool\":{\"must\":[{\"exists\":{\"field\":\"user.id\"}}]}}]},\"size\":1000,\"fields\":[\"user.id\", \"nationality\"]}' For example Data returns : id : 1 nationality : FA id : 2 nationality : CA",
    "website_area": "discuss"
  },
  {
    "id": "25846ab1-3f48-40cb-8933-51fed16bd146",
    "url": "https://discuss.elastic.co/t/searching-filtering-by-enums-tags-categories/195500",
    "title": "Searching/filtering by enums/tags/categories?",
    "category": [
      "App Search"
    ],
    "author": "Andrew_Bennett",
    "date": "August 16, 2019, 1:31pm August 16, 2019, 2:22pm August 16, 2019, 4:35pm August 19, 2019, 9:01pm August 19, 2019, 9:04pm August 20, 2019, 6:21pm August 20, 2019, 6:33pm September 17, 2019, 6:33pm",
    "body": "Hi all. I've got a large set of food dishes. I'd like to tag each dish with things like 'vegetarian', 'vegan', 'gluten free', etc. and be able to search by these tags. I.e., if a user wants to search 'pasta' as their query but also filter by 'vegetarian and gluten free'. As there are an indefinite number of tags it isn't feasible to give each tag its own field. I'd much prefer to put them all in one 'tags' field, comma separated. However, app-search seems to only filter/boost by exact match. I would like instead to do something like \"If my tags field does not contain the term 'vegetarian', do not return this result.\" Any ideas here? Thank you",
    "website_area": "discuss"
  },
  {
    "id": "078de9a5-7f03-49d4-80ed-ccd26b65c855",
    "url": "https://discuss.elastic.co/t/failed-authentication-after-period-of-time/194101",
    "title": "Failed authentication after period of time",
    "category": [
      "App Search"
    ],
    "author": "ayan",
    "date": "August 6, 2019, 8:34pm August 6, 2019, 10:44pm August 7, 2019, 6:27pm August 8, 2019, 7:44am August 8, 2019, 3:59pm August 17, 2019, 1:03am September 14, 2019, 1:03am",
    "body": "I generated a react app from the reference UI, and I am using npm start to start the react app. The react app works at first, but after a few hours, when I type some text, I am asked for a username/password because authentication fails. However, when I go to the server that hosts app-search, and then I try to search something again from the react app, it works. After a few hours, the search fails again. Is there a setting that fixes this issue? error.PNG18721014 20 KB error2.PNG18501084 81.7 KB error3.PNG18741045 26.6 KB error4.PNG1596850 63.5 KB Config Files: # ======================== Elasticsearch Configuration ========================= ... # ---------------------------------- Cluster ----------------------------------- # # Use a descriptive name for your cluster: # #cluster.name: my-application # # ------------------------------------ Node ------------------------------------ # # Use a descriptive name for the node: # #node.name: node-1 # # Add custom attributes to the node: # #node.attr.rack: r1 # # ----------------------------------- Paths ------------------------------------ # # Path to directory where to store the data (separate multiple locations by comma): # path.data: /elastic/lib/elasticsearch # # Path to log files: # path.logs: /elastic/log/elasticsearch # # ----------------------------------- Memory ----------------------------------- # # Lock the memory on startup: # # bootstrap.memory_lock: true # # Make sure that the heap size is set to about half the memory available # on the system and that the owner of the process is allowed to use this # limit. # # Elasticsearch performs poorly when the system is swapping the memory. # # ---------------------------------- Network ----------------------------------- # # Set the bind address to a specific IP (IPv4 or IPv6): # #network.host: 192.168.0.1 # # Set a custom port for HTTP: # #http.port: 9200 # # For more information, consult the network module documentation. # # --------------------------------- Discovery ---------------------------------- # # Pass an initial list of hosts to perform discovery when this node is started: # The default list of hosts is [\"127.0.0.1\", \"[::1]\"] # #discovery.seed_hosts: [\"host1\", \"host2\"] # # Bootstrap the cluster using an initial set of master-eligible nodes: # # cluster.initial_master_nodes: [\"node-1\"] # # For more information, consult the discovery and cluster formation module documentation. # # ---------------------------------- Gateway ----------------------------------- # # Block initial recovery after a full cluster restart until N nodes are started: # #gateway.recover_after_nodes: 3 # # For more information, consult the gateway module documentation. # # ---------------------------------- Various ----------------------------------- # # Require explicit names when deleting indices: # #action.destructive_requires_name: true # action.auto_create_index: \".app-search-*-logs-*,-.app-search-*,+*\" ## ===================== Elastic App Search Configuration ===================== # # NOTE: Elastic App Search comes with reasonable defaults. # Before adjusting the configuration, make sure you understand what you # are trying to accomplish and the consequences. # # NOTE: For passwords, the use of environment variables is encouraged # to keep values from being written to disk, e.g. # elasticsearch.password: ${ELASTICSEARCH_PASSWORD:changeme} # # ---------------------------------- Elasticsearch ---------------------------- ... # ------------------------------- Hosting & Network --------------------------- # # Define the exposed URL at which users will reach App Search. # Defaults to localhost:3002 for testing purposes. # Most cases will use one of: # # * An IP: http://255.255.255.255 # * A FQDN: http://example.com # * Shortname defined via /etc/hosts: http://app-search.search # app_search.external_url: http://ld-dbn-boref020:3002 # # Web application listen_host and listen_port. # Your application will run on this host and port. # # * app_search.listen_host: Must be a valid IPv4 or IPv6 address. # * app_search.listen_port: Must be a valid port number (1-65535). # app_search.listen_host: 10.31.150.40 app_search.listen_port: 3002 # # Background worker monitoring. # Diagnostic information will be served on `app_search.monitoring_port`. # # * app_search.monitoring_enabled: Set to false to disable monitoring. # * app_search.monitoring_port: Must be a valid port number (1-65535). # #app_search.monitoring_enabled: true #app_search.monitoring_port: 3003 # # ------------------------------ Authentication ------------------------------- ... # ---------------------------------- Email ----------------------------------- ... # ----------------------------------- APIs ------------------------------------ ... # ----------------------------- Diagnostics report ---------------------------- ... # ---------------------------------- Logging ---------------------------------- ... # ------------------------------- TLS/SSL ------------------------------- ... # ---------------------------------- Session ---------------------------------- # # Set key to persist user sessions through process restarts. # secret_session_key: ... #",
    "website_area": "discuss"
  },
  {
    "id": "f961f4d8-d3d5-481d-8146-eb87b3161ee6",
    "url": "https://discuss.elastic.co/t/multi-value-multi-level-hierarchical-facets-using-app-search/194805",
    "title": "Multi value multi level hierarchical facets using App Search",
    "category": [
      "App Search"
    ],
    "author": "abhishek.haith",
    "date": "August 12, 2019, 7:56am August 12, 2019, 10:24pm August 13, 2019, 5:54am August 14, 2019, 12:18pm August 14, 2019, 12:18pm August 14, 2019, 3:00pm August 15, 2019, 11:03am September 12, 2019, 11:03am",
    "body": "I have a form where user can select their favorite food. Options like below - Options Vegetables - Tomato - Pumpkin - Carrot Fruits - Apple - Banana - Orange Meats and Poultry - Lean meats - Beef - Lamb - Pork - Poultry - Chicken - Turkey - Fish and Seafood - Fish - Prawns - Lobster - Eggs - Chicken eggs - Duck eggs Users can select any number of option from any level. What should be the ideal Engine Schema in App Search and how to manage search query in API? Reference from: https://swiftype.com/documentation/app-search/guides/hierarchical-facets Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "415a7cb6-0e01-4ec3-a0cf-3cc6af838dab",
    "url": "https://discuss.elastic.co/t/webinterface-becomes-unusable-after-a-period-of-time/195262",
    "title": "Webinterface becomes unusable after a period of time",
    "category": [
      "App Search"
    ],
    "author": "MelvinRook",
    "date": "August 14, 2019, 9:00pm August 14, 2019, 9:36pm August 14, 2019, 10:02pm September 11, 2019, 10:02pm",
    "body": "The webinterface becomes unusable after a period of time. Not sure how to solve it. Checking the browser debug shows a 500 internal server error for some files: image.png1682590 117 KB In the 500 internal server error response: org.jruby.exceptions.TypeError: (TypeError) no implicit conversion of String into Integer at org.jruby.RubyArray.[]=(org/jruby/RubyArray.java:1558) at tmp.jruby7028956381679149336extract.gems.gems.rack_minus_1_dot_6_dot_11.lib.rack.body_proxy.method_missing(/tmp/jruby7028956381679149336extract/gems/gems/rack-1.6.11/lib/rack/body_proxy.rb:36) at tmp.jruby7028956381679149336extract.gems.gems.rack_minus_1_dot_6_dot_11.lib.rack.body_proxy.method_missing(/tmp/jruby7028956381679149336extract/gems/gems/rack-1.6.11/lib/rack/body_proxy.rb:36) .. <snip, repeating> In the logs: [2019-08-14T20:53:34.865+00:00][8110][2834][action_controller][INFO]: [ea5c2dc8-6617-4405-9766-94fc60431e83] Processing by ErrorsController#not_found as */* [2019-08-14T20:53:34.865+00:00][8110][2834][action_controller][INFO]: [ea5c2dc8-6617-4405-9766-94fc60431e83] Parameters: {\"host\"=>\"localhost:3002\", \"protocol\"=>\"http\"} [2019-08-14T20:53:34.873+00:00][8110][2834][action_controller][INFO]: [ea5c2dc8-6617-4405-9766-94fc60431e83] Completed 500 Internal Server Error in 8ms [2019-08-14T20:53:34.900+00:00][8110][2832][rails][FATAL]: [5dfb215f-3719-4723-a4fa-d9752bfd716e] ActionController::RoutingError (No route matches [GET] \"/packs/js/0-40694bdbb3917ca89f95.chunk.js\"): /tmp/jruby7028956381679149336extract/gems/gems/actionpack-4.2.11.1/lib/action_dispatch/middleware/debug_exceptions.rb:21:in `call' /tmp/jruby7028956381679149336extract/gems/gems/actionpack-4.2.11.1/lib/action_dispatch/middleware/show_exceptions.rb:30:in `call' /tmp/jruby7028956381679149336extract/gems/gems/railties-4.2.11.1/lib/rails/rack/logger.rb:38:in `call_app' /tmp/jruby7028956381679149336extract/gems/gems/railties-4.2.11.1/lib/rails/rack/logger.rb:20:in `block in call' /tmp/jruby7028956381679149336extract/gems/gems/activesupport-4.2.11.1/lib/active_support/tagged_logging.rb:68:in `block in tagged' /tmp/jruby7028956381679149336extract/gems/gems/activesupport-4.2.11.1/lib/active_support/tagged_logging.rb:26:in `tagged' /tmp/jruby7028956381679149336extract/gems/gems/activesupport-4.2.11.1/lib/active_support/tagged_logging.rb:68:in `tagged' /tmp/jruby7028956381679149336extract/gems/gems/railties-4.2.11.1/lib/rails/rack/logger.rb:20:in `call' config/initializers/quiet_assets.class:6:in `call_with_quiet_assets' /tmp/jruby7028956381679149336extract/gems/gems/request_store-1.4.1/lib/request_store/middleware.rb:19:in `call' /tmp/jruby7028956381679149336extract/gems/gems/actionpack-4.2.11.1/lib/action_dispatch/middleware/request_id.rb:21:in `call' /tmp/jruby7028956381679149336extract/gems/gems/rack-1.6.11/lib/rack/methodoverride.rb:22:in `call' /tmp/jruby7028956381679149336extract/gems/gems/rack-1.6.11/lib/rack/runtime.rb:18:in `call' /tmp/jruby7028956381679149336extract/gems/gems/actionpack-4.2.11.1/lib/action_dispatch/middleware/static.rb:120:in `call' /tmp/jruby7028956381679149336extract/gems/gems/rack-1.6.11/lib/rack/sendfile.rb:113:in `call' config/initializers/stats_middleware.class:10:in `call' shared_togo/lib/shared_togo/external_host_middleware.class:15:in `call' /tmp/jruby7028956381679149336extract/gems/gems/railties-4.2.11.1/lib/rails/engine.rb:518:in `call' /tmp/jruby7028956381679149336extract/gems/gems/railties-4.2.11.1/lib/rails/application.rb:165:in `call' uri:classloader:/rack/handler/servlet.rb:22:in `call",
    "website_area": "discuss"
  },
  {
    "id": "da3ece7f-4a18-476c-881b-56bd700fce70",
    "url": "https://discuss.elastic.co/t/advanced-setting-for-app-search-7-2-for-location-of-temp-work-space-for-jetty-and-other-process/193370",
    "title": "Advanced setting for app search 7.2 for location of temp work space for jetty and other process",
    "category": [
      "App Search"
    ],
    "author": "Dustin_Hughes",
    "date": "August 1, 2019, 3:52pm August 12, 2019, 10:24pm August 12, 2019, 5:43pm September 9, 2019, 5:43pm",
    "body": "During load testing of data ingestion last night on the 7.2 edition we notice the use of /tmp Example of what type of files we have seen: ''' drwxrwxr-x 4 app-search app-search 29 Aug 1 09:13 jetty-127.0.0.1-3002-app-search.war--any-14033323923130097253.dir drwxr-xr-x 4 app-search app-search 29 Aug 1 10:36 jetty-127.0.0.1-3002-app-search.war--any-15045452541766198933.dir drwxrwxr-x 4 app-search app-search 29 Aug 1 09:37 jetty-127.0.0.1-3002-app-search.war-_-any-3636654065492079126.dir drwxr-xr-x 2 app-search app-search 4.0K Aug 1 10:36 jruby-1111 drwxr-xr-x 2 app-search app-search 4.0K Aug 1 10:36 jruby-1112 drwxr-xr-x 2 app-search app-search 4.0K Aug 1 10:36 jruby-1113 drwxr-xr-x 14 app-search app-search 4.0K Aug 1 10:35 jruby11455094622311008353extract drwxr-xr-x 14 app-search app-search 4.0K Aug 1 09:40 jruby12054002575997581668extract drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:12 jruby12457954944277647398extract drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:36 jruby13846152299151519729extract drwxr-xr-x 14 app-search app-search 4.0K Aug 1 10:35 jruby14907057299340211513extract drwxrwxr-x 2 app-search app-search 4.0K Aug 1 09:13 jruby-163368 drwxrwxr-x 2 app-search app-search 4.0K Aug 1 09:12 jruby-163371 drwxrwxr-x 2 app-search app-search 4.0K Aug 1 09:37 jruby-164167 drwxrwxr-x 2 app-search app-search 4.0K Aug 1 09:37 jruby-164170 drwxr-xr-x 2 app-search app-search 4.0K Aug 1 09:41 jruby-164436 drwxr-xr-x 2 app-search app-search 4.0K Aug 1 09:40 jruby-164440 drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:12 jruby1689002271072179113extract drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:36 jruby2898632431355743206extract drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:36 jruby4620071894457058344extract drwxr-xr-x 14 app-search app-search 4.0K Aug 1 09:40 jruby5671705993438013238extract drwxr-xr-x 14 app-search app-search 4.0K Aug 1 10:35 jruby5845234341657106321extract drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:12 jruby8216675081117055363extract drwx------ 3 root root 16 Aug 1 10:35 systemd-private-94b6a3329d97441381df0d0d1ef30eaf-nginx.service-spyMfG drwx------ 3 root root 16 Aug 1 10:35 systemd-private-94b6a3329d97441381df0d0d1ef30eaf-ntpd.service-Gjhoy8 ''' For the time being, we have increased the /tmp directory size. Our sysadmin was asking if it was possible to change this location via an environmental variable.",
    "website_area": "discuss"
  },
  {
    "id": "76074001-564d-4dae-b768-18ca08491e91",
    "url": "https://discuss.elastic.co/t/newb-search-field-in-navbar-and-search-results-somewhere-else/192534",
    "title": "Newb - Search field in Navbar and search results somewhere else",
    "category": [
      "App Search"
    ],
    "author": "lesreaper",
    "date": "July 27, 2019, 7:25pm August 12, 2019, 12:37pm September 9, 2019, 12:37pm",
    "body": "Hey all, On my trial for a client, and I'm not understanding how to separate out a search field in the navbar and then putting the actual results on another page in the site. Using React, and there's no example from what I can see of how this is done. Am I missing something?",
    "website_area": "discuss"
  },
  {
    "id": "28e8272d-81df-41eb-b437-40923c49aaeb",
    "url": "https://discuss.elastic.co/t/issues-installing-app-search-locally-using-docker/192206",
    "title": "Issues installing App Search locally using docker",
    "category": [
      "App Search"
    ],
    "author": "jubstuff",
    "date": "July 25, 2019, 9:57am July 25, 2019, 9:43am July 25, 2019, 9:59am July 25, 2019, 12:06pm July 25, 2019, 2:18pm July 26, 2019, 2:08pm July 26, 2019, 2:23pm July 26, 2019, 2:37pm July 26, 2019, 3:18pm July 29, 2019, 6:22am July 29, 2019, 11:39am August 26, 2019, 11:39am",
    "body": "Hello, I wanted to give App Search a try and I wanted to install it locally using docker. I followed the instructions located at https://swiftype.com/documentation/app-search/self-managed/installation#docker-compose but it seems that no matter what, I cannot access the App Search app at http://localhost:3002 This is my docker-compose.yml: version: '2.2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0 environment: - \"node.name=es-node\" - \"discovery.type=single-node\" - \"cluster.name=app-search-docker-cluster\" - \"bootstrap.memory_lock=true\" - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - \"cluster.routing.allocation.disk.threshold_enabled=true\" - \"cluster.routing.allocation.disk.watermark.flood_stage=200mb\" - \"cluster.routing.allocation.disk.watermark.low=500mb\" - \"cluster.routing.allocation.disk.watermark.high=300mb\" ulimits: memlock: soft: -1 hard: -1 ports: - 9200:9200 - 9300:9300 volumes: - esdata01:/usr/share/elasticsearch/data appsearch: image: docker.elastic.co/app-search/app-search:7.2.0 environment: - \"elasticsearch.host=http://elasticsearch:9200\" - \"allow_es_settings_modification=true\" - \"JAVA_OPTS=-Xmx256m\" ports: - 3002:3002 volumes: esdata01: driver: local While this is the log I get when running docker-compose up:  docker-compose up appsearch Recreating appsearchtest_appsearch_1 ... done Attaching to appsearchtest_appsearch_1 appsearch_1 | Found java executable in PATH appsearch_1 | Java version: 1.8.0_212 appsearch_1 | appsearch_1 | App Search is starting. It will take a few moments. App Search includes the following stack components: appsearch_1 | - An application server appsearch_1 | - A pool of background workers appsearch_1 | - A filebeat instance for indexing logs appsearch_1 | appsearch_1 | forego | starting app-server.1 on port 5000 appsearch_1 | forego | starting worker.1 on port 5100 appsearch_1 | forego | starting filebeat.1 on port 5300 appsearch_1 | filebeat.1 | scripting container class loader urls: [file:/tmp/jruby8772327061860643696extract/lib/jruby-core-9.2.5.0-complete.jar, file:/tmp/jruby8772327061860643696extract/lib/jruby-rack-1.1.21.jar, file:/tmp/jruby8772327061860643696extract/lib/jruby-stdlib-9.2.5.0.jar] appsearch_1 | app-server.1 | scripting container class loader urls: [file:/tmp/jruby4781859431772385646extract/lib/jruby-core-9.2.5.0-complete.jar, file:/tmp/jruby4781859431772385646extract/lib/jruby-rack-1.1.21.jar, file:/tmp/jruby4781859431772385646extract/lib/jruby-stdlib-9.2.5.0.jar] appsearch_1 | worker.1 | scripting container class loader urls: [file:/tmp/jruby394617129645864115extract/lib/jruby-core-9.2.5.0-complete.jar, file:/tmp/jruby394617129645864115extract/lib/jruby-rack-1.1.21.jar, file:/tmp/jruby394617129645864115extract/lib/jruby-stdlib-9.2.5.0.jar] appsearch_1 | app-server.1 | setting GEM_HOME to /tmp/jruby4781859431772385646extract/gems appsearch_1 | app-server.1 | ... and BUNDLE_GEMFILE to /tmp/jruby4781859431772385646extract/Gemfile appsearch_1 | app-server.1 | loading resource: /tmp/jruby4781859431772385646extract/./META-INF/rails.rb appsearch_1 | app-server.1 | invoking /tmp/jruby4781859431772385646extract/./META-INF/rails.rb with: [runner, LocoTogo.start_app_server!] appsearch_1 | filebeat.1 | setting GEM_HOME to /tmp/jruby8772327061860643696extract/gems appsearch_1 | filebeat.1 | ... and BUNDLE_GEMFILE to /tmp/jruby8772327061860643696extract/Gemfile appsearch_1 | filebeat.1 | loading resource: /tmp/jruby8772327061860643696extract/./META-INF/rails.rb appsearch_1 | filebeat.1 | invoking /tmp/jruby8772327061860643696extract/./META-INF/rails.rb with: [runner, LocoTogo.start_filebeat!] appsearch_1 | worker.1 | setting GEM_HOME to /tmp/jruby394617129645864115extract/gems appsearch_1 | worker.1 | ... and BUNDLE_GEMFILE to /tmp/jruby394617129645864115extract/Gemfile appsearch_1 | worker.1 | loading resource: /tmp/jruby394617129645864115extract/./META-INF/rails.rb appsearch_1 | worker.1 | invoking /tmp/jruby394617129645864115extract/./META-INF/rails.rb with: [runner, LocoTogo.start_worker!] appsearch_1 | filebeat.1 | Creating log directory: /usr/share/app-search/log appsearch_1 | filebeat.1 | [2019-07-25T09:05:54.229+00:00][56][2000][rails][INFO]: App Search version=7.2.0, JRuby version=9.2.5.0, Ruby version=2.5.0, Rails version=4.2.11.1 appsearch_1 | app-server.1 | [2019-07-25T09:05:54.502+00:00][53][2000][rails][INFO]: App Search version=7.2.0, JRuby version=9.2.5.0, Ruby version=2.5.0, Rails version=4.2.11.1 appsearch_1 | worker.1 | [2019-07-25T09:06:02.025+00:00][55][2000][rails][INFO]: App Search version=7.2.0, JRuby version=9.2.5.0, Ruby version=2.5.0, Rails version=4.2.11.1 From the logs, it seems that the server is running on port 5000, but even exposing that port didn't work. Could someone help me setup it? Thank you",
    "website_area": "discuss"
  },
  {
    "id": "6116fb90-b367-4ce1-b72d-a7f91fc8b6fb",
    "url": "https://discuss.elastic.co/t/elastic-app-search-windows-installation/192301",
    "title": "Elastic App Search Windows installation",
    "category": [
      "App Search"
    ],
    "author": "rajithsam",
    "date": "July 25, 2019, 7:06pm July 25, 2019, 7:12pm July 25, 2019, 7:12pm July 25, 2019, 7:11pm July 25, 2019, 7:12pm July 25, 2019, 7:12pm August 22, 2019, 7:12pm",
    "body": "Is it possible to install elastic app search in windows? https://www.elastic.co/downloads/app-search",
    "website_area": "discuss"
  },
  {
    "id": "a3026fa8-1afc-4357-aeeb-d610f3de571b",
    "url": "https://discuss.elastic.co/t/app-search-not-returning-correct-curation-results/191694",
    "title": "App search not returning correct curation results",
    "category": [
      "App Search"
    ],
    "author": "brettg",
    "date": "July 22, 2019, 5:42pm July 23, 2019, 10:48am July 25, 2019, 4:18pm August 22, 2019, 4:18pm",
    "body": "Hey All, I have several curations set up in AppSearch and I'm trying to query them via postman by the curation search term to see if I can get them to show up in the correct order. The results that are coming back from endpoint: engines/myengine/search { \"query\": \"myquery\", } Are not in the order that the curation says they should be returned in. Is there something I'm missing? Thanks, B",
    "website_area": "discuss"
  },
  {
    "id": "3d57cbdb-7195-4852-8b48-526c863de6b2",
    "url": "https://discuss.elastic.co/t/issue-with-app-search-download-release-not-quite-a-connection-issue/191249",
    "title": "Issue with app search (download) release -- not quite a connection issue",
    "category": [
      "App Search"
    ],
    "author": "dhrp",
    "date": "July 18, 2019, 4:38pm July 18, 2019, 4:46pm July 18, 2019, 7:09pm July 18, 2019, 9:19pm July 18, 2019, 10:20pm July 19, 2019, 9:28pm August 16, 2019, 9:28pm",
    "body": "I'm interested in trying out app-search self hosted, but I'm having trouble getting started. I can submit an object using past JSON or upload. In the logs this is all well received. But then App search keeps returning me that I have 0 documents. Setup: Elasticsearch 7.2 with basic license (installed through elasticsearch kubernetes operator) available over https with authentication on my server with https on port 443 KIbana works fine App search is configured with the hostname, user and password of elasticsearch I can login fine on app search I can submit an object using past JSON or upload. In the logs this is all well received. App Search logs show the document(s) are successfully received. app-server.1 | [2019-07-18T16:14:26.892+00:00][12273][2366][rails][INFO]: [3dd6e664-73b0-48e8-b81f-4f5aa8d9939c] Engine[5d30804df841b462bf4f3c50]: Adding a batch of 2 documents to the index asynchronously app-server.1 | [2019-07-18T16:14:26.918+00:00][12273][2366][rails][INFO]: [3dd6e664-73b0-48e8-b81f-4f5aa8d9939c] [ActiveJob] Enqueueing a job into the '.app-search-esqueues-me_queue_v1_index_adder' index. {\"job_type\"=>\"ActiveJob::QueueAdapters::EsqueuesMeAdapter::JobWrapper\", \"payload\"=>{\"args\"=>[{\"job_class\"=>\"Work::Engine::IndexAdder\", \"job_id\"=>\"984576c7d5d00b376476f1aba9b863732290f8b3\", \"queue_name\"=>\"index_adder\", \"arguments\"=>[\"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"]], \"locale\"=>:en, \"executions\"=>1}]}, \"status\"=>\"pending\", \"created_at\"=>1563466466917, \"perform_at\"=>1563466466917, \"attempts\"=>0} app-server.1 | [2019-07-18T16:14:27.074+00:00][12273][2366][active_job][INFO]: [3dd6e664-73b0-48e8-b81f-4f5aa8d9939c] [ActiveJob] [2019-07-18 16:14:27 UTC] enqueued Work::Engine::IndexAdder job (984576c7d5d00b376476f1aba9b863732290f8b3) on `index_adder` app-server.1 | [2019-07-18T16:14:27.076+00:00][12273][2366][active_job][INFO]: [3dd6e664-73b0-48e8-b81f-4f5aa8d9939c] [ActiveJob] Enqueued Work::Engine::IndexAdder (Job ID: 984576c7d5d00b376476f1aba9b863732290f8b3) to EsqueuesMe(index_adder) with arguments: \"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"] app-server.1 | [2019-07-18T16:14:27.087+00:00][12273][2366][action_controller][INFO]: [3dd6e664-73b0-48e8-b81f-4f5aa8d9939c] Completed 200 OK in 723ms (Views: 2.0ms) But the worker returns the following errors: worker.1 | [2019-07-18T16:14:28.176+00:00][12274][2364][rails][WARN]: Failed to claim job 984576c7d5d00b376476f1aba9b863732290f8b3, claim conflict occurred worker.1 | [2019-07-18T16:14:28.176+00:00][12274][2370][rails][WARN]: Failed to claim job 984576c7d5d00b376476f1aba9b863732290f8b3, claim conflict occurred worker.1 | [2019-07-18T16:14:28.176+00:00][12274][2366][rails][WARN]: Failed to claim job 984576c7d5d00b376476f1aba9b863732290f8b3, claim conflict occurred worker.1 | [2019-07-18T16:14:28.181+00:00][12274][2368][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [984576c7d5d00b376476f1aba9b863732290f8b3] Performing Work::Engine::IndexAdder from EsqueuesMe(index_adder) with arguments: \"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"] worker.1 | [2019-07-18T16:14:28.546+00:00][12274][2368][rails][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [984576c7d5d00b376476f1aba9b863732290f8b3] Adding document 5d30813cf841b4e1a64f3c54 to index for engine 5d30804df841b462bf4f3c50 worker.1 | [2019-07-18T16:14:29.053+00:00][12274][2368][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [984576c7d5d00b376476f1aba9b863732290f8b3] Performed Work::Engine::IndexAdder from EsqueuesMe(index_adder) in 864.99ms worker.1 | [2019-07-18T16:14:29.055+00:00][12274][2368][rails][ERROR]: Retrying Work::Engine::IndexAdder in 300 seconds, due to a StandardError. The original exception was #<Faraday::ConnectionFailed wrapped=#<Manticore::SocketException: Connection refused (Connection refused)>>. worker.1 | [2019-07-18T16:14:29.058+00:00][12274][2368][rails][INFO]: [ActiveJob] Enqueueing a job into the '.app-search-esqueues-me_queue_v1_index_adder' index. {\"job_type\"=>\"ActiveJob::QueueAdapters::EsqueuesMeAdapter::JobWrapper\", \"payload\"=>{\"args\"=>[{\"job_class\"=>\"Work::Engine::IndexAdder\", \"job_id\"=>\"984576c7d5d00b376476f1aba9b863732290f8b3\", \"queue_name\"=>\"index_adder\", \"arguments\"=>[\"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"]], \"locale\"=>:en, \"executions\"=>2}]}, \"status\"=>\"pending\", \"created_at\"=>1563466469057, \"perform_at\"=>1563466769056, \"attempts\"=>0} worker.1 | [2019-07-18T16:14:29.079+00:00][12274][2368][rails][INFO]: [ActiveJob] Ignoring duplicate job class=Work::Engine::IndexAdder, id=984576c7d5d00b376476f1aba9b863732290f8b3, args=[\"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"]] worker.1 | [2019-07-18T16:14:29.080+00:00][12274][2368][active_job][INFO]: [ActiveJob] [2019-07-18 16:14:29 UTC] enqueued Work::Engine::IndexAdder job (984576c7d5d00b376476f1aba9b863732290f8b3) on `index_adder` worker.1 | [2019-07-18T16:14:29.084+00:00][12274][2368][active_job][INFO]: [ActiveJob] Enqueued Work::Engine::IndexAdder (Job ID: 984576c7d5d00b376476f1aba9b863732290f8b3) to EsqueuesMe(index_adder) at 2019-07-18 16:19:29 UTC with arguments: \"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"] worker.1 | [2019-07-18T16:14:29.086+00:00][12274][2368][rails][INFO]: Deleting: {:index=>\".app-search-esqueues-me_queue_v1_index_adder\", :type=>nil, :id=>\"984576c7d5d00b376476f1aba9b863732290f8b3\"} In the Elasticsearch logs I see that app search is connected, because it gives me the following deprecation warning every so much time. It does not seem related. {\"type\": \"deprecation\", \"timestamp\": \"2019-07-18T16:34:50,046+0000\", \"level\": \"WARN\", \"component\": \"o.e.d.s.a.b.h.DateHistogramAggregationBuilder\", \"cluster.name\": \"quickstart\", \"node.name\": \"quickstart-es-z92ct6ztpk\", \"cluster.uuid\": \"H77aFeQoQUqU7ckCMK_gVg\", \"node.id\": \"D7V-shukRdi9WyuxuWdHIQ\", \"message\": \"[interval] on [date_histogram] is deprecated, use [fixed_interval] or [calendar_interval] in the future.\" } I have checked settings, and it includes (not sure if app search or myself set it) { \"persistent\": { \"action\": { \"auto_create_index\": \".app-search-*-logs-*,-.app-search-*,+*\" }, \"discovery\": { \"zen\": { \"minimum_master_nodes\": \"1\" } } }, The end result is that there are some indexes that start with .app-search, but no data.... !?#! help?",
    "website_area": "discuss"
  },
  {
    "id": "2866c94b-5b8f-452c-aebf-29f006de9b96",
    "url": "https://discuss.elastic.co/t/different-ways-to-add-documents/191361",
    "title": "Different ways to add documents?",
    "category": [
      "App Search"
    ],
    "author": "gork",
    "date": "July 19, 2019, 9:37am July 19, 2019, 9:42am July 19, 2019, 9:59am July 19, 2019, 10:12am July 19, 2019, 10:26am July 19, 2019, 11:12am July 19, 2019, 11:29am July 19, 2019, 11:55am July 19, 2019, 12:33pm July 19, 2019, 2:11pm July 19, 2019, 2:22pm August 16, 2019, 2:22pm",
    "body": "Heya I've been playing around with App Search the last couple of days and so far I am really enjoying it! I would like to move the search functionality of my PHP-based forum to AS and have played around with adding some documents. Right now I have a PHP script that gets the data from MySQL, does a bit of manipulation (convert timestamps to ISO etc) and then ship 100 documents at a time to AS/ Now I am wondering, how to do it better / faster / easier using - I don't know - JDBC? Different shipping methods from MySQL to AS? But I am unsure of what would be the \"right\" way to do it. As far as I have found out, there is no Logstash in AS? Or is there? Any tips will be much appreciated! Seeing that this is quite a new (and awesome) product, I could not find any specific App Search related answers or tutorials Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "d55522f0-a71b-46d3-b04a-ee0810f12fa5",
    "url": "https://discuss.elastic.co/t/api-logs-app-search-self-managed-config-time-save-logs/189101",
    "title": "[API Logs] App Search Self Managed config time save logs?",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "July 5, 2019, 12:46pm July 18, 2019, 8:50am July 18, 2019, 11:58am July 18, 2019, 12:46pm July 18, 2019, 1:40pm July 18, 2019, 3:10pm July 18, 2019, 3:12pm July 19, 2019, 1:37am July 19, 2019, 2:59am August 16, 2019, 2:59am",
    "body": "Dear Team, I'm using App Search Self Managed 7.2.0 Please help me, config time save logs? (Current default 24h) Thanks Team",
    "website_area": "discuss"
  },
  {
    "id": "f92ea69b-db87-4b26-85a6-03739913d0f8",
    "url": "https://discuss.elastic.co/t/installing-app-search-7-2-ga/188103",
    "title": "Installing App Search 7.2 GA",
    "category": [
      "App Search"
    ],
    "author": "bpamiri",
    "date": "June 28, 2019, 7:47pm July 1, 2019, 11:58am July 3, 2019, 5:07pm July 3, 2019, 5:38pm July 8, 2019, 9:28pm August 5, 2019, 9:26pm",
    "body": "I've just finished standing up a 10 node ElasticSearch 7.2 cluster on CentOS VMs in our data center. Now I want to download App Search and play with it. I'm a little confused as to where it needs to be installed. The documentation for a production ready installation is very sparse and I don't think there are RMPs available yet. So here is my initial questions. Does App Search get installed on each node of a cluster or does it get installed on a single node? Should it be installed on a member node of the cluster or on it's own server? Are there any installation docs for a production ready installation?",
    "website_area": "discuss"
  },
  {
    "id": "b59a7921-15af-44fb-ada5-2b316deea9b4",
    "url": "https://discuss.elastic.co/t/how-to-make-special-character-insensitive/188567",
    "title": "How to make special character insensitive?",
    "category": [
      "App Search"
    ],
    "author": "Hesh",
    "date": "July 2, 2019, 6:46pm July 30, 2019, 6:46pm",
    "body": "We're using the \"query suggestion\" feature. We've realized that when searching for a title that contains a Special Character such a an apostrophe but the search query is missing the apostrophe, Swiftype will not return suggestions that contain an apostrophe. e.g. whats will not returns what's Is there a way to make special character insensitive?",
    "website_area": "discuss"
  },
  {
    "id": "a3628bcf-9537-4122-b656-9e53ed23034f",
    "url": "https://discuss.elastic.co/t/rank-feature-and-app-search/188461",
    "title": "Rank_feature and APP-Search",
    "category": [
      "App Search"
    ],
    "author": "bastimm",
    "date": "July 2, 2019, 9:05am July 2, 2019, 2:46pm July 30, 2019, 2:32pm",
    "body": "Hello, everybody, I just happened to come across the APP search and installed it. At first sight it looks very good! Just one question, currently I have extended my Elasticsearch-queries with a rank_feature query... Is it possible to include the rank_feature in the APP-search to calculate the relevance?",
    "website_area": "discuss"
  },
  {
    "id": "dda4d753-0b12-4eed-b458-ff77c2490098",
    "url": "https://discuss.elastic.co/t/modeling-non-tokenized-text-fields/187703",
    "title": "Modeling non-tokenized text fields",
    "category": [
      "App Search"
    ],
    "author": "omairkhawaja",
    "date": "June 27, 2019, 2:32am June 28, 2019, 3:51pm July 26, 2019, 3:51pm",
    "body": "Hi, While reading the App Search documentation, I was unable to find out how to model text fields that should not be tokenized/analyzed (e.g. ISBN numbers)? How should these be modeled in App Search?",
    "website_area": "discuss"
  },
  {
    "id": "fe3a862b-0884-40b6-9445-5e6912579e1e",
    "url": "https://discuss.elastic.co/t/default-password-for-elasticsearch-app-search/187625",
    "title": "Default password for ElasticSearch App Search",
    "category": [
      "App Search"
    ],
    "author": "omairkhawaja",
    "date": "June 26, 2019, 6:15pm June 28, 2019, 3:48pm June 28, 2019, 3:48pm June 28, 2019, 5:18am June 28, 2019, 6:14am June 28, 2019, 3:48pm July 26, 2019, 3:49pm",
    "body": "I've followed the documentation and successfully enabled standard authentication on my ElasticSearch and spun up an ElsaticSearch App Search instance. My question is: What is the default password for Elastic Search app search image.jpg703663 79.2 KB I tried app-search@example.com/changeme and that did not work.",
    "website_area": "discuss"
  },
  {
    "id": "df1a727a-b512-4262-8032-1c58661e99f7",
    "url": "https://discuss.elastic.co/t/initial-using-appsearch/186282",
    "title": "Initial using AppSearch",
    "category": [
      "App Search"
    ],
    "author": "tales",
    "date": "June 18, 2019, 2:27pm June 25, 2019, 11:12am June 25, 2019, 12:14pm June 26, 2019, 6:54am June 26, 2019, 1:06pm June 27, 2019, 3:25pm June 27, 2019, 4:19pm June 28, 2019, 12:39pm June 28, 2019, 1:12pm July 26, 2019, 1:12pm",
    "body": "I've started with AppSearch by Swiftype.com to tests, but i have some doubts. How many documents can i ingest with trial version? I trying to ingest around +- 5,000 documents, but i can't make it. There're no errors. If anybody can help me, i'll appreciate. Regards Tales Macedo",
    "website_area": "discuss"
  },
  {
    "id": "6273aaa1-187e-4010-8581-0b073457bae7",
    "url": "https://discuss.elastic.co/t/how-to-search-with-pagination-in-search-app/187709",
    "title": "How to search with pagination in search app?",
    "category": [
      "App Search"
    ],
    "author": "Ricky_Tokdis",
    "date": "June 27, 2019, 3:47am June 27, 2019, 6:52am June 27, 2019, 10:33am June 27, 2019, 4:05pm June 28, 2019, 2:29am June 28, 2019, 2:33am July 1, 2019, 10:45am July 26, 2019, 7:46am",
    "body": "https://xxxxxxxxxx.swiftype.com/api/as/v1/engines/xxxxxxxx/search?page[size]=15&page[current]=2&query=jam&filters[is_deleted]=0&sort[total_sold]=desc This my error : \"Page contains invalid option: size; must be an integer\", \"Page contains invalid option: current; must be an integer\" how to can i to fix this error help me please",
    "website_area": "discuss"
  },
  {
    "id": "c0bb53bc-4f59-4e50-944f-388686211f0f",
    "url": "https://discuss.elastic.co/t/modeling-data-to-support-nested-queries/187701",
    "title": "Modeling data to support nested queries",
    "category": [
      "App Search"
    ],
    "author": "omairkhawaja",
    "date": "June 27, 2019, 2:27am June 28, 2019, 7:44am July 26, 2019, 7:44am",
    "body": "Hi, I noticed that App Search does not support nested object types (Schema Design). What data model would you suggest in order to support the data model and query given below? e.g. my data model is: course (1) -- (*) section section properties: campus semester I would like the query to return all courses containing a section offered in Summer 2019 (semester) at the Loyalist campus",
    "website_area": "discuss"
  },
  {
    "id": "5ea3354d-a49b-40cb-a0c3-c53e6a9c2593",
    "url": "https://discuss.elastic.co/t/returning-results-with-a-relevancy-score-of-0-when-sort-field-is-passed/187314",
    "title": "Returning results with a relevancy score of 0 when sort field is passed",
    "category": [
      "App Search"
    ],
    "author": "Harry_Knowles",
    "date": "June 25, 2019, 11:24am June 26, 2019, 7:46am July 24, 2019, 7:46am",
    "body": "When passing a sort field, the client seems to be returning results with a relevancy score of 0. Should these not be excluded completely as they are not relevant to the search term? We have a sort by price field. When selecting either low or high, it seems to be returning our most expensive, or inexpensive indexed product and doesn't check if the relevancy score is above 0. Initially posted this as an issue on the javascript API client GitHub page, however, they have mentioned this is an API issue and should be posted here. https://github.com/swiftype/swiftype-app-search-javascript/issues/61 Thanks for any help.",
    "website_area": "discuss"
  },
  {
    "id": "8692e906-40ca-404f-8139-254f5711cd8d",
    "url": "https://discuss.elastic.co/t/instalacao-configuracao-inicial-app-search/186153",
    "title": "Instalao configurao inicial App Search",
    "category": [
      "App Search"
    ],
    "author": "tales",
    "date": "June 17, 2019, 8:36pm June 25, 2019, 11:23am June 26, 2019, 6:58am July 24, 2019, 6:58am",
    "body": "Boa tarde, Estou comeando testes com o App Search (on promisse), segui todos os procedimentos de instalao e configurao, porm quando vou iniciar o servio no consigo. Se algum puder me ajudar. image.png853420 28.2 KB",
    "website_area": "discuss"
  },
  {
    "id": "157dabd8-6ea4-4e74-babe-92decad8d674",
    "url": "https://discuss.elastic.co/t/when-elastic-app-search-support-elasticsearch-7-0/177511",
    "title": "When Elastic App Search support Elasticsearch 7.0?",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "April 23, 2019, 2:21am May 16, 2019, 4:31pm July 8, 2019, 5:15pm",
    "body": "Dear Team, Version current (Elastic App Search Beta 4) Verify these prerequisites: Java 8 or Java 11 is installed, and Elasticsearch >= 6.6.x, < 7 is installed and running. Elasticsearch requires at least a Basic license. When Elastic App Search support Elasticsearch 7.0? Best Regards",
    "website_area": "discuss"
  },
  {
    "id": "b22c0e4d-6049-4662-b11c-7a05deef0d1a",
    "url": "https://discuss.elastic.co/t/what-is-the-licensing-model-for-app-search/185732",
    "title": "What is the licensing model for App Search?",
    "category": [
      "App Search"
    ],
    "author": "",
    "date": "June 13, 2019, 10:24pm June 13, 2019, 10:15pm June 13, 2019, 10:24pm June 13, 2019, 10:37pm June 25, 2019, 8:11pm July 23, 2019, 8:10pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "6b31194e-28cd-40cc-8eae-311ce5642731",
    "url": "https://discuss.elastic.co/t/cant-use-filtering-negation/185325",
    "title": "Can't use filtering negation",
    "category": [
      "App Search"
    ],
    "author": "BestBenjamin",
    "date": "June 12, 2019, 5:22am June 12, 2019, 6:14am June 12, 2019, 10:50am July 10, 2019, 10:00am",
    "body": "Hello, I have a problem with filtering negation. It not return anything when get a result,but when I remove '!' out.It works properly.So,I want to know if this filter is still working. Thank you",
    "website_area": "discuss"
  },
  {
    "id": "ca76cadd-b699-4e1c-b3cc-1a3497a0b2e4",
    "url": "https://discuss.elastic.co/t/language-support-in-standard-plan/175733",
    "title": "Language support in Standard plan",
    "category": [
      "App Search"
    ],
    "author": "kuraga",
    "date": "April 7, 2019, 7:48pm April 8, 2019, 11:26am April 8, 2019, 11:26am May 6, 2019, 11:26am",
    "body": "Good day! As of plans' description, Standard plan doesn't have multilingual support. So, only one language is supported. But which? English only, or on my decision? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "489ef9f4-39be-4a24-b7d0-922138955b14",
    "url": "https://discuss.elastic.co/t/we-are-trying-to-figure-out-how-to-return-the-facets-summary-from-a-search/172563",
    "title": "We are trying to figure out how to return the facets summary from a search",
    "category": [
      "App Search"
    ],
    "author": "David_Williams1",
    "date": "March 15, 2019, 3:48pm April 8, 2019, 11:13pm March 15, 2019, 4:55pm March 15, 2019, 6:06pm April 12, 2019, 6:20pm",
    "body": "We are in the evaluation stage of using AppSearch for a web environment and are having trouble understanding the method of returning the facets summary. We build our query and successfully get results but at no time do we get a summary of facet item counts. I know we are likely missing something simple but we cannot see to find it in the documentation.",
    "website_area": "discuss"
  },
  {
    "id": "dbf4725c-44de-47ca-acd1-7fd8661e1891",
    "url": "https://discuss.elastic.co/t/time-mismatch-in-kibana/171922",
    "title": "Time mismatch in kibana",
    "category": [
      "App Search"
    ],
    "author": "Carrie4456",
    "date": "March 12, 2019, 10:40am March 12, 2019, 3:57pm March 12, 2019, 3:58pm",
    "body": "We are having ELK setup with Kibana version 5.6.10. We are facing a time mismatch in displaying logs from different servers. We are fetching log from 8 IIS server and parsing via Logstash to Elastic search Kibana. While filtering logs for past hour we could notice only 2 server logs were displayed. We have checked filebeat configuration in each IIS servers and found same configuration setup; kroger feedback also verified IIS log time format and other configurations. We could see indexing is happening properly in Elastic Search but while filtering the display option for an hour only throwing results for 2 servers. If we filter for four hours we can see multiple servers with the different time value in the display. Would like to know anyone facing a similar issue and welcoming solution for it.",
    "website_area": "discuss"
  },
  {
    "id": "d306029d-0e12-490a-b5f4-d2351f21d579",
    "url": "https://discuss.elastic.co/t/self-managed-app-search-roadmap-question/170836",
    "title": "Self-Managed App Search Roadmap Question",
    "category": [
      "App Search"
    ],
    "author": "rockybean",
    "date": "March 5, 2019, 8:00am March 6, 2019, 1:42am March 6, 2019, 1:42am April 3, 2019, 1:42am",
    "body": "Hi, I saw app search has released beta version and I tried a bit. We really enjoy this solution. I have follow up questions. 1.Will this product be commercial like ece? 2.Will this product run on ece or just be managed by user in their own env? 3.Can you share some roadmap for this self-managed app search product? For example, when will this product be released finally? How much will this product cost? thanks!",
    "website_area": "discuss"
  },
  {
    "id": "2fba03e4-1787-41c7-9584-88fdffafb8a9",
    "url": "https://discuss.elastic.co/t/pre-fetch-min-and-max-price-field-for-filtering-a-query/169370",
    "title": "Pre-fetch min and max price field for filtering a query",
    "category": [
      "App Search"
    ],
    "author": "itsliamjones",
    "date": "February 21, 2019, 10:06am February 21, 2019, 1:34pm February 21, 2019, 4:32pm March 21, 2019, 4:31pm",
    "body": "Hi, I'm looking at adding mix-max price sliders to our search instance and need to find out the mix and max price available for the initial query. In Elasticsearch I have the min, max, and stat aggregations that allow me to pre-fetch this data before filtering. Can't find anything similar for Swiftype? Note: Our catalogue is pretty large and the prices range from pennies to thousands, due to this I can't make a best guess at min and max. Thanks, Liam",
    "website_area": "discuss"
  },
  {
    "id": "ab139dbc-e29c-4edf-b491-0bee714548ca",
    "url": "https://discuss.elastic.co/t/filter-array-contains-only/167593",
    "title": "Filter array contains only",
    "category": [
      "App Search"
    ],
    "author": "steverob",
    "date": "February 8, 2019, 9:10am February 8, 2019, 5:03pm March 8, 2019, 5:04pm",
    "body": "Is it possible to use the Filter API to find documents with an array field that contains only the specified values? Eg. I need to find documents with {array_attr: [\"A\", \"B\", \"C\"]}. I do not want documents that may have {array_attr: [\"A\", \"B\", \"C\", \"D\"]}",
    "website_area": "discuss"
  },
  {
    "id": "f47ec4cf-fc40-4fc7-8c0d-c1a53811c5d6",
    "url": "https://discuss.elastic.co/t/self-managed-elastic-app-search-beta/167397",
    "title": "Self-Managed Elastic App Search Beta",
    "category": [
      "App Search"
    ],
    "author": "Ismail_Mayat",
    "date": "February 7, 2019, 8:42am February 7, 2019, 6:26pm March 7, 2019, 6:26pm",
    "body": "I have been having a play with app search beta and have something up and running. My question is for the elastic part of it any recommends on setup of elastic i.e no of clusters / shards etc? Regards Ismail",
    "website_area": "discuss"
  },
  {
    "id": "595d9f6c-ae4e-4b3c-9928-f5c8ecc8c780",
    "url": "https://discuss.elastic.co/t/how-can-we-import-sql-server-data-into-elastic-appsearch/163577",
    "title": "How can we import sql server data into elastic appsearch",
    "category": [
      "App Search"
    ],
    "author": "sanupadh",
    "date": "January 9, 2019, 7:55pm February 6, 2019, 4:13pm",
    "body": "how to call search API /engine (ex: https://host-api.swiftype.com/api/as/v1/engines/esappsearch/documents ) using c#. can we have textbox autocomplete search using code written in c#/ also how can we import sql server data into elastic appsearch",
    "website_area": "discuss"
  },
  {
    "id": "49a1d78b-3002-4bea-b0bd-8425dbb12d88",
    "url": "https://discuss.elastic.co/t/how-to-create-a-webhook-for-appsearch-from-contentful/162192",
    "title": "How to create a webhook for Appsearch from Contentful",
    "category": [
      "App Search"
    ],
    "author": "venu.pgi",
    "date": "January 3, 2019, 9:46pm January 2, 2019, 2:47pm January 2, 2019, 3:12pm January 30, 2019, 3:12pm",
    "body": "HI, I have created a webhook from contentful cms like below, using post method. POST: https://host-ucy3pj.api.swiftype.com/api/as/v1/engines/national-parks-demo/documents. But it fails to send data into the Appsearch. Below is the error I get in the Response body. Response body { \"error\": \"You need to sign in or sign up before continuing.\" } With a status : \"status\": \"401 Unauthorized\". Obviously I can understand we need to send the private key as well. However when I try to add the private key in the options, my contentful does not accept, it throws error \"Please provide a valid webhook URL.\" when I try to set the webhook in the below format. https://host-ucy3pj.api.swiftype.com/api/as/v1/engines/national-parks-demo/documents -H 'Content-Type: application/json' -H 'Authorization: Bearer private-REDACTED' Could you please suggest how to create a webhook from any system into the Appsearch Endpoint? This is very crucial for our project as we intend to send the data from our contentful into the Appsearch, and then our MobileApp must be able to show results from the Appsearch index. Please provide your valuable inputs.",
    "website_area": "discuss"
  },
  {
    "id": "4c595866-aefd-499b-a5c3-24552ab62f64",
    "url": "https://discuss.elastic.co/t/unable-to-display-appsearch-data-from-frontend-using-javascript-api-swiftypeappsearch/162196",
    "title": "Unable to display Appsearch data from frontend using javascript api (SwiftypeAppSearch)",
    "category": [
      "App Search"
    ],
    "author": "venu.pgi",
    "date": "December 27, 2018, 5:07am January 2, 2019, 3:08pm January 30, 2019, 3:08pm",
    "body": "we are evaluating the ElasticSearch / Appsearch to work with our MobileApp project for a US client. So I registered for the trail version and created a engine. Also trying to connect to the Appsearch using the sample code given at the github. GitHub swiftype/swiftype-app-search-javascript Swiftype App Search Javascript Client. Contribute to swiftype/swiftype-app-search-javascript development by creating an account on GitHub. Attached is the test code we have written, using the approach given in the github. However it is not working giving some errors. I am attaching the code, errors and the credentials. Kindly forward to some one who can help us to integrate our MobileApp with your Appsearch product. Here is the code with I am using: https://drive.google.com/file/d/19x1bwagtockP8sYtYm6KBmLjr3cVJUZJ/view?usp=sharing And the error message can be seen here: https://drive.google.com/file/d/1luD2qVZl1zZnX7CFcv4b6gSnppqPqYBO/view?usp=sharing Secondly, we are really looking for a example app code with a searchbox which can connect to your Appsearch Engine and get some results. Kindly help us with an example.",
    "website_area": "discuss"
  },
  {
    "id": "3f7f19e4-29e2-45a7-abb3-039abd5cf1fb",
    "url": "https://discuss.elastic.co/t/fetching-all-docs-in-an-app-search-index/159918",
    "title": "Fetching all docs in an app search index",
    "category": [
      "App Search"
    ],
    "author": "frankjoh2",
    "date": "December 7, 2018, 11:22am December 7, 2018, 11:56am December 7, 2018, 5:40pm December 10, 2018, 12:14pm December 10, 2018, 9:23pm January 7, 2019, 9:23pm",
    "body": "Hi, I'm trying to get the ids of all documents in my app search index. I tried to simply iteration of searches with an empty query and incrementally increasing the page-number. This seemed to work fine for the first 10 requests. This is what my requests looks like: { \"query\": \"\", \"result_fields\": { \"id\": { \"raw\": {} } }, \"page\": {\"size\": 1000,\"current\": [PAGENUMER] } } Where [PAGENUMBER] is 1 for the first request, 2 for second and so on This is the result of the 10th request: { \"meta\": { \"warnings\": [], \"page\": { \"current\": 10, \"total_pages\": 92, \"total_results\": 91035, \"size\": 1000 }, \"request_id\": \"39684c716fe14725a70406a1a71789e4\" }, \"results\": [...] <--- 1000 results here } Working as expected and showing all 91035 docs in the index and that there are a total of 92 pages. But the result of the 11th request: { \"meta\": { \"warnings\": [], \"page\": { \"current\": 11, \"total_pages\": 0, \"total_results\": 0, \"size\": 1000 }, \"request_id\": \"39684c716fe14725a70406a1a71789e4\" }, \"results\": [] <--- 0 results here } Suddenly it indicates that there are no docs found at all The documentation says that search-request should support up to 1000 in page size and up to 500 pages, but it seems like it supports max 10 pages when page-size is 1000. Or is there some setting I need to change to support more result-pages? Or is there some other way I can request ids of all docs in the index? Any help would be appreciated",
    "website_area": "discuss"
  },
  {
    "id": "d5bf4e7d-0365-4303-afd6-e2298921630e",
    "url": "https://discuss.elastic.co/t/about-the-enterprise-search-category/180081",
    "title": "About the Enterprise Search category",
    "category": [
      "Enterprise Search"
    ],
    "author": "warkolm",
    "date": "May 7, 2019, 10:48pm",
    "body": "Elastic Enterprise Search is a consumer grade enterprise search solution. It provides enjoyable, intuitive, high volume search across your entire organization. It's a fast, scalable, and relevant search bar for your everyday work life. And it's all Elastic under the hood.",
    "website_area": "discuss"
  },
  {
    "id": "ee80676d-0ff1-4f85-a764-b1141508a856",
    "url": "https://discuss.elastic.co/t/v7-5-tika-sax-error-property-com-ctc-wstx-maxentitycount-is-not-supported/215212",
    "title": "V7.5 Tika SAX error: Property com.ctc.wstx.maxEntityCount is not supported",
    "category": [
      "Enterprise Search"
    ],
    "author": "sgwillett",
    "date": "January 15, 2020, 9:04pm",
    "body": "I am getting this error. Pops up on the regular. Any ideas on a cause or what is going on? connectors.1 | Jan 15, 2020 3:57:14 PM org.apache.tika.utils.XMLReaderUtils trySetStaxSecurityManager connectors.1 | WARNING: SAX Security Manager could not be setup [log suppressed for 5 minutes] connectors.1 | java.lang.IllegalArgumentException: Property com.ctc.wstx.maxEntityCount is not supported",
    "website_area": "discuss"
  },
  {
    "id": "7172c1f6-62f4-43b3-8fa4-6df90b64d081",
    "url": "https://discuss.elastic.co/t/free-on-premise-option-after-release/210252",
    "title": "Free on premise option after release?",
    "category": [
      "Enterprise Search"
    ],
    "author": "Michael_C",
    "date": "December 2, 2019, 10:00pm December 5, 2019, 8:54am December 6, 2019, 4:10am",
    "body": "Similar to Elastic and APM - will there be a free, downloadable, on-premise option for Enterprise Search? Or what roughly might the feature breakdown by cost be? We currently use Searchblox but it continues to have rough edges. We love Kibana already and think Elastic overall is headed in an excellent direction. Having logs, metrics, apm and search within a unified and consistent system would be a dream.",
    "website_area": "discuss"
  },
  {
    "id": "24df1905-1706-43ab-8747-ea2f3a409c83",
    "url": "https://discuss.elastic.co/t/pricing-and-deployment-options/210652",
    "title": "Pricing and deployment options",
    "category": [
      "Enterprise Search"
    ],
    "author": "MichelZ",
    "date": "December 5, 2019, 8:59am",
    "body": "Do we already know what options we'll have to deploy this? Will it be part of the Elastic Cloud offering for free, or as a add-on with additional cost? Will there be a free version? Currently, a Premium license is required, will this be the requirement as well once this is GA? The product looks interesting, but without more details on the pricing/deployment front we're not going to invest the time to even test this",
    "website_area": "discuss"
  },
  {
    "id": "fbf1b6c6-4d0f-4cd5-b242-269ac17a45b9",
    "url": "https://discuss.elastic.co/t/what-is-ga-for-enterprise-search/200986",
    "title": "What is GA for Enterprise Search?",
    "category": [
      "Enterprise Search"
    ],
    "author": "Pierre_ALLYNDREE",
    "date": "September 25, 2019, 7:51am September 25, 2019, 3:17pm December 5, 2019, 8:56am",
    "body": "When is GA ? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "6492edf2-f5f1-4df9-8692-ae33e939c6ad",
    "url": "https://discuss.elastic.co/t/is-the-confluence-cloud-documentation-correct/208547",
    "title": "Is the Confluence Cloud documentation correct?",
    "category": [
      "Enterprise Search"
    ],
    "author": "jporter",
    "date": "November 19, 2019, 4:49pm",
    "body": "I was following the documentation here Swiftype Confluence Cloud Connector Guide | Swiftype Documentation Learn how to get the most out of Swiftype Everything goes as described until I click 'add' on the section Click **Add** under the Confluence source. First, you'll need to click **I understand** . Why the hurdle? We want to make it clear that you're adding a *public source* . (this is on the page with url http://localhost:3002/ent/org/sources/add#/ ) The response to this is a server error status 500. Following the logs within the terminal hosting enterprise search doesn't yield anything obvious except that there was an error with the get request which can be replicated with - the stacktrace was then followed through but it's hard to say without seeing the code. Happy to look into attaching logs if that would help: curl localhost:3002/ent/org/sources/confluence_cloud/new -u enterprise_search Important to note that I have tried the custom API and dropbox and these added fine. Any help appreciated, even if just a confirmation that this works/doesn't work for someone else Thanks in advance",
    "website_area": "discuss"
  },
  {
    "id": "7c3c0f91-99dc-44e3-a843-8f1c0b573856",
    "url": "https://discuss.elastic.co/t/beta-3-search-inside-pptx-and-images/207133",
    "title": "Beta 3 Search Inside .PPTX and Images",
    "category": [
      "Enterprise Search"
    ],
    "author": "sgwillett",
    "date": "November 8, 2019, 3:06pm",
    "body": "Beta 3 is up and running well. It is indexing inside of everything I expect except .ppt or .pptx and images. Images I understand as that requires OCR. Are the .ppt and .pptx known or am I missing a switch? OSX Mojave. Using ES 7.4 with platinum license. Thanks! -Steve",
    "website_area": "discuss"
  },
  {
    "id": "8f957462-66e3-4549-bbb6-06f084bd8acb",
    "url": "https://discuss.elastic.co/t/any-solution/204984",
    "title": "Any solution?",
    "category": [
      "Enterprise Search"
    ],
    "author": "pbona",
    "date": "October 24, 2019, 1:59am October 24, 2019, 2:58am October 24, 2019, 2:58am",
    "body": "[root@elastic ~]# auditbeat setup --dashboards Loading dashboards (Kibana must be running and reachable) Exiting: Failed to import dashboard: Failed to load directory /usr/share/auditbeat/kibana/7/dashboard: error loading /usr/share/auditbeat/kibana/7/dashboard/auditbeat-file-integrity.json: index [.kibana_2] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];. Response: {\"objects\":[{\"id\":\"AV0tVcg6g1PYniApZa-v-ecs\",\"type\":\"visualization\",\"error\":{\"message\":\"index [.kibana_2] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];\"}},{\"id\":\"AV0tV05vg1PYniApZbA2-ecs\",\"type\":\"visualization\",\"error\":{\"message\":\"... (truncated) strong text",
    "website_area": "discuss"
  },
  {
    "id": "8bac4236-9ff2-4f20-a42b-8e7b4fa11a0b",
    "url": "https://discuss.elastic.co/t/announcement-elastic-enterprise-search-beta-3-released/204088",
    "title": "[ANNOUNCEMENT] Elastic Enterprise Search Beta 3 Released",
    "category": [
      "Enterprise Search"
    ],
    "author": "goodroot",
    "date": "October 17, 2019, 3:22pm October 17, 2019, 3:22pm",
    "body": "It's live! Read more about it. Download it.",
    "website_area": "discuss"
  },
  {
    "id": "fa8e91a9-10b8-41f8-8ff0-a396d586f529",
    "url": "https://discuss.elastic.co/t/search-within-the-body-of-documents/183764",
    "title": "Search Within the Body of documents",
    "category": [
      "Enterprise Search"
    ],
    "author": "sgwillett",
    "date": "May 31, 2019, 4:26pm May 31, 2019, 4:29pm May 31, 2019, 4:45pm September 9, 2019, 4:09am September 9, 2019, 4:43pm September 11, 2019, 10:10am September 24, 2019, 5:24pm September 25, 2019, 7:49am September 25, 2019, 2:49pm September 25, 2019, 3:02pm",
    "body": "This version doesn't seem to be able to access the content of the body. Is the connector to dropbox or a configuration within elasticsearch necessary?",
    "website_area": "discuss"
  },
  {
    "id": "2efbe94f-12fc-4151-b201-089f47362f83",
    "url": "https://discuss.elastic.co/t/not-able-to-search-documents-in-custom-content-source-in-enterprise-search-beta-2/200828",
    "title": "Not able to search documents in custom Content Source in Enterprise Search Beta 2",
    "category": [
      "Enterprise Search"
    ],
    "author": "SriAkash",
    "date": "September 24, 2019, 9:18am",
    "body": "I' able to search documents that have been indexed via connectors like Jira and Confluence. But unable to search documents indexed via Custom API. Pasting the content of enterprise-search.log : [action_controller][INFO]: [9ba5fa63-3745-43e1-9d57-9e02fe1e3f85] Parameters: {\"q\"=>\"Sample doc\", \"host\"=>\"xx.xxx.xxx.xx\", \"protocol\"=>\"http\"} [action_controller][INFO]: [9ba5fa63-3745-43e1-9d57-9e02fe1e3f85] Completed 200 OK in 318ms (Views: 0.3ms) [rails][ERROR]: [da767131-c4da-4ec4-8421-690a13696499] None of 0 processors could process the event: {\"info\":{\"datacenter\":\"local\",\"node_name\":\"xx.xxx.xxx.xx\",\"event_id\":\"5d89d1b5dcc84dcdabf2b6b1\",\"timestamp\":\"2019-09-24T08:20:05Z\",\"client_platform\":\"browser\",\"content_source_ids\":[\"5d7b3816dcc84df5454a808d\",\"5d7b9c11dcc84daa0e4a809e\",\"5d7ba453dcc84db96c4a80c0\",\"5d888922dcc84dcaa8f2b683\",\"5d88adb3dcc84d0083f2b69b\"],\"federated\":false,\"grouped\":true,\"organization_id\":\"5d728965dcc84d0793f2ed8f\",\"query\":\"Sample doc\",\"sort_field\":{\"all\":\"_score\"},\"total_result_count\":0},\"type\":\"frito_pie_organization_query\"} [action_controller][INFO]: [da767131-c4da-4ec4-8421-690a13696499] Completed 200 OK in 931ms (Views: 0.4ms)",
    "website_area": "discuss"
  },
  {
    "id": "a74d40ab-d526-4270-ba23-035481e87c8a",
    "url": "https://discuss.elastic.co/t/enterprise-search-connect-and-search-my-index-in-elasticsearch/200106",
    "title": "Enterprise Search connect and search my index in elasticsearch",
    "category": [
      "Enterprise Search"
    ],
    "author": "111207",
    "date": "September 19, 2019, 3:53am September 19, 2019, 3:24pm",
    "body": "I have old Index And I want to connect with Enterprise search.Please suggest how to connect to me. Or is there another way to suggest me",
    "website_area": "discuss"
  },
  {
    "id": "dd259db8-edf4-48bd-8604-ac988a027f92",
    "url": "https://discuss.elastic.co/t/cannot-get-enterprise-search-beta1-to-work/180584",
    "title": "Cannot Get Enterprise Search Beta1 to work",
    "category": [
      "Enterprise Search"
    ],
    "author": "christopher.farmer",
    "date": "May 10, 2019, 3:27pm May 10, 2019, 4:04pm May 10, 2019, 3:58pm May 10, 2019, 3:59pm May 10, 2019, 4:01pm May 10, 2019, 4:15pm May 10, 2019, 5:54pm May 10, 2019, 6:30pm May 10, 2019, 7:01pm May 10, 2019, 8:12pm May 10, 2019, 10:25pm May 11, 2019, 1:25am May 11, 2019, 3:05am May 13, 2019, 2:08pm May 13, 2019, 7:21pm May 14, 2019, 8:03am May 14, 2019, 12:55pm May 17, 2019, 3:33pm June 12, 2019, 6:39pm August 8, 2019, 8:07pm",
    "body": "I have a clean build of Fedora/ELK running with Oracle 8 JDK, but cannot get EES to work - getting the following; Found java executable in PATH Java version: 1.8.0_211 Starting the following Elastic Enterprise Search stack components: An application server A pool of background workers A pool of connectors A filebeat instance for indexing logs forego | starting app-server.1 on port 5000 forego | starting background-worker.1 on port 5100 forego | starting filebeat.1 on port 5300 forego | starting connectors.1 on port 5600 background-worker.1 | ERROR: org.jruby.exceptions.SystemExit: (SystemExit) exit app-server.1 | app-server.1 | -------------------------------------------------------------------------------- app-server.1 | app-server.1 | Invalid config file (/usr/downloads/enterprisesearch-0.1.0-beta1/config/enterprise_search.yml): app-server.1 | The setting '#/' contains additional properties [\"listen_host\", \"listen_port\"] outside of the schema when none are allowed app-server.1 | app-server.1 | -------------------------------------------------------------------------------- app-server.1 | app-server.1 | ERROR: org.jruby.exceptions.SystemExit: (SystemExit) exit forego | sending SIGTERM to app-server.1 forego | sending SIGTERM to connectors.1 forego | sending SIGTERM to filebeat.1 connectors.1 | ERROR: org.jruby.exceptions.LoadError: (LoadError) load error: /tmp/jruby7238240652883867351extract/frito_togo/lib/frito_togo -- java.lang.NoClassDefFoundError: jnr/constants/platform/linux/Errno$StringTable filebeat.1 | ERROR: org.jruby.exceptions.LoadError: (LoadError) load error: /tmp/jruby9169685294815892085extract/config/application -- java.lang.NoClassDefFoundError: org/jruby/RubyEnumerable$57 Two things; I set the listen address and port... I also have not yet set up security in the ELK Trial. (I will set up security just to rule it out) Help! (Thanks!)",
    "website_area": "discuss"
  },
  {
    "id": "2609c4a6-1da2-4976-a585-d1a8d38fca90",
    "url": "https://discuss.elastic.co/t/linux-osx-zip-missing-simlinks/180411",
    "title": "Linux / OSX zip missing simlinks",
    "category": [
      "Enterprise Search"
    ],
    "author": "sgwillett",
    "date": "May 9, 2019, 4:47pm May 9, 2019, 8:11pm",
    "body": "When running enterprise search the config doesn't seen right BIN_DIR=$(dirname \"$0\") BIN_DIR=$(realpath \"$BIN_DIR\") export APP_ROOT=\"(cd \"(dirname \"$BIN_DIR\")\"; pwd)\" CONFIG_DIR=\"$APP_ROOT/config\" LIB_DIR=\"$APP_ROOT/lib\" This config causes the system to look in the bin directory for lib and filebeat I had to create simlinks or copy the lib and filebeat directories to bin. I don't believe this is the desired state.",
    "website_area": "discuss"
  },
  {
    "id": "77fdafd4-e517-41da-947b-b3391a2db9a3",
    "url": "https://discuss.elastic.co/t/preview-images-for-indexed-documents/180404",
    "title": "Preview Images for indexed documents",
    "category": [
      "Enterprise Search"
    ],
    "author": "sgwillett",
    "date": "May 9, 2019, 4:05pm May 9, 2019, 4:31pm",
    "body": "I see the previews on your documentation and screen grabs. But I don't see any previews in my installation. Am I missing something in the configuration?",
    "website_area": "discuss"
  },
  {
    "id": "a9f2b83d-a60d-4eb8-b1a4-0129201bcf9a",
    "url": "https://discuss.elastic.co/t/about-the-site-search-category/159241",
    "title": "About the Site Search category",
    "category": [
      "Site Search"
    ],
    "author": "warkolm",
    "date": "December 3, 2018, 11:12pm",
    "body": "Site Search (powered by Swiftype) provides all the tools you need to build a powerful search experience for your website, without the learning curve. All that, at scale, backed by Elasticsearch.",
    "website_area": "discuss"
  },
  {
    "id": "fd0d3971-a918-4c48-a0db-1c28bedb9cb7",
    "url": "https://discuss.elastic.co/t/change-order-to-return-web-pages-first/216090",
    "title": "Change order to return web pages first",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "January 22, 2020, 3:58pm",
    "body": "HI, is there a way to change the weight (or some other method) so that all web pages typs get a higher score? thanks",
    "website_area": "discuss"
  },
  {
    "id": "dfbf3b2e-5418-4e79-893d-64249f328cf9",
    "url": "https://discuss.elastic.co/t/implementing-elastic-site-search-in-place-of-searchblox/213994",
    "title": "Implementing Elastic Site search in place of SearchBlox",
    "category": [
      "Site Search"
    ],
    "author": "Yogesh_Chandra",
    "date": "January 7, 2020, 7:20am January 7, 2020, 6:33pm",
    "body": "Hi, We are currently using searchblox to crawl our website and Alfresco (CMS), this also crawls the metadata of documents and presents the results. Now we are planning to move to elastic site search and wanted to understand if same crawling is feasible with vanilla elastic search.",
    "website_area": "discuss"
  },
  {
    "id": "ae480014-d188-4d64-abab-4d9ca5bf6e75",
    "url": "https://discuss.elastic.co/t/crawler-ip-address-or-block/213756",
    "title": "Crawler IP Address or Block",
    "category": [
      "Site Search"
    ],
    "author": "Ronno",
    "date": "January 3, 2020, 10:41pm",
    "body": "Hi, my site is still in development and public traffic is blocked by default. What is your crawler's IP block so I can whitelist you?",
    "website_area": "discuss"
  },
  {
    "id": "57f9f1b4-0b7d-4fe8-9567-ae77c73aeea1",
    "url": "https://discuss.elastic.co/t/site-search-engine-not-supporting-autocompletesuggestions/211483",
    "title": "Site Search Engine not supporting autocompleteSuggestions",
    "category": [
      "Site Search"
    ],
    "author": "sarrame",
    "date": "December 11, 2019, 11:07pm December 19, 2019, 6:11pm December 19, 2019, 6:25pm January 16, 2020, 6:25pm",
    "body": "Hi All, I have code implemented with react elastic search ui component \"SearchBox\" to have autocompleteSuggestions. With documentation seems this should work for all engine types. But I am stuck on achieving this feature, can anyone help on this !! Config : export const searchEngineConfig = { apiConnector: searchEngineConnector, autocompleteQuery: { suggestions: { types: { documents: { fields: ['title'] } }, size: 4 } } } };",
    "website_area": "discuss"
  },
  {
    "id": "90855286-0a5d-4c40-aec5-25da9b503ab8",
    "url": "https://discuss.elastic.co/t/highlight-fields-not-matching-multiple-words/211383",
    "title": "Highlight fields not matching multiple words",
    "category": [
      "Site Search"
    ],
    "author": "Dileep_Ratnayake",
    "date": "December 10, 2019, 10:01pm December 11, 2019, 10:32am January 8, 2020, 10:32am",
    "body": "I'm trying to get swiftype to highlight search results in a multiple keywords search. If a result body and/or title has atleast one of the keywords matched, they need to be highlighted. I also noticed that if a result has matching keywords, they are not highlighted either. It only highlights if the search is only one keyword which leads me to believe that multiple keyword highlights are ignored? data: { engine_key : 'EngineKeyGoesHere', q: keyword, per_page: 10, page: pageNum, spelling: \"strict\", highlight_fields: {'page': {'title': {'size': maxStrLen, 'fallback': true },'body': {'size': maxStrLen, 'fallback': true }}}, }, I have verified that there are matching keywords with in the maxStrLen limit. Any idea on what I can do get matching keywords (individual) to be highlighted in a multiple keyword search?",
    "website_area": "discuss"
  },
  {
    "id": "9267f94d-9e17-4f4a-916f-124a0a82e620",
    "url": "https://discuss.elastic.co/t/filtering-by-nonexistent-value-returns-all-results/207752",
    "title": "Filtering by nonexistent value returns all results",
    "category": [
      "Site Search"
    ],
    "author": "Ryan_Peters",
    "date": "November 13, 2019, 5:11pm November 15, 2019, 12:02pm December 13, 2019, 12:02pm",
    "body": "My query: { engine_key: 'ENGINE_KEY', q: '', page: 1, per_page: 12, sort_field: { page: 'published_date' }, filters: { page: { search_category: 'How-To Tips & Tricks' } } } If there are no pages with this search_category tag on the page, it will return all results. If there is 1 page tagged with this search_category, it will return that one page. I would expect no results if no pages are tagged with this category. Is it possible to run a query where no results are returned on a filter that has no matches?",
    "website_area": "discuss"
  },
  {
    "id": "224cdb82-38ad-4852-95df-c199ac0527a8",
    "url": "https://discuss.elastic.co/t/search-ui-styles-are-not-read-in-production-build-of-gatsby/206501",
    "title": "Search UI styles are not read in production build of Gatsby",
    "category": [
      "Site Search"
    ],
    "author": "sarrame",
    "date": "November 5, 2019, 1:01am November 5, 2019, 1:33pm November 5, 2019, 2:16pm November 5, 2019, 2:38pm November 6, 2019, 1:26pm November 6, 2019, 7:38pm November 12, 2019, 2:18pm November 12, 2019, 5:24pm November 12, 2019, 5:52pm November 12, 2019, 6:06pm November 14, 2019, 8:27pm December 12, 2019, 8:28pm",
    "body": "Hi Team, As i am actively building a Site Search with SearchUI and did a production build of Gatsby, interestingly Paging is the only component i used directly with built-in styles. I could see default pagination in development code like <1 2 > . but in Gatsby production build there are no styles applied and i see unordered list like below 1 2 Implementation ways : import { Paging } from '@elastic/react-search-ui'; import '@elastic/react-search-ui-views/lib/styles/styles.css'; And simply imported in the render component. Am not sure did i miss anything in implementation ., could any one help me on this.",
    "website_area": "discuss"
  },
  {
    "id": "db3d1e7f-6e64-4aea-a8d1-173259fdbb1f",
    "url": "https://discuss.elastic.co/t/cant-configure-search-results-to-show-up/207401",
    "title": "Can't configure search results to show up",
    "category": [
      "Site Search"
    ],
    "author": "Cosmin_Mazilu",
    "date": "November 11, 2019, 5:31pm November 13, 2019, 12:48am November 13, 2019, 12:44pm December 11, 2019, 12:44pm",
    "body": "Hello, Im trying to configure site search for our company. Ive set up the search engine which has crawled the site and seems to work fine (checked the preview and its OK). I cant seem to get the search results to show up based on the instructions provided. For instance, if I create the following htm page, the search results wont load. <!DOCTYPE html> <html> <body> <script type=\"text/javascript\"> (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){ (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t); e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e); })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st'); _st('install','ECV_azmEAz1k44a7uf-y','2.0.0'); </script> <input type=\"text\" class=\"st-default-search-input\"> <div class=\"st-search-container\"></div> </body> </html> What am I doing wrong?",
    "website_area": "discuss"
  },
  {
    "id": "72e5669f-bb5b-42cd-b614-61c11de6a769",
    "url": "https://discuss.elastic.co/t/crawler-not-reading-class-swifttype/206095",
    "title": "Crawler not reading class \"swifttype\"",
    "category": [
      "Site Search"
    ],
    "author": "sarrame",
    "date": "October 31, 2019, 7:55pm October 31, 2019, 8:01pm October 31, 2019, 8:02pm October 31, 2019, 8:05pm October 31, 2019, 8:17pm October 31, 2019, 8:50pm November 4, 2019, 10:10pm November 5, 2019, 10:27pm November 5, 2019, 11:34pm November 8, 2019, 9:52pm November 8, 2019, 10:45pm December 6, 2019, 10:45pm",
    "body": "Hi Team, As per the documentation meta tags with class \"swifttype\" will be read by Swiftbot, but it is not working . I added this class to meta tag name \"description\" like below and i don't see this data in the document created after crawling. Can anyone help on this ? Thanks !",
    "website_area": "discuss"
  },
  {
    "id": "f5d33e2f-9795-433b-a246-93d9e8001862",
    "url": "https://discuss.elastic.co/t/customize-style-for-paging-component-of-search-ui/205542",
    "title": "Customize style for paging component of Search UI",
    "category": [
      "Site Search"
    ],
    "author": "sarrame",
    "date": "October 28, 2019, 8:03pm October 29, 2019, 6:52pm October 29, 2019, 7:11pm October 29, 2019, 7:41pm October 31, 2019, 7:57pm December 3, 2019, 12:34am",
    "body": "Hello, I am using Site Search with SearchUI and I want to create a custom styles for react-search-ui component. I went over the documentation https://github.com/elastic/search-ui/blob/master/ADVANCED.md#component-views-and-html this looks like letting you modify the onChange, but i looking to have custom styling. for example, I want different styles and different arrows to be used. I am using styled-components. Please help !",
    "website_area": "discuss"
  },
  {
    "id": "838a5ae7-bfb5-49ff-be1b-ddc50c4f991f",
    "url": "https://discuss.elastic.co/t/autocomplete-for-sitesearch-in-searchui/205283",
    "title": "Autocomplete for siteSearch in SearchUI",
    "category": [
      "Site Search"
    ],
    "author": "sarrame",
    "date": "October 25, 2019, 2:42pm October 25, 2019, 4:50pm October 25, 2019, 5:44pm October 28, 2019, 8:02pm October 29, 2019, 11:39am November 26, 2019, 11:39am",
    "body": "I am trying to fix autocomplete for the SiteSearch Engine with SearchUI. I am unable to fix autocomplete with headless core components(my own build component) whereas it works for \"SearchBox\" SearchUI built-in component. I tried to add \"beforeAutocompleteResultsCall\" but not working. beforeAutocompleteResultsCall: (options, next) => next({ ...options, group: { field: 'title' } }) Please provide help !",
    "website_area": "discuss"
  },
  {
    "id": "ee4c191e-338b-4106-88ba-a27ffb26ed72",
    "url": "https://discuss.elastic.co/t/bulk-upload-documents-to-site-search-from-a-json-file/205147",
    "title": "Bulk upload documents to Site Search from a JSON file",
    "category": [
      "Site Search"
    ],
    "author": "JasonStoltz",
    "date": "October 24, 2019, 7:45pm November 21, 2019, 7:41pm",
    "body": "I just wanted to share some code I helped put together for a customer. They had a JSON document of records that they wanted to upload in bulk to Site Search via our API. books.json [ { \"id\": \"1\", \"title\": \"Tom Sawyer\", \"pages\": 200 }, { \"id\": \"2\", \"title\": \"Catcher in the Rye\", \"pages\": 300 } ] The first challenge was converting their JSON file to a format that Site Search requires. We came up with the following: convert.js var FILE_NAME = \"books.json\"; var isValidNumber = num => { return !isNaN(num); }; var fs = require(\"fs\"); var obj = JSON.parse(fs.readFileSync(FILE_NAME, \"utf8\")); var convert = o => { return Object.entries(o).reduce( (p, [k, v]) => { if (k === \"id\") { return { external_id: v, ...p }; } return { ...p, fields: p.fields.concat({ name: k, value: v, type: isValidNumber(v) ? \"number\" : \"string\" }) }; }, { fields: [] } ); }; var updated = obj.map(convert); fs.writeFile(\"updated.json\", JSON.stringify(updated), err => { if (err) throw err; }); Assuming that convert.js is located in the same directory as books.json, you would run: node convert.js This produces a file in the correct format for Site Search: updated.json [ { \"external_id\": \"1\", \"fields\": [ { \"name\": \"title\", \"value\": \"Tom Sawyer\", \"type\": \"string\" }, { \"name\": \"pages\", \"value\": 200, \"type\": \"number\" } ] }, { \"external_id\": \"2\", \"fields\": [ { \"name\": \"title\", \"value\": \"Catcher in the Rye\", \"type\": \"string\" }, { \"name\": \"pages\", \"value\": 300, \"type\": \"number\" } ] } ] We then created another file in the same directory, and configured our API key, document type, and engine name: upload.js var ENGINE_NAME = \"book-engine\"; var DOCUMENT_TYPE = \"books\"; var API_KEY = \"{YOUR_KEY_HERE}\"; var fs = require(\"fs\"); var json = JSON.parse(fs.readFileSync(\"updated.json\", \"utf8\")); var SiteSearchClient = require(\"@elastic/site-search-node\"); var client = new SiteSearchClient({ apiKey: API_KEY }); client.documents.batchCreate( { engine: ENGINE_NAME, documentType: DOCUMENT_TYPE }, json, 100, (e, o) => { console.log(e); console.log(o); } ); We then installed the site search node client, and then ran our script: npm install @elastic/site-search-node node upload.js And voil, our documents have now been indexed.",
    "website_area": "discuss"
  },
  {
    "id": "04d1770c-54a5-4eab-9040-1b562ab62563",
    "url": "https://discuss.elastic.co/t/rate-limit-exceeded/203867",
    "title": "Rate limit exceeded",
    "category": [
      "Site Search"
    ],
    "author": "Patrick_Simard",
    "date": "October 16, 2019, 3:00pm October 22, 2019, 8:02pm November 19, 2019, 8:02pm",
    "body": "I currently have a demo account. I am trying to make a POC using SwifType for my employer. We have a very big database that is indexec every 1h and creates a JSON file. I thought the integration with Elastic would be very easy considering it's only a mater of sending the string when it's generated. I used PHP Curl and got a connection to the API. The code sends out part of the data and then freaks out with a \"Rate limit exceeded\" error. How can I manage around that error and get the full JSON indexed? My code looks like this at the moment: // SENDING DATA TO ELASTIC SEARCH $arr = array_change_key_case($arr, CASE_LOWER); // Keys to lower case $arrlist = array_chunk($arr,100); // Split to chunks of 100 foreach($arrlist as $key=>$arr){ $json = json_encode($arr); // Making the JSON string from the array $ch = curl_init('https://host-***.api.swiftype.com/api/as/v1/engines/***/documents'); curl_setopt($ch, CURLOPT_POST, 1); curl_setopt($ch, CURLOPT_POSTFIELDS, $json); curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1); curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false); $headers = array(); $headers[] = 'Content-Type: application/json'; $headers[] = 'Authorization: Bearer private-***'; curl_setopt($ch, CURLOPT_HTTPHEADER, $headers); $result = curl_exec($ch); if (curl_errno($ch)) { echo 'Error:' . curl_error($ch); } curl_close($ch); echo $result.\"<hr>\"; } Also, considering this code is going to be indexed every hour, if I am sending the same data over and over, will it UPDATE the previous one or will it duplicate it? If so, how can I manage that?",
    "website_area": "discuss"
  },
  {
    "id": "04457364-93db-4568-9cd9-bf50de6ad9d9",
    "url": "https://discuss.elastic.co/t/filtering-from-default-javascript/203265",
    "title": "Filtering from default Javascript",
    "category": [
      "Site Search"
    ],
    "author": "DeDev",
    "date": "October 11, 2019, 3:56pm October 22, 2019, 7:57pm November 19, 2019, 7:57pm",
    "body": "Is there any way to filter results using the Javascript snippet below? Instead of creating a lot of files for my project. (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){(w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t); e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e); })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');_st('install','xxx-xxxx-xxx','2.0.0');",
    "website_area": "discuss"
  },
  {
    "id": "05287c91-f430-4bd8-b12c-4ae450c29801",
    "url": "https://discuss.elastic.co/t/are-document-types-still-a-thing/202382",
    "title": "Are document_types still a thing?",
    "category": [
      "Site Search"
    ],
    "author": "webinc",
    "date": "October 7, 2019, 7:02am October 7, 2019, 7:04am November 4, 2019, 7:04am",
    "body": "I'm just starting with swiftype and elastic search (are they really the same now?). And I followed the instructions here to add document types to my engine, so I could bulk-upload documents that way. Its not clear to me that it worked however, and I don't see any references to 'document_types' in my swiftype dashboard. Neither can I get the api call to work showing me document_types in my engine, and nor can I find a way to add documents to a particular document_type using the Documents API. So either I'm missing something, or doing it wrong (very likely) or is the current documentation at odds with the old? Something which makes me wonder that is that I can't get either of my swiftype api keys (public or private) to work with the call to display existing document_types: curl -X GET 'https://api.swiftype.com/api/v1/engines/bookstore/document_types.json?auth_token=YOUR_API_KEY' This is documented here. Any help and advice would be much welcomed. I like the idea of document_types for my data, but I'm unsure if they exist... Thanks",
    "website_area": "discuss"
  },
  {
    "id": "ac0e39e2-a842-44c8-a496-4a05edc58758",
    "url": "https://discuss.elastic.co/t/filter-fields-on-list-documents/201893",
    "title": "Filter fields on list documents?",
    "category": [
      "Site Search"
    ],
    "author": "Sophistifunk",
    "date": "October 2, 2019, 4:56am October 3, 2019, 3:04am October 3, 2019, 3:04am October 31, 2019, 3:04am",
    "body": "Is it possible to filter the list of returned fields when GETting ..../documents.json? I need a way to get a list of documents that are in the index without having to download the complete content of every document.",
    "website_area": "discuss"
  },
  {
    "id": "c0b73859-f774-4d46-b0e2-7434ac4e5caf",
    "url": "https://discuss.elastic.co/t/how-to-install-two-search-engines-in-one-site/200167",
    "title": "How to Install two search engines in one site",
    "category": [
      "Site Search"
    ],
    "author": "Carlos_Lozada",
    "date": "September 19, 2019, 10:44am September 19, 2019, 3:25pm September 20, 2019, 9:04am October 18, 2019, 9:04am",
    "body": "Hello Guys, I have created two search engines on site-search each specific to the corresponding sitemaps according to the language of the search Engine Search for Spanish looking only at the sitemaps for Spanish Engine Search for English looking only at the sitemaps for English This has been done correctly, now the problem resides to install it on my e-commerce I have tried using both scripts given but I got the error message to install only one So I did it in this way <!-- Swiftype --> <script type=\"text/javascript\"> (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){ (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t); e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e); })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st'); if (window.location.href.indexOf('/es') !== -1) { _st('install', '<CODE-ES>', '2.0.0'); } else if (window.location.href.indexOf('/en-ie') !== -1) { _st('install','<CODE-EN>', '2.0.0'); } </script> <!-- End Swiftype --> Now I only get the search result in one of the two engines added and according to the Url were I'm at For example: If I'm in /es give me results for Spanish (this is Ok as my main language for the store) if I'm/en-ie give me results for English: Thanks for your time",
    "website_area": "discuss"
  },
  {
    "id": "99859f8b-5110-4a0c-9a2f-2c5087e05d14",
    "url": "https://discuss.elastic.co/t/to-whitelist-swiftype-search/192659",
    "title": "To Whitelist Swiftype Search",
    "category": [
      "Site Search"
    ],
    "author": "khaingsh",
    "date": "July 29, 2019, 9:33am July 29, 2019, 3:38pm July 30, 2019, 2:11am November 19, 2019, 1:51pm July 31, 2019, 3:45pm August 1, 2019, 6:28am August 1, 2019, 1:56pm August 21, 2019, 5:20am September 18, 2019, 5:24am",
    "body": "Hi, Swifttype search is set up for the website and it is working fine at the public but in our office network, it is being blocked and showing 403. So, I need to whitelist swifttype at our office network to make it works. May I know what url/service/IP need to be whitelisted? Regards, Khaing Su Hlaing",
    "website_area": "discuss"
  },
  {
    "id": "78905361-ae3b-41a0-87eb-b7aec5d7af27",
    "url": "https://discuss.elastic.co/t/how-to-get-same-results-in-autocomplete-search-results-in-swiftype/191721",
    "title": "How to get same results in autocomplete & search results in swiftype?",
    "category": [
      "Site Search"
    ],
    "author": "nabtron",
    "date": "July 23, 2019, 12:35am August 20, 2019, 12:35am",
    "body": "I am using swiftype for WordPress and can customize it as needed. The query basically is, how to achieve the same results ranking in autocomplete (when the query is complete) and then we press enter and then get the results on the page? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "6461bdb2-eb6d-4e38-972b-51aae8c09904",
    "url": "https://discuss.elastic.co/t/search-result-description-issue/190123",
    "title": "Search result description issue",
    "category": [
      "Site Search"
    ],
    "author": "bluejay0018",
    "date": "July 11, 2019, 11:43pm July 12, 2019, 12:05am July 12, 2019, 12:45am July 12, 2019, 12:57am July 12, 2019, 2:10pm July 12, 2019, 3:12pm August 9, 2019, 3:12pm",
    "body": "Search result description is showing \"Loading error messages\" which are part of the page. This is happening even after adding the \"data-swiftype-index=false\" to those tags in the page. See screenshot below, any recommendation will be great? We are using SFDC for our communities portal. image.png1244234 58.9 KB",
    "website_area": "discuss"
  },
  {
    "id": "b14d42a2-8ced-4ff3-b548-1847f426a8e2",
    "url": "https://discuss.elastic.co/t/error-when-using-search-ui-and-site-search-map-is-undefined/189786",
    "title": "Error when using search ui and site search - map is undefined",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "July 10, 2019, 2:34pm July 10, 2019, 7:18pm July 11, 2019, 5:35am July 11, 2019, 12:12pm July 11, 2019, 1:32pm July 11, 2019, 1:52pm July 11, 2019, 2:10pm July 11, 2019, 2:17pm August 8, 2019, 2:26pm",
    "body": "hi im using the latest search-ui (0.12) with the example code that works fine. when changing it to use the site search connector and updating the .env file (and some other small stuff to match it) i went on to remove all the facets etc. to have the minimum needed to have a search page. i used a search engine that already scanned one of my sites (apx. 2k pages) I get this error in the page: An unexpected error occurred: Cannot read property 'map' of undefined this is the app.js: https://jsfiddle.net/viper123/7h1wx2vo/",
    "website_area": "discuss"
  },
  {
    "id": "af0a3623-40ad-4faa-a0e1-b77f01138997",
    "url": "https://discuss.elastic.co/t/error-when-trying-to-implement-jqeury-for-siteseaerch/186684",
    "title": "Error when trying to implement jqeury for siteseaerch",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "June 20, 2019, 12:30pm June 23, 2019, 2:25pm June 27, 2019, 12:48pm July 1, 2019, 7:17pm July 4, 2019, 6:03pm July 5, 2019, 5:24am July 8, 2019, 3:22pm July 9, 2019, 11:33am July 9, 2019, 7:30pm July 10, 2019, 2:05pm August 7, 2019, 2:05pm",
    "body": "Hi. im am following this official guide for jQuery Plugin Guide: Swiftype jQuery Plugin Guide | Swiftype Documentation Learn how to get the most out of Swiftype when loading the needed scripts i get an error: Uncaught TypeError: Cannot read property 'msie' of undefined at jquery.ba-hashchange.min.js:9 is this guide out of date? has anyone tried to use it? thanks",
    "website_area": "discuss"
  },
  {
    "id": "a5915fc1-4a32-4628-ab16-368d6f2f9309",
    "url": "https://discuss.elastic.co/t/more-endpoints-for-site-search-analytics-api/188785",
    "title": "More endpoints for Site Search Analytics API?",
    "category": [
      "Site Search"
    ],
    "author": "katyd",
    "date": "July 3, 2019, 5:29pm July 3, 2019, 7:19pm July 31, 2019, 7:19pm",
    "body": "We're in the process of automating our Site Search analytics to bring visibility to our team and track action items. https://swiftype.com/documentation/site-search/analytics has valuable endpoints, but there are few that we'd also like to have to streamline this process. Are there any plans to add endpoints for the following: Most commonly clicked documents from search Top searches with zero clickthroughs % of search with no results (I think we could calculate this via the Searches and Top no result queries endpoints, but would be helpful to not need to make two requests.) Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "25d3bf57-a5b0-42fc-8831-5ac98ff6ddf6",
    "url": "https://discuss.elastic.co/t/trademark-in-title-overridden/184399",
    "title": "Trademark in title overridden?",
    "category": [
      "Site Search"
    ],
    "author": "buckwebdev",
    "date": "June 5, 2019, 2:54pm June 5, 2019, 3:35pm July 3, 2019, 3:35pm",
    "body": "I'm having an issue with my trademarks () being overwritten with questions marks (?) in my titles. The display issue only occurs in the search field. My product titles are fine on WP backend and show no issue. Any advice would be appreciated.",
    "website_area": "discuss"
  },
  {
    "id": "191f2a70-676e-48f4-b94a-fb6e8edd35c1",
    "url": "https://discuss.elastic.co/t/case-sensitivity-question/183737",
    "title": "Case sensitivity question",
    "category": [
      "Site Search"
    ],
    "author": "jamesgrobertson",
    "date": "May 31, 2019, 2:30pm May 31, 2019, 4:28pm June 28, 2019, 4:28pm",
    "body": "I have a question about case sensitivity in autocomplete searches vs. in search filters. If I type \"search term\", \"Search Term\", or \"SEARCH TERM\" in the autocomplete (implemented using the Swiftype jQuery autocomplete plugin) I get the same results. I have implemented the filtering functionality of the Search API using some form fields, and if I enter the same terms as above, I only get results when the case matches what is in the index. I suspect this is because autocomplete searches are suggest queries while filters are full text queries, but I can't find where it states this in the documentation. If that is the case, is there a way to make filters work as suggest queries (i.e. case-insensitive)?",
    "website_area": "discuss"
  },
  {
    "id": "e9d9aaf2-3b3b-4eab-9592-8bc78905d52b",
    "url": "https://discuss.elastic.co/t/uncaught-typeerror-window-ga-is-not-a-function/181066",
    "title": "Uncaught TypeError: window.ga is not a function",
    "category": [
      "Site Search"
    ],
    "author": "highered",
    "date": "May 14, 2019, 8:16pm May 14, 2019, 8:25pm May 15, 2019, 2:06pm May 14, 2019, 8:50pm May 14, 2019, 9:02pm May 14, 2019, 9:27pm May 15, 2019, 2:06pm May 15, 2019, 2:08pm May 15, 2019, 2:36pm June 12, 2019, 2:36pm",
    "body": "We're trying to implement Site Search with a custom search field. However, if a user types something in the field and then hits \"return\", this error appears in the console: st.js:73 Uncaught TypeError: window.ga is not a function at Object.n.Utils.pushToGA (st.js:73) at Swiftype.QueryContext.pQueryContext.pushQueryToGA (st.js:74) at Swiftype.QueryComposer.pQueryComposer.runSearch (st.js:74) at HTMLInputElement.<anonymous> (st.js:74) at HTMLInputElement.dispatch (st.js:25) at HTMLInputElement.y.handle (st.js:24) What is this error and how do we fix it? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "72c393b1-b1ee-4a4e-8649-5befa981e5c6",
    "url": "https://discuss.elastic.co/t/presenting-two-different-type-results-separately-on-the-same-page/180895",
    "title": "Presenting two different type results separately on the same page",
    "category": [
      "Site Search"
    ],
    "author": "bobclewell",
    "date": "May 13, 2019, 9:26pm June 10, 2019, 9:31pm",
    "body": "I've got a database of two types, products and blog entries. Right now we are using Swiftype to search the full database and return all the results merged together as a single result set. I'd like to separate products, so that I can highlight them at the top of the page, with images, and then have the blog entries listed under them, in a separate part of the page, as text results only. How would I do this?",
    "website_area": "discuss"
  },
  {
    "id": "4ab6e4dc-71fa-42ff-8e68-f24954d93fec",
    "url": "https://discuss.elastic.co/t/no-documents-after-20-hours-of-crawling/179504",
    "title": "No documents after 20 hours of crawling",
    "category": [
      "Site Search"
    ],
    "author": "Hiren_Patel",
    "date": "May 3, 2019, 9:21am May 3, 2019, 5:03pm May 3, 2019, 11:29pm May 7, 2019, 6:13pm May 7, 2019, 8:08pm May 7, 2019, 8:11pm May 7, 2019, 10:22pm June 4, 2019, 10:22pm",
    "body": "I've initiated a manual crawl 20 hours ago however I see no documents in the control panel. I see a \"Swiftype is currently indexing content for domains in this search engine. Pages will become available as they are indexed.\" message, but I suspect there's some issue that I'm not seeing. I have a valid sitemap generated based on the guides provided, and I can see Swiftbot hitting our servers. Is there a problem, because I don't see what I'm doing wrong. We're on the Pro Plan and the engine key is: HApht89NAVsMfYA37u4p Kind regards, Hiren",
    "website_area": "discuss"
  },
  {
    "id": "d7daf82a-dbd8-4bca-8a27-70b06fa33c22",
    "url": "https://discuss.elastic.co/t/request-for-site-search-react-demo-demonstrating-pagination/179176",
    "title": "Request for site-search react demo demonstrating pagination",
    "category": [
      "Site Search"
    ],
    "author": "shameer-rahman",
    "date": "May 1, 2019, 7:18am May 1, 2019, 12:20pm May 29, 2019, 12:19pm",
    "body": "I'm trying to implement pagination using site-search api on my gatsby website, can you please share me the demo repo for the same like you have it for app-search: https://github.com/swiftype/app-search-demo-react",
    "website_area": "discuss"
  },
  {
    "id": "85218dec-6d07-40f4-bfc3-8ab64886d92c",
    "url": "https://discuss.elastic.co/t/australian-datacenter/176499",
    "title": "Australian Datacenter?",
    "category": [
      "Site Search"
    ],
    "author": "Kim_Pepper",
    "date": "April 11, 2019, 9:12pm April 11, 2019, 10:21pm April 30, 2019, 5:50am April 30, 2019, 3:25pm May 28, 2019, 3:25pm",
    "body": "We are evaluating Site search for clients based in Australia. Is the managed Site search (Swiftype) located in Australia? We would like to minimise latency. Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "cd6a5914-e808-4a8a-b8d4-1b33c298587d",
    "url": "https://discuss.elastic.co/t/search-response-in-a-progressive-way/177248",
    "title": "Search response in a progressive way",
    "category": [
      "Site Search"
    ],
    "author": "luka7",
    "date": "April 17, 2019, 9:13am May 15, 2019, 9:12am",
    "body": "Hi, getPayload() { let textsearch = this.textsearch.replace(/ /g,\" AND \"); return { q: textsearch, lang: this.$store.getters.getLang || 'en', engine: this.getSwiftypeEngine } } I am trying to query swiftype engine and get the response in a progressive way. It means that: When I search for: elastic I get an array of objects with 20 results and a property title starting with \"elastic\" word When I search for: elastic s I get an array of objects with 20 results and a property title starting with exactly elastic c I tried already to change the algorithm by passing AND between two keywords (that works better but not exactly as I would expect). Free-text Query Syntax Text searches support a basic subset of the standard Lucene query syntax. The supported functions are: double quoted strings, + and -, AND, OR, and NOT. Visit the Lucene documentation for more information. Do I need to pass anything additional? Is it possible to omit completely the score and return results only containing exact amount of letters?",
    "website_area": "discuss"
  },
  {
    "id": "696b6bcb-f940-4341-a943-a8e53c69cf67",
    "url": "https://discuss.elastic.co/t/only-exact-search-matches-for-quoted-phrases/176698",
    "title": "Only Exact Search Matches for Quoted Phrases",
    "category": [
      "Site Search"
    ],
    "author": "timlwhite",
    "date": "April 12, 2019, 7:35pm April 12, 2019, 8:04pm April 15, 2019, 2:47pm May 13, 2019, 2:47pm",
    "body": "Hello - I need to enable searches that return only exact matches for quoted phrases. For example \"Bank of America\" should not return results that only contain \"America\" or \"bank\" as these are common words in the search corpus. I have tried using escaped quotations, \"Bank of America\" as well as other various versions of quoting strings, and I continue to get non-exact matches. Google does this quite well, I am trying to determine how to coerce SwiftType to do the same. Thank you!",
    "website_area": "discuss"
  },
  {
    "id": "1ea9ddd3-9a83-4368-b9f2-73565f7a081d",
    "url": "https://discuss.elastic.co/t/how-to-get-metrics-of-a-single-domain/173820",
    "title": "How to get metrics of a single domain",
    "category": [
      "Site Search"
    ],
    "author": "MaryP",
    "date": "March 25, 2019, 7:11pm March 25, 2019, 7:21pm March 25, 2019, 7:27pm March 25, 2019, 10:38pm April 11, 2019, 4:19pm May 9, 2019, 4:19pm",
    "body": "When I export metrics from Swiftype, it gives me some of the data but not on the domain I need it from. We currently have five domains listed, but I only need data from Support.veeva.com domain. How do I get that?",
    "website_area": "discuss"
  },
  {
    "id": "91d5c45e-04fa-46dd-928e-a888e3fc477a",
    "url": "https://discuss.elastic.co/t/swiftype-node-cors/175025",
    "title": "Swiftype-node & CORS",
    "category": [
      "Site Search"
    ],
    "author": "katyd",
    "date": "April 2, 2019, 3:20pm April 2, 2019, 4:09pm April 2, 2019, 4:37pm April 10, 2019, 3:57pm April 2, 2019, 7:11pm April 8, 2019, 11:15pm April 9, 2019, 12:00am April 9, 2019, 12:07pm April 9, 2019, 3:31pm April 10, 2019, 12:43pm May 8, 2019, 12:43pm",
    "body": "I'm having trouble developing locally with https://github.com/swiftype/swiftype-node as I'm running into CORS errors. I know I can open an instance of Chrome with web security disabled, but I'm hoping to avoid that. Anyone have suggestions or examples?",
    "website_area": "discuss"
  },
  {
    "id": "935d1f8d-c390-48cb-a81f-d8463255fd66",
    "url": "https://discuss.elastic.co/t/multiple-range-searches-on-one-search-field-in-query/175263",
    "title": "Multiple range searches on one search field in query",
    "category": [
      "Site Search"
    ],
    "author": "ashtonlance",
    "date": "April 3, 2019, 6:26pm April 4, 2019, 3:56pm May 2, 2019, 3:56pm",
    "body": "I'm trying to run a query that on a field called bookable_date . I want to be able to return results that have a bookable_date in May and a bookable_date in July and not return results for the month of June. I'm using the first-party PHP client here: https://github.com/swiftype/swiftype-site-search-php Here's one of my approaches -- no results. $args['bookable_date'] = array( \"type\"=> \"or\", \"values\"=> array( array( \"type\"=>\"range\", \"from\"=>'2019-01-01T00:00:00+00:00', \"to\"=> '2019-01-31T00:00:00+00:00', ), array ( \"type\"=>\"range\", \"from\"=>'2019-05-01T00:00:00+00:00', \"to\"=> '2019-05-31T00:00:00+00:00', ) ) ); The $args variable is later passed to $st_args in the filters array. Which is then sent to Swiftype like this: $response = $client->search($engine , stripslashes($s), $st_args);",
    "website_area": "discuss"
  },
  {
    "id": "ea6c6696-b7c2-4d57-8541-7206ce44f085",
    "url": "https://discuss.elastic.co/t/need-explanation-of-swiftype-reports-ootb/173773",
    "title": "Need explanation of Swiftype reports OOTB",
    "category": [
      "Site Search"
    ],
    "author": "MaryP",
    "date": "March 25, 2019, 2:33pm March 25, 2019, 2:36pm March 25, 2019, 2:45pm March 25, 2019, 2:48pm March 25, 2019, 3:19pm March 25, 2019, 3:42pm March 25, 2019, 3:53pm March 25, 2019, 3:54pm March 25, 2019, 3:55pm April 22, 2019, 3:55pm",
    "body": "Is there any documentation regarding the reports that are created in Swiftype?",
    "website_area": "discuss"
  },
  {
    "id": "e7632a72-6bc3-4596-a4ba-d32a273f307a",
    "url": "https://discuss.elastic.co/t/sitemap-in-swiftype/173569",
    "title": "Sitemap in Swiftype",
    "category": [
      "Site Search"
    ],
    "author": "Amit_Jain1",
    "date": "March 22, 2019, 11:23pm March 22, 2019, 11:40pm April 19, 2019, 11:40pm",
    "body": "Hi Guys I am new to Swiftype and I have set up my first engine in the dashboard. Swiftype is complaining that its not able to find sitemap on my website https://broadcastseo.com.au when I definitely know that there is a sitemap. I even verified using an external tool - and sitemap exists here - https://broadcastseo.com.au/sitemap.xml How to get past this problem? Amit",
    "website_area": "discuss"
  },
  {
    "id": "5293b60f-9dc7-4397-8487-c1bf3249352f",
    "url": "https://discuss.elastic.co/t/site-or-app-search-indexing-issues/173306",
    "title": "Site or App Search? Indexing issues",
    "category": [
      "Site Search"
    ],
    "author": "3morrow",
    "date": "March 21, 2019, 1:05pm March 21, 2019, 1:05pm March 21, 2019, 2:26pm April 18, 2019, 2:26pm",
    "body": "Trying to enable site-wide search across a database of Shakespeare films; each film has slightly different timings for these lines, and the interactive transcripts are stored along with their corresponding video as JSONs in an S3 bucket. Lines in a script are stored as anchor tags for users to jump to a particular line and share it (eg line 606 of Macbeth has an anchor link of https://scriptspeare.co.uk/Tragedy/Macbeth/#606=line when clicked on). Search bar should display each line as a result (for example, typing in to be should bring up to be or not to be, that is the question (#1611=line). Currently stuck on a few bugs in Elastic's Site Search trial (no lines show up! photo attached), but thinking this might not be the correct format since the queries are to individual word indices, rather than the IDs of lines within a page. Highly likely that I'm missing something obvious as well; Shakespeare fans, help appreciated! https://scriptspeare.co.uk/Tragedy/Macbeth/0/ Screen Shot 2019-03-21 at 12.31.50.png2026650 34 KB",
    "website_area": "discuss"
  },
  {
    "id": "034fc092-a959-4373-b9fd-84cf4f9f8554",
    "url": "https://discuss.elastic.co/t/how-do-you-block-bots/169853",
    "title": "How do you block bots?",
    "category": [
      "Site Search"
    ],
    "author": "slhuber",
    "date": "February 28, 2019, 11:58am March 13, 2019, 8:46pm April 10, 2019, 8:45pm",
    "body": "We are starting to get a lot of bot hits in our Swiftype search and I was wondering if anyone else has had this issue and what you did to solve it? I don't see any captcha feature available and the volume of this bot traffic is messing up our query limit as well as reporting.",
    "website_area": "discuss"
  },
  {
    "id": "426426a1-d7c0-4dda-8af6-18b3c83e914b",
    "url": "https://discuss.elastic.co/t/how-do-i-sort-of-filter-based-on-score/168911",
    "title": "How do I sort of filter based on _score?",
    "category": [
      "Site Search"
    ],
    "author": "mt-at-td",
    "date": "February 18, 2019, 11:51pm February 19, 2019, 5:09pm March 12, 2019, 5:55pm April 9, 2019, 5:52pm",
    "body": "If I follow the syntax of https://swiftype.com/documentation/site-search/searching/sorting and use sort_field: {page: \"_score\"} I get the error {sort_field: {page: [\"DocumentType 'page' does not have a field '_score'.\"]}} If I try to filter with filters: {page: {_score: '!1'}} as in https://swiftype.com/documentation/site-search/searching/filtering I get the error {filters: {page: [\"DocumentType 'page' does not have a field '_score'.\"]}}",
    "website_area": "discuss"
  },
  {
    "id": "bbfc855b-6ab3-4ead-bd5b-6e85dd792e56",
    "url": "https://discuss.elastic.co/t/changing-filters-dynamically/170085",
    "title": "Changing filters dynamically",
    "category": [
      "Site Search"
    ],
    "author": "cakePlease",
    "date": "February 27, 2019, 1:39am February 27, 2019, 6:33am February 27, 2019, 3:17pm March 27, 2019, 3:17pm",
    "body": "Hi there, We're using the jquery client library for our custom search along with using \"domain-identifier\" to filter our results from 3 different domains. Im having a hard time trying to figure out how to use the jquery swiftype-search-jquery to change the filter parameter and have it picked up by the plugin object. Currently have 3 call to actions, each with a click event to build a new filter object, however i dont know how to pass this changed object to the existing $el.swifttypeSearch({}) object. I hope that makes sense. var searchScope = function(){ ....some biz logic here... if(onClickSearchScope>0){ return {'page': {'domain-identifier':onClickSearchScope}}; } else { return {}; } }; $('.st-default-search-input').swiftypeSearch({ resultContainingElement: '.st-search-container', engineKey: 'myEngineIDHere', renderFunction: customRenderFunction, perPage: 20, filters: searchScope() }); Any help would be appreciated",
    "website_area": "discuss"
  },
  {
    "id": "23215e5f-8edb-42b8-9b30-d47bc1f69c81",
    "url": "https://discuss.elastic.co/t/swifttype-crawl-rate/169709",
    "title": "Swifttype Crawl Rate",
    "category": [
      "Site Search"
    ],
    "author": "legislat.io",
    "date": "February 24, 2019, 9:06am February 24, 2019, 5:56pm February 24, 2019, 6:45pm February 24, 2019, 6:59pm March 24, 2019, 6:59pm",
    "body": "I'm trialing the service to see if it will help a project we're working on. but I don't seem to get the crawler working across the site. I fed it the seed and a sitemap, but it only crawls 20 pages in 12 hours, vs the many thousand pages that exist. Is there a better way to get the crawler working? or crawl with another tool then upload a url list?",
    "website_area": "discuss"
  },
  {
    "id": "aaadac3b-97db-4d7c-b7c5-c924b2e5c3d7",
    "url": "https://discuss.elastic.co/t/bulk-import-synonyms/168018",
    "title": "Bulk import synonyms",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "February 12, 2019, 11:24am February 12, 2019, 5:48pm March 12, 2019, 5:48pm",
    "body": "Hi, after buying the site search my customer would like to have some way to import a large amount of synonyms. is there any ways to do this? i didnt see an Api reference to do this. (even if its to create one at a time...) thanks",
    "website_area": "discuss"
  },
  {
    "id": "11b61d38-5b2b-4d27-8887-ac5e94172112",
    "url": "https://discuss.elastic.co/t/inconsistencies-in-search-results-with-and-without-special-characters/165091",
    "title": "Inconsistencies in search results with and without special characters",
    "category": [
      "Site Search"
    ],
    "author": "haraldurkarls",
    "date": "January 21, 2019, 4:49pm February 18, 2019, 4:49pm",
    "body": "I am getting different results depending if I input the search term with Icelandic characters or replace them with \"fallback\" characters using only the English alphabet. Query: framtarreikningur image.png973394 49.8 KB Notice how matching words in response are not in bold font. Query: framtidarreikningur image.png976389 52.9 KB Matching words are correctly in bold font. When querying the API directly I am getting same results. When using Icelandic characters the highlight object is empty.",
    "website_area": "discuss"
  },
  {
    "id": "a9a79170-cfac-47c3-974a-241c2b2cd17b",
    "url": "https://discuss.elastic.co/t/get-all-documents-within-document-type-not-working/163332",
    "title": "Get all documents within document_type not working",
    "category": [
      "Site Search"
    ],
    "author": "Marvin1",
    "date": "January 8, 2019, 10:05am January 10, 2019, 12:00pm January 10, 2019, 4:55pm January 10, 2019, 4:55pm",
    "body": "today I tried to use the api-call for listing all documents within a document_type but I didnt get all documents, I just get about 50 documents but I have atleast more than 65. Is there a limit for receiving documents? How I can get all documents within a document_type? API-Call: /api/v1/engines/{engine_id}/document_types/{document_type_id}/documents.json",
    "website_area": "discuss"
  },
  {
    "id": "c6da0416-235f-41b6-8849-1f9db0d76a31",
    "url": "https://discuss.elastic.co/t/add-a-new-page-to-be-indexed-maunaly-via-api/160783",
    "title": "Add a new page to be indexed maunaly via api",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "December 13, 2018, 4:47pm December 13, 2018, 5:04pm December 13, 2018, 5:09pm December 13, 2018, 7:05pm December 16, 2018, 10:52am January 13, 2019, 10:52am",
    "body": "Hello. i am trying to use swiftype web search that already crawled my site and manualy add a page. when trying to add content like this: { \"auth_token\": \"________\", \"document\": { \"external_id\": \"2\", \"fields\": [ {\"title\": \"title\", \"body\": \"viper123\", \"type\": \"string\"} ]} } i get: \"error\": \"This API action is prohibited on Engines created using the Swiftype web crawler.\" why is that? thanks",
    "website_area": "discuss"
  },
  {
    "id": "f763426a-f14a-4106-98c6-0288ac7d48ba",
    "url": "https://discuss.elastic.co/t/about-the-elastic-cloud-enterprise-category/66274",
    "title": "About the Elastic Cloud Enterprise category",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "nordbergm",
    "date": "November 10, 2018, 11:40pm July 6, 2017, 1:47pm",
    "body": "The Elastic Cloud Enterprise forum is dedicated to all questions related to Elastics on-premise Elastic Stack service.",
    "website_area": "discuss"
  },
  {
    "id": "29f533e4-98ff-4a51-a2c5-a190696b9ef6",
    "url": "https://discuss.elastic.co/t/azure-hosted-elasticsearch-in-my-vnet/216100",
    "title": "Azure- hosted elasticsearch in my vNet",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "liorg2",
    "date": "January 22, 2020, 4:33pm",
    "body": "hello , is it possible to use hosted elastic (cloud.elastic.co), but to configure it to run in the same azure vNet? that will reduce data transfer, and will give logstash access to sql server thanks!",
    "website_area": "discuss"
  },
  {
    "id": "6d88bf61-0197-40d8-b4ba-0e88110ced24",
    "url": "https://discuss.elastic.co/t/ece-elasticsearch-keystore/215901",
    "title": "ECE Elasticsearch Keystore",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "January 21, 2020, 2:57pm January 21, 2020, 3:26pm January 21, 2020, 3:53pm January 21, 2020, 3:59pm January 21, 2020, 4:05pm January 21, 2020, 4:29pm January 21, 2020, 5:09pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "aaab60d1-6774-4a6a-8208-217a319df69e",
    "url": "https://discuss.elastic.co/t/checking-runner-ip-connectivity-failed/215634",
    "title": "Checking runner ip connectivity... FAILED",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "sasikumar",
    "date": "January 19, 2020, 5:26pm January 19, 2020, 9:55pm January 20, 2020, 3:37am January 21, 2020, 3:35pm January 21, 2020, 4:47pm",
    "body": "We have already ECE setup on environment and we are trying to add another to existing ECE setup. While installing ECE on the new host we are getting the below errors. Can't connect $RUNNER_HOST_IP [xx.xx.xx.xx:22000]: Connection refused Could any one help on this ? Host Details: Operating System: RHEL 7 Docker Version :1.13 ECE Version:2.4.2",
    "website_area": "discuss"
  },
  {
    "id": "29371e60-18a9-48e3-a3f6-9ccd67b3f3c3",
    "url": "https://discuss.elastic.co/t/bootstrap-monitoring-process-did-not-exit-as-expected-again/215228",
    "title": "Bootstrap monitoring process did not exit as expected - Again",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "lcarr",
    "date": "January 16, 2020, 12:24am January 16, 2020, 4:05pm January 16, 2020, 4:43pm January 16, 2020, 5:34pm January 16, 2020, 5:33pm January 16, 2020, 6:16pm January 16, 2020, 6:21pm January 17, 2020, 4:25pm January 17, 2020, 4:29pm January 17, 2020, 6:25pm",
    "body": "I am asking to reopen this ticket. I need some kind of resolution on how to fix it. I have 7 deployments to do in the next 6 months. I had no issues with doing the install on Centos 7 but when I tried in RHEL 7.5 Maipo - with Docker 1.13 I am getting the error. Could someone please put in the commands for how they fixed it. Also, where does the IP address come into play with the services? Why is it used in the /mnt/data/elastic/xxx.xxx.1.187/services. Is there somewhere in the configuration that I should have made a change so that the IP address is not the point for services. I am asking as one of the errors that shows up when I am not directly in that network, I get a message that it is on the whitelist and I should change it. Any help would be greatly appreciated. Linda Carr",
    "website_area": "discuss"
  },
  {
    "id": "622bf9fc-4626-4dad-a873-d57567f7bb9e",
    "url": "https://discuss.elastic.co/t/disable-built-in-users/215002",
    "title": "Disable built-in users",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "January 14, 2020, 2:19pm January 14, 2020, 2:45pm January 14, 2020, 3:10pm January 14, 2020, 4:03pm January 14, 2020, 4:10pm January 15, 2020, 1:42pm January 15, 2020, 3:10pm January 15, 2020, 3:35pm January 15, 2020, 4:07pm January 16, 2020, 11:09am January 16, 2020, 2:39pm January 17, 2020, 10:44am January 17, 2020, 2:00pm January 17, 2020, 2:05pm January 17, 2020, 2:44pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c534130c-2bae-4df3-b5ca-e4a4baef7603",
    "url": "https://discuss.elastic.co/t/json-in-api-to-change-platform-endpoint/215326",
    "title": "JSON in API to Change Platform Endpoint",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "balogan",
    "date": "January 16, 2020, 2:15pm January 16, 2020, 2:43pm January 16, 2020, 5:36pm",
    "body": "I'm using the configuration store API to change the platform endpoint and having trouble getting the JSON formatting right. This is the JSON example from the docs. { \"value\" : \"string\" } When I use the API to list the config store it looks like this. { \"values\": [{ \"changed\": false, \"name\": \"https_port\", \"value\": \"9243\" }, { \"changed\": true, \"name\": \"cname\", \"value\": \"prod2\" }] } I've tried a combination of values[1].value with no success.",
    "website_area": "discuss"
  },
  {
    "id": "2c6cf8da-8690-4e1e-9757-5bd8ccc4c60b",
    "url": "https://discuss.elastic.co/t/launch-elasticsearch-really-an-error/215336",
    "title": "Launch Elasticsearch - really an error?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "newKibanaUser",
    "date": "January 16, 2020, 3:03pm January 16, 2020, 4:03pm January 16, 2020, 4:15pm January 16, 2020, 4:37pm",
    "body": "Hello - I am getting 401 error while launching elasticsearch as below from my ECE deployment. I am able to add/view indices onto this elasticsearch from my local instance using curl. Is this really an error or what should really happen after clicking on this Launch button? image1197392 54.9 KB",
    "website_area": "discuss"
  },
  {
    "id": "636ff49e-95fb-4054-a191-28f01184670e",
    "url": "https://discuss.elastic.co/t/gmail-problems/213467",
    "title": "Gmail problems",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Rob_wylde",
    "date": "December 31, 2019, 7:54pm January 2, 2020, 8:26pm January 2, 2020, 8:26pm January 15, 2020, 5:57pm January 15, 2020, 7:44pm January 16, 2020, 5:50am January 16, 2020, 11:00am",
    "body": "I'm trying to enable watcher to send email when conditions require an email to be sent. however i'm encounter issues and have been bashing my head against the wall for a while and decided its time to ask for assistance. I'm using an app specific password and i'm 100% confident app security settings in gmail are correct as i'm using this same email address with a different app password to send email from Zabbix and its working. Below are my elastic user setting overrides. I do not think xpack.watcher.enabled is required but i saw it in an example and gave it a try. xpack.watcher.enabled : true xpack.notification.email.account: gmail_account: profile: gmail smtp: auth: true starttls.enable: true host: smtp.gmail.com port: 587 user: donotreply@notimportant.com The password is saved in the key store with the setting name of: xpack.notification.email.account.gmail_account.smtp.secure_password Below is the error i find in the logging-and-metrics instances after i push the \"send test email\" after creating a temporary watcher rule. [2019-12-31T18:33:10,986][ERROR][org.elasticsearch.xpack.watcher.actions.email.ExecutableEmailAction] [instance-0000000005] failed to execute action [inlined/email_1] javax.mail.MessagingException: failed to send email with subject [Watch [asf] has exceeded the threshold] via account [work] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:171) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:163) ~[?:?] at org.elasticsearch.xpack.watcher.actions.email.ExecutableEmailAction.execute(ExecutableEmailAction.java:76) ~[?:?] at org.elasticsearch.xpack.core.watcher.actions.ActionWrapper.execute(ActionWrapper.java:164) [x-pack-core-7.5.1.jar:7.5.1] at org.elasticsearch.xpack.watcher.execution.ExecutionService.executeInner(ExecutionService.java:534) [x-pack-watcher-7.5.1.jar:7.5.1] On the nodes running docker root@ELK-ECE-NODE-4:~# telnet smtp.gmail.com 587 Trying 74.125.20.109... Connected to smtp.gmail.com. Escape character is '^]'. 220 smtp.gmail.com ESMTP c14sm35697244pfn.8 - gsmtp ^] telnet> quit However when i tcpdump on the node while snooping for traffic on 587 i see nothing and the above error does not indicate that it is even trying to make a connection. Any suggestions? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "67a295c9-3487-480c-909f-8de88038f034",
    "url": "https://discuss.elastic.co/t/suggestion-ece-ansible-role-addition/215285",
    "title": "Suggestion: ECE Ansible Role Addition",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "TWhitehouse",
    "date": "January 16, 2020, 9:47am",
    "body": "I've used the Ansible role to configure a number of ECE hosts running on RHEL7 I noticed that the docker installation on RHEL7 doesn't include docker-procy, docker-runc, or docker-init (only *-current e.g docker-proxy-current) This causes the script to fail when starting any of the containers. To fix this I added the following ansible code to the install docker script in (ansibledir)/roles/ece/tasks/system/RedHat-7/install_docker.yml which has allowed the install to happen - name: SymLink for docker-runc file: src: \"/usr/libexec/docker/docker-runc-current\" path: \"/usr/libexec/docker/docker-runc\" state: link - name: SymLink for docker-proxy file: src: \"/usr/libexec/docker/docker-proxy-current\" path: \"/usr/libexec/docker/docker-proxy\" state: link - name: SymLink for docker-init file: src: \"/usr/libexec/docker/docker-init-current\" path: \"/usr/libexec/docker/docker-init\" state: link I hope this helps someone and it may be worth considering adding to the ansible file, or at least having a note to ensure people know about this.",
    "website_area": "discuss"
  },
  {
    "id": "b254ede2-5bb5-4d3d-9338-8c22be587112",
    "url": "https://discuss.elastic.co/t/ece-and-monitoring-remote-exporters-indicate-a-possible-misconfiguration-misleading/215128",
    "title": "ECE and monitoring: Remote exporters indicate a possible misconfiguration - misleading",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "January 15, 2020, 12:10pm January 16, 2020, 10:10am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "bd203ea4-5146-438a-bc95-0bab3c6aceff",
    "url": "https://discuss.elastic.co/t/delegate-access-on-cloud-elastic/214850",
    "title": "Delegate access on cloud.elastic",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "AQ_Amra",
    "date": "January 13, 2020, 2:28pm January 13, 2020, 2:49pm January 14, 2020, 6:18am",
    "body": "Hi, Is it possible to delegate access to other members of our team on the cloud.elastic console?",
    "website_area": "discuss"
  },
  {
    "id": "dc20e804-ebd9-49eb-baac-6d481445ab27",
    "url": "https://discuss.elastic.co/t/customizing-kibana-deployment-logos-css-etc/214305",
    "title": "Customizing kibana deployment (logos, css, etc.)",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ebalmeida",
    "date": "January 8, 2020, 9:07pm January 8, 2020, 9:35pm January 8, 2020, 9:56pm January 10, 2020, 1:14am January 13, 2020, 8:21pm",
    "body": "Hello team, On a local kibana deployment it's possible (however unwieldy, because version updates) to customize logos and other graphical settings by directly changing the relevant files (svg logos or related css). Is there any way to do it (persistently) on an ECE kibana deployment? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "663a509b-6605-4606-b4fc-3d4f8891e6a4",
    "url": "https://discuss.elastic.co/t/ece-2-4-change-deployement-name-impact/214678",
    "title": "ECE 2.4 change deployement name impact",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "tazikm",
    "date": "January 10, 2020, 10:29pm January 10, 2020, 10:42pm January 11, 2020, 12:58am",
    "body": "Hi, is there any impact if i change the deployement name of my clusters in an ECE 2.4 environnement ? Thank you in advance Kamal",
    "website_area": "discuss"
  },
  {
    "id": "fd76663b-1e76-42f5-9314-61846ef284c8",
    "url": "https://discuss.elastic.co/t/kibana-endpoint-not-passing-through-haproxy-using-shortened-url/213597",
    "title": "Kibana Endpoint not Passing through Haproxy Using Shortened URL",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "balogan",
    "date": "January 2, 2020, 6:39pm January 2, 2020, 6:41pm January 2, 2020, 7:29pm January 2, 2020, 9:57pm January 3, 2020, 3:29pm January 6, 2020, 7:26pm January 6, 2020, 7:40pm January 20, 2020, 7:40pm",
    "body": "ECE 2.4.3 Haproxy 1.8 Kibana/Elasticsearch 7.5.4 I can get to Kibana using the endpoint prepended to the domain name like this: https://708....vcp-ecelab-log.mon.vzwops.com But we want a more user friendly url like this: https://vcp-ecelab-log.mon.vzwops.com/kibana Here's the result: {\"ok\":false,\"message\":\"Unknown deployment.\"} Haproxy setup: Frontend: use_backend be_ecelab_log if { hdr(host) -i vcp-ecelab-log.mon.vzwops.com } { path_beg -i /kibana } or { path_beg -i /kibana/ } Backend: backend be_ecelab_log mode http balance source server ecelab-log-1 708.....eceproxylab-1-southlake.mon.vzwops.com:9200 check verify none server ecelab-log-2 708.....eceproxylab-2-southlake.mon.vzwops.com:9200 check verify none server ecelab-log-3 708.....eceproxylab-3-southlake.mon.vzwops.com:9200 check verify none I've tried different combinations of the following on the frontend with no luck: #acl ece_kibana path_beg -i /kibana #redirect location 708.....eceproxylab-1-southlake.mon.vzwops.com append-slash code 301 if ece_kibana #acl ece_kibana { hdr(host) -i vcp-ecelab-log.mon.vzwops.com } { path_beg -i /kibana } or { path_beg -i /kibana/ } #http-request set-var(req.kibana_endpoint) req.hdr(host),lower,regsub(.vcp-ecelab-log.mon.vzwops.com$,) if { hdr(host) -i vcp-ecelab-log.mon.vzwops.com } { path_beg -i /kibana } #http-request set-path /%[var(req.rewrite_kibendpoint)]%[path] if { var(req.rewrite_kibendpoint) -m found } #http-request set-header Host vcp-ecelab-log.mon.vzwops.com if { var(req.rewrite_kibendpoint) -m found } #http-request redirect location 708.....eceproxylab-1-southlake.mon.vzwops.com if ece_kibana #http-request redirect code 301 location http://%[url,regsub(^/,/708....,)].%[hdr(host)] if ece_kibana #use_backend be_ecelab_log ece_kibana #redirect prefix 708.....eceproxylab-1-southlake.mon.vzwops.com code 301 if { hdr(host) -i vcp-ecelab-log.mon.vzwops.com }",
    "website_area": "discuss"
  },
  {
    "id": "8f24c2da-6191-49fa-a2a9-2da012ac184a",
    "url": "https://discuss.elastic.co/t/does-elastic-could-run-on-their-own-infrastructure-or-on-other-cloud-service-providers-like-aws-gcp-or-azure/213401",
    "title": "Does Elastic could run on their own infrastructure or on other cloud service providers like AWS, GCP or Azure?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Manjula_Piyumal",
    "date": "December 31, 2019, 1:57am December 31, 2019, 3:51am January 3, 2020, 5:30am January 17, 2020, 5:30am",
    "body": "Hi, I want to understand more about the elastic cloud and how does it works. Is it runs on their on infrastructure or on other cloud service providers? And if it's possible to run on AWS, can I integrate it with my existing AWS account and VPCs. As per the information available, I'm not clear whether they offer both options or not. So appreciate your help on this to clarify. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "8a781147-5fb8-4d8b-b2cb-bda5ce9a7d62",
    "url": "https://discuss.elastic.co/t/failing-to-restoring-from-snapshot/212978",
    "title": "Failing to restoring from snapshot",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Rob_wylde",
    "date": "December 24, 2019, 7:04pm December 29, 2019, 8:37pm December 29, 2019, 9:07pm December 30, 2019, 1:11am December 30, 2019, 1:16am January 2, 2020, 7:05pm January 16, 2020, 7:05pm",
    "body": "So I'm evaluating ECE and testing its abilities to restore from snapshot. So i had a functional ECE install with a number of deployments. All of which were sending snaps to minio regularly without issue. Before uninstalling ECE I ensure all of the deployments had recently been backed up to the S3 minio instance. I reinstall ECE and add my repository information again. I enable snapshots for one of the default deployments to ensure minio connectivity. The new deployment was successfully able to create and upload a snapshot. I move on to try and restore from a snapshot and none of my previous snaps are listed. I do not know what is going on here. Are there limitations to the ECE trial license that prevent me from restoring? I can confirm that the my previous deployments were running ELK stack 7.5 and that is the same target version that i'm trying to restore to. Snapshot Recovery on a Separate ECE Setup Does this still apply here? Are there any plans to make this process any better? As a user i expect to see the snapshots to be listed perhaps if they are unusable they can have a strike through or link to information as to why its being marked as unusable. Leaving me completely in the dark and searching forums for hours to ultimately find no good answers is very frustrating. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "32d20c29-853e-4ea6-b4a9-38cb95fdb911",
    "url": "https://discuss.elastic.co/t/ece-not-recognizing-docker-config-json/211844",
    "title": "ECE not recognizing docker config.json",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "December 13, 2019, 8:17pm December 14, 2019, 12:07am December 16, 2019, 1:49pm December 18, 2019, 7:10pm December 18, 2019, 7:48pm December 19, 2019, 2:23pm December 19, 2019, 3:02pm January 2, 2020, 3:01pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "27a764f3-8d8b-495c-81a2-a09bd7427213",
    "url": "https://discuss.elastic.co/t/moving-nodes-updates-in-2-4-3/212240",
    "title": "Moving Nodes Updates in 2.4.3",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "daniel_a",
    "date": "December 17, 2019, 11:53pm December 18, 2019, 1:27pm December 18, 2019, 5:52pm December 18, 2019, 7:33pm December 18, 2019, 7:36pm December 18, 2019, 7:40pm December 18, 2019, 9:54pm January 1, 2020, 9:54pm",
    "body": "From the 2.4.3 ECE release note: Moving nodes off allocators for the system-owned deployments admin-console-elasticsearch and logging-and-metrics now works as expected. I've been trying to move Kibana, the one from logging-and-metrics from the existing allocators to the new ones, but I can't do it. Even though, I specify the new target location, I'm failing to move off Kibana. I moved all other nodes (10-15 nodes), but not Kibana node.",
    "website_area": "discuss"
  },
  {
    "id": "8f448523-7216-485e-84b5-82290fd0f35f",
    "url": "https://discuss.elastic.co/t/bootstrap-monitoring-process-did-not-exit-as-expected/208673",
    "title": "Bootstrap monitoring process did not exit as expected",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 20, 2019, 10:15am November 20, 2019, 3:22pm November 20, 2019, 3:46pm November 20, 2019, 3:47pm November 20, 2019, 4:10pm November 20, 2019, 4:23pm November 21, 2019, 8:07am November 21, 2019, 8:17am November 21, 2019, 8:22am November 21, 2019, 3:36pm November 22, 2019, 2:13pm November 22, 2019, 6:49pm November 26, 2019, 12:53pm November 29, 2019, 9:57am December 2, 2019, 6:33am December 6, 2019, 5:58pm December 9, 2019, 1:47pm December 10, 2019, 5:10pm December 10, 2019, 8:25pm December 15, 2019, 10:52pm",
    "body": "Hi there, I try to install a \"large \". I started a few month ago with a small Installation and now we need to expand. For now I just try to install 2.4.2 on a new VM and it is not exiting without errors. But there are no errors. I used \"debug\" and evertything but there are no errors. I get a few warnings: -- Verifying Prerequisites -- Checking runner container does not exist... PASSED Checking host storage root volume path is not root... PASSED Checking host storage path is accessible... PASSED Checking host storage path contents matches whitelist... PASSED Checking Docker version... PASSED Checking Docker file system... PASSED Checking Docker storage driver... PASSED - The installation with overlay2 can proceed; however, we recommend using overlay Checking whether 'setuser' works inside a Docker container... PASSED Checking memory settings... PASSED Checking runner ip connectivity... PASSED Checking OS IPv4 IP forward setting... PASSED Checking OS max map count setting... PASSED Checking OS kernel version... PASSED - OS kernel version is 3.10.0-1062.4.1.el7.x86_64 but we recommend 4.4. Checking minimum required memory... PASSED Checking OS kernel cgroup.memory... PASSED - OS setting 'cgroup.memory' should be set to cgroup.memory=nokmem Checking OS minimum ephemeral port... PASSED Checking OS max open file descriptors per process... PASSED Checking OS max open file descriptors system-wide... PASSED Checking OS file system and Docker storage driver compatibility... PASSED Checking OS file system storage driver permissions... PASSED -- Completed Verifying Prerequisites -- And the exit notification is: - Exiting bootstrapper {} [2019-11-20 10:06:32,958][INFO ][no.found.util.LogApplicationExit$] Application is exiting {} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Errors have caused Elastic Cloud Enterprise installation to fail Bootstrap monitoring process did not exit as expected Traceback (most recent call last): File \"/elastic_cloud_apps/bootstrap-initiator/initiator.py\", line 88, in <module> exitcode = monitor.logging_and_bootstrap_monitor(bootstrap_properties, bootstrap_container_id, enable_debug) File \"/elastic_cloud_apps/bootstrap-initiator/bootstrap_initiator/monitor.py\", line 24, in logging_and_bootstrap_monitor raise BootstrapMonitoringException('Bootstrap monitoring process did not exit as expected') bootstrap_initiator.monitor.BootstrapMonitoringException: Bootstrap monitoring process did not exit as expected ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Now I dont know what to do. I tried removing and reinstalling everything new. So docker, ece, all permissions, etc.",
    "website_area": "discuss"
  },
  {
    "id": "ac2903a3-b0c0-4241-a1b0-d02786b3df8d",
    "url": "https://discuss.elastic.co/t/ece-confusion-about-docker-storage-driver-to-use-overlay-or-overlay2/210927",
    "title": "ECE - Confusion about docker storage driver to use - overlay or overlay2?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ygirouard_stm",
    "date": "December 6, 2019, 6:58pm December 11, 2019, 2:42pm December 11, 2019, 4:07pm December 25, 2019, 4:09pm",
    "body": "Docker's documentation is clear: if possible and your kernel supports it, use overlay2 as the storage driver, as stated here: https://docs.docker.com/storage/storagedriver/overlayfs-driver/ Note : If you use OverlayFS, use the overlay2 driver rather than the overlay driver, because it is more efficient in terms of inode utilization. To use the new driver, you need version 4.0 or higher of the Linux kernel, or RHEL or CentOS using version 3.10.0-514 and above. However, ECE's documentation and installation script says otherwise and recommends overlay over overlay2 during the prereq check phase: Checking Docker storage driver... The installation with overlay2 can proceed; however, we recommend using overlay ... and, confusingly, the official Ansible playbook sets overlay2 as the storage driver when installing ECE on RHEL7. Why the discrepancy, and why recommend overlay over overlay2 when Docker themselves recommends otherwise?",
    "website_area": "discuss"
  },
  {
    "id": "4d080915-dd4d-4d41-9847-a6feedc4536b",
    "url": "https://discuss.elastic.co/t/ece-2-4-3-released/211338",
    "title": "ECE 2.4.3 released",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Przemyslaw_H",
    "date": "December 10, 2019, 4:58pm December 24, 2019, 4:58pm",
    "body": "We are pleased to announce that ECE 2.4.3 has been released today. This is a bug fix release. Release notes are available here",
    "website_area": "discuss"
  },
  {
    "id": "f641ad92-d84e-48e5-8fd4-df0874fc3c83",
    "url": "https://discuss.elastic.co/t/trouble-with-dashboard-import-in-ece-cluster/209983",
    "title": "Trouble with dashboard import in ECE cluster",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "November 29, 2019, 3:33pm November 29, 2019, 3:41pm December 2, 2019, 2:41pm December 2, 2019, 5:40pm December 2, 2019, 6:50pm December 3, 2019, 8:31am December 3, 2019, 9:12am December 17, 2019, 9:12am",
    "body": "Hi, I have an ECE deployment including one cluster that seems to be working fine. I have used APIs to interact with the kibana instance for example using curl to upload an index template. I tried to import a saved dashboard using curl, which gave me an error : 504 Gateway Time-out 504 Gateway Time-out The curl syntax has worked elsewhere but I tried doing the same from the Kibana UI in person and also got timeout behaviour. I tried splitting the dashboard file into its component parts and uploading them one at a time, but even a small file gives the same timeout behaviour. Could there be a reason the dashboard import is not working for me? How can I go about diagnosing what sounds like a load balancer / reverse proxy issue within ECE? Any ideas?",
    "website_area": "discuss"
  },
  {
    "id": "ee666028-ca68-49dc-9b9a-637130fab405",
    "url": "https://discuss.elastic.co/t/two-allocators-on-the-same-hardware/209502",
    "title": "Two allocators on the same hardware",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "lovegronvall",
    "date": "November 26, 2019, 12:17pm November 26, 2019, 1:54pm November 27, 2019, 6:39am November 27, 2019, 4:23pm December 11, 2019, 4:26pm",
    "body": "Did anyone successfully run more than one allocator on the same hardware? For example one using spinning disks and one on SSD?",
    "website_area": "discuss"
  },
  {
    "id": "9b8ad1ea-a01d-4260-8d19-5e812469c80a",
    "url": "https://discuss.elastic.co/t/node-move-failing-for-es/209203",
    "title": "Node Move Failing for ES",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Rob_wylde",
    "date": "November 24, 2019, 10:34pm November 25, 2019, 2:15pm November 25, 2019, 8:12pm November 27, 2019, 12:09am November 27, 2019, 12:09am December 10, 2019, 5:14pm",
    "body": "So i just setup another brand new instance of ECE and I'm following the same process i've used successfully in the past. That being install an 'admin' node with less resources than the other 3 nodes. Add the new allocators and once they all show up under platform -> allocators i try and Platform -> allocator -> {select first installed node} -> move nodes -> select all -> Move nodes but i get this error on the ES migrations. KB migation appears to work and can be validated on the allocator screen ece2.PNG2146513 25.2 KB no.found.constructor.validation.ValidationException: 1. Can't apply a move_only plan with topology / setting changes. Actions: [settings] at no.found.constructor.validation.Validation$EveryError.asFailedFuture(Validation.scala:238) at no.found.constructor.steps.ValidatePlanPrerequisites$$anonfun$no$found$constructor$steps$ValidatePlanPrerequisites$$validateWithRetries$1$3.apply(ValidatePlanPrerequisites.scala:101) at no.found.constructor.steps.ValidatePlanPrerequisites$$anonfun$no$found$constructor$steps$ValidatePlanPrerequisites$$validateWithRetries$1$3.apply(ValidatePlanPrerequisites.scala:84) at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) at no.found.concurrent.WrappedRunnable.run(ControllableExecutionContextWrapper.scala:80) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ece.PNG29121831 309 KB I've tried starting fresh and reinstalled a number of times, but i always get this error. Any suggestions? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "22c62f4d-55c1-492a-a029-a663a775483b",
    "url": "https://discuss.elastic.co/t/snapshots-are-failing-in-ece-minio/207553",
    "title": "Snapshots are failing in ECE minio",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 12, 2019, 3:42pm November 12, 2019, 5:07pm November 13, 2019, 6:34am November 13, 2019, 7:06am November 13, 2019, 3:20pm November 15, 2019, 7:56pm November 19, 2019, 12:24pm November 19, 2019, 4:31pm November 20, 2019, 6:04am November 21, 2019, 5:32pm December 5, 2019, 5:32pm",
    "body": "Hi there, I have integrated a repository with minio. After I recognized that elasticsearch 6.x was not able to resolve hostnames I changed to IP. Elasticsearch 7.x worked fine for a while. I use version 2.4.1 and updated yesterday from 2.3.1. But the error has already been in 2.3.1. Now one deployment has some error by creating a snapshot. But it has 100 working snapshots already: Unknown s3 client name [cloud-REPONAME]. Existing client configs: Default In the advanced settings I see: \"snapshot\": { \"enabled\": true, \"repository\": { \"config\": { \"repository_id\": \"REPONAME\", \"snapshot_config_type\": \"reference\" } }, \"suspended\": {} } Why is in the Error message a \"cloud-\" prefix?",
    "website_area": "discuss"
  },
  {
    "id": "39809d7f-0f86-48b1-99a8-f0bbc81eee86",
    "url": "https://discuss.elastic.co/t/elastic-cloud-architecture-selection-and-system-requirements-suggestion/208644",
    "title": "Elastic Cloud Architecture Selection and System Requirements Suggestion",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "firat.tekin",
    "date": "November 20, 2019, 7:43am November 20, 2019, 4:09pm December 4, 2019, 3:36pm",
    "body": "Hey there, I just tried Elastic Cloud service and looks like it meets my expectations. I want to ask some questions before further steps. I've got approximately 600GB of data currently, which strictly defined as \"keyword\" mapping for all of the fields. My data may be grow up to 1.4TB -even may be more in the future- and needed to be actively searchable in all indices. I can't decide some questions on my own. So I need suggestions about; 1- I liked the \"Hot-Warm Architecture\" with high storage offering. But I think, I need an architecture with no-warm nodes. As I said before, I need to actively search inside all indices with high search speed. Should I choose \"I/O Optimized Architecture\" instead of this? 2- If I need further storage in the future, can I easily increase the size of nodes or append new nodes without a need of extra configuration? Is Elastic Cloud supports distribution of data with the old nodes and the new? Like automatic reorganizing? 3- I really don't understood about \"Fault Tolerance\". Is that means, creating a new data node for replica shard usage only? Or, synchronizes my primary shards on my current data node with another machine in another zone - If your current data machine went down, it automatically serve you from our backup machine in another zone - status? Thanks, Note: I previously open a topic inside 'Elasticsearch' with same text. But after I found this section, I thought that my topic is more appropriate for here.Flagged old topic by, \"Content on wrong section\" already, but didn't get any response to this. So, I reopened it again here.",
    "website_area": "discuss"
  },
  {
    "id": "eb1fcede-c45c-4f5a-80a5-e6287736bc65",
    "url": "https://discuss.elastic.co/t/hot-warm-cord-architecture/208176",
    "title": "Hot/Warm/Cord Architecture",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "daniel_a",
    "date": "November 16, 2019, 5:11pm November 18, 2019, 3:47pm November 18, 2019, 3:58pm November 18, 2019, 8:08pm December 2, 2019, 5:26pm",
    "body": "I'm trying to figure out what are the most optional ratios for each data storage. I'm aware about 30/100 ratios used for hot/warm storages in Elastic Cloud. I can't really find much information about the cold storage. What would be the recommended ratio for the cold storage in ECE? If I increase it, let's say to 120 and it's running on a slower hardware, would this downgrade performance to the point that my searches would take very long time to return? How searchable would be the data in the cold storage? If I use SSD drives for the hot storage, how much can I increase the hot storage ratio and still have solid performance? I totally understand that this will depend on specific requirements for each customer. I'm trying to see how much performance downgrade/upgrade with the ratio change. Is this something I can easily measure before making final decisions? --Thansk",
    "website_area": "discuss"
  },
  {
    "id": "91fa6758-61d9-4a30-9430-6cdaa3d0b739",
    "url": "https://discuss.elastic.co/t/api-key-ip-filtering-on-ece/207660",
    "title": "API Key, IP Filtering on ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "November 13, 2019, 8:57am November 14, 2019, 2:23am November 14, 2019, 2:23am November 28, 2019, 12:57am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e80086e5-dbdf-4a56-b9dd-21574458a0b1",
    "url": "https://discuss.elastic.co/t/api-console-auditing/207313",
    "title": "API Console Auditing",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "robrotheram",
    "date": "November 11, 2019, 9:26am November 11, 2019, 7:28pm November 12, 2019, 12:07pm November 12, 2019, 2:19pm November 26, 2019, 2:19pm",
    "body": "Hi there all deployments have a api console that uses the default elastic user to execute commands to the clusters. Is there any where that this is audited. IE something like user bob has executed command Post _search on cluster id XXXXXX If not is there away to disable this console. We have some very strict auditing requirements based on the data we hold",
    "website_area": "discuss"
  },
  {
    "id": "9eb544f9-0892-4549-aa92-ee189b3d6f35",
    "url": "https://discuss.elastic.co/t/ece-aws-load-balancer-port-configuration/207129",
    "title": "ECE + AWS load balancer: port configuration?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "November 8, 2019, 2:31pm November 12, 2019, 10:04am November 12, 2019, 10:04am November 26, 2019, 10:04am",
    "body": "I have a load balancer in front of ECE to front the cluster URLs for kibana and elasticsearch. These all use port 9243 and the config in the ECE UI allows me to set a CNAME to be used instead of the ip.es.io service... This works and I can use the CloudID to configure beats etc. So far so good. But I would like to present the services not only on the domain of my choosing but move the port to a standard one - eg. 443, which would allow me to use the service through a corporate web proxy. I configure the load balancer to listen on 443 and target 9243. I can add the port assignment to the endpoint configuration / CNAME and it is accepted and the resulting CloudID seems right. But none of the links in the ECE UI work then. I suspect this is a bug. (I can manually make a CloudID including the port 443 using base64 that works with beats.) So far the least broken approach is to configure the endpoint without port, the links in ECE point to xxx:9243 which doesn't get through the proxy, but removing the :9243 then makes the link work. The manually modified CloudID also works to get beats to access elasticsearch over port 443. Is there any way to configure ECE to build URLs and CloudID with a specific port in addition to the specific endpoint?",
    "website_area": "discuss"
  },
  {
    "id": "2f929e5d-ffde-44f7-b2ac-8fc22974ac02",
    "url": "https://discuss.elastic.co/t/waiting-for-admin-cluster-to-become-reachable-after-os-upgrade-from-rhek-7-6-to-rhel-7-7/207343",
    "title": "Waiting for admin cluster to become reachable, after OS upgrade from rhek 7.6 to rhel 7.7",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 11, 2019, 11:30am November 11, 2019, 7:17pm November 12, 2019, 7:49am November 26, 2019, 7:49am",
    "body": "Hi there, I had to upgrade the OS and for this I deleted the ECE completely like mentioned in the docs. It worked and I reinstalled and rejoined the node. In the UI I can see the runner and configure the roles. But it seems that it has an issue with the admin cluster and the Proxy. [2019-11-11 11:17:06,115][WARN ][spray.can.client.HttpHostConnectionSlot] Connection attempt to containerhost:9244 failed in response to POST request to /allocator-metricbeat-*/_search with no retries left, dispatching error... {} [2019-11-11 11:17:06,549][WARN ][spray.can.client.HttpHostConnectionSlot] Connection attempt to containerhost:9244 failed in response to GET request to /.migration/doc/lock with no retries left, dispatching error... {} [2019-11-11 11:17:06,549][INFO ][no.found.adminconsole.elasticsearch.IndexConfigurationActor] Waiting for admin cluster to become reachable ([Connection attempt to containerhost:9244 failed]). Retrying every [5 seconds]. {} spray.can.Http$ConnectionAttemptFailedException: Connection attempt to containerhost:9244 failed at spray.can.client.HttpHostConnectionSlot$$anonfun$connecting$1.applyOrElse(HttpHostConnectionSlot.scala:87) at akka.actor.Actor$class.aroundReceive(Actor.scala:502) at spray.can.client.HttpHostConnectionSlot.aroundReceive(HttpHostConnectionSlot.scala:33) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526) at akka.actor.ActorCell.invoke(ActorCell.scala:495) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) at akka.dispatch.Mailbox.run(Mailbox.scala:224) at akka.dispatch.Mailbox.exec(Mailbox.scala:234) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) It is not possible to expand the admin console to the host. Other deployments are working. And I cannot Login with its IP:12433. On this hosts I have added IPtables rules: Chain INPUT (policy ACCEPT) target prot opt source destination ACCEPT tcp -- anywhere anywhere multiport dports 12191:12201 /* 900 profile::linux::firewall::deny_unused accept 12191-12201 */ ACCEPT tcp -- anywhere anywhere multiport dports 12343 /* 900 profile::linux::firewall::deny_unused accept 12343 */ ACCEPT tcp -- anywhere anywhere multiport dports 12443 /* 900 profile::linux::firewall::deny_unused accept 12443 */ ACCEPT tcp -- anywhere anywhere multiport dports 12898:12908 /* 900 profile::linux::firewall::deny_unused accept 12898-12908 */ ACCEPT tcp -- anywhere anywhere multiport dports 13898:13908 /* 900 profile::linux::firewall::deny_unused accept 13898-13908 */ ACCEPT tcp -- anywhere anywhere multiport dports 18000:21999 /* 900 profile::linux::firewall::deny_unused accept 18000-21999 */ ACCEPT tcp -- anywhere anywhere multiport dports 2112/* 900 profile::linux::firewall::deny_unused accept 2112 */ ACCEPT tcp -- anywhere anywhere multiport dports ssh /* 900 profile::linux::firewall::deny_unused accept 22 */ ACCEPT tcp -- anywhere anywhere multiport dports 22191:22195 /* 900 profile::linux::firewall::deny_unused accept 22191-22195 */ ACCEPT tcp -- anywhere anywhere multiport dports 9243 /* 900 profile::linux::firewall::deny_unused accept 9243 */ ACCEPT tcp -- anywhere anywhere multiport dports 9343/* 900 profile::linux::firewall::deny_unused accept 9343 */ Does anyone has experience with this? Greetings Malte",
    "website_area": "discuss"
  },
  {
    "id": "805b03bd-a784-408f-9812-b73f0943cb75",
    "url": "https://discuss.elastic.co/t/requirements-for-coordinator-and-director-hosts-and-proxies/207379",
    "title": "Requirements for coordinator and director hosts and proxies",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 11, 2019, 3:15pm November 12, 2019, 6:02am November 25, 2019, 7:14pm",
    "body": "Hi, we want to switch from small \"small baseline installation\" to large. for this I need to know what disk requirements are needed for coordinator, director and proxies. I didnt find anything in the docs. Only about RAM. Greetings Malte",
    "website_area": "discuss"
  },
  {
    "id": "05e780f8-739d-4b71-b142-81c2b4a4251a",
    "url": "https://discuss.elastic.co/t/sample-installation-runs-out-of-disk-indices-readonly/206953",
    "title": "Sample installation runs out of disk, indices readonly",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "November 7, 2019, 11:09am November 7, 2019, 9:48pm November 8, 2019, 9:57am November 12, 2019, 9:56am November 25, 2019, 7:11pm",
    "body": "A single host installation is running out of disk well before I would expect given the amount of data stored in clusters. However I am struggling to understand the contact points to use to diagnose this with ECE or eventually to control it. GET /_cluster/allocation/explain tells \"can_allocate\" : \"no, allocate_explanation\" : \"cannot allocate because allocation is not permitted to any of the nodes GET _cat/shards shows only some indices running to low hundreds of MB - totalling probably a couple of GB at the most GET _cat/allocation shows disk used 18.4GB avail 1.5GB Using df and du on the underlying ECE node on AWS suggests, the disk is filling. Surprisingly I found the proxyv2 directory was comparable in size to the allocator one - this looked suspicious. Drilling down deeper I found that the majority of the proxyv2 usage was uncompressed logs which were huge. So tactical question - how can I change the logging policy for proxy nodes to avoid storing these relatively useless logs or at the very least, compress them? And strategic question, what is best practice for storage management within ECE? I will delete some of these logs to get the ECE off its knees but would like a better solution.",
    "website_area": "discuss"
  },
  {
    "id": "c7209951-c2b3-4c70-b10d-682a6b534a4e",
    "url": "https://discuss.elastic.co/t/ece-upgrade-v2-3-2-to-v2-4-1/207262",
    "title": "ECE Upgrade v2.3.2 to v2.4.1",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Jugsofbeer",
    "date": "November 11, 2019, 1:49am November 25, 2019, 1:49am",
    "body": "Hi, Has anyone tried to do an upgrade on-premise, from ECE 2.3.2 to v2.4.1 on RHEL 7.5 with Docker 1.13? Does anyone have any feedback on how the new proxy has improved things in your environment ? We are just wanting to hear about other people's migrations and if there were any lessons learned that you can share. We have done a lot of reading of the documentation, but keen to hear from others.",
    "website_area": "discuss"
  },
  {
    "id": "3dda1802-3afb-4b8b-9ba9-135a1558df2a",
    "url": "https://discuss.elastic.co/t/ece-medium-installation-example-aws-runner-ip-connectivity/206040",
    "title": "ECE medium installation example, AWS, runner ip connectivity",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "October 31, 2019, 1:02pm October 31, 2019, 6:45pm November 1, 2019, 6:51am November 7, 2019, 6:10pm November 7, 2019, 9:51pm November 8, 2019, 10:24am November 8, 2019, 10:25am November 22, 2019, 10:26am",
    "body": "I'm installing the second instance (step 3 here: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-topology-example2.html) after the first instance seemed to install without problems. The second instance complains: Checking runner ip connectivity... FAILED Can't connect $RUNNER_HOST_IP [10.5.3.195:22000]: Connection refused [...] Errors have caused Elastic Cloud Enterprise installation to fail Some of the prerequisites failed: [runner ip connectivity], please fix before continuing I checked on the first instance and there isn't anything listening on this port: netstat -anpt|grep LISTEN|grep 22000 Any advice? Possibly relevant context. I'm using r5.xlarge instances. I had to manually set up docker as the cloud-init script to do this failed. Failed running /var/lib/cloud/scripts/per-instance/00-format-drives-enable-docker Possibly because of the storage being /dev/nvme1n1 Instead I manually did parted mklabel, mkpart, mkfs.xfs, mkdir /mnt/data, install /mnt/data, mount, make the sysctl edits, systemctl restart docker...",
    "website_area": "discuss"
  },
  {
    "id": "55c1ff65-da67-4217-ab04-c2ecc2b9e7c0",
    "url": "https://discuss.elastic.co/t/cluster-health-not-reported-accurately-in-ece/206966",
    "title": "Cluster health not reported accurately in ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "November 7, 2019, 1:44pm November 7, 2019, 10:00pm November 8, 2019, 9:52am November 22, 2019, 9:52am",
    "body": "I have clusters where the health is degraded by indices in read only state. Clicking through to the individual deployment UI for those clusters, the health is often not reportedly the same state. Within ECE deployments top level page and within specific deployment UIs separately, the health is consistent, so I don't think the status is fluctuating. ECE top page seems to have a snapshot from some time in the past that bears little relation to the current status in Kibana. Clicking through to Kibana, one cluster that is Unhealthy at the top level and is Healthy at the individual level, has lifecycle errors against several indices. As context, the readonly state was probably induced by running out of storage. This has been addressed by deleting excessive uncompressed log files. The cluster in question has had: PUT _all/_settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } } to clear the readonly status. I believe there are no longer readonly indices on this cluster. What is going on with cluster health reporting?",
    "website_area": "discuss"
  },
  {
    "id": "0706ec97-d958-42c1-a3f9-a12e5963fd36",
    "url": "https://discuss.elastic.co/t/change-docker-registry-and-repo-path/206422",
    "title": "Change docker registry and repo path",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 4, 2019, 2:38pm November 4, 2019, 3:10pm November 6, 2019, 3:10pm November 20, 2019, 3:10pm",
    "body": "Hi there, we have a satellite server for our internal docker registry. When I setup ECE I can change the registry server but not the repository path. Unfortunatley we have a static path and cannot change it. satellite.company-registry.de:5000/company-name-elastic_cloud_enterprise-cloud-assets_elasticsearch:7.4.1-1 But the Syntax of the Installation script is docker pull ${DOCKER_REGISTRY}/${ECE_DOCKER_REPOSITORY}/elastic-cloud-enterprise:${CLOUD_ENTERPRISE_VERSION} As you can see we cannot combine this. Is there another way or another config where I can change this. Or do I have to change the Script within? Greetings Malte",
    "website_area": "discuss"
  },
  {
    "id": "26fcacf8-3803-4608-83fe-bb70d01f1e2e",
    "url": "https://discuss.elastic.co/t/ip-filter-on-cloud-ui/206636",
    "title": "IP Filter on Cloud UI",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 5, 2019, 3:54pm November 8, 2019, 7:09am November 20, 2019, 3:08pm",
    "body": "Hi there, I have tried a few things with IP filtering and I have a question about how can I restrict traffic to the cloud UI. We are using Terminalserver where we want to access the Cloud UI and block every other connection. For now I just tried to create a ruleset and added it to the \"admin-console-elasticsearch\". After that I was able to login but when I Login I get: There was a problem communicating with the system cluster. Origin status code [403 Forbidden]. So my question is, is there a way to restrict the Cloud UI with the IP-Filter and how?",
    "website_area": "discuss"
  },
  {
    "id": "84fbfafd-54c3-48f4-9ca1-7ddd3f2ba804",
    "url": "https://discuss.elastic.co/t/collecting-specific-data-from-within-each-virtual-machine/206309",
    "title": "Collecting specific data from within each virtual machine",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "greavette",
    "date": "November 3, 2019, 11:53pm November 4, 2019, 10:49am November 18, 2019, 7:17am",
    "body": "Hello Forum, I've just begun to use Kibana (ELK) so my understanding is very limited at our company but I'm hoping this forum can educate me or lead me in the right direction. I have a need to collect various data from within each of our windows and Linux virtual machines such as what applications are installed and the version of these applications as well as other data to create various reports. Currently we use PowerShell and Bash scripts to connect to each server to collect this data. I'd like to have this data collected and placed into our ELK stack. Can ELK do this and if so how? Thank you in advance for any direction this forum can provide me",
    "website_area": "discuss"
  },
  {
    "id": "a8c826d8-8f96-43f8-bec2-4fe1a7a450aa",
    "url": "https://discuss.elastic.co/t/snapshot-settings-interval/206097",
    "title": "Snapshot settings interval",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ryan_Downey",
    "date": "October 31, 2019, 8:13pm October 31, 2019, 8:17pm October 31, 2019, 9:14pm November 1, 2019, 12:28pm November 15, 2019, 12:14pm",
    "body": "Am I missing something here or are the directions to change the Snapshot settings interval just that poor. The 3rd, 4h and 5th steps dont even seem to exist when I'm on the deployment that I want to snapshot. I don't need the default 30 minute settings for this deployment. https://www.elastic.co/guide/en/cloud-enterprise/current/ece-snapshots.html#ece-change-snapshot-interval",
    "website_area": "discuss"
  },
  {
    "id": "19bfe7d0-e62e-4047-8a15-916fa684e72c",
    "url": "https://discuss.elastic.co/t/ip-filtering-api-not-showing-rulesets/205644",
    "title": "IP-Filtering API not showing rulesets",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "October 31, 2019, 9:33pm October 29, 2019, 4:34pm October 29, 2019, 4:44pm October 29, 2019, 5:32pm October 29, 2019, 6:25pm October 30, 2019, 6:49am October 31, 2019, 7:26pm November 14, 2019, 7:26pm",
    "body": "Hi there, I am currently trying to script a few things for our ECE. Right now I am hurdling with the API for IP-filtering. curl -k -X GET -H \"Authorization: Bearer mytoken\" https://localhost:12443/api/v1/deployments/ip-filtering/rulesets { \"rulesets\": } I have a few rulesets created via the UI and I am able to put it manually to the deployments. But I need to script it. Is there a bug or am I doing it wrong. Greetings Malte",
    "website_area": "discuss"
  },
  {
    "id": "b84f809c-c883-4648-b43f-5e571d7dad01",
    "url": "https://discuss.elastic.co/t/proxy-forwarder-security-issue/205082",
    "title": "Proxy / Forwarder security issue",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "robrotheram",
    "date": "October 24, 2019, 1:55pm October 24, 2019, 4:07pm October 24, 2019, 9:09pm October 24, 2019, 9:22pm October 24, 2019, 9:23pm November 7, 2019, 9:23pm",
    "body": "Hi I am trialing ECE and I have setup secure route to a new cluster and that works fine. I get a access denied when I try and access it from anywhere except the host I allowed. But I can hit the frc-services-forwarders-services-forwarder (port 9244) on a server that is only a allocator. If I spoof the address for the cluster ie. add the cluster-id.ece-address.local to the allocator ip in the host file I can access the cluster anywhere I do this change. This raise some security concerns, also that communication looks to be frc-services-forwarders-services-forwarder http only. Does the poxy terminate the tls connection and its http to the elasticsearch cluster or is the something missing.",
    "website_area": "discuss"
  },
  {
    "id": "348d4869-223a-47e9-9e69-16f3b4bf7874",
    "url": "https://discuss.elastic.co/t/kibana-no-instances-running/204263",
    "title": "Kibana - No Instances Running",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "justinw",
    "date": "October 18, 2019, 6:12pm October 18, 2019, 6:37pm October 18, 2019, 6:48pm October 18, 2019, 9:05pm October 18, 2019, 9:23pm October 18, 2019, 9:32pm October 21, 2019, 2:30pm October 24, 2019, 8:08pm November 7, 2019, 8:08pm",
    "body": "Hi, All of a sudden my kibana deployment disappeared. There's not much info in the UI - has anyone seen this before? Any tips on resolution? I have terminated the deployment, re-created it, tried scaling up to more nodes, and more, however each time ends up in a failed kibana deployment. Screenshot from 2019-10-18 10-45-14.png1898946 75.3 KB Thanks, Justin",
    "website_area": "discuss"
  },
  {
    "id": "4f887522-1256-4938-bae6-2f0fb0a87305",
    "url": "https://discuss.elastic.co/t/does-ece-have-user-audit-trail/204435",
    "title": "Does ECE have user audit trail",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "robrotheram",
    "date": "October 21, 2019, 10:27am October 21, 2019, 2:25pm October 21, 2019, 3:08pm October 21, 2019, 4:27pm October 21, 2019, 7:01pm October 22, 2019, 10:51am October 22, 2019, 12:44pm October 22, 2019, 12:46pm November 5, 2019, 12:46pm",
    "body": "Hello, I am experimenting with ECE and I trying to see if the admin console stores a audit trail of user actions. I can see the activity field but there is no user attribute. What I am looking for is something like \"User A performed cluster restart on deployment \" so we can work out which user did which action. Secondly is there a way I can export this data?",
    "website_area": "discuss"
  },
  {
    "id": "6033c068-6ffa-41bf-8a32-ea0d8d49f696",
    "url": "https://discuss.elastic.co/t/monitor-status-of-filebeat-sin-elastic-cloud/203429",
    "title": "Monitor Status of Filebeat sin elastic cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "AndresL",
    "date": "October 14, 2019, 10:31am October 22, 2019, 10:01pm October 28, 2019, 2:57pm",
    "body": "Hi, how can i know the status of Filebeats across my organization in an elastic cloud deployment? Checking into the documentation, looks like is for hosted cluster.",
    "website_area": "discuss"
  },
  {
    "id": "25ced6e3-0504-4b5d-8b15-5bf0833b1a7b",
    "url": "https://discuss.elastic.co/t/resubscribe-alerting-email/201912",
    "title": "Resubscribe alerting email",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "October 2, 2019, 9:14am October 2, 2019, 4:33pm October 16, 2019, 4:40pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "e400cbf8-bfcf-44f0-a00b-78650ba9c086",
    "url": "https://discuss.elastic.co/t/kibana-will-not-take-saml-config-changes-in-the-user-settings-overrides/202002",
    "title": "Kibana will not take SAML config changes in the User Settings Overrides",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ryan_Downey",
    "date": "October 2, 2019, 4:13pm October 2, 2019, 4:29pm October 16, 2019, 4:29pm",
    "body": "I have a deployment that will not take the SAML settings that I'm trying to implement. I have tried to implement the Kibana settings needed to allow SAML access and the changes fail every time. This is occurring on another deployment as well. Out of the three deployments that I've tried to implement SAML access to only one has taken the changes and allows SAML access. There was a problem applying this configuration change [rolling-upgrade]: [java.lang.IllegalStateException: The state did not become the desired one before [600000 milliseconds] elapsed. Last error was: [Instance is not running [instance-0000000016]. Please check allocator/docker logs.]] { \"cluster_topology\": [ { \"instance_configuration_id\": \"kibana\", \"size\": { \"resource\": \"memory\", \"value\": 4096 }, \"zone_count\": 2 } ], \"kibana\": { \"system_settings\": { \"elasticsearch_password\": \"xxxxxx\", \"elasticsearch_url\": \"xxxxxx\", \"elasticsearch_username\": \"found-internal-kibana4-server\" }, - \"user_settings_yaml\": \"# Note that the syntax for user settings can change between major versions.\\n# You might need to update these user settings before performing a major version upgrade.\\n#\\n# Use OpenStreetMap for tiles:\\n# tilemap:\\n# options.maxZoom: 18\\n# url: http://a.tile.openstreetmap.org/{z}/{x}/{y}.png\\n#\\n# To learn more, see the documentation.\\n#xpack.security.authc.providers: [saml, basic]\\n#xpack.security.authc.saml.realm: saml1\\n#server.xsrf.whitelist: [/api/security/v1/saml]\\n#xpack.security.authProviders: [saml, basic]\\n#server.xsrf.whitelist: [/api/security/v1/saml]\\n#xpack.security.public:\\n# protocol: https\\n# hostname: xxxxxx\\n# port: 9243\\n#\\nxpack.ilm.enabled: true\\n#\\n#xpack.security.authProviders: [saml, basic]\\n#server.xsrf.whitelist: [/api/security/v1/saml]\\n#xpack.security.public:\\n# protocol: https\\n# hostname: xxxxxx\\n# port: 9243\", + \"user_settings_yaml\": \"# Note that the syntax for user settings can change between major versions.\\n# You might need to update these user settings before performing a major version upgrade.\\n#\\n# Use OpenStreetMap for tiles:\\n# tilemap:\\n# options.maxZoom: 18\\n# url: http://a.tile.openstreetmap.org/{z}/{x}/{y}.png\\n#\\n# To learn more, see the documentation.\\n#xpack.security.authc.providers: [saml, basic]\\n#xpack.security.authc.saml.realm: saml1\\n#server.xsrf.whitelist: [/api/security/v1/saml]\\n#xpack.security.authProviders: [saml, basic]\\n#server.xsrf.whitelist: [/api/security/v1/saml]\\n#xpack.security.public:\\n# protocol: https\\n# hostname: xxxxxx\\n# port: 9243\\n#\\nxpack.ilm.enabled: true\\n#\\nxpack.security.authProviders: [saml, basic]\\nserver.xsrf.whitelist: [/api/security/v1/saml]\\nxpack.security.public:\\n protocol: https\\n hostname: xxxxxx\\n port: 9243\", \"version\": \"7.3.0\" }, \"transient\": { \"strategy\": { \"rolling\": {} } } }",
    "website_area": "discuss"
  },
  {
    "id": "4b1a9f2f-ddb1-4956-b7e6-b2fa5b518543",
    "url": "https://discuss.elastic.co/t/ece-saml-error-still-persists-after-upgrade/188104",
    "title": "ECE Saml Error still persists after upgrade",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "afuggetta",
    "date": "June 28, 2019, 8:22pm July 1, 2019, 12:57pm July 8, 2019, 2:20pm July 8, 2019, 2:37pm July 8, 2019, 2:44pm July 9, 2019, 1:05pm July 23, 2019, 1:05pm October 1, 2019, 8:58pm",
    "body": "Hi @Alex_Piggott, I saw that ECE 2.2.3 came out and it had a SAML bug fix. However, after upgrading I still see the same error as explained in here: ECE - SAML integration error . Any suggestions? Thanks, Andrea",
    "website_area": "discuss"
  },
  {
    "id": "4b8c09fc-4992-4d6e-8f26-630371410b86",
    "url": "https://discuss.elastic.co/t/disable-elastic-security-plugin-in-ece/201650",
    "title": "Disable Elastic Security Plugin in ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "robrotheram",
    "date": "September 30, 2019, 1:47pm September 30, 2019, 2:15pm September 30, 2019, 2:46pm October 14, 2019, 2:46pm",
    "body": "Hi, I am trialing out ECE and we have a our own custom plugin that implements the rest filter class. This is incompatible with elastic security plugin since it seems that only one plugin can implement the rest filter class. In our clusters we just disable the plugin and run the cluster in its own VLAN can we do this in ECE",
    "website_area": "discuss"
  },
  {
    "id": "4a3b6419-ca3b-45d1-88f6-16cb00ed28ce",
    "url": "https://discuss.elastic.co/t/endpoints-on-elastic-cloud/200672",
    "title": "Endpoints on Elastic Cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "AndresL",
    "date": "September 23, 2019, 11:31am September 23, 2019, 1:22pm September 23, 2019, 1:54pm September 23, 2019, 2:34pm September 23, 2019, 2:54pm September 23, 2019, 3:39pm September 23, 2019, 3:39pm September 23, 2019, 4:02pm October 7, 2019, 4:02pm",
    "body": "HI, i just deployed a cluster in ECE. Im trying to edit the settings for and endpoint following this instructions: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-administering-endpoints.html But i cant see any of this; From the Platform menu, select Settings . Specify the CNAME value for your cluster and Kibana endpoints. Click Update Deployment endpoints . The new endpoint becomes effective immediately.",
    "website_area": "discuss"
  },
  {
    "id": "f9414044-954e-4b56-bc83-c3592893f192",
    "url": "https://discuss.elastic.co/t/ece-installation-script-error/200288",
    "title": "ECE Installation script error",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rajuleo",
    "date": "September 19, 2019, 6:21pm September 22, 2019, 4:28pm September 23, 2019, 7:09am October 7, 2019, 7:09am",
    "body": "Hi Team, While running the installation bash script, I am getting below error? any insight? bash-4.2$ bash <(curl -fsSL https://download.elastic.co/cloud/elastic-cloud-enterprise.sh) install Elastic Cloud Enterprise Installer Start setting up a new Elastic Cloud Enterprise installation by installing the software on your first host. This first host becomes the initial coordinator and provides access to the Cloud UI, where you can manage your installation. To learn more about the options you can specify, see the documentation. NOTE: If you want to add this host to an existing installation, please specify the --coordinator-host and --roles-token flags -- Verifying Prerequisites -- Checking runner container does not exist... Errors have caused Elastic Cloud Enterprise installation to fail Error while fetching server API version: ('Connection aborted.', PermissionError(13, 'Permission denied'))",
    "website_area": "discuss"
  },
  {
    "id": "45555aaf-9d88-42ff-b714-38d377eee19c",
    "url": "https://discuss.elastic.co/t/elastic-cloud-enterprise-setup-for-exisiting-elk-solution/200180",
    "title": "Elastic Cloud Enterprise setup for exisiting ELK solution",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rajuleo",
    "date": "September 19, 2019, 10:34am September 19, 2019, 2:17pm October 3, 2019, 2:17pm",
    "body": "Hi Team, We already have a ELK solution running in our environment for Log analytics, If we need to use Elastic Cloud Enterprise setup basically the Cloud UI self service portal for the existing solution, is it supported, Can ECE discover the existing ELK deployment and start managing it?",
    "website_area": "discuss"
  },
  {
    "id": "e9c50529-24a8-49e2-b098-de8b21458bb3",
    "url": "https://discuss.elastic.co/t/resync-runner-doesnt-appear-to-do-anything/199883",
    "title": "Resync runner doesn't appear to do anything",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ClydeM",
    "date": "September 17, 2019, 8:59pm October 1, 2019, 9:00pm",
    "body": "Continuing the discussion from ECE Cloud UI shows incorrect info of node roles: I ran into this issue. I tried running a resync on the runner and it doesn't help any. Add/remove a role didn't help either",
    "website_area": "discuss"
  },
  {
    "id": "9e6edd15-f5dc-46e5-9ec8-ab3cb896b484",
    "url": "https://discuss.elastic.co/t/docker-updates-causing-issues-with-the-ece/198131",
    "title": "Docker Updates Causing Issues with the ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "daniel_a",
    "date": "September 5, 2019, 1:10am September 6, 2019, 3:12pm September 9, 2019, 1:58am September 9, 2019, 12:43pm September 16, 2019, 10:18am September 16, 2019, 5:43pm September 30, 2019, 5:43pm",
    "body": "I noticed connection issues with the ECE after Google updated docker to its latest version \"Docker version 19.03.2, build\", and right after that everything in the ECE has stopped working. I can still log into the UI, but nothing else is showing up. I'm getting a few errors while clicking on things from the left side menu, nothing is populating though: -> There was a problem communicating with the system cluster. Origin status code [502 Bad Gateway]. -> Fetching region ece-region failed",
    "website_area": "discuss"
  },
  {
    "id": "bd67f48e-6260-451b-a212-44543b65635c",
    "url": "https://discuss.elastic.co/t/using-python-to-connect-to-elastic-cloud/199350",
    "title": "Using Python to Connect To Elastic Cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "michaelberg",
    "date": "September 13, 2019, 3:31am September 13, 2019, 5:37am September 13, 2019, 1:49pm September 13, 2019, 2:17pm September 13, 2019, 2:34pm September 27, 2019, 2:34pm",
    "body": "Greetings Everyone! I am building a project for University and want to do some experimenting with connecting to Elastic Cloud using Python ... I already have Elastic Cloud and Kibana set up and it's working perfectly well and ingesting data from a couple of different servers I'm running ... I am having a hard time finding documentation that outlines the steps you need to take in order to create whatever certificates you need to connect securely from Python code and then, I think, you need to upload those to the cloud platform and install them locally as well on the machine you'll be connecting \"from\" ... Not looking for someone to \"hand hold\" but perhaps kick me in the right direction for where to find some good documentation on what needs to be done to make this connection! I appreciate any and all advice/direction I can get! Thanks so much!",
    "website_area": "discuss"
  },
  {
    "id": "bf6dc354-de91-443a-8d0a-641e98908a9d",
    "url": "https://discuss.elastic.co/t/snapshot-recovery-on-a-separate-ece-setup/199353",
    "title": "Snapshot Recovery on a Separate ECE Setup",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "September 13, 2019, 3:39am September 13, 2019, 1:23pm September 27, 2019, 1:23pm",
    "body": "I have an ECE 2.3 setup which has gone down due to a damaged zookeeper. Because of this, I set up a new ECE and created a couple of new ES cluster deployments on the new ECE, also on version 2.3. How do I restore data from the previous deployments to the new ES clusters on the new ECE? On the Snapshot recovery UI, I am only able to select from deployments created on the new ECE and not from the previous one. The snapshots are stored on Minio.",
    "website_area": "discuss"
  },
  {
    "id": "fcca325c-3a2c-420f-9d46-54fcd3263f02",
    "url": "https://discuss.elastic.co/t/installing-custom-driver-on-cloud/199339",
    "title": "Installing custom driver on cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "September 13, 2019, 12:37am September 14, 2019, 7:11am September 27, 2019, 12:51pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ffcd0d6e-015f-4b04-8b66-8b66cc49fee5",
    "url": "https://discuss.elastic.co/t/filebeat-modules-elastic-cloud/197906",
    "title": "Filebeat modules + Elastic Cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "gerard1",
    "date": "September 3, 2019, 4:22pm September 3, 2019, 6:30pm September 12, 2019, 2:18pm September 26, 2019, 2:18pm",
    "body": "I'm following the official documentation to try out Filebeat modules, together with Elastic Cloud, but I'm getting this error when generating some log files. filebeat | 2019-09-03T16:18:31.368Z ERROR elasticsearch/client.go:343 Failed to perform any bulk index operations: 500 Internal Server Error: {\"error\":{\"root_cause\":[{\"type\":\"illegal_state_exception\",\"reason\":\"There are no ingest nodes in this cluster, unable to forward request to an ingest node.\"}],\"type\":\"illegal_state_exception\",\"reason\":\"There are no ingest nodes in this cluster, unable to forward request to an ingest node.\"},\"status\":500} That I'm missing?",
    "website_area": "discuss"
  },
  {
    "id": "2568d2f1-7888-4e02-856f-a4dda116e20a",
    "url": "https://discuss.elastic.co/t/ldap-resultcode-89-simple-bind-operations-are-not-allowed-to-contain-a-bind-dn-without-a-password/198919",
    "title": "Ldap resultCode=89 Simple bind operations are not allowed to contain a bind DN without a password",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "September 10, 2019, 2:50pm September 11, 2019, 5:41am September 11, 2019, 1:59pm September 12, 2019, 9:43am September 26, 2019, 9:43am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "a345b193-2cbd-4a68-b2c9-e60f7d5e10d5",
    "url": "https://discuss.elastic.co/t/about-the-elastic-cloud-on-kubernetes-eck-category/183899",
    "title": "About the Elastic Cloud on Kubernetes (ECK) category",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "uricohen",
    "date": "June 3, 2019, 7:04am",
    "body": "All things related to your ECK and running the Elastic stack on Kubernetes.",
    "website_area": "discuss"
  },
  {
    "id": "198c281c-cda6-4a45-b0ec-75ccae02da1c",
    "url": "https://discuss.elastic.co/t/what-ver-of-elastic-is-able-to-be-run-in-eck/215917",
    "title": "What Ver of Elastic is able to be run in ECK?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "bigdamhero",
    "date": "January 21, 2020, 4:27pm January 21, 2020, 5:09pm January 21, 2020, 5:20pm January 21, 2020, 5:41pm January 21, 2020, 5:44pm January 21, 2020, 7:54pm January 21, 2020, 8:38pm",
    "body": "We are trying to upgrade our docker container based clusters into the newly released ECK. I am not seeing what newest version of ECK and support running. We want to try to migrate as much as we can and I am just wondering if there is a limitation on what we should upgrade and when. Thank you.",
    "website_area": "discuss"
  },
  {
    "id": "d07e8ebf-e95e-4fa4-81c6-d4c81e551d7e",
    "url": "https://discuss.elastic.co/t/best-option-for-persistent-volume-of-an-on-premise-eck/215611",
    "title": "Best option for persistent volume of an on-premise ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Ali_Reza_Haghighat",
    "date": "January 19, 2020, 9:50am January 21, 2020, 10:37am",
    "body": "We have a cluster with multiple elastic instances deployed on a few servers. We decided to use ECK to manage our cluster. We are looking for the best option for persistent volume for an ECK cluster. We have tried NFS and Local Volumes. However, NFS volume is not recommended to be used with Elasticsearch (here and here). We also have problems using Local volume (Pod crashes after a few document insertion). Could you please help us to choose best volume option for our case? We use Debian on our servers.",
    "website_area": "discuss"
  },
  {
    "id": "671c952d-97f0-4b75-8ace-5ea1e5379f31",
    "url": "https://discuss.elastic.co/t/gke-upgrade-and-pdb/215661",
    "title": "GKE Upgrade and PDB",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Vincent_Ngai",
    "date": "January 20, 2020, 2:44am January 20, 2020, 8:23am January 20, 2020, 8:51am January 20, 2020, 9:21am January 20, 2020, 9:20am January 20, 2020, 10:18am January 20, 2020, 10:48am January 20, 2020, 11:00am",
    "body": "Hey Guys, I am install my ECK on GKE as you may know GKE provide a method for cluster auto upgrade or no matter as a normal upgrade for node pool (it does 1 by 1 in each zone) if we setup PDB , it will follow the rules as max 1 hour However i dont know if 1 hour is enough for the replica relocate to another know What will happen to the case and do ECK correspond react for this ? https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster",
    "website_area": "discuss"
  },
  {
    "id": "31c1d649-5fc9-4f06-b171-9c8cbb60e274",
    "url": "https://discuss.elastic.co/t/filebeat-to-eck-on-gke/215575",
    "title": "Filebeat to ECK on GKE",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Zero_Day",
    "date": "January 18, 2020, 8:58pm January 20, 2020, 5:47am January 20, 2020, 7:43am January 20, 2020, 10:03am",
    "body": "I built a cluster on GKE with ECK operator and am trying to send logs from an on premises Filebeat installation to the cloud. Elasticsearch has LoadBlancer IP. I specified certificate, password and necessary things, but I couldn't make it work. Is there a tutorial?",
    "website_area": "discuss"
  },
  {
    "id": "42e800f5-490c-4750-a9ca-2a1b2a0845a5",
    "url": "https://discuss.elastic.co/t/user-role-management-best-practices/187197",
    "title": "User/role management best practices",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "t0ffel",
    "date": "June 24, 2019, 8:55pm June 25, 2019, 7:24am January 15, 2020, 2:50pm January 18, 2020, 10:41am January 20, 2020, 9:28am",
    "body": "What is the best practice for the user and role management with the operator? Is it preferable to create users k8s resources or is it better to create users via Kibana? is there a way to do custom roles via k8s resources?",
    "website_area": "discuss"
  },
  {
    "id": "f5e94650-517b-40b4-b586-0afe4ae05e0b",
    "url": "https://discuss.elastic.co/t/uninstalling-eck-hangs-forever/215521",
    "title": "Uninstalling ECK - hangs forever",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "January 17, 2020, 8:29pm January 17, 2020, 9:25pm",
    "body": "Hello World! I'm tried to follow Uninstalling ECK | Elastic Cloud on Kubernetes [1.0] | Elastic, however bellow commands hangs forever: $ kubectl get namespaces --no-headers -o custom-columns=:metadata.name \\ > | xargs -n1 kubectl delete elastic --all -n elasticsearch.elasticsearch.k8s.elastic.co \"quickstart\" deleted kibana.kibana.k8s.elastic.co \"quickstart\" deleted Please advise.",
    "website_area": "discuss"
  },
  {
    "id": "cf427170-b2b2-45fa-a911-96a373f3cfb0",
    "url": "https://discuss.elastic.co/t/eck-and-prometheus/215495",
    "title": "Eck and Prometheus",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "jdambly",
    "date": "January 17, 2020, 4:45pm",
    "body": "What using eck what is the best way to send metrics to Prometheus for monitoring? I see there are a couple of projects for this GitHub vvanholl/elasticsearch-prometheus-exporter Prometheus exporter plugin for ElasticSearch. Contribute to vvanholl/elasticsearch-prometheus-exporter development by creating an account on GitHub. And GitHub justwatchcom/elasticsearch_exporter Elasticsearch stats exporter for Prometheus. Contribute to justwatchcom/elasticsearch_exporter development by creating an account on GitHub. Are there any other that are better?",
    "website_area": "discuss"
  },
  {
    "id": "2f3f5460-cb52-4206-9a9c-9f29627cd890",
    "url": "https://discuss.elastic.co/t/elastic-stack-helm-charts/187598",
    "title": "Elastic Stack Helm Charts",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LaurentGoderre",
    "date": "June 26, 2019, 2:48pm June 27, 2019, 3:27pm June 27, 2019, 5:20pm July 1, 2019, 8:29pm January 15, 2020, 5:31pm January 17, 2020, 3:29pm",
    "body": "I had started an epic to fix the elastic-stack helm chart in helm/stable: https://github.com/helm/charts/issues/14564 I think this work is still desirable because it should be easy to install the stack using helm without having to manually link the pods together. Is this something the community is interested in?",
    "website_area": "discuss"
  },
  {
    "id": "477e17fd-ba3c-4375-b14e-605064e7ac9b",
    "url": "https://discuss.elastic.co/t/change-defauit-volume-size-for-elastic/215421",
    "title": "Change defauit volume size for elastic",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "KapitanPlaneta",
    "date": "January 17, 2020, 7:29am January 17, 2020, 8:01am January 17, 2020, 9:34am",
    "body": "Elasticsearch comes with a 1GB storage by default > NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE > elasticsearch-data-elasticsearch-es-default-0 Bound pvc-4d565cd7-3894-11ea-a77b-12b06dd324bb 1Gi RWO gp2 12 Is there a way to change the size without configuring my own pvc, volume and volume mount? thanks",
    "website_area": "discuss"
  },
  {
    "id": "e143e8ba-5617-4156-ac01-901c37aee893",
    "url": "https://discuss.elastic.co/t/picking-up-a-configuration-file-change/214219",
    "title": "Picking up a configuration file change",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JanKowalik",
    "date": "January 8, 2020, 11:37am January 8, 2020, 12:00pm January 8, 2020, 2:42pm January 8, 2020, 3:35pm January 8, 2020, 3:48pm January 8, 2020, 4:17pm January 8, 2020, 5:10pm January 9, 2020, 11:08am January 9, 2020, 11:08am January 9, 2020, 11:36am January 9, 2020, 1:11pm January 9, 2020, 3:03pm January 9, 2020, 3:10pm January 9, 2020, 3:31pm January 9, 2020, 3:39pm January 9, 2020, 3:43pm January 10, 2020, 1:46pm January 10, 2020, 2:10pm January 10, 2020, 3:54pm January 13, 2020, 3:50pm",
    "body": "Hi, It seems that ECK does not pickup configuration changes. How do I make it pick it up? Do I restart the nodes? How can I do it via ECK? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "bcc6ae80-9b56-43e3-9782-9644df7e65a3",
    "url": "https://discuss.elastic.co/t/dont-want-to-use-https-and-user-password/202332",
    "title": "Don't want to use https and user:password",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Kumar_Saurabh_Srivas",
    "date": "October 4, 2019, 11:15am October 4, 2019, 12:47pm October 4, 2019, 1:35pm October 4, 2019, 5:55pm October 16, 2019, 4:30pm January 16, 2020, 5:16pm",
    "body": "Hi I am deploying ECK on GKE in a private Kubernetes cluster. That cluster has the only service which will talk to Elasticsearch. So I don't need to have any https or user:password authentication. All I want is a simple clusterIP service which can be directly accessed by the service within the kubernetes cluster. Please let me know how to do that.",
    "website_area": "discuss"
  },
  {
    "id": "b0fbf55a-75f7-4422-a154-8811839ab5ec",
    "url": "https://discuss.elastic.co/t/setup-snapshot-for-s3-plugin/211661",
    "title": "Setup snapshot for s3 plugin",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dg_hivebrite",
    "date": "December 12, 2019, 2:12pm December 12, 2019, 2:30pm December 12, 2019, 2:54pm December 13, 2019, 2:11pm December 13, 2019, 2:58pm December 17, 2019, 8:57am December 17, 2019, 9:18am December 17, 2019, 3:21pm December 19, 2019, 9:59am December 19, 2019, 12:04pm December 20, 2019, 8:49am December 20, 2019, 10:53am December 20, 2019, 2:44pm December 20, 2019, 3:15pm January 15, 2020, 5:27pm January 15, 2020, 5:51pm",
    "body": "Hi, I'm trying to setup the s3 plugin but I have an issue. I don't how to format the secret for aws credentials for the operator. Actually I tried that (with terraform): resource \"kubernetes_secret\" \"aws_credentials_datawarehouse\" { type = \"kubernetes.io/generic\" metadata { name = \"datawarehouse-aws-credentials\" } data = { \"s3.client.default.access_key\" = \".....\" \"s3.client.default.secret_key\" = \".....\" } } The operator log an error: E1212 13:40:26.641312 1 reflector.go:126] pkg/mod/k8s.io/client-go@v11.0.1-0.20190409021438-1a26190bd76a+incompatible/tools/cache/reflector.go:94: Failed to list *v1beta1.Elasticsearch: v1beta1.ElasticsearchList.Items: []v1beta1.Elasticsearch: v1beta1.Elasticsearch.Spec: v1beta1.ElasticsearchSpec.SecureSettings: []v1beta1.SecretSource: readObjectStart: expect { or n, but found \", error found in #10 byte of ...|ttings\":[\"datawareho|..., bigger context ...|rageClassName\":\"standard\"}}]}],\"secureSettings\":[\"datawarehouse-aws-credentials\"],\"updateStrategy\":{|... I tried to check in the github repository but I'm not sure of which format is expected inside the secret",
    "website_area": "discuss"
  },
  {
    "id": "a16e8444-23fb-488a-9406-a4311631c8b5",
    "url": "https://discuss.elastic.co/t/deploy-eck-to-a-single-namespace/211495",
    "title": "Deploy ECK to a Single Namespace",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "nobound",
    "date": "December 11, 2019, 4:07pm December 11, 2019, 4:45pm December 11, 2019, 4:55pm December 12, 2019, 8:30pm December 12, 2019, 9:10pm December 12, 2019, 9:25pm January 8, 2020, 9:20pm January 10, 2020, 3:17pm",
    "body": "Hi, I would like to deploy ECK within a single namespace, but it is not clear to me how to do so after reading https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-ns-config.html and https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-operator-config.html. Not sure exactly how to specify thoes options. Can someone give me a quick example? Thanks Kc",
    "website_area": "discuss"
  },
  {
    "id": "3722c1fd-e2d1-4289-af62-3a45cd52d5e4",
    "url": "https://discuss.elastic.co/t/how-to-configure-nfs-in-eck-k8s-rancher/213936",
    "title": "How to configure nfs in ECK k8s Rancher",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Marinho_DevOPs",
    "date": "January 6, 2020, 8:53pm January 6, 2020, 10:46pm January 7, 2020, 8:31am January 7, 2020, 5:26pm January 8, 2020, 8:57am January 8, 2020, 10:43am January 8, 2020, 10:50am January 8, 2020, 10:55am January 8, 2020, 3:50pm January 8, 2020, 4:56pm January 10, 2020, 3:06pm",
    "body": "Hi I have a problem setting a volume in the all-in-one.yaml file. I am trying to map an ntfs to persist the external volume and not use the Elastic default. Could someone help me in this case. My file. apiVersion: apps/v1 kind: StatefulSet metadata: name: elastic-operator namespace: elasticsearch-funcional labels: control-plane: elastic-operator spec: selector: matchLabels: control-plane: elastic-operator serviceName: elastic-operator template: metadata: labels: control-plane: elastic-operator spec: serviceAccountName: elastic-operator containers: - image: docker.elastic.co/eck/eck-operator:1.0.0-beta1 volumeMounts: - name: elasticsearch-data mountPath: /usr/share/elasticsearch/data name: manager args: [\"manager\", \"--operator-roles\", \"all\", \"--enable-debug-logs=true\"] env: - name: OPERATOR_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: WEBHOOK_SECRET value: webhook-server-secret - name: WEBHOOK_PODS_LABEL value: elastic-operator - name: OPERATOR_IMAGE value: docker.elastic.co/eck/eck-operator:1.0.0-beta1 resources: limits: cpu: 1 memory: 150Mi requests: cpu: 100m memory: 50Mi ports: - containerPort: 9876 name: webhook-server protocol: TCP terminationGracePeriodSeconds: 10 volumes: - name: elasticsearch-data nfs: server: 192.168.134.137 path: /nfs/dev/elastick8",
    "website_area": "discuss"
  },
  {
    "id": "25741c7d-d425-454f-9f06-c970cc86223d",
    "url": "https://discuss.elastic.co/t/how-can-i-remove-the-es-default-service-installed-by-cloud-on-k8s/214358",
    "title": "How can I remove the es-default service installed by cloud-on-k8s",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Jone",
    "date": "January 9, 2020, 7:32am January 9, 2020, 1:16pm",
    "body": "How can I remove the es-default service installed by cloud-on-k8s, or add port to the es-default service?",
    "website_area": "discuss"
  },
  {
    "id": "167cc91d-4de4-4fcd-9a03-34fc14083b27",
    "url": "https://discuss.elastic.co/t/transport-profiles-with-es-7-5-0/214407",
    "title": "Transport profiles with ES 7.5.0",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "akappler",
    "date": "January 9, 2020, 10:09am January 9, 2020, 11:12am",
    "body": "Hi, we configured ES 6.8.3 with a transport profile without mutual TLS authentication for use with a legacy client. Now we want to upgrade to ES 7.5.0, but the same configuration stopped working. Apparently no server certificate is generated, and the TLS handshake fails. Here is the configuration: transport.profiles.client: port: 9500 xpack.security: type: client ssl: client_authentication: none And here is the output of openssl when trying to connect to the transport profile: openssl s_client -connect localhost:9500 Do 09 Jan 2020 11:09:18 CET CONNECTED(00000003) 140007703086336:error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure:ssl/record/rec_layer_s3.c:1543:SSL alert number 40 --- no peer certificate available --- No client certificate CA names sent --- SSL handshake has read 7 bytes and written 293 bytes Verification: OK --- New, (NONE), Cipher is (NONE) Secure Renegotiation IS NOT supported Compression: NONE Expansion: NONE No ALPN negotiated Early data was not sent Verify return code: 0 (ok) ---",
    "website_area": "discuss"
  },
  {
    "id": "c4da96e2-3192-4d47-863f-6ebc2e49238c",
    "url": "https://discuss.elastic.co/t/can-the-elastic-stack-run-on-the-kubernetes-1-10/213970",
    "title": "Can the Elastic stack run on the Kubernetes: 1.10?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ted_ye",
    "date": "January 7, 2020, 3:32am January 7, 2020, 8:36am January 7, 2020, 9:03am",
    "body": "Hello, Can the Elastic stack run on the Kubernetes: 1.10 ? why the Kubernetes need version 1.11+? thanks.",
    "website_area": "discuss"
  },
  {
    "id": "9fc97f8f-3aa4-4f16-a2b6-959a8a14b513",
    "url": "https://discuss.elastic.co/t/elasticsearch-upgrade-stuck-skipping-deletion-because-of-migrating/213568",
    "title": "Elasticsearch upgrade stuck - Skipping deletion because of migrating",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "pabloborh",
    "date": "January 2, 2020, 12:28pm January 2, 2020, 4:48pm January 3, 2020, 7:26am January 3, 2020, 2:04pm January 7, 2020, 8:39am",
    "body": "Hello, I've modified request, limits and xms xmx Java options of my cluster yaml. The upgrade is stuck and operators logs are {\"level\":\"info\",\"ts\":1577967197.4311237,\"logger\":\"elasticsearch-controller\",\"msg\":\"Updating status\",\"iteration\":5457,\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967197.4311762,\"logger\":\"generic-reconciler\",\"msg\":\"Aggregated reconciliation results complete\",\"result\":{\"Requeue\":true,\"RequeueAfter\":10000000000}} {\"level\":\"info\",\"ts\":1577967197.4311981,\"logger\":\"elasticsearch-controller\",\"msg\":\"End reconcile iteration\",\"iteration\":5457,\"took\":0.784774123,\"namespace\":\"elasticsearch\",\"es_ame\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967207.4313903,\"logger\":\"elasticsearch-controller\",\"msg\":\"Start reconcile iteration\",\"iteration\":5458,\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967207.4348965,\"logger\":\"transport\",\"msg\":\"Reconciling transport certificate secrets\",\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967208.1634543,\"logger\":\"driver\",\"msg\":\"Calculated all required changes\",\"to_create:\":5,\"to_keep:\":4,\"to_delete:\":6,\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967208.1636055,\"logger\":\"driver\",\"msg\":\"Calculated performable changes\",\"schedule_for_creation_count\":0,\"schedule_for_deletion_count\":1,\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967208.2052684,\"logger\":\"driver\",\"msg\":\"Skipping deletion because of migrating data\",\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\",\"pod_name\":\"elasticsearch-es-data-sx54vph58q\"} {\"level\":\"info\",\"ts\":1577967208.2053738,\"logger\":\"elasticsearch-controller\",\"msg\":\"Updating status\",\"iteration\":5458,\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} I don't know what means \"Skipping deletion because of migrating\" Cluster previous state was yellow, because of one data node was \"evicted\" because of request threshold Operators image is: docker.elastic.co/eck/eck-operator:0.9.0",
    "website_area": "discuss"
  },
  {
    "id": "c3c230d7-9a5b-4ce9-8154-cadb1651430e",
    "url": "https://discuss.elastic.co/t/ldap-needs-a-license-for-use/213536",
    "title": "LDAP needs a license for use",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JuYeong_Park",
    "date": "January 2, 2020, 8:45am January 2, 2020, 9:17am January 2, 2020, 11:54am",
    "body": "When i try to use ldap in elastic cloud on k8s according to guide(https://github.com/elastic/cloud-on-k8s/issues/40#issuecomment-565412446), they returned to me below message. version 6.8 [2020-01-02T07:55:27,091][WARN ][o.e.x.s.a.AuthenticationService] [master-1] Authentication failed using realms [reserved/reserved,file/file1]. Realms [ldap/ldap1] were skipped because they are not permitted on the current license version 7.2 {\"type\": \"server\", \"timestamp\": \"2020-01-02T08:09:21,612+0000\", \"level\": \"WARN\", \"component\": \"o.e.x.s.a.AuthenticationService\", \"cluster.name\": \"cluster-name\", \"node.name\": \"master-name\", \"cluster.uuid\": \"L8Xl5EMGT9qtE_oUIRifwQ\", \"node.id\": \"dtGj32qpS7WG227FO9aiKw\", \"message\": \"Authentication failed using realms [reserved/reserved,file/file1]. Realms [ldap/ldap1] were skipped because they are not permitted on the current license\" } So, I want to know that there are any method to use ldap in elastic cloud on k8s. Here is my yaml file for ldap setting. apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: jay-p-es spec: http: tls: selfSignedCertificate: disabled: True version: 7.2.0 nodeSets: - name: master count: 3 config: node.master: true node.data: false node.ingest: false xpack.security.authc.realms: ldap: ldap1: order: 0 url: \"ldap://hostname:389\" bind_dn: \"bind_dn\" user_search: base_dn: \"base_dn\" filter: \"(uid={0})\" group_search: base_dn: \"base_dn\" unmapped_groups_as_roles: false bind_password: \"password\" podTemplate: spec: hostNetwork: true volumes: - name: elasticsearch-data emptyDir: {} containers: - name: elasticsearch resources: requests: memory: 7Gi cpu: 7 limits: memory: 7Gi cpu: 7 - name: data count: 3 config: node.master: false node.data: true node.ingest: true xpack.security.authc.realms: ldap: ldap1: order: 0 url: \"ldap://hostname:389\" bind_dn: \"bind_dn\" user_search: base_dn: \"base_dn\" filter: \"(uid={0})\" group_search: base_dn: \"base_dn\" unmapped_groups_as_roles: false bind_password: \"password\" podTemplate: spec: hostNetwork: true volumes: - name: elasticsearch-data emptyDir: {} containers: - name: elasticsearch resources: requests: memory: 7Gi cpu: 7 limits: memory: 7Gi cpu: 7",
    "website_area": "discuss"
  },
  {
    "id": "91f783d4-5894-4012-a7e9-5b570c12a2ac",
    "url": "https://discuss.elastic.co/t/pod-status-stuck-at-terminating-when-k8s-node-not-ready/212907",
    "title": "Pod status stuck at Terminating when k8s node not ready",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LoadingZhang",
    "date": "December 24, 2019, 3:26am December 26, 2019, 7:36am December 26, 2019, 8:17am",
    "body": "I try to simulate k8s environment fault to test eck operator self-healing ability, like stop kubelet or shutdown node. But es pod will stuck at Terminating status when node goes down, is it by desgin? And is there solution to solve k8s node fault by itself? I know it's hard to handle, we can solve a part of at first. Such as when the k8s nodes get failure and es nodes left cluster, operator try to schedul pod to other k8s node adter few minutes.",
    "website_area": "discuss"
  },
  {
    "id": "dc221330-5efb-445f-ae3f-f5a6c2b2c9e2",
    "url": "https://discuss.elastic.co/t/init-container-fails/193684",
    "title": "Init container fails",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ransoor",
    "date": "August 4, 2019, 2:20pm August 5, 2019, 7:55am August 5, 2019, 6:05pm August 6, 2019, 9:04am August 7, 2019, 12:50pm December 20, 2019, 4:52pm December 20, 2019, 5:08pm December 23, 2019, 2:25pm December 25, 2019, 3:39pm",
    "body": "I followed the quickstart guide version 0.9. I applied the elastic operator CRD. I deployed the elastic cluster with one node, but its status is Init:CrashLoopBackOff. After some debugging, I found that the init container elastic-internal-init-filesystem fails, specifically in the script prepare-fs.sh: \"chowning /usr/share/elasticsearch/data to elasticsearch:elasticsearch chown: changing ownership of '/usr/share/elasticsearch/data': Operation not permitted\" The chown command is being run by root. Am I missing anything?",
    "website_area": "discuss"
  },
  {
    "id": "83911206-0d70-4011-b17e-c916d3de4516",
    "url": "https://discuss.elastic.co/t/credentials-for-elastic-operator/212628",
    "title": "Credentials for elastic-operator",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "December 20, 2019, 8:39am December 20, 2019, 10:34am December 20, 2019, 2:28pm December 20, 2019, 4:19pm December 20, 2019, 5:46pm December 22, 2019, 2:40pm December 23, 2019, 7:59am December 23, 2019, 9:22am December 23, 2019, 1:52pm December 23, 2019, 1:57pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "ab1d1cbc-dcd7-48ea-bc8f-bf49d46b48cc",
    "url": "https://discuss.elastic.co/t/init-container-mount-problem-in-eck/212382",
    "title": "Init container mount problem in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "December 18, 2019, 6:28pm December 19, 2019, 11:10am December 20, 2019, 8:59am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "31154a21-92ea-41bd-b170-9fec1920e2f6",
    "url": "https://discuss.elastic.co/t/apm-on-eck-overwriting-kubernetes-secret-token/211729",
    "title": "APM on ECK overwriting kubernetes secret token",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Andrew_Nichols",
    "date": "December 12, 2019, 9:58pm December 16, 2019, 9:04am December 16, 2019, 10:41pm",
    "body": "We recently added APM to our ECK cluster andrew we're running into an issue. If we remove APM and re-add it the operator will overwrite any existing apm-token set. I would assume it worked like the es-elastic-user secret and just use it if it exists and not overwrite the value. Is this how it's supposed to work? Thanks, Andrew",
    "website_area": "discuss"
  },
  {
    "id": "07962f7a-3c6f-4077-a807-297277ab8f86",
    "url": "https://discuss.elastic.co/t/fails-to-deploy-es-on-kubernetes/211499",
    "title": "Fails to deploy ES on Kubernetes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "nobound",
    "date": "December 11, 2019, 4:36pm December 16, 2019, 9:20am December 16, 2019, 4:30pm",
    "body": "Hi, I am using the elasticsearch helm chart (https://github.com/elastic/helm-charts/tree/master/elasticsearch) to deploy to Kubernetes cluster. It fails to come up properly. I think it is due to the health check (via http) with its hostname. I am just wondering if anyone has seen the same issue. Not sure how to debug the issue. When I run \"kubectl describe pods ...\" ....... Containers: test-es-master-vcqdc-test: Container ID: docker://28b6383270b600ef103edd7d1a83b35374fd8fe80d5aba9a673c029a0e86571c Image: docker.elastic.co/elasticsearch/elasticsearch:7.4.0 Image ID: docker-pullable://docker.elastic.co/elasticsearch/elasticsearch@sha256:ccacb1463adc6daee970ed45e34cc46c14ba22116b64d5d4fac58044dfd61e8c Port: <none> Host Port: <none> Command: sh -c #!/usr/bin/env bash -e curl -XGET --fail 'test-es-master:9200/_cluster/health?wait_for_status=green&timeout=60s' State: Terminated Reason: Error Exit Code: 7 Started: Thu, 21 Nov 2019 12:36:16 -0500 Finished: Thu, 21 Nov 2019 12:36:19 -0500 Ready: False Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-tg24t (ro)",
    "website_area": "discuss"
  },
  {
    "id": "6f05f5a5-f23c-4c44-a50f-5753bb60de5a",
    "url": "https://discuss.elastic.co/t/elastic-7-5-s3-plugin-install-warning-message/211875",
    "title": "Elastic 7.5 S3 plugin install warning message",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sfgroups1",
    "date": "December 14, 2019, 5:06pm December 16, 2019, 8:24am December 16, 2019, 3:15pm",
    "body": "on my 7.5 cluster when install s3 plugin its giving below warning. Any idea what permission needed for java # bin/elasticsearch-plugin install repository-s3 -> Downloading repository-s3 from elastic [=================================================] 100%?? @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: plugin requires additional permissions @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ * java.lang.RuntimePermission accessDeclaredMembers * java.lang.RuntimePermission getClassLoader * java.lang.reflect.ReflectPermission suppressAccessChecks * java.net.SocketPermission * connect,resolve * java.util.PropertyPermission es.allow_insecure_settings read,write See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks. Continue with installation? [y/N]y -> Installed repository-s3",
    "website_area": "discuss"
  },
  {
    "id": "8938e1ea-57cd-4cd6-b333-a146b3b10ab8",
    "url": "https://discuss.elastic.co/t/gcs-snapshots-suddenly-failing-in-eck/210949",
    "title": "GCS Snapshots suddenly failing in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "December 7, 2019, 3:58am December 13, 2019, 5:36am",
    "body": "Hello Elastic Wizards! I'm on ECK 0.8 at the moment, and I have been using the snapshot functionality on GCP quite successfully, with no issues for a long time. I followed these instructions and it has been working quite well. I literally have not touched the cluster in forever. I went to check on my snapshots, and I now notice that since a few weeks ago, I get partial shard failures . 2pGEVje.png1190582 76.3 KB Nothing changed in the cluster state during this time. I've gone over elastic logs for those days, nothing was upgraded or added. The errors look like this: Y9x3xMm.png534747 41.9 KB If there were an error accessing the bucket, you would think that all of the indices would fail. Am I missing something obvious here? Thanks for reading, --Tadgh",
    "website_area": "discuss"
  },
  {
    "id": "6a7d9620-1bfe-4d9d-9d83-0573f168bbf5",
    "url": "https://discuss.elastic.co/t/unable-to-import-kibana-dashboard/210605",
    "title": "Unable to import kibana dashboard",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "jdambly",
    "date": "December 4, 2019, 11:11pm December 10, 2019, 10:05pm December 10, 2019, 10:21pm",
    "body": "I am trying to import a 1.3mb bashoard and am getting the following error Screen Shot 2019-12-04 at 3.03.12 PM.png796798 32.7 KB I set server.maxPayloadBytes to 8388608, here is my kibana manifest apiVersion: kibana.k8s.elastic.co/v1beta1 kind: Kibana metadata: name: kibana spec: version: 7.4.1 count: 1 elasticsearchRef: name: \"elastic\" config: server.maxPayloadBytes: 8388608 http: service: metadata: annotations: external-dns.alpha.kubernetes.io/hostname: kibana.prime.iad0.netskope.com spec: type: LoadBalancer # default is ClusterIP tls: selfSignedCertificate: disabled: true podTemplate: spec: volumes: - name: kibana-plugins emptyDir: {} - name: logtrail-configs configMap: name: log-trail containers: - name: kibana resources: limits: memory: 4Gi volumeMounts: - name: kibana-plugins mountPath: /usr/share/kibana/plugins - name: logtrail-configs mountPath: /usr/share/kibana/plugins/logtrail/logtrail.json subPath: logtrail.json initContainers: - name: install-plugins image: docker.elastic.co/kibana/kibana:7.4.1 command: - sh - -c - bin/kibana-plugin install https://github.com/sivasamyk/logtrail/releases/download/v0.1.31/logtrail-7.4.1-0.1.31.zip volumeMounts: - name: kibana-plugins mountPath: /usr/share/kibana/plugins what I think is odd is that once the pod spawns the value for maxPayLoadBytes seems to have been munged? is this perhaps what is causing my issue? it's set to maxPayloadBytes: 8.388608e+06 is this right? kubectl exec -it kibana-kb-b865c898b-mzb72 -- cat config/kibana.yml elasticsearch: hosts: http://elastic-es-http.logging.svc:9200 password: fgbvvrhw2xz2mh9smkmkl5qq ssl: certificateAuthorities: /usr/share/kibana/config/elasticsearch-certs/ca.crt verificationMode: certificate username: logging-kibana-kibana-user server: host: \"0\" maxPayloadBytes: 8.388608e+06 name: kibana xpack: monitoring: ui: container: elasticsearch: enabled: true",
    "website_area": "discuss"
  },
  {
    "id": "155344b3-24b3-4fb4-8711-1d96fd84f09e",
    "url": "https://discuss.elastic.co/t/eck-1-0-0-beta1-during-node-startup-pod-goes-to-crashloopbackoff/211154",
    "title": "ECK 1.0.0-beta1 during node startup pod goes to CrashLoopBackOff",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sfgroups1",
    "date": "December 9, 2019, 4:12pm December 9, 2019, 5:01pm December 9, 2019, 10:57pm December 10, 2019, 10:15am December 10, 2019, 1:46pm December 10, 2019, 2:01pm December 10, 2019, 7:53pm",
    "body": "Hi, i have 6 node ECK cluster, when ever node reboots elastic 7.5 pod is not starting it goes in 'crashloop node'. If I delete the pod, then its starting properly. I am un shure why POD goes into crashloop. # kgpw -w NAME READY STATUS RESTARTS AGE elastic-operator-0 1/1 Running 8 2d22h elk-prd-es-default-0 1/1 Running 0 22h elk-prd-es-default-1 1/1 Running 0 2d16h elk-prd-es-default-2 1/1 Running 0 2d16h elk-prd-es-default-3 1/1 Running 0 17s elk-prd-es-default-4 0/1 Init:CrashLoopBackOff 9 22h elk-prd-es-default-4 0/1 Init:1/3 10 22h elk-prd-es-default-4 0/1 Init:Error 10 22h elk-prd-es-default-4 0/1 Init:CrashLoopBackOff 10 22h Here is the events: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning BackOff 21m (x5767 over 21h) kubelet, ecknode04 Back-off restarting failed container Normal SandboxChanged 17m kubelet, ecknode04 Pod sandbox changed, it will be killed and re-created. Normal Pulled 17m kubelet, ecknode04 Container image \"docker.elastic.co/elasticsearch/elasticsearch:7.5.0\" already present on machine Normal Created 17m kubelet, ecknode04 Created container elastic-internal-init-filesystem Normal Started 17m kubelet, ecknode04 Started container elastic-internal-init-filesystem Normal Pulled 17m (x4 over 17m) kubelet, ecknode04 Container image \"docker.elastic.co/elasticsearch/elasticsearch:7.5.0\" already present on machine Normal Created 17m (x4 over 17m) kubelet, ecknode04 Created container elastic-internal-init-keystore Normal Started 17m (x4 over 17m) kubelet, ecknode04 Started container elastic-internal-init-keystore Warning BackOff 2m50s (x77 over 17m) kubelet, ecknode04 Back-off restarting failed container any help to resolve this issue?",
    "website_area": "discuss"
  },
  {
    "id": "a223e68f-0b6a-4120-bb1b-0c75c018584c",
    "url": "https://discuss.elastic.co/t/ingress-for-kibana-in-eck/211163",
    "title": "Ingress for Kibana in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "francesc.recasens",
    "date": "December 9, 2019, 5:10pm December 10, 2019, 9:31am",
    "body": "Good afternoon, I'm trying to deploy an Elasticsearch cluster (v1.16.3) using ECK on premise. I'm using OpenEBS for StorageClass. I created a one-node Elasticsearch cluster and also a Kibana, so I can test if it's working or not. When I create the Ingress, I'm always having a HTTP 502 error. I'm using nginxinc/kubernetes-ingress and I tried different solutions (#942). It is also identified here and here. This is the YAML I'm using: apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: elastic-lm spec: version: 7.5.0 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false apiVersion: kibana.k8s.elastic.co/v1beta1 kind: Kibana metadata: name: kibana-lm spec: version: 7.5.0 http: service: spec: type: ClusterIP count: 1 elasticsearchRef: name: elastic-lm apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kibana-lm-ingress namespace: default spec: tls: - hosts: - kibana.example.com rules: - host: kibana.example.com http: paths: - path: / backend: serviceName: kibana-lm-kb-http servicePort: 5601 Thank you so much,",
    "website_area": "discuss"
  },
  {
    "id": "c39feaac-9cf8-47aa-babe-5d8811bebf54",
    "url": "https://discuss.elastic.co/t/use-s3-elastic-plugins/211214",
    "title": "Use S3 elastic plugins",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "pingz",
    "date": "December 10, 2019, 6:18am",
    "body": "Hi: I installed elastic operator to install elastic search and kibana in AWS EKS. I would like to register a S3 Repository for taking snapshot. How do I get the repository-s3 plugin added to the elastic search image? I used kubectl exec -it quickstart-es-default-0 /bin/bash and noticed the plugins directory is empty. Elastic 7.5.0 image is in use. Found the document myself now. https://www.elastic.co/guide/en/cloud-on-k8s/master/k8s-init-containers-plugin-downloads.html",
    "website_area": "discuss"
  },
  {
    "id": "3b1472a6-bdb2-464d-b83d-49632a345c30",
    "url": "https://discuss.elastic.co/t/filebeat-and-metricbeat-as-daemonset-with-eck/210935",
    "title": "Filebeat and metricbeat as daemonset with eck",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ikaposi",
    "date": "December 6, 2019, 7:16pm December 6, 2019, 7:32pm December 6, 2019, 7:54pm",
    "body": "Hi, I've deployed an eck elastic cluster and now I'd like to enable the metricbeat and filebeat daemonsets. The daemonsets are however complaining the x509 certificate is not trusted. I'd prefer to somehow reference the CA utilizing the already defined secrets by ECK, but I could use some help with that. I'm not quite sure what the best approach is here. Anybody already got this working?",
    "website_area": "discuss"
  },
  {
    "id": "98fe9c5b-e4fd-4009-ab75-ca392cc38150",
    "url": "https://discuss.elastic.co/t/feature-change-default-docker-registry-in-operator-command-line-flags/210801",
    "title": "[feature] Change default docker registry in operator command line flags",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LoadingZhang",
    "date": "December 6, 2019, 2:06am December 6, 2019, 8:57am December 6, 2019, 10:32am",
    "body": "I can not download docker images from docker.elastic.co in private network eviroment. So if we could set operator command line flags such as: --default-registry=xxx.xxx.xxx, it would be more convenience to use eck where network can not access internet.",
    "website_area": "discuss"
  },
  {
    "id": "79f60fe7-d10f-4ea9-99f9-02a0206c7ffd",
    "url": "https://discuss.elastic.co/t/how-to-assign-nodeselector-to-kibana/210636",
    "title": "How to assign nodeSelector to Kibana",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LoadingZhang",
    "date": "December 5, 2019, 7:50am December 5, 2019, 8:06am December 5, 2019, 8:55am",
    "body": "I try to add spec: version: 7.5.0 count: 1 nodeSelector: appType: stateless but it didn't work.",
    "website_area": "discuss"
  },
  {
    "id": "f33585e9-d0de-4e87-93c5-5ca9b103fa62",
    "url": "https://discuss.elastic.co/t/specify-known-password-for-the-elasticsearch-user/210239",
    "title": "Specify known password for the elasticsearch user",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "trondhindenes",
    "date": "December 2, 2019, 8:22pm December 2, 2019, 11:55pm December 4, 2019, 7:21am",
    "body": "As far as I understand, ECK will generate a password and place it in a secrets file. This doesn't suit our workflow, we want to provide known values for all things upfront, including the elastic user password. Is there a way to provide it instead of having it auto-generated? As far as I can see, the documentation at least doesn't mention it.",
    "website_area": "discuss"
  },
  {
    "id": "952e2f28-e146-402e-8ddd-23a175e41bb5",
    "url": "https://discuss.elastic.co/t/kibana-no-longer-working/208399",
    "title": "Kibana no longer working",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "getorca",
    "date": "November 18, 2019, 11:12pm November 19, 2019, 10:19am November 19, 2019, 6:44pm November 21, 2019, 6:13pm November 22, 2019, 10:47pm November 23, 2019, 6:31am November 25, 2019, 8:44am November 25, 2019, 8:35pm November 26, 2019, 9:23am November 26, 2019, 6:03pm November 26, 2019, 6:15pm November 27, 2019, 8:46am December 3, 2019, 8:10pm December 3, 2019, 8:51pm",
    "body": "As of a few hours ago, Kibana is no longer working. it crashed suddenly and no the pod for Kibana is failing to achieve a Ready state. The ES pods are still working fine. The most obvious errors from the kibana pods seem to relate to getting a licence from Elasticsearch for xpack: {\"type\":\"log\",\"@timestamp\":\"2019-11-18T23:08:21Z\",\"tags\":[\"warning\",\"task_manager\"],\"pid\":1,\"message\":\"PollError Request Timeout after 30000ms\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-18T23:08:40Z\",\"tags\":[\"license\",\"warning\",\"xpack\"],\"pid\":1,\"message\":\"License information from the X-Pack plugin could not be obtained from Elasticsearch for the [data] cluster. Error: Request Timeout after 30000ms\"} ES version: 7.2 Kubernetes version: v1alpha1 I'm not sure why this would occur suddenly. Any ideas to diagnose?",
    "website_area": "discuss"
  },
  {
    "id": "3b925bcc-cc07-4f7a-acec-1de19844ee38",
    "url": "https://discuss.elastic.co/t/fresh-cluster-all-shards-are-unavailable/210216",
    "title": "Fresh cluster all shards are unavailable",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dg_hivebrite",
    "date": "December 2, 2019, 4:31pm December 2, 2019, 7:01pm December 3, 2019, 8:06am December 3, 2019, 10:38am December 3, 2019, 10:47am December 3, 2019, 11:02am December 3, 2019, 12:13pm December 3, 2019, 1:28pm December 3, 2019, 1:43pm December 3, 2019, 2:34pm",
    "body": "Hi, When my cluster is started, his status is in yellow: { \"cluster_name\" : \"datawarehouse\", \"status\" : \"yellow\", \"timed_out\" : false, \"number_of_nodes\" : 6, \"number_of_data_nodes\" : 0, \"active_primary_shards\" : 0, \"active_shards\" : 0, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 9, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 0.0 } Also in parallel I check the kibana logs (because the service can't start), the error is: {\"type\":\"log\",\"@timestamp\":\"2019-12-02T15:58:39Z\",\"tags\":[\"security\",\"error\"],\"pid\":6,\"message\":\"Error registering Kibana Privileges with Elasticsearch for kibana-.kibana: [unavailable_shards_exception] at least one primary shard for the index [.security-7] is unavailable\"} I check my shards: curl -k -u \"elastic:xxxxx\" \"https://datawarehouse-es-http:9200/_c at/shards?h=index,shard,prirep,state,unassigned.reason\" --silent .security-7 0 p UNASSIGNED INDEX_CREATED .kibana_task_manager_1 0 p UNASSIGNED INDEX_CREATED .kibana_task_manager_1 0 r UNASSIGNED INDEX_CREATED .kibana_1 0 p UNASSIGNED INDEX_CREATED .kibana_1 0 r UNASSIGNED INDEX_CREATED .apm-agent-configuration 0 p UNASSIGNED INDEX_CREATED .apm-agent-configuration 0 r UNASSIGNED INDEX_CREATED Then I check the cluster allocation: curl -k -u \"elastic:xxxxx\" \"https://datawarehouse-es-http:9200/_c luster/allocation/explain?pretty\" { \"index\" : \".kibana_1\", \"shard\" : 0, \"primary\" : true, \"current_state\" : \"unassigned\", \"unassigned_info\" : { \"reason\" : \"INDEX_CREATED\", \"at\" : \"2019-12-02T15:34:31.969Z\", \"last_allocation_status\" : \"no_attempt\" }, \"can_allocate\" : \"no\", \"allocate_explanation\" : \"cannot allocate because allocation is not permitted to any of the nodes\" } Also I found this error in the elasticsearch logs: org.elasticsearch.action.UnavailableShardsException: at least one primary shard for the index [.security-7] is unavailable I check the cluster settings: curl -k -u \"elastic:xxxxx\" https://datawarehouse-es-http:9200/_cl uster/settings?pretty { \"persistent\" : { }, \"transient\" : { \"cluster\" : { \"routing\" : { \"allocation\" : { \"exclude\" : { \"_name\" : \"none_excluded\" } } } } } } ECK version: 1.0.0-beta elasticsearch version: 7.4.2 elasticsearch config: https://gist.github.com/Dudesons/f4af00f15790175e3f360577b8f04f15 (it's the config of 1 nodes) cluster: 3 master & 3 data nodes I tried to boot a new cluster, the problem persists If someone can help me to understand / fix the issue it would be awesome.",
    "website_area": "discuss"
  },
  {
    "id": "5070cdce-143b-4b07-8b8d-0ce32af23ed8",
    "url": "https://discuss.elastic.co/t/horizontal-and-vertical-upgrade-of-eck-cluster/210309",
    "title": "Horizontal and vertical upgrade of eck cluster",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dineshabbi",
    "date": "December 3, 2019, 8:50am December 3, 2019, 9:08am",
    "body": "As per the documentation, I tried doing upgrades of the cluster using some of the examples given in https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-orchestration.html a) Horizontal upgrade: I see a new service getting created with every statefulset creation, but under the hood they all belong to one cluster and is accessible via http service 9200 right ? How can I confirm that the shards are being rebalanced after horizontal upgrade ?  kubectl get svc -n elastic-system   NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch-exporter ClusterIP 172.20.72.204 9108/TCP 6d1h elasticsearch-sample-es-data-nodes ClusterIP None 5d20h elasticsearch-sample-es-data-nodes-2 ClusterIP None 41h elasticsearch-sample-es-http ClusterIP 172.20.133.254 9200/TCP 5d20h kibana-sample-kb-http ClusterIP 172.20.94.101 5601/TCP 5d20h b) Vertical upgrade I tried, but I did specify a wrong RAM size while specifying, and the cluster got into a state where it could not recover/add any new pods after that. I am using 1.0 beta version, and I wonder vertical upgrade is fully supported in this release. create Pod elasticsearch-sample-es-data-nodes-2-0 in StatefulSet elasticsearch-sample-es-data-nodes-2 failed error: Pod \"elasticsearch-sample-es-data-nodes-2-0\" is invalid: spec.containers[0].resources.requests: Invalid value: \"3Gi\": must be less than or equal to memory limit",
    "website_area": "discuss"
  },
  {
    "id": "663576df-0e3e-49dc-95b9-2e5fcddc147a",
    "url": "https://discuss.elastic.co/t/eck-and-es-on-eks-1-12-installation-setup-issue/206401",
    "title": "ECK and ES on EKS 1.12 installation / setup issue",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sathia.thayathil",
    "date": "November 4, 2019, 12:34pm December 2, 2019, 10:47am",
    "body": "{\"type\": \"server\", \"timestamp\": \"2019-11-04T12:12:27,916Z\", \"level\": \"WARN\", \"component\": \"o.e.c.c.ClusterFormationFailureHelper\", \"cluster.name\": \"elastic\", \"node.name\": \"elastic-es-default-0\", \"message\": \"master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and [cluster.initial_master_nodes] is empty on this node: have discovered [{elastic-es-default-0}{R6HYNNymSF-jhqz5y3EJuQ}{kWMPQe05TUmH9jii7PVzcA}{10.32.0.32}{10.32.0.32:9300}{dilm}{ml.machine_memory=4294967296, xpack.installed=true, ml.max_open_jobs=20}]; discovery will continue using [127.0.0.1:9300, 127.0.0.1:9301, 127.0.0.1:9302, 127.0.0.1:9303, 127.0.0.1:9304, 127.0.0.1:9305] from hosts providers and [{elastic-es-default-0}{R6HYNNymSF-jhqz5y3EJuQ}{kWMPQe05TUmH9jii7PVzcA}{10.32.0.32}{10.32.0.32:9300}{dilm}{ml.machine_memory=4294967296, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\" } {\"type\": \"server\", \"timestamp\": \"2019-11-04T12:12:28,008Z\", \"level\": \"WARN\", \"component\": \"r.suppressed\", \"cluster.name\": \"elastic\", \"node.name\": \"elastic-es-default-0\", \"message\": \"path: /_bulk, params: {}\", \"stacktrace\": [\"org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized, SERVICE_UNAVAILABLE/2/no master];\",",
    "website_area": "discuss"
  },
  {
    "id": "5d594115-7906-4c79-aff0-244ae348bbd6",
    "url": "https://discuss.elastic.co/t/eck-1-0-0-beta1-doesnt-start-pod/209886",
    "title": "ECK 1.0.0-beta1 doesn't start pod",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dg_hivebrite",
    "date": "November 28, 2019, 4:16pm November 29, 2019, 8:08am December 2, 2019, 10:37am",
    "body": "Hi, I'm beginning to use ECK, I deploy it on my kubernetes cluster hosted on GKE. The problem is when I'm checking if the elasticsearch cluster is running I can find my stateful sets but without pod. So I check, the count is set to 0 (in my manifest it sets to 1). When I tried to set the replica to 1, I have an other error: create Pod datawarehouse-es-master-europe-west1-c-0 in StatefulSet datawarehouse-es-master-europe-west1-c failed error: Pod \"datawarehouse-es-master-europe-west1-c-0\" is invalid: spec.containers[0].image: Required value So it seems I miss something but I don't understand where is the issue, my elasticsearch resource have an image and count, if someone can help i would be awesome. I share with you my manifests: https://gist.github.com/Dudesons/e042c7e42b94bb8f14b3e2e5537831a0",
    "website_area": "discuss"
  },
  {
    "id": "e68d2211-b65d-41ff-94a9-ff8c40361b7a",
    "url": "https://discuss.elastic.co/t/eck-compatibility-with-docker-for-windows-kubernetes/209873",
    "title": "ECK compatibility with Docker for Windows (Kubernetes)",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "olc-olivier",
    "date": "November 28, 2019, 2:19pm November 29, 2019, 8:57am November 29, 2019, 2:43pm",
    "body": "Hi, Someone could tell me if I can install ECK with Docker for Windows (compliance) ? Thx. PS: I have this error. \"logger\":\"transport\",\"message\":\"Skipping pod because it has no IP yet\",\"ver\":\"1.0.0-beta1-84792e30\",\"namespace\":\"default\",\"pod_name\":\"quickstart-es-default-0\"} Events: Type Reason Age From Message Warning FailedScheduling 67s (x2 over 67s) default-scheduler pod has unbound immediate PersistentVolumeClaims",
    "website_area": "discuss"
  },
  {
    "id": "d5487135-3f7d-474e-b481-644a0a16f6d6",
    "url": "https://discuss.elastic.co/t/eck-1-0-with-on-premise-k8s/209745",
    "title": "ECK 1.0 with on premise K8s",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ikaposi",
    "date": "November 27, 2019, 6:32pm November 27, 2019, 7:51pm",
    "body": "Hi, I have installed a basic k8s cluster with kubeadm; cluster node state is ready and I can deploy an nginx image.. so far so good. Next I installed the elastic operator (1.0.0-beta1) using the steps described on: https://www.elastic.co/downloads/elastic-cloud-kubernetes The operator seems to install fine and I can launch a kibana instance. However elasticsearch pods remain in pending state and the error message I get is: pod has unbound immediate PersistentVolumeClaims If i type kubectl describe pvc elastic i get: no persistent volumes available for this claim and no storage class is set I tried creating a pv but with no success. Does anybody have an idea what I am doing wrong?",
    "website_area": "discuss"
  },
  {
    "id": "4a623578-08bc-42d0-8101-38a4f9fc457e",
    "url": "https://discuss.elastic.co/t/waiting-for-the-transport-certificates-mnt-elastic-internal-transport-certificates-elasticsearch-sample-es-default-0-tls-key/209313",
    "title": "Waiting for the transport certificates (/mnt/elastic-internal/transport-certificates/elasticsearch-sample-es-default-0.tls.key)",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ftyuuu",
    "date": "November 25, 2019, 1:36pm November 26, 2019, 9:11am November 26, 2019, 11:28am",
    "body": "I want to debug es, so i commented on the following code in cmd/manager/main.go. But pod stop at init container (elastic-internal-init-filesystem). if operator.HasRole(operator.NamespaceOperator, roles) { //if err = apmserver.Add(mgr, params); err != nil { // log.Error(err, \"unable to create controller\", \"controller\", \"ApmServer\") // os.Exit(1) //} if err = elasticsearch.Add(mgr, params); err != nil { log.Error(err, \"unable to create controller\", \"controller\", \"Elasticsearch\") os.Exit(1) } //if err = kibana.Add(mgr, params); err != nil { // log.Error(err, \"unable to create controller\", \"controller\", \"Kibana\") // os.Exit(1) //} //if err = asesassn.Add(mgr, params); err != nil { // log.Error(err, \"unable to create controller\", \"controller\", \"ApmServerElasticsearchAssociation\") // os.Exit(1) //} //if err = kbassn.Add(mgr, params); err != nil { // log.Error(err, \"unable to create controller\", \"controller\", \"KibanaAssociation\") // os.Exit(1) //} } Logs: Starting init script ... '/usr/share/elasticsearch/bin/x-pack' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack' '/usr/share/elasticsearch/bin/x-pack/certgen' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/certgen' '/usr/share/elasticsearch/bin/x-pack/users' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/users' '/usr/share/elasticsearch/bin/x-pack/saml-metadata.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/saml-metadata.bat' '/usr/share/elasticsearch/bin/x-pack/sql-cli.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/sql-cli.bat' '/usr/share/elasticsearch/bin/x-pack/croneval' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/croneval' '/usr/share/elasticsearch/bin/x-pack/certutil' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/certutil' '/usr/share/elasticsearch/bin/x-pack/sql-cli' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/sql-cli' '/usr/share/elasticsearch/bin/x-pack/migrate.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/migrate.bat' '/usr/share/elasticsearch/bin/x-pack/certutil.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/certutil.bat' '/usr/share/elasticsearch/bin/x-pack/setup-passwords' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/setup-passwords' '/usr/share/elasticsearch/bin/x-pack/setup-passwords.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/setup-passwords.bat' '/usr/share/elasticsearch/bin/x-pack/certgen.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/certgen.bat' '/usr/share/elasticsearch/bin/x-pack/croneval.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/croneval.bat' '/usr/share/elasticsearch/bin/x-pack/syskeygen.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/syskeygen.bat' '/usr/share/elasticsearch/bin/x-pack/saml-metadata' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/saml-metadata' '/usr/share/elasticsearch/bin/x-pack/syskeygen' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/syskeygen' '/usr/share/elasticsearch/bin/x-pack/migrate' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/migrate' '/usr/share/elasticsearch/bin/x-pack/users.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/users.bat' '/usr/share/elasticsearch/bin/x-pack-env' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-env' '/usr/share/elasticsearch/bin/x-pack-env.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-env.bat' '/usr/share/elasticsearch/bin/x-pack-security-env' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-security-env' '/usr/share/elasticsearch/bin/x-pack-security-env.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-security-env.bat' '/usr/share/elasticsearch/bin/x-pack-watcher-env' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-watcher-env' '/usr/share/elasticsearch/bin/x-pack-watcher-env.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-watcher-env.bat' Files copy duration: 0 sec. chowning /usr/share/elasticsearch/data to elasticsearch:elasticsearch ownership of '/usr/share/elasticsearch/data' retained as elasticsearch:elasticsearch chowning /usr/share/elasticsearch/logs to elasticsearch:elasticsearch changed ownership of '/usr/share/elasticsearch/logs' from root:root to elasticsearch:elasticsearch chown duration: 0 sec. waiting for the transport certificates (/mnt/elastic-internal/transport-certificates/elasticsearch-sample-es-default-0.tls.key) what should i do?",
    "website_area": "discuss"
  },
  {
    "id": "f474275f-212b-4cea-b41b-ab2d4ada4b73",
    "url": "https://discuss.elastic.co/t/new-user-cant-login-kibana/204810",
    "title": "New user can't login kibana",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "lonelyleaf",
    "date": "October 23, 2019, 7:57am October 23, 2019, 8:11am October 23, 2019, 8:49am November 19, 2019, 3:37pm November 26, 2019, 10:08am November 26, 2019, 10:26am November 26, 2019, 10:42am",
    "body": "I setup eck and login by default \"elastic\" account successfully.But I or logstash can't login by new account created in kibana. {\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"[security_exception] unable to authenticate user [gmt-log-logstash] for REST request [/_security/_authenticate], with { header={ WWW-Authenticate={ 0=\"Bearer realm=\\\"security\\\"\" & 1=\"ApiKey\" & 2=\"Basic realm=\\\"security\\\" charset=\\\"UTF-8\\\"\" } } }\"}",
    "website_area": "discuss"
  },
  {
    "id": "64e8a939-90c8-45d0-a415-490ae1dfc98b",
    "url": "https://discuss.elastic.co/t/cant-change-xpack-security-realms-in-eck-1-0/209118",
    "title": "Can't change xpack security realms in ECK 1.0",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "November 22, 2019, 7:45pm November 25, 2019, 8:21am November 25, 2019, 9:44pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "690efa65-4206-4dc6-b030-a796c1ff124e",
    "url": "https://discuss.elastic.co/t/eck-data-node-type-selection/209277",
    "title": "ECK data node type selection",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Komal_Agarwal",
    "date": "November 25, 2019, 10:20am November 25, 2019, 10:37am",
    "body": "We have a cluster holding ~400gbs of data. We have divided this using 6 data nodes(2 nodes X 3 zones). Currently, in Elastic Cloud, all these are I3 machines. We want to understand what would be better in ECK - I3 nodes using local storage or another node such as R5/M5 using EBS volumes. It seems I3 might be better keeping memory in mind, but then EBS volumes would be faster to restore data in case of node failure. Please suggest.",
    "website_area": "discuss"
  },
  {
    "id": "d2595059-3cac-47f3-b4d4-cf1d136ace04",
    "url": "https://discuss.elastic.co/t/ssl-handshake-fails-between-kibana-apmserver-and-elasticsearch-after-custom-certificate-setting-on-elasticsearch/208604",
    "title": "SSL handshake fails between Kibana/APMServer and Elasticsearch after custom certificate setting on Elasticsearch",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Bingu_Shim",
    "date": "November 20, 2019, 4:15pm November 20, 2019, 1:37pm November 21, 2019, 2:17am",
    "body": "I'm trying to configure our own certificate to the Elasticsearch, Kibana and APM Server, and got ssl handshake errors. I referenced This document for setting. The symptoms and my environments are as follow, and if there is any more information please let me know. Environment ECK Version : 1.0.0Beta1 Elastic Image Version : 7.4.2 Certificate Info : Issued by a well-known CA to the *.mycompany.com domain Certificate is add to the k8s cluster with this name : my-cert Referenced this manual Domain names for service are registered to the DNS Server as follow Elasticsearch : es.mycompany.com (10.0.0.2) Kibana : kb.mycompany.com (10.0.0.3) APM Server : apm.mycompany.com (10.0.0.4) (I replaced the domain name as mycompany.com) Symptoms I can connect with Chrome browser without any certificate error with this url https://es.mycompany.com:9200 However, Kibana readness fails and the logs are: {\"type\":\"log\",\"@timestamp\":\"2019-11-19T07:59:41Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":6,\"message\":\"No living connections\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-19T07:59:43Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":6,\"message\":\"Unable to revive connection: https://es-cluster-es-http.default.svc:9200/\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-19T07:59:43Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":6,\"message\":\"No living connections\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-19T07:59:46Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":6,\"message\":\"Unable to revive connection: https://es-cluster-es-http.default.svc:9200/\"} APMServer passed the readness check but doen't work properly, and the logs are: 2019-11-19T09:27:47.607Z ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(https://es-cluster-es-http.default.svc:9200)): Get https://es-cluster-es-http.default.svc:9200: x509: certificate is valid for *.mycompany.com, mycompany.com, not es-cluster-es-http.default.svc 2019-11-19T09:27:47.607Z INFO pipeline/output.go:93 Attempting to reconnect to backoff(elasticsearch(https://es-cluster-es-http.default.svc:9200)) with 149 reconnect attempt(s) Yaml Files The followings are the yaml files that I used. (I replaced domain and IP address) # Source: apm.yaml apiVersion: apm.k8s.elastic.co/v1beta1 kind: ApmServer metadata: name: es-cluster spec: version: 7.4.2 http: service: spec: type: LoadBalancer loadBalancerIP: 10.0.0.4 tls: selfSignedCertificate: subjectAltNames: - ip: 10.0.0.4 - dns: apm.mycompany.com certificate: secretName: my-cert count: 1 elasticsearchRef: name: es-cluster podTemplate: metadata: labels: project: paas idc: app : apmServer spec: containers: - name: apm-server resources: request: memory: 12Gi cpu: 1 limits: memory: 12Gi cpu: 3 env: - name: ES_JAVA_OPTS value: \"-Xms6g -Xmx6g\" --- # Source: es.yaml apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: es-cluster spec: http: service: spec: type: LoadBalancer loadBalancerIP: 10.0.0.2 tls: selfSignedCertificate: subjectAltNames: - ip: 10.0.0.2 - dns: es.mycompany.com certificate: secretName: my-cert version: 7.4.2 nodeSets: - name: node count: 1 config: node.master: true node.ingest: true node.data: true node.store.allow_mmap: true podTemplate: metadata: labels: name: master spec: initContainers: - name: sysctl securityContext: privileged: true command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144'] - name: install-gcs-plugins command: ['sh', '-c', 'bin/elasticsearch-plugin install --batch repository-gcs'] - name: install-hdfs-plugins command: ['sh', '-c', 'bin/elasticsearch-plugin install --batch repository-hdfs'] containers: - name: elasticsearch resources: request: memory: 12Gi cpu: 1 limits: memory: 12Gi cpu: 3 env: - name: ES_JAVA_OPTS value: \"-Xms6g -Xmx6g\" volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Gi storageClassName: fast --- # Source: kb.yaml apiVersion: kibana.k8s.elastic.co/v1beta1 kind: Kibana metadata: name: es-cluster spec: version: 7.4.2 count: 1 elasticsearchRef: name: es-cluster http: service: spec: type: LoadBalancer loadBalancerIP: 10.0.0.3 tls: selfSignedCertificate: subjectAltNames: - ip: 10.0.0.3 - dns: kb.mycompany.com certificate: secretName: my-cert podTemplate: metadata: labels: name: kb-alpha spec: containers: - name: kibana resources: request: memory: 12Gi cpu: 1 limits: memory: 12Gi cpu: 3 env: - name: ES_JAVA_OPTS value: \"-Xms6g -Xmx6g\" I tried these Elasticsearch http configs and it all fail with same error http: ... tls: selfSignedCertificate: disabled: true certificate: secretName: my-cert http: ... tls: certificate: secretName: my-cert",
    "website_area": "discuss"
  },
  {
    "id": "15d51156-d28c-49eb-9022-3861d8d23d83",
    "url": "https://discuss.elastic.co/t/kibana-cant-connect-to-es-having-openstack-loadbalancer-setting/206691",
    "title": "Kibana can't connect to ES having (openstack) LoadBalancer setting",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JuYeong_Park",
    "date": "November 5, 2019, 11:21pm November 7, 2019, 2:35pm November 10, 2019, 10:28pm November 18, 2019, 11:44am November 19, 2019, 5:00am",
    "body": "I want to use ES and Kibana on LoadBalancer Env. So, i added load balancer options to ES yaml as below. apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: lb spec: http: service: metadata: annotations: service.beta.kubernetes.io/openstack-internal-load-balancer: \"true\" spec: externalTrafficPolicy: Local type: LoadBalancer ... And then, i executed Kibana yaml(default) as below. apiVersion: kibana.k8s.elastic.co/v1beta1 kind: Kibana metadata: name: test spec: version: 6.8.0 count: 1 elasticsearchRef: name: \"lb\" podTemplate: spec: containers: - name: kibana resources: limits: memory: 1Gi cpu: 1 On this settings, kibana coudln't be executed normally having error as below. kubernetes error Warning Unhealthy 2m43s (x179 over 32m) kubelet, dkosv3-ingress-test-worker-4 Readiness probe failed: HTTP probe failed with statuscode: 503 docker error {\"type\":\"log\",\"@timestamp\":\"2019-11-05T22:57:32Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"Unable to revive connection: https://lb-es-http.default.svc:9200/\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-05T22:57:32Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"No living connections\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-05T22:57:32Z\",\"tags\":[\"warning\",\"task_manager\"],\"pid\":1,\"message\":\"PollError No Living connections\"} So, i want to know that what options is needed to use kibana for es having load balancer settings.",
    "website_area": "discuss"
  },
  {
    "id": "c1037806-bcd3-4672-8304-01ab45281151",
    "url": "https://discuss.elastic.co/t/trouble-installing-kibana-plugin/207984",
    "title": "Trouble installing kibana plugin",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "terryE",
    "date": "November 15, 2019, 2:34am November 15, 2019, 8:05pm",
    "body": "I'm trying to use an init container to install a kibana plugin but I'm getting an error. Events: Type Reason Age From Message Warning ReconciliationError 11m (x18 over 30m) kibana-controller Reconciliation error: Deployment.apps \"logs-kb\" is invalid: spec.template.spec.initContainers[0].image: Required value I'm hoping I don't need to specify the image in the podtemplate and can leave it to eck to populate this correctly. Am I doing something wrong? Here's my kibana spec apiVersion: kibana.k8s.elastic.co/v1beta1 kind: Kibana metadata: name: logs namespace: tools spec: version: 7.4.2 count: 1 elasticsearchRef: name: logs http: tls: selfSignedCertificate: disabled: true podTemplate: spec: initContainers: - name: install-plugins command: ['sh', '-c', '|bin/kibana-plugin install https://github.com/sivasamyk/logtrail/releases/download/v0.1.31/logtrail-7.4.1-0.1.31.zip' ]",
    "website_area": "discuss"
  },
  {
    "id": "a638dc00-036b-463c-8a36-2db165f03fec",
    "url": "https://discuss.elastic.co/t/having-issues-with-more-than-1-instance/207154",
    "title": "Having issues with more than 1 instance",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "booboothefool",
    "date": "November 8, 2019, 5:27pm November 8, 2019, 9:16pm November 13, 2019, 9:30pm",
    "body": "Hi there so I've gotten the single instance configuration to work. And Kibana is able to connect to it. apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: elastic spec: version: 7.4.2 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false http: tls: selfSignedCertificate: disabled: true However as soon as I try to add more, in order to scale out reads e.g.: apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: elastic spec: version: 7.4.2 nodeSets: - name: default count: 3 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false http: tls: selfSignedCertificate: disabled: true apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: elastic spec: version: 7.4.2 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false - name: data count: 2 config: node.master: false node.data: true node.ingest: false node.store.allow_mmap: false http: tls: selfSignedCertificate: disabled: true All of my Elasticsearch pods start crashing/erroring out, and Kibana is not able to connect. If I start with 1 Elasticsearch + 1 Kibana it is able to connect, then when I scale up to 3 Elasticsearch, the pods stay running but eventually Kibana shows everything in a red state and it no longer works. I am running on an 8 vCPU x 32GB mem + 4 vCPU x 16GB mem + 4 vCPU x 16GB mem, so I believe I have enough resources. Any suggestions?",
    "website_area": "discuss"
  },
  {
    "id": "0c6180fc-75c5-43f2-b13a-8dd140acf5da",
    "url": "https://discuss.elastic.co/t/cross-cluster-search-on-eck-how-to-set-ca-cert/202113",
    "title": "Cross Cluster Search on ECK - how to set ca cert",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "iremmats",
    "date": "October 3, 2019, 8:24am October 16, 2019, 11:54pm October 17, 2019, 5:54am October 17, 2019, 7:32pm October 17, 2019, 8:21pm October 18, 2019, 8:26am October 18, 2019, 9:01am October 18, 2019, 5:53pm October 20, 2019, 6:39pm October 20, 2019, 11:52pm October 20, 2019, 11:57pm October 21, 2019, 6:03am October 21, 2019, 9:10am October 21, 2019, 9:11am October 23, 2019, 8:48am October 25, 2019, 6:48pm November 12, 2019, 9:40pm",
    "body": "Hi, So we are playing around with ECK on multiple clusters in different region. We want to establish cross cluster search from one central cluster. How can we set the ca-cert for our clusters during creation? We followed the instructions here to set the http certificate but how do we set the CA cert? https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-custom-http-certificate.html This is the error we get right now. { \"error\": { \"root_cause\": [ { \"type\": \"transport_exception\", \"reason\": \"handshake failed because connection reset\" } ], \"type\": \"connect_transport_exception\", \"reason\": \"[100.102.0.8:9300] general node connection failure\", \"caused_by\": { \"type\": \"transport_exception\", \"reason\": \"handshake failed because connection reset\" } }, \"status\": 500 }",
    "website_area": "discuss"
  },
  {
    "id": "6a5084d8-2e13-4b64-8a40-07af559db4d1",
    "url": "https://discuss.elastic.co/t/logstash-and-eck/206997",
    "title": "Logstash and ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dbwest",
    "date": "November 7, 2019, 6:09pm November 7, 2019, 9:54pm November 7, 2019, 10:32pm",
    "body": "Is Logstash part of ECK? How do I add it in? Is it currently on any dev branches or are there any ways to get edge releases that include it? I do not see it in stable. Are there any plans to add it in?",
    "website_area": "discuss"
  },
  {
    "id": "03fac409-9afe-4460-a8c0-cc9f2e7f3404",
    "url": "https://discuss.elastic.co/t/does-helm-use-official-eck-under-the-hood/206067",
    "title": "Does Helm use official ECK under the hood?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LeFrancois",
    "date": "October 31, 2019, 3:04pm November 6, 2019, 7:47am",
    "body": "Hello, After reading the doc of Helm charts I saw it was grounded on the Docker images of ElasticSearch. But under the hood it seem strange that the features / codebase is not shared with ECK. I would like to clarifiy to know if the official Helm charts provide the same features as describe here ? Or this only a normal version of ES. I know the question can seem weird, but the number of features worth to put aside your official Chart . Thank you in advance",
    "website_area": "discuss"
  },
  {
    "id": "e3128b48-928c-4453-a98f-94ffeaaaa27c",
    "url": "https://discuss.elastic.co/t/failed-to-get-api-group-resources/206482",
    "title": "Failed to get API Group-Resources",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "November 5, 2019, 4:29pm November 5, 2019, 10:46am November 5, 2019, 8:28pm",
    "body": "Hello World! I'm trying to Deploy ECK in your Kubernetes cluster, yet running into following issue: $ microk8s.kubectl apply -f https://download.elastic.co/downloads/eck/1.0.0-beta1/all-in-one.yaml customresourcedefinition.apiextensions.k8s.io/apmservers.apm.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/elasticsearches.elasticsearch.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co created clusterrole.rbac.authorization.k8s.io/elastic-operator created clusterrolebinding.rbac.authorization.k8s.io/elastic-operator created namespace/elastic-system created statefulset.apps/elastic-operator created serviceaccount/elastic-operator created $ microk8s.kubectl -n elastic-system logs -f statefulset.apps/elastic-operator Error from server: Get https://noc.uftwf.local:10250/containerLogs/elastic-system/elastic-operator-0/manager?follow=true: dial tcp: lookup noc.uftwf.local on 208.67.222.222:53: no such host $ I thought maybe this would help: $ echo \"X.X.X.X noc.uftwf.local\" >> /etc/hosts $ microk8s.kubectl -n elastic-system logs -f statefulset.apps/elastic-operator {\"level\":\"info\",\"@timestamp\":\"2019-11-04T21:30:03.518Z\",\"logger\":\"manager\",\"message\":\"Setting up client for manager\",\"ver\":\"1.0.0-beta1-84792e30\"} {\"level\":\"info\",\"@timestamp\":\"2019-11-04T21:30:03.518Z\",\"logger\":\"manager\",\"message\":\"Setting up scheme\",\"ver\":\"1.0.0-beta1-84792e30\"} {\"level\":\"info\",\"@timestamp\":\"2019-11-04T21:30:03.518Z\",\"logger\":\"manager\",\"message\":\"Setting up manager\",\"ver\":\"1.0.0-beta1-84792e30\"} {\"level\":\"error\",\"@timestamp\":\"2019-11-04T21:30:04.563Z\",\"logger\":\"controller-runtime.manager\",\"message\":\"Failed to get API Group-Resources\",\"ver\":\"1.0.0-beta1-84792e30\",\"error\":\"Get https://10.152.183.1:443/api?timeout=32s: dial tcp 10.152.183.1:443: connect: no route to host\",\"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\nsigs.k8s.io/controller-runtime/pkg/manager.New\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.2.1/pkg/manager/manager.go:212\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.execute\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:227\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.glob..func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:74\\ngithub.com/spf13/cobra.(*Command).execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:830\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:914\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:864\\nmain.main\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/main.go:27\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\"} {\"level\":\"error\",\"@timestamp\":\"2019-11-04T21:30:04.564Z\",\"logger\":\"manager\",\"message\":\"unable to create controller manager\",\"ver\":\"1.0.0-beta1-84792e30\",\"error\":\"Get https://10.152.183.1:443/api?timeout=32s: dial tcp 10.152.183.1:443: connect: no route to host\",\"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.execute\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:229\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.glob..func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:74\\ngithub.com/spf13/cobra.(*Command).execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:830\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:914\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:864\\nmain.main\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/main.go:27\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\"} $ but it didn't( Please advise.",
    "website_area": "discuss"
  },
  {
    "id": "cc989a23-b3ef-4919-a96e-5c534363c155",
    "url": "https://discuss.elastic.co/t/adding-a-custom-volume-for-backups/205324",
    "title": "Adding a Custom Volume for Backups",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Zorlack",
    "date": "October 25, 2019, 8:21pm October 25, 2019, 8:49pm October 25, 2019, 9:03pm October 26, 2019, 3:32am October 28, 2019, 6:01am October 29, 2019, 12:54pm October 29, 2019, 1:32pm October 29, 2019, 2:23pm October 29, 2019, 2:48pm October 30, 2019, 9:05am November 4, 2019, 1:33pm",
    "body": "Hello, I'm interested in mounting a shared NFS PersistentVolume across an Elasticsearch ECK cluster so that I can add it has a Snapshot Repository. (This seems to be the obvious solution for on-prem snapshotting) I've created a deployment (by cribbing from the Synonym configmap example) which attempts to use podTemplate to accomplish this: [SNIP] podTemplate: metadata: labels: es-role: \"data-search\" spec: containers: - name: elasticsearch resources: limits: memory: 60G cpu: 7 env: - name: ES_JAVA_OPTS value: \"-Xms30g -Xmx30g\" volumeMounts: - name: snapshot-claim-volume mountPath: /mnt/snapshots volumes: - name: snapshot-claim-volume persistentVolumeClaim: claimName: snapshot-claim [SNIP] When I try to deploy this using ECK, It gets stuck starting with: create Pod fusion-search-a-es-data-searchers-0 in StatefulSet fusion-search-a-es-data-searchers failed error: Pod \"fusion-search-a-es-data-searchers-0\" is invalid: [spec.containers[0].volumeMounts[12].name: Not found: \"snapshot-claim-volume\", spec.initContainers[0].volumeMounts[12].name: Not found: \"snapshot-claim-volume\"] Have I misunderstood something? What am I doing wrong here? Many thanks! -Z",
    "website_area": "discuss"
  },
  {
    "id": "4a3fcc66-424f-425e-8e20-eb7177cc755f",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-use-oss-docker-images-without-xpack/184410",
    "title": "Is it possible to use OSS docker images (without xpack)?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ismailbaskin",
    "date": "June 5, 2019, 3:20pm June 5, 2019, 3:34pm July 25, 2019, 7:04am November 2, 2019, 5:37pm",
    "body": "Hi, I'm not sure but it seems ECK operator is strictly depends on xpack features. Is there any way to use it open source version of ElasticSearch with ECK operator?",
    "website_area": "discuss"
  },
  {
    "id": "ec3e7ab0-0582-4a35-b8b9-953832cd5155",
    "url": "https://discuss.elastic.co/t/eck-istio-mtls/205070",
    "title": "ECK + Istio mTLS",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Spencer_Gilbert",
    "date": "October 24, 2019, 12:54pm October 24, 2019, 2:55pm October 24, 2019, 4:38pm October 25, 2019, 12:49pm October 25, 2019, 1:36pm October 28, 2019, 3:58pm October 28, 2019, 2:49pm November 1, 2019, 11:01am",
    "body": "I saw on the release notes a mention of using ECK with Istio with disabling HTTP level TLS - I've been trying to use mTLS with Istio and ES and running into problems both with the transport layer (which I think I've resolved by using network.bind_host = 127.0.0.1) and the ECK managed Kibana which seems to fail to communicate with the ES cluster when mTLS is enabled. Is this supported, and if so is there any guidance on making this work? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "a90bb26e-eab0-4df1-a926-ae8ff625a829",
    "url": "https://discuss.elastic.co/t/upgrade-of-custom-image-statefulset-causes-cluster-wide-readiness-probe-failed/205952",
    "title": "Upgrade of custom-image statefulset causes cluster-wide Readiness probe failed",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "October 30, 2019, 9:57pm October 30, 2019, 10:10pm October 31, 2019, 9:31am October 31, 2019, 5:12pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "42571002-b76c-4e53-875c-d6e1aa317937",
    "url": "https://discuss.elastic.co/t/if-physical-mechine-down-how-to-move-the-bad-pod-to-other-working-node-when-using-local-volume-storageclass/205718",
    "title": "If physical mechine down, how to move the bad pod to other working node when using local-volume storageclass",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LoadingZhang",
    "date": "October 29, 2019, 4:55pm October 30, 2019, 7:07am October 30, 2019, 8:42am",
    "body": "Im using elastic-local-volume as storageclass. I shutdown a physical mechine to simulate hardware malfunction. I found the pod can not move to other node because of volume node affinity. Could we move it to other node manually? Or operator can move it automatically few minutes later?",
    "website_area": "discuss"
  },
  {
    "id": "d29c2aa8-8755-4843-a6f1-19543826982e",
    "url": "https://discuss.elastic.co/t/authentication-to-realm-default-file-failed/205237",
    "title": "Authentication to realm default_file failed",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "pabloborh",
    "date": "October 25, 2019, 10:38am October 28, 2019, 6:11am October 28, 2019, 12:31pm October 28, 2019, 12:31pm",
    "body": "Hi! I have a elasticsearch cluster deployed with ECK. The pods are logging: level\": \"WARN\", \"component\": \"o.e.x.s.a.AuthenticationService\", \"cluster.name\": \"elasticsearch\", \"node.name\": \"elasticsearch-es-data-6ff4cgt49v\", \"message\": \"Authentication to realm default_file failed - Password authentication failed for elastic\", \"cluster.uuid\": \"ixv-beimTw-glVn8x3-TEA\", \"node.id\": \"guIZqSwRSUm7zU1ZHCu8iw\" } I don't understand de error. Cluster is working (3 master, 3 ingest, 3 data). Also i have other question. it's possible preserve data volumes from data nodes? I had troubles and I had lost all data. Luckily i had a snapshot of the most important index.",
    "website_area": "discuss"
  },
  {
    "id": "ac12c0ed-26a1-4eef-9cb1-fabac1040756",
    "url": "https://discuss.elastic.co/t/maximum-number-of-data-disk-exceeded/204883",
    "title": "Maximum number of data disk exceeded",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Alwandy",
    "date": "October 23, 2019, 1:43pm October 28, 2019, 6:15am",
    "body": "Hey guys, For my initial creation of K8s, I have already assigned 4 data disks (this is our QA env) during the creation process. When running the template to create the master node to act as ingress and data. I hit on an error: Failure sending request: StatusCode=0 -- Original Error: autorest/azure: Service returned an error. Status= Code=\"OperationNotAllowed\" Message=\"The maximum number of data disks allowed to be attached to a VM of this size is 4.\" Target=\"dataDisks\" How can we through CRD / Master template I created to use the existing disk(s)?",
    "website_area": "discuss"
  },
  {
    "id": "f026f62e-69a9-4201-8190-4715ea0e0361",
    "url": "https://discuss.elastic.co/t/automating-es-installation-with-eck-via-helm/205335",
    "title": "Automating ES installation with ECK via Helm",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Aravind",
    "date": "October 25, 2019, 11:24pm October 28, 2019, 5:51am",
    "body": "I would like to automate the installation of Elasticsearch via ECK on a cloud provider via helm. I don't want to prematurely go and install the custom kubernetes resource of kind: Elasticsearch before the operator is fully initialized and running. So my question is what guarantees ECK is fully up and running ? Asking this so that I could test for that before proceeding to install Elasticsearch Appreciate any response.",
    "website_area": "discuss"
  },
  {
    "id": "7aabe026-d1ce-42a1-b9a1-230ba4bcecc8",
    "url": "https://discuss.elastic.co/t/installing-es-with-custom-image-from-private-repository-fails/205331",
    "title": "Installing ES with Custom image from private repository fails",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Aravind",
    "date": "October 25, 2019, 10:10pm October 25, 2019, 10:11pm October 25, 2019, 10:44pm October 28, 2019, 5:44am",
    "body": "Here is my file - apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: jaeger-elasticsearch namespace: test spec: image: phx.ocir.io/<>/elasticsearch/elasticsearch:7.3.2 version: 7.3.2 nodes: - nodeCount: 1 config: node.master: true node.data: true node.ingest: true I've verified that the image exists and does not require secrets for pull. After installation I see this - NAME HEALTH NODES VERSION PHASE AGE jaeger-elasticsearch 7.3.2 Invalid 6m Can someone pls help ? How do I debug/get logs as to what's going on ?",
    "website_area": "discuss"
  },
  {
    "id": "07fd028c-cd44-4c6f-aee9-7df53a7bfaee",
    "url": "https://discuss.elastic.co/t/change-version-and-custom-images-in-eck/205306",
    "title": "Change version and custom images in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LoadingZhang",
    "date": "October 25, 2019, 5:26pm October 28, 2019, 5:41am",
    "body": "Now I set configuration as below: spec: version: 7.4.0 image: my_custom_image:7.4.0 What would happen if I change version to 7.4.1 but image not? And what would happen if change image tag to 7.4.1 but version not?",
    "website_area": "discuss"
  },
  {
    "id": "6e3fb49b-6bc8-4d4d-919d-c26e38b76466",
    "url": "https://discuss.elastic.co/t/cross-cluster-replication-on-eck-cross-region-replication/205311",
    "title": "Cross Cluster Replication on ECK - cross region replication",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "October 25, 2019, 6:39pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "c4ecb85d-7e02-46e6-85b1-ed87039e769c",
    "url": "https://discuss.elastic.co/t/setup-snapshots-with-minio-unable-to-retrieve-secrets/202351",
    "title": "Setup snapshots with minio, unable to retrieve secrets",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Kent_Brake",
    "date": "October 4, 2019, 1:42pm October 10, 2019, 11:11am October 24, 2019, 8:44pm",
    "body": "Hi, I'm trying to setup snapshot with a local minion instance, but I can't seem to imports secrets as documented here. I setup the secrets with: kubectl create secret generic minio-credentials --from-file=s3.client.default.access_key --from-file=s3.client.default.secret_key Then added it to the elasticsearch yaml file: secureSettings: secretName: \"minio-credentials\" I can see in the operator logs that its picked up: {\"level\":\"info\",\"ts\":1570195646.0524664,\"logger\":\"license-validation\",\"msg\":\"ValidationHandler handler called\",\"operation\":\"CREATE\",\"name\":\"**minio-credentials**\",\"namespace\":\"default\"} {\"level\":\"info\",\"ts\":1570195684.2096705,\"logger\":\"es-validation\",\"msg\":\"ValidationHandler handler called\",\"operation\":\"UPDATE\",\"name\":\"quickstart\",\"namespace\":\"default\"} But when I setup via console PUT /_snapshot/my_minio_repository { \"type\": \"s3\", \"settings\": { \"bucket\": \"esbackups\", \"endpoint\": \"10.1.1.220:9000\", \"protocol\": \"http\" } } I get this error: { \"error\": { \"root_cause\": [ { \"type\": \"repository_verification_exception\", \"reason\": \"[my_minio_repository] path is not accessible on master node\" } ], \"type\": \"repository_verification_exception\", \"reason\": \"[my_minio_repository] path is not accessible on master node\", \"caused_by\": { \"type\": \"i_o_exception\", \"reason\": \"Unable to upload object [tests-86_qgPYEQ3KdDtlhf6kK2g/master.dat] using a single upload\", \"caused_by\": { \"type\": \"sdk_client_exception\", \"reason\": \"sdk_client_exception: Unable to load credentials from service endpoint\", \"caused_by\": { \"type\": \"i_o_exception\", \"reason\": \"Connect timed out\" } } } }, \"status\": 500 }",
    "website_area": "discuss"
  },
  {
    "id": "7a3cd429-9dfe-40e1-a9d7-d6881d2222b3",
    "url": "https://discuss.elastic.co/t/quickstart-example-v0-8-ssl-error/183992",
    "title": "Quickstart Example (v0.8) SSL error",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "MooseyMersa",
    "date": "June 3, 2019, 2:32pm June 4, 2019, 1:26pm June 4, 2019, 10:48am June 4, 2019, 11:26am June 4, 2019, 12:58pm October 21, 2019, 9:30am October 21, 2019, 11:03am October 22, 2019, 8:48am",
    "body": "Hi, I've followed the guide https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html and deployed the items described, everything seems to have been correctly created. However when the Elasticsearch logs are inspected there is a problem connecting to it which looks like it relates to an SSL certificate. Any advice on how to resolve this? The relevant extract from the Elasticsearch log is as follows: {\"type\": \"server\", \"timestamp\": \"2019-06-03T14:25:47,157+0000\", \"level\": \"WARN\", \"component\": \"o.e.h.AbstractHttpServerTransport\", \"cluster.name\": \"quickstart\", \"node.name\": \"quickstart-es-p27jq468fh\", \"cluster.uuid\": \"zu07afsqR4Su51dScM6MYw\", \"node.id\": \"vmGa6_pnRaqjagdW3v4GeA\", \"message\": \"caught exception while handling client http traffic, closing connection Netty4HttpChannel{localAddress=0.0.0.0/0.0.0.0:9200, remoteAddress=/10.244.0.100:45094}\" , \"stacktrace\": [\"io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate\" The filebeat log is shown below: 2019-06-03T14:27:28.643Z INFO [monitoring] log/log.go:144 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":30310,\"time\":{\"ms\":6}},\"total\":{\"ticks\":153690,\"time\":{\"ms\":47},\"value\":153690},\"user\":{\"ticks\":123380,\"time\":{\"ms\":41}}},\"handles\":{\"limit\":{\"hard\":1048576,\"soft\":1048576},\"open\":16},\"info\":{\"ephemeral_id\":\"bbd1dd2f-d815-4003-829b-182cc3d8ce3f\",\"uptime\":{\"ms\":268890029}},\"memstats\":{\"gc_next\":46221424,\"memory_alloc\":23183016,\"memory_total\":3499909608}},\"filebeat\":{\"harvester\":{\"open_files\":9,\"running\":21}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":2,\"events\":{\"active\":4117,\"retry\":50}}},\"registrar\":{\"states\":{\"current\":26}},\"system\":{\"load\":{\"1\":0.23,\"15\":0.03,\"5\":0.07,\"norm\":{\"1\":0.115,\"15\":0.015,\"5\":0.035}}}}}} 2019-06-03T14:27:58.037Z ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(https://quickstart-es.default.svc.cluster.local:9200)): Get https://quickstart-es.default.svc.cluster.local:9200: x509: certificate signed by unknown authority 2019-06-03T14:27:58.037Z INFO pipeline/output.go:93 Attempting to reconnect to backoff(elasticsearch(https://quickstart-es.default.svc.cluster.local:9200)) with 5947 reconnect attempt(s)",
    "website_area": "discuss"
  },
  {
    "id": "a806ccff-19dd-4c05-b702-49a1dc0f2e03",
    "url": "https://discuss.elastic.co/t/how-to-create-role-mapping-file/199459",
    "title": "How to create role mapping file",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sfgroups1",
    "date": "September 13, 2019, 3:39pm September 30, 2019, 12:07pm October 8, 2019, 5:16pm October 18, 2019, 8:40am",
    "body": "Hi, I am adding the LDAP authentication to my cluster, I need to create the group_to_role_mapping.yml file for the roles. Is there an example I can follow? files: role_mapping: \"/mnt/elasticsearch/group_to_role_mapping.yml\" unmapped_groups_as_roles: false Thanks",
    "website_area": "discuss"
  },
  {
    "id": "050da91a-1604-4985-b99b-3f3b62bfad3c",
    "url": "https://discuss.elastic.co/t/eck-install-kubernetes-on-premise-k8s-cluster-with-local-nfs/204065",
    "title": "ECK install kubernetes on-premise k8s cluster with local NFS",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "umim9",
    "date": "October 17, 2019, 1:19pm October 17, 2019, 3:01pm",
    "body": "Hi, I want install one ECK cluster in local on-premise k8s HA cluster. I use local network NFS share persistent store for k8s HA cluster. I am a beginner enginer. Someone can send install yaml file for me? Thank you, bye. Steven",
    "website_area": "discuss"
  },
  {
    "id": "3dd165bd-4bd2-40a2-ac5f-704b9b674426",
    "url": "https://discuss.elastic.co/t/using-s3-repository-with-ec2-instance-profile-and-kube2iam/203252",
    "title": "Using s3-repository with EC2 Instance Profile and kube2iam",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "akappler",
    "date": "October 11, 2019, 2:46pm October 14, 2019, 10:29pm October 16, 2019, 11:11am October 16, 2019, 3:28pm",
    "body": "I am trying to configure my Elasticsearch instance for access to a S3 bucket. I want to secure access via an AWS Role, which should be assumed automatically via kube2iam. I am running into two problems: For kube2iam to work, it is necessary to add an annotation to the Elasticsearch Pod. I tried via the \"podTemplate\", but this did not work. Is there any way to do this or do you plan to add support? If I add the annotation for kube2iam manually to the Pod, I can exec into the container and verify that the AWS Role is assigned correctly. I can access to S3 bucket via aws cli. However when I try to create the repository, it fails with the following error message: curl -H \"Content-Type: application/json\" -X PUT --user elastic:XXX -k https://localhost:8000/_snapshot/s3 --data '{\"type\":\"s3\", \"settings\": {\"endpoint\": \"s3.eu-central-1.amazonaws.com\", \"bucket\":\"backup\", \"server_side_encryption\": true}}' {\"error\":{\"root_cause\":[{\"type\":\"repository_verification_exception\",\"reason\":\"[s3] path is not accessible on master node\"}],\"type\":\"repository_verification_exception\",\"reason\":\"[s3] path is not accessible on master node\",\"caused_by\":{\"type\":\"i_o_exception\",\"reason\":\"Unable to upload object [tests-hOTEv96NS1yoA4mgpNgzxg/master.dat] using a single upload\",\"caused_by\":{\"type\":\"amazon_s3_exception\",\"reason\":\"Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: XXX; S3 Extended Request ID: XXX\"}}},\"status\":500}",
    "website_area": "discuss"
  },
  {
    "id": "712d841c-e554-41a3-ba83-bae91208c071",
    "url": "https://discuss.elastic.co/t/runaway-memory-usage/203483",
    "title": "Runaway memory usage",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JoeyLemur",
    "date": "October 14, 2019, 3:30pm October 14, 2019, 3:59pm October 14, 2019, 4:17pm October 14, 2019, 6:15pm October 14, 2019, 7:11pm",
    "body": "I am deploying Elastic 7.4.0 in my cluster, specifying -Xms4096M -Xmx4096M for the nodes, but I continue to run into pods getting evicted for memory pressure. Examples: The node was low on resource: memory. Container elasticsearch was using 36874144Ki, which exceeds its request of 8Gi. The node was low on resource: memory. Container elasticsearch was using 49884448Ki, which exceeds its request of 8Gi. The top output from one of the nodes shows this rather impressive virtual address space usage: PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 63500 1000 20 0 97.810g 4.584g 170780 S 46.0 8.3 912:46.37 java Anyone know why java is bloating so badly, and what I can do to stop it?",
    "website_area": "discuss"
  },
  {
    "id": "c2a4e1e2-a000-46f7-bbce-991683d5f0c5",
    "url": "https://discuss.elastic.co/t/stuck-upgrade/203087",
    "title": "Stuck upgrade",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Claus_Strommer",
    "date": "October 10, 2019, 6:19pm October 11, 2019, 5:55am",
    "body": "I 've used eck to set up an elasticsearch cluster at v7.2.0. I've successfully upgraded it to 7.3.0 and 7.3.1. However, when I attempted to upgrade it to 7.4.0 the change to the CRD was accepted but the operator did not upgrade the pods. Now the operator won't let me downgrade the CRD to match the pods, and it won't upgrade the pods because it thinks they're already at v7.4.0. Deleting a pod will only recreate it with the same version it's already at. I'd like to either rectify the operator's data to match the cluster state, or manually update the pods to match the CRD, and am looking for recommendations.",
    "website_area": "discuss"
  },
  {
    "id": "d30e7639-4232-4cff-9c45-41a8e0729b3f",
    "url": "https://discuss.elastic.co/t/elastic-operator-juju-charm/203117",
    "title": "Elastic Operator Juju Charm",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "jamesbeedy",
    "date": "October 10, 2019, 10:00pm",
    "body": "Hello, I want to introduce the lifecycle ops we are building around ECK using Juju charms. elastic-operator charm home page discourse post explaining the elastic-operator charm and modeling custom objects via Juju layer-elastic-operator-k8s github My team and I will be working on modeling the Elasticsearch, Kibana and ApmServer objects in Juju over the coming weeks. In the mean time, we welcome and appreciate any feedback on how we are encapsulating the elastic-operator as a Juju charm. Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "fab47fdf-05a1-48ee-a524-33f6f40b38b5",
    "url": "https://discuss.elastic.co/t/custom-kibana-settings-in-eck/202006",
    "title": "Custom kibana settings in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Mike_Eklund",
    "date": "October 2, 2019, 4:31pm October 2, 2019, 7:57pm October 2, 2019, 10:06pm",
    "body": "It appears that you have to do manual configuration of ES if you want to change items in the kibana.yml such as elasticsearch.requestHeadersWhitelist? is this correct and we cant use elasticsearchRef to do all the username and cert stuff for us?",
    "website_area": "discuss"
  },
  {
    "id": "392dcfa4-b146-4702-bf4b-b94ccbb74bd3",
    "url": "https://discuss.elastic.co/t/gcs-repository-creation-issue-for-elasticsearch-on-gke/201519",
    "title": "Gcs-repository-creation-issue for elasticsearch on gke",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Kumar_Saurabh_Srivas",
    "date": "September 30, 2019, 5:53pm September 29, 2019, 6:55am September 30, 2019, 2:59am September 30, 2019, 6:51am September 30, 2019, 8:31am September 30, 2019, 11:17am September 30, 2019, 2:09pm September 30, 2019, 2:26pm September 30, 2019, 3:14pm September 30, 2019, 5:42pm September 30, 2019, 5:54pm September 30, 2019, 6:10pm October 1, 2019, 4:14am October 29, 2019, 4:14am",
    "body": "We are setting up an elasticsearch cluster on GKE with the following format: Master nodes as kubernetes deployments Client nodes as kubernetes deployments with HPA Data nodes as stateful sets with PVs We are able to set up the cluster well. But then we are struggling in configuring the snapshot backup mechanism. Essentially, we are following this guide. We are able to follow this upto the step of getting the secret json key. Afterwards, we are not sure how to add this to the elasticsearch keystore and proceed further. We are really stuck on this for quite some and the documentation have not been great. All docs mention that add this json key to elasticsearch.keystore but we don't know how to do that. The json file is on our local shell while the keystore is on es pods. Also, we have created a custom dockerfile to install gcs plugin. Really looking for some help here.",
    "website_area": "discuss"
  },
  {
    "id": "e76b1ccd-2122-4f76-b87b-b18ec3fe59b5",
    "url": "https://discuss.elastic.co/t/upgrading-elasticsearch-version-on-eck/201222",
    "title": "Upgrading elasticsearch version on ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Bozo_Tegeltija",
    "date": "September 26, 2019, 12:07pm September 26, 2019, 12:58pm",
    "body": "How should I approach upgrading the version of Elasticsearch? Can operator handle just changing the configuration from version: 7.2.0 -> 7.3.0 and kubectl apply?",
    "website_area": "discuss"
  },
  {
    "id": "d37ffc42-b898-4a75-94ce-63a65c90e94a",
    "url": "https://discuss.elastic.co/t/how-to-override-configuration-in-runtime/200688",
    "title": "How to override configuration in runtime",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Bozo_Tegeltija",
    "date": "September 23, 2019, 12:07pm September 23, 2019, 2:22pm September 26, 2019, 7:33am September 26, 2019, 7:49am",
    "body": "How can you use the predefined configuration(elasticsearch.yml) and override just one field (e.g. xpack.security.secureCookies=false?",
    "website_area": "discuss"
  },
  {
    "id": "0324f6b2-acf0-49f0-912e-d30583a00101",
    "url": "https://discuss.elastic.co/t/whats-the-default-registry-and-images-for-eck-and-es/201083",
    "title": "What's the default registry and images for ECK and ES",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "gearoidibm",
    "date": "September 25, 2019, 4:15pm September 25, 2019, 4:47pm September 25, 2019, 7:45pm",
    "body": "What's the default registry and images used for both the operator and ES itself. Where can I find the Dockerfiles for the ES images ? I've skimmed the operator repo but not sure where I'd find these in the code.",
    "website_area": "discuss"
  },
  {
    "id": "561b6582-3992-4279-93b9-eb9bb28872ef",
    "url": "https://discuss.elastic.co/t/is-there-a-target-release-date-for-1-0/200704",
    "title": "Is there a target release date for 1.0?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "gearoidibm",
    "date": "September 23, 2019, 2:12pm September 23, 2019, 4:47pm September 24, 2019, 10:06am",
    "body": "Is there a target date for release of 1.0 or any indicative timeframe ?",
    "website_area": "discuss"
  },
  {
    "id": "0a857549-da3f-46ea-bdec-6a5dc7984347",
    "url": "https://discuss.elastic.co/t/readiness-check-failure-when-enabling-oidc-authentication/200554",
    "title": "Readiness check failure when enabling OIDC authentication",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "robcoward",
    "date": "September 22, 2019, 12:52am September 22, 2019, 8:14am September 22, 2019, 6:16pm",
    "body": "I'm trying to enable OIDC authentication in elastic/kibana deployed by the operator, having enabled the platinum subscription trial. The operator is starting the required number of pods and they are coming up as running but not ready. Checking the pod description to find what the readiness check is defined as and exec'ing into the container, I can see that elasticsearch is running and responds to curl on port 9200, however the PROBE_USERNAME and password in the PROBE_PASSWORD_FILE is failing to authenticate: sh-4.2# curl -vk -u elastic-internal-probe:hmmnr8rchqfp8dm9w4fs87cq https://127.0.0.1:9200 * About to connect() to 127.0.0.1 port 9200 (#0) * Trying 127.0.0.1... * Connected to 127.0.0.1 (127.0.0.1) port 9200 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 * Server certificate: * subject: CN=edp-mgt-elasticsearch-es-http.monitoring.es.local,OU=edp-mgt-elasticsearch * start date: Sep 19 20:31:52 2019 GMT * expire date: Sep 18 20:41:52 2020 GMT * common name: edp-mgt-elasticsearch-es-http.monitoring.es.local * issuer: CN=edp-mgt-elasticsearch-http,OU=edp-mgt-elasticsearch * Server auth using Basic with user 'elastic-internal-probe' > GET / HTTP/1.1 > Authorization: Basic ZWxhc3RpYy1pbnRlcm5hbC1wcm9iZTpobW1ucjhyY2hxZnA4ZG05dzRmczg3Y3E= > User-Agent: curl/7.29.0 > Host: 127.0.0.1:9200 > Accept: */* > < HTTP/1.1 401 Unauthorized < WWW-Authenticate: Bearer realm=\"security\" < WWW-Authenticate: ApiKey * Authentication problem. Ignoring this. < WWW-Authenticate: Basic realm=\"security\" charset=\"UTF-8\" < content-type: application/json; charset=UTF-8 < content-length: 495 < * Connection #0 to host 127.0.0.1 left intact {\"error\":{\"root_cause\":[{\"type\":\"security_exception\",\"reason\":\"unable to authenticate user [elastic-internal-probe] for REST request [/]\",\"header\":{\"WWW-Authenticate\":[\"Bearer realm=\\\"security\\\"\",\"ApiKey\",\"Basic realm=\\\"security\\\" charset=\\\"UTF-8\\\"\"]}}],\"type\":\"security_exception\",\"reason\":\"unable to authenticate user [elastic-internal-probe] for REST request [/]\",\"header\":{\"WWW-Authenticate\":[\"Bearer realm=\\\"security\\\"\",\"ApiKey\",\"Basic realm=\\\"security\\\" charset=\\\"UTF-8\\\"\"]}},\"status\":401} Does anyone have any suggestions on what I can check next ?",
    "website_area": "discuss"
  },
  {
    "id": "fc1c3aa7-5b2f-4bfd-af3b-f865184200fa",
    "url": "https://discuss.elastic.co/t/unknown-secure-setting-secure-bind-password-please-check-that-any-required-plugins-are-installed/199329",
    "title": "Unknown secure setting [secure_bind_password] please check that any required plugins are installed",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sfgroups1",
    "date": "September 12, 2019, 10:00pm September 13, 2019, 10:59am September 13, 2019, 1:54pm",
    "body": "Hi, when I deploy below in my kubernetes cluster 1.15.3. POD is failing with secure setting error message. How to fix this error? apiVersion: v1 kind: Secret metadata: name: secure-bind-password type: Opaque data: xpack.security.authc.realms.ldap.ldap1.secure_bind_password: XXXXXXaW4= --- apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.2.0 secureSettings: secretName: secure-bind-password image: docker.elastic.co/elasticsearch/elasticsearch:7.3.1 nodes: - nodeCount: 1 config: node.master: true node.data: true node.ingest: true xpack.security.authc.realms: &xpack-realms # explicitly enable file and native realm, otherwise it's disabled implicitly when other realms are used. file.file1: { order: 0 } native.native1: { order: 1 } ldap.ldap1: order: 2 url: \"ldap://example:389\" bind_dn: \"\" secure_bind_password: xpack.security.authc.realms.ldap.ldap1.secure_bind_password --- error messae \"stacktrace\": [\"org.elasticsearch.bootstrap.StartupException: java.lang.IllegalArgumentException: unknown secure setting [secure_bind_password] please check that any required plugins are installed, or check the breaking changes documentation for removed settings\", \"at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[elasticsearch-cli-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"Caused by: java.lang.IllegalArgumentException: unknown secure setting [secure_bind_password] please check that any required plugins are installed, or check the breaking changes documentation for removed settings\", \"at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:531) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:476) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:447) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:418) ~[elasticsearch-7.3.1.jar:7.3.1]\",",
    "website_area": "discuss"
  },
  {
    "id": "adab31b5-3209-428c-a06e-e087bd8a46c8",
    "url": "https://discuss.elastic.co/t/cant-connect-from-fluentd-notsslrecordexception-error/199221",
    "title": "Can't connect from fluentd: NotSslRecordException error",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Paul_F",
    "date": "September 12, 2019, 9:49am September 12, 2019, 12:32pm September 13, 2019, 1:48pm",
    "body": "I have deployed Elastic stack on k8s [running with kops on AWS] with almost default configuration [just different namespace]. I have deployed a fluentd daemonset in the same namespace and I am trying to connect to Elastic, but from fluentd I am getting: [warn]: #0 [out_es] Could not communicate to Elasticsearch, resetting connection and trying again. end of file reached (EOFError) And from elastic instance I am getting: Caused by: io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record What am I missing in my configuration? How can I make it work? My fluentd config is fairly default: <match **> @type elasticsearch @id out_es @log_level \"info\" include_tag_key true host \"elastic-es-http.elastic-system.svc.cluster.local\" port 9200 path \"\" scheme http ssl_verify true ssl_version TLSv1 reload_connections false reconnect_on_error true reload_on_failure true log_es_400_reason false logstash_prefix \"logstash\" logstash_format true index_name \"logstash\" type_name \"fluentd\"",
    "website_area": "discuss"
  },
  {
    "id": "ffff2091-1689-4abf-be79-8db523b09095",
    "url": "https://discuss.elastic.co/t/error-from-server-timeout-error-when-creating-stdin-timeout-request-did-not-complete-within-requested-timeout-30s/196680",
    "title": "Error from server (Timeout): error when creating \"STDIN\": Timeout: request did not complete within requested timeout 30s",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "August 26, 2019, 3:18pm August 25, 2019, 11:41pm August 26, 2019, 3:30pm August 29, 2019, 2:05pm September 7, 2019, 6:24pm September 12, 2019, 12:48pm September 12, 2019, 1:48pm September 12, 2019, 7:36pm September 12, 2019, 7:38pm September 12, 2019, 8:47pm",
    "body": "Hello World! I'm trying to follow Quickstart | Elastic Cloud on Kubernetes [0.9] | Elastic: Deploy ECK in your Kubernetes cluster $ kubectl apply -f https://download.elastic.co/downloads/eck/0.9.0/all-in-one.yaml customresourcedefinition.apiextensions.k8s.io/apmservers.apm.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/elasticsearches.elasticsearch.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/trustrelationships.elasticsearch.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co created clusterrole.rbac.authorization.k8s.io/elastic-operator created clusterrolebinding.rbac.authorization.k8s.io/elastic-operator created namespace/elastic-system created statefulset.apps/elastic-operator created secret/webhook-server-secret created serviceaccount/elastic-operator created $ $ kubectl -n elastic-system logs statefulset.apps/elastic-operator | tail {\"level\":\"info\",\"ts\":1566775664.8577778,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting Controller\",\"controller\":\"license-controller\"} {\"level\":\"info\",\"ts\":1566775664.8577247,\"logger\":\"kubebuilder.webhook\",\"msg\":\"installing webhook configuration in cluster\"} {\"level\":\"info\",\"ts\":1566775664.957982,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"apmserver-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.958148,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"kibana-association-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.958176,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"kibana-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.9581378,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"license-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.9582195,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"apm-es-association-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.958255,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"trial-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.9582841,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"elasticsearch-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.9925923,\"logger\":\"kubebuilder.webhook\",\"msg\":\"starting the webhook server.\"} $ Deploy the Elasticsearch cluster $ cat <<EOF | kubectl apply -f - > apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 > kind: Elasticsearch > metadata: > name: quickstart > spec: > version: 7.2.0 > nodes: > - nodeCount: 1 > config: > node.master: true > node.data: true > node.ingest: true > EOF Error from server (Timeout): error when creating \"STDIN\": Timeout: request did not complete within requested timeout 30s $ How does one troubleshoot ECK? Please advise.",
    "website_area": "discuss"
  },
  {
    "id": "d4d376ea-7d89-4d9a-9a29-7ceeb805e6cf",
    "url": "https://discuss.elastic.co/t/quickstart-health-and-phase-are-empty/198164",
    "title": "Quickstart \"health\" and \"phase\" are empty",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "shirishatideal",
    "date": "September 5, 2019, 8:31am September 5, 2019, 9:06am September 6, 2019, 6:49am September 6, 2019, 7:22am September 6, 2019, 8:13am September 10, 2019, 7:29pm September 12, 2019, 2:20am September 12, 2019, 12:45pm September 12, 2019, 5:23pm",
    "body": "I am trying ECK but got stuck right at start, eck_install.png1509219 14 KB $ kubectl get elasticsearch quickstart NAME HEALTH NODES VERSION PHASE AGE quickstart 7.2.0 24m Kubernetes - v1.15.3 Centos7 AWS Instance (t3.large) Operator logs show timeout as seems to be trying to pull non-existent GitHub resources github.com/elastic/cloud-on-k8s/operators/ I can't see \"operators\" in cloud-on-k8s. Any guidance on troubleshooting appreciated ! Shirish",
    "website_area": "discuss"
  },
  {
    "id": "58d31806-053c-41bf-814e-60c4b850d263",
    "url": "https://discuss.elastic.co/t/error-on-cluster-downsizing-if-the-master-node-is-stopped/195766",
    "title": "Error on cluster downsizing if the master node is stopped",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "gboanea",
    "date": "August 19, 2019, 3:29pm September 12, 2019, 12:51pm",
    "body": "Hi, I am using the quickstart steps from: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html. I am using EKS 1.13. If I upgrade to 3 nodes and then go back to 2 nodes, if the node that is shut down the master node is, then I can see the following error: {\"type\": \"server\", \"timestamp\": \"2019-08-19T14:24:40,552+0000\", \"level\": \"ERROR\", \"component\": \"o.e.x.s.a.TokenService\", \"cluster.name\": \"quickstart\", \"node.name\": \"quickstart-es-bsdgq9mjrx\", \"cluster.uuid\": \"IpkatT_qTaqok3H0pIiqRQ\", \"node.id\": \"EppdBoAFT2ajg5hrLwba5g\", \"message\": \"unable to install token metadata\" , \"stacktrace\": [\"org.elasticsearch.cluster.NotMasterException: no longer master. source: [install-token-metadata]\"] } In the tab where 'kubectl port-forward service/quickstart-es-http 9200' is started: E0819 17:15:44.177539 84646 portforward.go:362] error creating forwarding stream for port 9200 -> 9200: Timeout occured Handling connection for 9200 The curl commands end with: curl: (35) LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to localhost:9200 The problem occurs only if the master node is shut down at resizing. If I restart 'kubectl port-forward service/quickstart-es-http 9200' everything works again. Is this a problem on my side? Thank you, Georgeta",
    "website_area": "discuss"
  },
  {
    "id": "cf48cde7-fd0a-4c50-9155-28e2057089df",
    "url": "https://discuss.elastic.co/t/how-can-we-patch-the-eck-operator-to-watch-multiple-namespaces/198925",
    "title": "How can we patch the ECK operator to watch multiple namespaces",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "shahtab",
    "date": "September 10, 2019, 3:25pm September 11, 2019, 7:31am",
    "body": "I ahve patched it ONCE to watch one namespace other than default ... But how do you patch for multiple ... ECK worked for the first one but without patching doesn't for the second one. How do we add to the list of namespaces for ECK to watch... What would be the JSON string for that patch statefulset/elastic-operator -n elastic-system --type='json' --patch '[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/0/env/-\",\"value\": {\"name\": \"NAMESPACE\", \"value\": \"elastic\"}}]'",
    "website_area": "discuss"
  },
  {
    "id": "dee2321b-a82a-4343-985c-ce81a1e1fe3d",
    "url": "https://discuss.elastic.co/t/missing-documentation-for-eck-stack-in-non-default-namespace/198802",
    "title": "Missing documentation for ECK stack in non-default namespace",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sarjeet",
    "date": "September 10, 2019, 2:23am September 10, 2019, 12:07am September 10, 2019, 2:25am September 10, 2019, 5:16pm September 10, 2019, 5:30pm",
    "body": "I haven't found any documentation that states on how to run ECK stack on k8s with non-default namespace. Currently, Elasticsearch and kibana samples also provided with default namespace. I tried to run ECK stack with non-default namespace, but it failed at different places and had to debug what the issue is. It would have been helpful if there is a documentation that can state what is needed to run ECK stack with non-default namespace.",
    "website_area": "discuss"
  },
  {
    "id": "dff0d6ba-8c1c-4306-8936-49506272c598",
    "url": "https://discuss.elastic.co/t/elastic-cloud-with-auth-but-without-https/198337",
    "title": "Elastic cloud with auth but without https",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "O_K",
    "date": "September 6, 2019, 4:12am September 6, 2019, 7:33am September 6, 2019, 7:36am September 6, 2019, 7:58am September 7, 2019, 8:43pm",
    "body": "hello, I wonder the proper way to disable https but remain authentication for this example https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html#k8s-deploy-eck",
    "website_area": "discuss"
  },
  {
    "id": "8f174879-1984-41fa-a5e2-20a33486f8f9",
    "url": "https://discuss.elastic.co/t/trouble-installing-plugins-on-eck-with-initcontainers-method/198291",
    "title": "Trouble installing plugins on ECK with initContainers method",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "getorca",
    "date": "September 5, 2019, 4:40pm September 5, 2019, 9:05pm September 5, 2019, 5:34pm September 5, 2019, 7:06pm September 6, 2019, 8:06pm",
    "body": "I'm attempting to install plugins using the initContainers method, my code is as follows: version: 7.2.0 nodes: - podTemplate: spec: initContainers: - name: install-plugins command: ['sh', '-c', '| bin/elasticsearch-plugin install --batch repository-s3'] - nodeCount: 3 config: node.master: true node.data: true node.ingest: true volumeClaimTemplates: - metadata: name: elasticsearch-data # note: elasticsearch-data must be the name of the Elasticsearch volume spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: do-block-storage updateStrategy: changeBudget: maxSurge: 0 maxUnavailable: 1 However the plugins aren't install and the pods don't update. I can see the version has updated with kubectl describe -f elasticsearch.yaml How do I get the plugin to install?",
    "website_area": "discuss"
  },
  {
    "id": "86348298-20cf-499a-bab7-41a99f8c2d57",
    "url": "https://discuss.elastic.co/t/timeout-request-did-not-complete-within-requested-timeout-30s/197432",
    "title": "Timeout: request did not complete within requested timeout 30s",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "August 30, 2019, 1:51am August 30, 2019, 2:47pm September 1, 2019, 1:40am September 3, 2019, 1:56pm September 5, 2019, 5:37pm",
    "body": "Hello World! I'm just Deploy the Elasticsearch cluster, yet unable to delete it: $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"13+\", GitVersion:\"v1.13.7-gke.24\", GitCommit:\"2ce02ef1754a457ba464ab87dba9090d90cf0468\", GitTreeState:\"clean\", BuildDate:\"2019-08-12T22:05:28Z\", GoVersion:\"go1.11.5b4\", Compiler:\"gc\", Platform:\"linux/amd64\"} $ time kubectl delete elasticsearch quickstart elasticsearch.elasticsearch.k8s.elastic.co \"quickstart\" deleted elasticsearch cluster never gets deleted, meanwhile in Monitor the operator logs: {\"level\":\"info\",\"ts\":1567129334.4552548,\"logger\":\"license-controller\",\"msg\":\"Start reconcile iteration\",\"iteration\":35,\"namespace\":\"default\",\"es_name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4554448,\"logger\":\"license-controller\",\"msg\":\"End reconcile iteration\",\"iteration\":35,\"took\":0.000193777,\"namespace\":\"default\",\"es_name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4555485,\"logger\":\"elasticsearch-controller\",\"msg\":\"Start reconcile iteration\",\"iteration\":75,\"namespace\":\"default\",\"es_name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4557135,\"logger\":\"finalizer\",\"msg\":\"Executing finalizer\",\"finalizer_name\":\"expectations.finalizers.elasticsearch.k8s.elastic.co\",\"namespace\":\"default\",\"name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4557407,\"logger\":\"finalizer\",\"msg\":\"Executing finalizer\",\"finalizer_name\":\"observer.finalizers.elasticsearch.k8s.elastic.co\",\"namespace\":\"default\",\"name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4557512,\"logger\":\"finalizer\",\"msg\":\"Executing finalizer\",\"finalizer_name\":\"secure-settings.finalizers.elasticsearch.k8s.elastic.co\",\"namespace\":\"default\",\"name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4557755,\"logger\":\"finalizer\",\"msg\":\"Executing finalizer\",\"finalizer_name\":\"dynamic-watches.finalizers.k8s.elastic.co/http-certificates\",\"namespace\":\"default\",\"name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129364.4594567,\"logger\":\"elasticsearch-controller\",\"msg\":\"Updating status\",\"iteration\":75,\"namespace\":\"default\",\"es_name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129364.4595168,\"logger\":\"generic-reconciler\",\"msg\":\"Aggregated reconciliation results complete\",\"result\":{\"Requeue\":false,\"RequeueAfter\":0}} {\"level\":\"info\",\"ts\":1567129364.4595733,\"logger\":\"elasticsearch-controller\",\"msg\":\"End reconcile iteration\",\"iteration\":75,\"took\":30.00402465,\"namespace\":\"default\",\"es_ame\":\"quickstart\"} {\"level\":\"error\",\"ts\":1567129364.4596124,\"logger\":\"kubebuilder.controller\",\"msg\":\"Reconciler error\",\"controller\":\"elasticsearch-controller\",\"request\":\"default/quickstart\",\"error\":\"Timeout: request did not complete within requested timeout 30s\",\"errorCauses\":[{\"error\":\"Timeout: request did not complete within requested timeout 30s\"}],\"stacktrace\":\"github.com/elastic/cloud-on-k8s/operators/vendor/github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/github.com/go-logr/zapr/zapr.go:128\\ngithub.com/elastic/cloud-on-k8s/operators/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:217\\ngithub.com/elastic/cloud-on-k8s/operators/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:158\\ngithub.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil.func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133\\ngithub.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134\\ngithub.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait.Until\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88\"} Please advise.",
    "website_area": "discuss"
  },
  {
    "id": "aaf9412d-d9ab-4620-a155-dbb461d3dc3d",
    "url": "https://discuss.elastic.co/t/using-the-transportclient-with-eck/197767",
    "title": "Using the TransportClient with ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "akappler",
    "date": "September 3, 2019, 6:52am September 4, 2019, 10:12am September 4, 2019, 1:05pm",
    "body": "We have a legacy application which uses the Java TransportClient to access the ES API on port 9300, which we would like to operate with the ES operator. It seems that the ES instance created by ECK only exposes the REST API on port 9200. Is it possible to use the API on port 9300? I tried to expose it via a Service, but it is secured by client certificate authentication and I cannot figure out how to create a valid certificate.",
    "website_area": "discuss"
  },
  {
    "id": "605c11b5-e413-4e97-aa09-9c5df2459761",
    "url": "https://discuss.elastic.co/t/backup-logs/197069",
    "title": "Backup logs",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "mindsinthecloud",
    "date": "August 28, 2019, 8:42am August 28, 2019, 6:15pm August 28, 2019, 6:18pm August 29, 2019, 3:16am August 29, 2019, 9:26am September 2, 2019, 6:47am",
    "body": "Hi there, I have set up the ECK operator on my k8s cluster on my local machines with 3 nodes and integrated with Fluent Bit for log shipping. May I ask how can I backup the logs? I understand that duplicating the data directory is not recommended and the snapshot is the way to go. I have read this link: https://www.elastic.co/guide/en/cloud-on-k8s/0.9/k8s-snapshot.html but I did not see any example with local storage. Since I'm running on local machines, how can I set it up? Any help is appreciated! thanks",
    "website_area": "discuss"
  },
  {
    "id": "2c7b2813-228b-45c5-9334-ad29fccea05c",
    "url": "https://discuss.elastic.co/t/runaway-resource-usage/197560",
    "title": "Runaway Resource Usage",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "j-rewerts",
    "date": "August 30, 2019, 5:18pm August 30, 2019, 5:27pm August 30, 2019, 5:36pm",
    "body": "Hi! I've deployed a cluster following this guide to Kubernetes in GKE. One of my ES nodes was evicted for using too much memory. Screen Shot 2019-08-30 at 11.11.26 AM.png1262404 43.9 KB Screen Shot 2019-08-30 at 11.11.51 AM.png692552 8.12 KB I wasn't running anything overnight on the cluster. How can I ensure Elasticsearch doesn't get evicted in the future?",
    "website_area": "discuss"
  },
  {
    "id": "f504db87-2aa0-4596-a81a-eb5ff4195717",
    "url": "https://discuss.elastic.co/t/resource-was-created-with-older-version-of-operator-will-not-take-action-cause-es-cluster-pods-to-stuck/197381",
    "title": "\"Resource was created with older version of operator, will not take action\" cause ES cluster pods to stuck",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sarjeet",
    "date": "August 29, 2019, 6:36pm August 29, 2019, 6:04pm August 29, 2019, 7:01pm",
    "body": "I am facing an issue with the Elasticsearch (w/o kibana or apm) cluster deployment with the custom operator image compiled and built locally (using latest master without any changes) and tried to deploy a simple Elasticsearch yaml, the statefulset pod is stuck in the Init:1/2 state. I am seeing the following in the operator log which might be the issue here: {\"level\":\"info\",\"ts\":1567096658.9352784,\"logger\":\"annotation\",\"msg\":\"Resource was created with older version of operator, will not take action\",\"controller_version\":\"0.0.0\",\"resource_controller_version\":\"0.0.0\",\"namespace\":\"default\",\"name\":\"quickstart\"} Here is the kubectl output for the pod: default quickstart-es-test-0 0/1 Init:1/2 0 4m51s 172.17.0.6 minikube <none> <none> The version: 7.3.0 is specified in the elasticsearch yaml and using following operaor image: sarjesingh/eck-operator:0.10.0-SNAPSHOT-d07c6f08 Please let me know if you'd need me to debug into something to see the issue. Note: I am able to deploy elasticsearch cluster fine from the quickstart guide.",
    "website_area": "discuss"
  },
  {
    "id": "fa178e0d-808c-4747-ab59-dfc08c3c4e6a",
    "url": "https://discuss.elastic.co/t/eks-internal-load-balancer-for-kibana/195955",
    "title": "EKS internal load balancer for kibana",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "mario-mazo",
    "date": "August 20, 2019, 2:17pm August 28, 2019, 6:03pm",
    "body": "Hello we are trying to use serviceType LoadBalancer for kibana but it create a public ELB which is not an option for us. How could we add the annotations required like service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0 and so on thanks",
    "website_area": "discuss"
  },
  {
    "id": "fbd6c9aa-5dd1-4b76-81b6-750d9f4995b9",
    "url": "https://discuss.elastic.co/t/kubernetes-and-plugins/196935",
    "title": "Kubernetes and plugins",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tomma100",
    "date": "August 27, 2019, 10:58am August 28, 2019, 5:43pm",
    "body": "We are a team quite new to Kubernetes. Recently we have created an ECK installation running on Google Kubernetes Engine using the recommended deployment instructions (https://www.elastic.co/elasticsearch-kubernetes). So far all is very good, we are comfortable with upgrading versions, changing Elasticsearch node counts and applying basic config changes. Next up, we are looking to install Kibana plugins which does not seem quite so straightforward. All online documentation that we can find for this perform actions directly on the VMs, which seems irrelevant for Kubernetes installs. For this type of install, we would have thought that the Kubernetes yaml definition files will need updating... Can someone confirm this is correct, and if so which files and what changes need to be made to add plugins to our Kibana install?",
    "website_area": "discuss"
  },
  {
    "id": "28675510-8fc9-477d-a96e-626e26a3f834",
    "url": "https://discuss.elastic.co/t/custom-kibana-image-readiness-woes/196007",
    "title": "Custom Kibana Image readiness woes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "August 20, 2019, 9:10pm August 21, 2019, 5:10pm August 28, 2019, 5:32pm",
    "body": "Hey all, for an internal deploy of ECK, we are using a custom docker image that we host on GCR (gcr.io/etc/etc/kibana:latest). in the Dockerfile, I replace some images that are part of the assets, then remove the optimize folder so that they can be built on boot. Unfortunately, it seems as though kibana's readinessProbe only has an initialDelay of 10 seconds. And unfortunately, the optimizing process takes fairly longer than this, causing this pod to go into a boot loop. Two questions: Is there some way for me to force the optimize in the Dockerfile during build time? If not, any way to modify the readinessProbe for the Kibana CRD? I am on 0.8 Cheers,",
    "website_area": "discuss"
  },
  {
    "id": "e495d896-d23d-43f2-af09-ed6dca318ac4",
    "url": "https://discuss.elastic.co/t/error-from-server-notfound-services-quickstart-es-http-not-found/193658",
    "title": "Error from server (NotFound): services \"quickstart-es-http\" not found",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "August 4, 2019, 3:23am August 4, 2019, 11:25am August 8, 2019, 3:57am August 7, 2019, 8:02am August 8, 2019, 3:55am August 8, 2019, 6:06am August 9, 2019, 5:16am August 9, 2019, 6:20am August 9, 2019, 5:48pm August 9, 2019, 7:01pm August 12, 2019, 6:53pm August 21, 2019, 8:10pm August 21, 2019, 8:42pm August 22, 2019, 2:27pm August 22, 2019, 8:49pm August 23, 2019, 7:33pm August 25, 2019, 11:46pm August 26, 2019, 3:50pm August 26, 2019, 4:01pm",
    "body": "Hello World! I'm trying to follow: #### Request Elasticsearch access yet, running into following error: $ kubectl get service quickstart-es-http Error from server (NotFound): services \"quickstart-es-http\" not found $ $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.37.0.1 <none> 443/TCP 6d10h quickstart-es ClusterIP 10.37.4.20 <none> 9200/TCP 4d23h quickstart-es-discovery ClusterIP None <none> 9300/TCP 4d23h quickstart-kibana ClusterIP 10.37.6.26 <none> 5601/TCP 4d23h $ Please advise.",
    "website_area": "discuss"
  },
  {
    "id": "02012c84-26b7-45e5-b2cb-b8da382db4fd",
    "url": "https://discuss.elastic.co/t/eck-on-iks-with-fluentd-how-to-deal-with-secrets/195757",
    "title": "ECK on IKS with fluentd. How to deal with secrets",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "O_K",
    "date": "August 19, 2019, 2:27pm August 19, 2019, 3:11pm August 21, 2019, 1:28am August 21, 2019, 4:16pm August 21, 2019, 5:03pm August 25, 2019, 7:28pm",
    "body": "Hello, I'm checking https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html and deployed elasticsearch + kibana on IKS with no major issues. Now I'm trying to deploy fluentd with helm chart helm install stable/fluentd-elasticsearch and obviously receive some security error. Please let me know how to deal with this 2019-08-19 14:13:17 +0000 [warn]: suppressed same stacktrace 2019-08-19 14:13:17 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=22 next_retry_seconds=2019-08-19 14:13:48 +0000 chunk=\"590753a76d953933a14e9c6883b6892e\" error_class=Fluent::Plugin::ElasticsearchOutput::ConnectionFailure error=\"Can not reach Elasticsearch cluster ({:host=>\"elasticsearch-client\", :port=>9200, :scheme=>\"http\"})!\" 2019-08-19 14:13:17 +0000 [warn]: suppressed same stacktrace 2019-08-19 14:13:32 +0000 [warn]: [kubelet.log] got incomplete line before first line from /var/log/kubelet.log: \"Aug 19 14:13:32 kube-bld65cjd0gighkrkfff0-mycluster-default-00000172 kubelet.service[24449]: W0819 14:13:32.675534 24449 kubelet_pods.go:832] Unable to retrieve pull secret kube-system/bluemix-default-secret for kube-system/ibm-keepalived-watcher-pcb7h due to secrets \"bluemix-default-secret\" not found. The image pull may not succeed.\\n\" 2019-08-19 14:13:37 +0000 [warn]: [kubelet.log] got incomplete line before first line from /var/log/kubelet.log: \"Aug 19 14:13:37 kube-bld65cjd0gighkrkfff0-mycluster-default-00000172 kubelet.service[24449]: W0819 14:13:37.674962 24449 kubelet_pods.go:832] Unable to retrieve pull secret kube-system/bluemix-default-secret for kube-system/public-crbld65cjd0gighkrkfff0-alb1-566c8969f6-wrcwf due to secrets \"bluemix-default-secret\" not found. The image pull may not succeed.\\n\"",
    "website_area": "discuss"
  },
  {
    "id": "6b8a3c8a-dddb-4314-9a9d-b37eece65016",
    "url": "https://discuss.elastic.co/t/roadmap-and-feature-planning-documentation/196412",
    "title": "Roadmap and feature planning documentation?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sarjeet",
    "date": "August 22, 2019, 9:46pm August 22, 2019, 10:52pm August 23, 2019, 11:19pm",
    "body": "Hi, My name is Sarjeet Singh, and I have recently started to evaluate ECK for Kubernetes, specifically Elasticsearch portion of operator. I have been looking around to find some documentation around the development contribution or what's the current roadmap and features being planned in near term but couldn't find yet. It would also be helpful if there is a slack/groups to discuss any users/devs related questions as well. Thanks, Sarjeet Singh",
    "website_area": "discuss"
  },
  {
    "id": "7096ded3-9e05-42bf-bbe8-e42369449a88",
    "url": "https://discuss.elastic.co/t/elastic-cloud-kibana-port-forward-throws-503-with-default-tls-enabled/195866",
    "title": "Elastic Cloud Kibana port-forward throws 503 with default TLS enabled",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "O_K",
    "date": "August 20, 2019, 11:35am",
    "body": "Following this guidance, it seems that Kibana throws 503 error while connecting with kubectl port-forward service/quickstart-kb-http 5601 https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html#k8s-deploy-eck but it started working after adding below line into /etc/hosts and accessing Kibana URL as https://quickstart-kb-http.default.kb.local:5601 127.0.0.1 quickstart-kb-http.default.kb.local into /etc/hosts Errors: 2019-08-19 23:57:06 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=33 next_retry_seconds=2019-08-19 23:57:33 +0000 chunk=\"590806ab8757ebe018dd5f180f845528\" error_class=Faraday::SSLError error=\"SSL_connect returned=1 errno=0 state=error: certificate verify failed (OpenSSL::SSL::SSLError) Unable to verify certificate. This may be an issue with the remote host or with Excon. Excon has certificates bundled, but these can be customized:\\n\\n Excon.defaults[:ssl_ca_path] = path_to_certs\\n ENV['SSL_CERT_DIR'] = path_to_certs\\n Excon.defaults[:ssl_ca_file] = path_to_file\\n ENV['SSL_CERT_FILE'] = path_to_file\\n Excon.defaults[:ssl_verify_callback] = callback\\n (see OpenSSL::SSL::SSLContext#verify_callback)\\nor:\\n Excon.defaults[:ssl_verify_peer] = false (less secure).\\n\" {\"type\":\"log\",\"@timestamp\":\"2019-08-20T08:40:00Z\",\"tags\":[\"error\",\"task_manager\"],\"pid\":1,\"message\":\"Failed to poll for work: Error: Request Timeout after 30000ms\"} {\"type\":\"response\",\"@timestamp\":\"2019-08-20T08:40:07Z\",\"tags\":,\"pid\":1,\"method\":\"get\",\"statusCode\":200,\"req\":{\"url\":\"/login\",\"method\":\"get\",\"headers\":{\"host\":\"172.30.60.200:5601\",\"user-agent\":\"kube-probe/1.13\",\"accept-encoding\":\"gzip\",\"connection\":\"close\"},\"remoteAddress\":\"10.176.253.77\",\"userAgent\":\"10.176.253.77\"},\"res\":{\"statusCode\":200,\"responseTime\":13,\"contentLength\":9},\"message\":\"GET /login 200 13ms - 9.0B\"} {\"type\":\"error\",\"@timestamp\":\"2019-08-20T08:42:50Z\",\"tags\":[\"connection\",\"client\",\"error\"],\"pid\":1,\"level\":\"error\",\"error\":{\"message\":\"139907533506432:error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown:../deps/openssl/openssl/ssl/record/rec_layer_s3.c:1407:SSL alert number 46\\n\",\"name\":\"Error\",\"stack\":\"Error: 139907533506432:error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown:../deps/openssl/openssl/ssl/record/rec_layer_s3.c:1407:SSL alert number 46\\n\"},\"message\":\"139907533506432:error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown:../deps/openssl/openssl/ssl/record/rec_layer_s3.c:1407:SSL alert number 46\\n\"}",
    "website_area": "discuss"
  },
  {
    "id": "cafae8a9-c364-427c-8508-12a2f809f6f7",
    "url": "https://discuss.elastic.co/t/how-to-backup-restore/194541",
    "title": "How to backup/restore?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tew",
    "date": "August 9, 2019, 5:17am August 9, 2019, 6:18am August 9, 2019, 6:32am",
    "body": "I installed the elasticsearch operator using the quickstart guide and have a 3 node cluster running. I can't find any documentation on how to schedule regular snapshots of the ES cluster or how to restore from snapshot. Can someone point me to a doc or guide for backup/restore?",
    "website_area": "discuss"
  },
  {
    "id": "e506b0dc-8493-4e39-a256-f1cf9c475c6f",
    "url": "https://discuss.elastic.co/t/cant-find-keystore-data/192134",
    "title": "Can't find keystore data?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JoeyLemur",
    "date": "July 24, 2019, 8:52pm August 2, 2019, 12:13am August 2, 2019, 3:00pm August 2, 2019, 3:14pm",
    "body": "I'm trying to get OpenID working with ECK, but its not finding the client_secret key/value in the keystore. I have verified that it (seems to be) populating the keystore. If I set up elasticsearch without the OpenID realm: apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: elasticsearch-poc spec: version: 7.2.0 secureSettings: secretName: azure-openid-secret nodes: - nodeCount: 1 config: node.master: true node.data: true node.ingest: true xpack.security.authc.token.enabled: true volumeClaimTemplates: - metadata: name: elasticdata spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: default You can see the keystore gets the appropriate data: [root@elasticsearch-poc-es-5kvf75d48q elasticsearch]# cd /mnt/elastic/secure-settings/ [root@elasticsearch-poc-es-5kvf75d48q secure-settings]# ls -al total 4 drwxrwxrwt 3 root root 100 Jul 24 20:18 . drwxr-xr-x 9 root root 4096 Jul 24 20:20 .. drwxr-xr-x 2 root root 60 Jul 24 20:18 ..2019_07_24_20_18_31.193342258 lrwxrwxrwx 1 root root 31 Jul 24 20:18 ..data -> ..2019_07_24_20_18_31.193342258 lrwxrwxrwx 1 root root 62 Jul 24 20:18 xpack.security.authc.realms.oidc.oidc1.rp.client_secret -> ..data/xpack.security.authc.realms.oidc.oidc1.rp.client_secret [root@elasticsearch-poc-es-5kvf75d48q config]# elasticsearch-keystore list keystore.seed xpack.security.authc.realms.oidc.oidc1.rp.client_secret (I looked in the xpack.security.authc.realms.oidc.oidc1.rp.client_secret file, and it has the correct data.) Now, if I add the OpenID stuff into nodes config (just under the xpack.security.authc.token.enabled line): xpack.security.authc.realms.oidc.oidc1: order: 10 rp.client_id: \"REDACTED\" rp.response_type: \"code\" rp.redirect_uri: \"REDACTED\" op.issuer: \"REDACTED\" op.authorization_endpoint: \"REDACTED\" op.token_endpoint: \"REDACTED\" op.userinfo_endpoint: \"REDACTED\" op.endsession_endpoint: \"REDACTED\" op.jwkset_path: \"REDACTED\" rp.post_logout_redirect_uri: \"REDACTED\" claims.principal: sub And kubectl apply the changes, the new pod spins up, only to give me a stacktrace, the relevant line being: \"Caused by: org.elasticsearch.common.settings.SettingsException: The configuration setting [xpack.security.authc.realms.oidc.oidc1.rp.client_secret] is required\", So, what am I missing that it can't find what is obviously there?",
    "website_area": "discuss"
  },
  {
    "id": "57d67315-d0cd-4722-9ba3-cd744de03c98",
    "url": "https://discuss.elastic.co/t/cluster-never-recovers-on-baremetal-cloud-on-k8s-instance/189840",
    "title": "Cluster never recovers on baremetal cloud-on-k8s instance",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Alexei_Smirnov",
    "date": "July 10, 2019, 5:46pm July 10, 2019, 5:53pm July 11, 2019, 8:12am July 12, 2019, 3:00am August 2, 2019, 2:25pm",
    "body": "tried to install it several times starting from 0.8.0 operator, and 0.8.1, always the same issue. whenever we try to reboot one or few machines, cluster never recovers. all masters become unavailable and it continues to try to recover, but fails indefinitely, here's a snippet of log from kubetail'ing all pods. So far ready to give up on the operator, looks like we don't have enough technical knowledge to be able to run this in production and should issues occur - we have no way: not sure if these events would be any helpful: LAST SEEN TYPE REASON OBJECT MESSAGE 1s Warning BackOff pod/kibana-kibana-c98867586-4594g Back-off restarting failed container 1s Warning BackOff pod/kibana-kibana-c98867586-4594g Back-off restarting failed container 1s Warning Unhealthy pod/kibana-kibana-c98867586-4594g Readiness probe failed: HTTP probe failed with statuscode: 503 114s Warning FailedToUpdateEndpoint endpoints/elastic-es-discovery Failed to update endpoint ops/elastic-es-discovery: Operation cannot be fulfilled on endpoints \"elastic-es-discovery\": the object has been modified; please apply your changes to the latest version and try again 114s Warning FailedToUpdateEndpoint endpoints/elastic-es Failed to update endpoint ops/elastic-es: Operation cannot be fulfilled on endpoints \"elastic-es\": the object has been modified; please apply your changes to the latest version and try again 1s Normal Killing pod/elastic-es-qqtm956plr Stopping container elasticsearch 0s Normal Killing pod/elastic-es-k2bcb29q68 Stopping container elasticsearch 115s Warning FailedToUpdateEndpoint endpoints/elastic-es Failed to update endpoint ops/elastic-es: Operation cannot be fulfilled on endpoints \"elastic-es\": the object has been modified; please apply your changes to the latest version and try again 0s Normal Killing pod/elastic-es-pzthtpb4mh Stopping container elasticsearch 0s Normal Killing pod/elastic-es-9hdbl2tzj7 Stopping container elasticsearch 0s Normal Killing pod/elastic-es-4vkcnm5kxv Stopping container elasticsearch 0s Warning Unhealthy pod/elastic-es-9hdbl2tzj7 Readiness probe failed: 0s Warning Unhealthy pod/elastic-es-pzthtpb4mh Readiness probe failed: 0s Warning Unhealthy pod/elastic-es-9hdbl2tzj7 Readiness probe failed: 0s Warning Unhealthy pod/elastic-es-9hdbl2tzj7 Readiness probe failed: 1s Warning Unhealthy pod/kibana-kibana-c98867586-4594g Readiness probe failed: HTTP probe failed with statuscode: 503 Or these logs: [elastic-es-655t98ckzr cert-initializer] 2019-07-10T16:46:32.499Z INFO certificate-initializer No private key found on disk, will create one {\"reason\": \"open /mnt/elastic/private-key/node.key: no such file or directory\"} [elastic-es-655t98ckzr prepare-fs] at org.elasticsearch.cli.Command.main(Command.java:90) [elastic-es-655t98ckzr prepare-fs] at org.elasticsearch.plugins.PluginCli.main(PluginCli.java:47) [elastic-es-655t98ckzr cert-initializer] 2019-07-10T16:46:32.499Z INFO certificate-initializer Creating a private key on disk [elastic-es-655t98ckzr prepare-fs] Installed plugins: [elastic-es-655t98ckzr cert-initializer] 2019-07-10T16:46:32.815Z INFO certificate-initializer Generating a CSR from the private key [elastic-es-655t98ckzr prepare-fs] Plugins installation duration: 52 sec. [elastic-es-655t98ckzr cert-initializer] 2019-07-10T16:46:32.818Z INFO certificate-initializer Serving CSR over HTTP {\"port\": 8001} [elastic-es-655t98ckzr cert-initializer] 2019-07-10T16:46:32.818Z INFO certificate-initializer Watching filesystem for cert update",
    "website_area": "discuss"
  },
  {
    "id": "f5167504-c015-4566-bc65-724e889e5b52",
    "url": "https://discuss.elastic.co/t/public-ssled-access-with-ingress-not-working/189634",
    "title": "Public SSL'ed access with Ingress not working",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "July 9, 2019, 10:17pm July 10, 2019, 7:33am July 10, 2019, 4:28pm July 11, 2019, 7:35pm July 19, 2019, 3:00pm July 19, 2019, 10:55pm July 25, 2019, 2:44am July 25, 2019, 8:57am July 25, 2019, 2:48pm July 29, 2019, 4:25pm",
    "body": "hey all, I have followed the quickstart guide from master branch of the docs, and all works perfectly well when i set the network type to LoadBalancer for kibana and elastic. I am able to curl the endpoints (with the self-signed cert). However, when I create an ingress resource, it appears as though all the backends fail the health checks, and the ingress refuses to route traffic to any of the pods. Here are my related configs: elastic.yaml apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.1.0 http: service: spec: type: NodePort ports: - port: 9200 - targetPort: 9200 - protocol: TCP tls: selfSignedCertificate: subjectAltNames: - dns: myuniquedomain.ca ip: 34.98.124.3 nodes: - nodeCount: 3 config: node.master: true node.data: true node.ingest: true volumeClaimTemplates: - metadata: name: data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: standard elastic_ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: myuniquedomain-ingress annotations: kubernetes.io/ingress.global-static-ip-name: myuniquedomain-static-ip spec: rules: - http: paths: - path: /elastic backend: serviceName: quickstart-es servicePort: 9200 However, the ingress shows all the backend as unhealthy. Attempting to curl with the new cert now returns:  curl --cacert ca.pem -u elastic:$PW https://myuniquedomain.ca/elastic curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to myuniquedomain.ca:443 I have an A-record on the domain correctly pointing to the static IP as well. Am I missing something obvious? Thanks, --Gary",
    "website_area": "discuss"
  },
  {
    "id": "d0a0cc10-8774-462f-b9b4-0d8bf0340bc9",
    "url": "https://discuss.elastic.co/t/pvc-reuse-when-removing-pod/191898",
    "title": "PVC reuse when removing pod",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LaurentGoderre",
    "date": "July 23, 2019, 6:55pm July 23, 2019, 7:21pm July 24, 2019, 1:32pm",
    "body": "When a pod is removed the previous PVC is not used and as new one is created. How do I ensure persistence when deleting pods?",
    "website_area": "discuss"
  },
  {
    "id": "1e372e9b-3465-4004-8b9c-2728adb18c1a",
    "url": "https://discuss.elastic.co/t/air-gap-system-internal-error-occurred-failed-calling-webhook-validation-elasticsearch-elastic-co-post-https-elastic-webhook-service-elastic-system-svc-443-validate-elasticsearches-timeout-30s-service-unavailable/186396",
    "title": "Air Gap System: Internal error occurred: failed calling webhook \"validation.elasticsearch.elastic.co\": Post https://elastic-webhook-service.elastic-system.svc:443/validate-elasticsearches?timeout=30s: Service Unavailable",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "asp",
    "date": "June 19, 2019, 7:19am June 24, 2019, 8:25pm July 10, 2019, 1:19pm July 10, 2019, 1:22pm July 12, 2019, 10:42am July 24, 2019, 7:57am",
    "body": "Hi, I am following the quick start guide: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html When I want to deploy the single node elasticsearch I get following error: Internal error occurred: failed calling webhook \"validation.elasticsearch.elastic.co\": Post https://elastic-webhook-service.elastic-system.svc:443/validate-elasticsearches?timeout=30s: Service Unavailable I assume that the problem is that my cluster is behind a proxy server. It can access a private docker registry and can communicate to internet via proxy. https_proxy is set in docker config and in environment. But later on production there will be NO proxy available. Should ECK work in air-gapped-systems? Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "c9372e83-9aad-4061-acaf-323b178ea847",
    "url": "https://discuss.elastic.co/t/operator-updates/191933",
    "title": "Operator Updates",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "July 24, 2019, 1:55am July 24, 2019, 7:32am",
    "body": "With how quickly ECK is updating/evolving, I'm wondering if we will be receiving upgrade path documentation for updating the operator(to 0.9 and beyond, for those of us who started with 8.0). This would definitely help me out. I could just snapshot the cluster, blow it away, rebuild it and restore with the newest version, but I would like to keep everything if at all possible. Is this possible? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "98732544-416a-4fd1-93e8-8e0c4eb8f156",
    "url": "https://discuss.elastic.co/t/customizing-roles-yml/191882",
    "title": "Customizing roles.yml",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LaurentGoderre",
    "date": "July 23, 2019, 5:47pm July 23, 2019, 6:26pm July 23, 2019, 6:51pm",
    "body": "I an following the docs here to add a custom roles to Elasticsearch (via the roles.yml) but whatever I try doesn't get applied. github.com elastic/cloud-on-k8s/blob/f47a4259eed761f3bb93c9a5bc624a49d8c428f2/docs/elasticsearch-spec.asciidoc#custom-configuration-files-and-plugins [id=\"{p}-elasticsearch-specification\"] == Elasticsearch Specification There are a number of settings which need to be considered before going into production related to Elasticsearch but also to Kubernetes. Basic settings - JVM heap size - Node configuration - HTTP settings & TLS SANs - Resource limits - Pod Template - Volume claim templates Advanced settings - Virtual memory - Custom HTTP certificate - Secure settings - Custom plugins and bundles This file has been truncated. show original",
    "website_area": "discuss"
  },
  {
    "id": "87a9f579-88aa-470c-8866-5bd347948632",
    "url": "https://discuss.elastic.co/t/elastic-webhook-service-not-found/191233",
    "title": "Elastic-webhook-service not found",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "t638403",
    "date": "July 18, 2019, 3:48pm July 18, 2019, 4:08pm July 23, 2019, 3:07pm July 23, 2019, 5:41pm",
    "body": "Hi, My elastic web hook service does not seem to work. I followed the guide for installing elk via operator using this guide: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html But I have been so stupid to do the following. I downloaded the custom resource definitions and changed the image docker.elastic.co/eck/eck-operator:0.8.1 to docker.elastic.co/eck/eck-operator:0.9.0-rc2 and tried to deploy elasticsearch which kind of worked. However, when I tried to delete elasticsearch using kubectl delete -f ... I noticed that nothing changed. So I deleted the secrets and services manually. I changed the image back to 0.8.1 and tried to add the custom resources again, but i get this now: Error from server (InternalError): error when creating \"custom-resource-definition.elastic-system.yml\": Internal error occurred: failed calling webhook \"validation.license.elastic.co\": Post https://elastic-webhook-service.elastic-system.svc:443/validate-secrets?timeout=30s: service \"elastic-webhook-service\" not found Any help to fix this mess I created would be endlessly appreciated!",
    "website_area": "discuss"
  },
  {
    "id": "49b589da-4c46-4fe2-8967-83996c0268b3",
    "url": "https://discuss.elastic.co/t/single-instance-quickstart-cluster-crashes-after-10-minutes-with-eck-0-8-1/191086",
    "title": "Single Instance Quickstart Cluster Crashes after 10 Minutes With ECK 0.8.1",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Zorlack",
    "date": "July 17, 2019, 8:40pm July 18, 2019, 8:42am July 18, 2019, 1:40pm",
    "body": "I've been having some difficulty following the ECK Quickstart. To begin with I install the operator and instantiate a single node instance: [root@a0002-flexnet ~]# kubectl apply -f https://download.elastic.co/downloads/eck/0.8.1/all-in-one.yaml [SNIP] [root@a0002-flexnet ~]# cat <<EOF | kubectl apply -f - > apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 > kind: Elasticsearch > metadata: > name: quickstart > spec: > version: 7.1.0 > nodes: > - nodeCount: 1 > config: > node.master: true > node.data: true > node.ingest: true > EOF elasticsearch.elasticsearch.k8s.elastic.co/quickstart created [root@a0002-flexnet ~]# kubectl get elasticsearches.elasticsearch.k8s.elastic.co NAME HEALTH NODES VERSION PHASE AGE quickstart red 7.1.0 Pending 35s [root@a0002-flexnet ~]# kubectl get elasticsearches.elasticsearch.k8s.elastic.co NAME HEALTH NODES VERSION PHASE AGE quickstart green 1 7.1.0 Operational 81s At this point I can open a connection to the quickstart-es service and authenticate correctly. So far so good! A describe reveals that it looks healthy: [root@a0002-flexnet ~]# kubectl describe elasticsearches.elasticsearch.k8s.elastic.co quickstart Name: quickstart Namespace: default Labels: <none> Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"elasticsearch.k8s.elastic.co/v1alpha1\",\"kind\":\"Elasticsearch\",\"metadata\":{\"annotations\":{},\"name\":\"quickstart\",\"namespace\":... API Version: elasticsearch.k8s.elastic.co/v1alpha1 Kind: Elasticsearch Metadata: Creation Timestamp: 2019-07-17T20:21:03Z Finalizers: expectations.finalizers.elasticsearch.k8s.elastic.co observer.finalizers.elasticsearch.k8s.elastic.co secure-settings.finalizers.elasticsearch.k8s.elastic.co licenses.finalizers.elasticsearch.k8s.elastic.co Generation: 2 Resource Version: 37578333 Self Link: /apis/elasticsearch.k8s.elastic.co/v1alpha1/namespaces/default/elasticsearches/quickstart UID: 661930de-a8d0-11e9-9f84-ac1f6b7678a2 Spec: Http: Service: Metadata: Spec: Tls: Nodes: Config: Node . Data: true Node . Ingest: true Node . Master: true Node Count: 1 Pod Template: Metadata: Creation Timestamp: <nil> Spec: Containers: <nil> Update Strategy: Version: 7.1.0 Status: Available Nodes: 1 Cluster UUID: GguK3wIwSAe_W2hWWIiVsg Health: green Master Node: quickstart-es-gbkkpdr7lm Phase: Operational Service: quickstart-es Zen Discovery: Minimum Master Nodes: 1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 11m elasticsearch-controller Created pod quickstart-es-gbkkpdr7lm Normal StateChange 10m elasticsearch-controller Master node is now quickstart-es-gbkkpdr7lm Here's where things go off the rails a bit: After about 10 minutes, the elasticsearch pod falls over. {\"type\": \"server\", \"timestamp\": \"2019-07-17T20:32:22,527+0000\", \"level\": \"INFO\", \"component\": \"o.e.x.m.p.NativeController\", \"cluster.name\": \"quickstart\", \"node.name\": \"quickstart-es-gbkkpdr7lm\", \"cluster.uuid\": \"GguK3wIwSAe_W2hWWIiVsg\", \"node.id\": \"pUETgQReRNuWE0mvNi6q-A\", \"message\": \"Native controller process has stopped - no new native processes can be started\" } {\"level\":\"info\",\"ts\":1563395552.5241575,\"logger\":\"process-manager\",\"msg\":\"Update process state\",\"action\":\"terminate\",\"id\":\"es\",\"state\":\"failed\",\"pid\":15} {\"level\":\"info\",\"ts\":1563395552.531126,\"logger\":\"process-manager\",\"msg\":\"HTTP server closed\"} {\"level\":\"info\",\"ts\":1563395552.5324,\"logger\":\"process-manager\",\"msg\":\"Exit\",\"reason\":\"process failed\",\"code\":-1} Afterwards, the operator shows the service is degraded and it never recovers: [root@a0002-flexnet ~]# kubectl describe elasticsearches.elasticsearch.k8s.elastic.co quickstart Name: quickstart Namespace: default Labels: <none> Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"elasticsearch.k8s.elastic.co/v1alpha1\",\"kind\":\"Elasticsearch\",\"metadata\":{\"annotations\":{},\"name\":\"quickstart\",\"namespace\":... API Version: elasticsearch.k8s.elastic.co/v1alpha1 Kind: Elasticsearch Metadata: Creation Timestamp: 2019-07-17T20:21:03Z Finalizers: expectations.finalizers.elasticsearch.k8s.elastic.co observer.finalizers.elasticsearch.k8s.elastic.co secure-settings.finalizers.elasticsearch.k8s.elastic.co licenses.finalizers.elasticsearch.k8s.elastic.co Generation: 2 Resource Version: 37583256 Self Link: /apis/elasticsearch.k8s.elastic.co/v1alpha1/namespaces/default/elasticsearches/quickstart UID: 661930de-a8d0-11e9-9f84-ac1f6b7678a2 Spec: Http: Service: Metadata: Spec: Tls: Nodes: Config: Node . Data: true Node . Ingest: true Node . Master: true Node Count: 1 Pod Template: Metadata: Creation Timestamp: <nil> Spec: Containers: <nil> Update Strategy: Version: 7.1.0 Status: Cluster UUID: GguK3wIwSAe_W2hWWIiVsg Health: red Master Node: quickstart-es-gbkkpdr7lm Phase: Pending Service: quickstart-es Zen Discovery: Minimum Master Nodes: 1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 11m elasticsearch-controller Created pod quickstart-es-gbkkpdr7lm Normal StateChange 10m elasticsearch-controller Master node is now quickstart-es-gbkkpdr7lm Warning Unhealthy 5s elasticsearch-controller Elasticsearch cluster health degraded After some time the pod goes into Waiting: CrashLoopBackOff. How do I start to troubleshoot this? What would cause this single-instance test cluster to crash reliably after 10 minutes? Many thanks! -Z",
    "website_area": "discuss"
  },
  {
    "id": "3f67fb16-5c58-40f3-8848-187824c12ae6",
    "url": "https://discuss.elastic.co/t/excessive-garbage-collection-when-i-try-to-vertically-scale-the-pods/190628",
    "title": "Excessive Garbage collection when I try to vertically scale the pods",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "July 16, 2019, 2:01am July 16, 2019, 7:36am July 17, 2019, 4:05pm",
    "body": "Hello there, I've recently gotten the cluster up and running via ECK. in order to increase performance, I have allocated 3 nodes, each on an n1-highmem-2 instance. I have also increased the memory limit. I am now seeing a ton of garbage collection logs in ES. Here is my elastic service yaml: apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.1.0 nodes: - nodeCount: 3 config: node.master: true node.data: true node.ingest: true podTemplate: spec: containers: - name: elasticsearch resources: limits: memory: \"6Gi\" cpu: \"100m\" volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: standard My logs are absolutely filled with [gc][10417] overhead, spent [503ms] collecting in the last [1.1s] and it is making performance worse than on smaller clusters. Is there some way I'm supposed to increase memory limitations that isn't through the pod template? Cheers and thanks for the assistance.",
    "website_area": "discuss"
  },
  {
    "id": "603c743a-34a4-451a-ba72-d81e38d5947b",
    "url": "https://discuss.elastic.co/t/how-to-give-built-password-in-eck-cr/190742",
    "title": "How to give built password in ECK CR",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "bikkina_mahesh",
    "date": "July 16, 2019, 1:32pm July 16, 2019, 3:06pm",
    "body": "Hi, How to configure password for user elastic while bootup of cluster through ECK CR. Thanks, Mahesh",
    "website_area": "discuss"
  },
  {
    "id": "b49040c4-88cf-457f-9150-058ca1f98633",
    "url": "https://discuss.elastic.co/t/how-to-use-persistent-storage-on-bare-metal-no-minikube/190224",
    "title": "How to use persistent Storage on bare metal? (no minikube)",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "asp",
    "date": "July 12, 2019, 11:29am July 12, 2019, 12:30pm",
    "body": "Hi, I am trying to bring a cluster alive using Elastic cloud on kubernetes. I'v setup a bare metal kubernetes cluster (no minikube). I am a little stuck with the persistent local storage descibed here: https://github.com/elastic/cloud-on-k8s/tree/master/local-volume Since I am not using GCE nor minikube, how do I setup this? Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "51fa43a6-39e2-468c-a586-e532d851ecee",
    "url": "https://discuss.elastic.co/t/master-not-discovery-exception/188344",
    "title": "Master not discovery exception",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "bikkina_mahesh",
    "date": "July 2, 2019, 3:57am July 10, 2019, 9:31am July 9, 2019, 5:25am July 9, 2019, 8:21am July 9, 2019, 4:58pm July 10, 2019, 5:30am July 10, 2019, 7:52am July 10, 2019, 9:45am",
    "body": "Using elastic search 7.1.0 My eck crd configuration : apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: labels: controller-tools.k8s.io: \"1.0\" name: moss-es-cluster spec: version: \"7.1.0\" nodes: config: node.master: true node.data: true node.ingest: true podTemplate: metadata: labels: app: moss-es-node spec: containers: - name: elasticsearch resources: limits: memory: 4Gi cpu: 1 nodeCount: 3 this shows how to request 2Gi of persistent data storage for pods in this topology element volumeClaimTemplates: metadata: name: data spec: accessModes: ReadWriteOnce resources: requests: storage: 50Gi storageClassName: rook-block I am getting master not discovered exception and cluster is red state. logs of pods: ter-es-phkl755hgg}{z_DYKZ60Sl-3miLEm7oiuA}{p0-DzHkHRjS66M_APpnXXw}{10.2.1.23}{10.2.1.23:9300}{ml.machine_memory=12884901888, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\" } {\"type\": \"server\", \"timestamp\": \"2019-06-28T06:04:15,345+0000\", \"level\": \"DEBUG\", \"component\": \"o.e.a.a.c.s.TransportClusterUpdateSettingsAction\", \"cluster.name\": \"moss-es-cluster\", \"node.name\": \"moss-es-cluster-es-phkl755hgg\", \"message\": \"timed out while retrying [cluster:admin/settings/update] after failure (timeout [30s])\" } {\"type\": \"server\", \"timestamp\": \"2019-06-28T06:04:15,345+0000\", \"level\": \"WARN\", \"component\": \"r.suppressed\", \"cluster.name\": \"moss-es-cluster\", \"node.name\": \"moss-es-cluster-es-phkl755hgg\", \"message\": \"path: /_cluster/settings, params: {}\" , \"stacktrace\": [\"org.elasticsearch.discovery.MasterNotDiscoveredException: null\", \"at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$4.onTimeout(TransportMasterNodeAction.java:259) [elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:322) [elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:249) [elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:555) [elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:681) [elasticsearch-7.1.0.jar:7.1.0]\", \"at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\", \"at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\",",
    "website_area": "discuss"
  },
  {
    "id": "1ee4ff4f-eaa1-42cc-8e73-33bb252471df",
    "url": "https://discuss.elastic.co/t/public-availability-of-eck-operator-0-9-0-snapshot-images/189663",
    "title": "Public availability of eck-operator 0.9.0-SNAPSHOT images",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ron18219",
    "date": "July 10, 2019, 6:16am July 10, 2019, 7:39am",
    "body": "I see PR https://github.com/elastic/cloud-on-k8s/pull/1184 was recently resolved implying that ECK snapshot releases are now being pushed to a docker registry. Are these eck-operator 0.9.0-SNAPSHOT images publicly available for testing? If so, how can they be accessed and is there a way to query a elastic docker registry for them?",
    "website_area": "discuss"
  },
  {
    "id": "5e868c0e-39df-4436-9099-664ecedb2b14",
    "url": "https://discuss.elastic.co/t/installing-eck-crds-on-gke-1-12-without-validations/187927",
    "title": "Installing ECK CRDs on GKE 1.12 without Validations",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Nitish_Krishna",
    "date": "June 27, 2019, 8:50pm July 9, 2019, 8:24am",
    "body": "I am trying to deploy ECK resources on a GKE cluster as documented here: https://www.elastic.co/elasticsearch-kubernetes In the YAML file Elastic Cloud K8S Resources YAML there are 8 CRDs defined these have validation sections. The CRD Validation is a beta feature in K8s 1.15 and not supported yet in GKE 1.12 : https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#validation Will there be any issue installing these CRD resources WITHOUT the validation section? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "44a76a83-407b-4927-a3fb-b3a88ac35319",
    "url": "https://discuss.elastic.co/t/helm-chart-and-es-5-6/189420",
    "title": "Helm chart and ES 5.6",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "mfcanovas",
    "date": "July 8, 2019, 7:52pm",
    "body": "Is it possible to use official helm chart with the 5.6 docker image? I tried it by setting image and major version values but the pod remains on unready status with no errors in log. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "666a2a63-7280-4f2c-8466-f044214166d3",
    "url": "https://discuss.elastic.co/t/cant-configure-a-multi-node-cluster-using-elastic-kubernetes/188562",
    "title": "Can't configure a multi-node cluster using Elastic-Kubernetes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Chaitanya_Kaul",
    "date": "July 2, 2019, 7:56pm July 2, 2019, 7:22pm July 30, 2019, 7:22pm",
    "body": "Hello guys, I was trying to configure a multi node cluster using cloud-on-k8s but I'm unable to achieve that. Basically I created a NFS server as a pod and created two seperate PVs to mount the same end-point so that my 2 node cluster can create 2 PVcs to lock PVs. But it seems I'm getting this error {\"level\":\"info\",\"ts\":1562096740.5542755,\"logger\":\"process-manager\",\"msg\":\"Update process state\",\"action\":\"initialization\",\"id\":\"es\",\"state\":\"failed\",\"pid\":0} {\"level\":\"info\",\"ts\":1562096740.554345,\"logger\":\"process-manager\",\"msg\":\"Starting...\"} {\"level\":\"info\",\"ts\":1562096740.5548103,\"logger\":\"process-manager\",\"msg\":\"Update process state\",\"action\":\"start\",\"id\":\"es\",\"state\":\"started\",\"pid\":18} {\"level\":\"info\",\"ts\":1562096740.554846,\"logger\":\"process-manager\",\"msg\":\"Started\"} {\"level\":\"info\",\"ts\":1562096740.55715,\"logger\":\"keystore-updater\",\"msg\":\"Waiting for Elasticsearch to be ready\"} OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. {\"type\": \"server\", \"timestamp\": \"2019-07-02T19:45:41,922+0000\", \"level\": \"WARN\", \"component\": \"o.e.b.ElasticsearchUncaughtExceptionHandler\", \"cluster.name\": \"ad-tools-cluster\", \"node.name\": \"ad-tools-cluster-es-hkj7lr8khw\", \"message\": \"uncaught exception in thread [main]\" , \"stacktrace\": [\"org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: failed to obtain node locks, tried [[/usr/share/elasticsearch/data]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?\", \"at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[elasticsearch-cli-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"Caused by: java.lang.IllegalStateException: failed to obtain node locks, tried [[/usr/share/elasticsearch/data]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?\", \"at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:297) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:272) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:252) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:211) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:211) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:325) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"... 6 more\"] } {\"level\":\"info\",\"ts\":1562096741.9779282,\"logger\":\"process-manager\",\"msg\":\"Update process state\",\"action\":\"terminate\",\"id\":\"es\",\"state\":\"failed\",\"pid\":18} {\"level\":\"info\",\"ts\":1562096741.9779992,\"logger\":\"process-manager\",\"msg\":\"Exit\",\"reason\":\"process failed\" This is what I see on Kubernetes logs for the second node which is trying to start-up. I tested my NFS volume(by creating a pod that mounts and writes dates) and it does work. I think I'm going wrong with respect to the way PVs are supposed to be configured.",
    "website_area": "discuss"
  },
  {
    "id": "2789066a-5770-448f-b9c0-cefd41d16f2d",
    "url": "https://discuss.elastic.co/t/es-pod-crashloopbackoff-likely-root-cause-java-nio-file-filealreadyexistsexception/187824",
    "title": "ES pod CrashLoopBackOff , Likely root cause: java.nio.file.FileAlreadyExistsException",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Zanoubia",
    "date": "June 27, 2019, 12:37pm June 27, 2019, 12:44pm",
    "body": "Using elasticsearch docker image 7.2.0, in a kubernetes cluster: ES describe: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 45m elasticsearch-controller Created pod quickstart-es-9smrn67vmv Normal StateChange 44m elasticsearch-controller Master node is now quickstart-es-9smrn67vmv Warning Unhealthy 32m elasticsearch-controller Elasticsearch cluster health degraded Pod logs: {\"level\":\"info\",\"ts\":1561636379.3319728,\"logger\":\"keystore-updater\",\"msg\":\"Waiting for Elasticsearch to be ready\"} OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. Exception in thread \"main\" org.elasticsearch.bootstrap.BootstrapException: java.nio.file.FileAlreadyExistsException: /usr/share/elasticsearch/config/elasticsearch.keystore.tmp Likely root cause: java.nio.file.FileAlreadyExistsException: /usr/share/elasticsearch/config/elasticsearch.keystore.tmp at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:94) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219) at java.base/java.nio.file.spi.FileSystemProvider.newOutputStream(FileSystemProvider.java:478) at java.base/java.nio.file.Files.newOutputStream(Files.java:222) at org.apache.lucene.store.FSDirectory$FSIndexOutput.<init>(FSDirectory.java:411) at org.apache.lucene.store.FSDirectory$FSIndexOutput.<init>(FSDirectory.java:407) at org.apache.lucene.store.FSDirectory.createOutput(FSDirectory.java:255) at org.elasticsearch.common.settings.KeyStoreWrapper.save(KeyStoreWrapper.java:462) at org.elasticsearch.bootstrap.Bootstrap.loadSecureSettings(Bootstrap.java:242) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:305) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) Refer to the log for complete error details. {\"level\":\"info\",\"ts\":1561636381.358021,\"logger\":\"process-manager\",\"msg\":\"Update process state\",\"action\":\"terminate\",\"id\":\"es\",\"state\":\"failed\",\"pid\":14} {\"level\":\"info\",\"ts\":1561636381.3581822,\"logger\":\"process - manager\",\"msg\":\"Exit\",\"reason\":\"process failed\",\"code\":1}",
    "website_area": "discuss"
  },
  {
    "id": "e780edc3-20e1-434c-aee9-92e69d57fa53",
    "url": "https://discuss.elastic.co/t/eck-0-8-1-released/187396",
    "title": "ECK 0.8.1 Released",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Anurag_Gupta",
    "date": "June 25, 2019, 5:11pm",
    "body": "Elastic Cloud on Kubernetes 0.8.1 released! This release adds support for version 7.2. Info about 7.2 can be found here: https://www.elastic.co/blog/elastic-stack-7-2-0-released",
    "website_area": "discuss"
  },
  {
    "id": "5b11fc7d-90eb-42ea-b5d3-5d0063ae3db7",
    "url": "https://discuss.elastic.co/t/killing-pod/186745",
    "title": "Killing pod",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "t0ffel",
    "date": "June 20, 2019, 6:36pm June 24, 2019, 8:32pm June 24, 2019, 8:50pm June 25, 2019, 7:39am June 25, 2019, 1:41pm June 25, 2019, 3:15pm",
    "body": "I'm trying a simple 3-node cluster. All 3 are masters/data/ingest. If I delete all 3 pods manually the pods will spin-up again, they will have the same names, however they'll fail to join the cluster. According to the logs they'll be looking for some completely different non-existent master nodes. Is there a way to resolve this issue? i.e. to ensure that even in case of deletion the pod try to join the existing masters. Where does the operator save information about the masters?",
    "website_area": "discuss"
  },
  {
    "id": "1064849f-2d92-4c38-99bf-f031f2abd140",
    "url": "https://discuss.elastic.co/t/elasticsearch-nodes-get-killed-by-kubernetes-due-to-oom/186682",
    "title": "Elasticsearch nodes get killed by kubernetes due to OOM",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Bozo_Tegeltija",
    "date": "June 20, 2019, 12:28pm June 25, 2019, 7:44am June 25, 2019, 8:05am",
    "body": "I have installed ES cluster on my k8s cluster and no matter how much RAM limit i give to the nodes, they eventually get killed. The load on the cluster is low (approx. 50 docs/min). I have used the default template for Elasticsearch operator with 3 nodes (all masters + data). Monitoring shows that memory is constantly growing for the particular container and when it reaches the limit, kubernetes kills it. I have tried with limits of 6Gi, 12Gi, 40Gi... The kubernetes node has 188GB RAM and it seems to me like the container does not care about the limit and wants to use all of it. Does this have to do with lucene's memory mapped files and how do you solve this?",
    "website_area": "discuss"
  },
  {
    "id": "356d549a-40b5-460d-b6fc-e34c471c3434",
    "url": "https://discuss.elastic.co/t/air-gap-systems-running-with-private-docker-registry/186392",
    "title": "Air-Gap-Systems: Running with private docker registry?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "asp",
    "date": "June 19, 2019, 7:10am June 20, 2019, 2:39pm",
    "body": "Hi, can I configure ECK to use a private docker registry? Target infrastructure will be a bare-metal kubernetes cluster which is air-gap, so the elastic registry will not be available for me. Thanks, Andreas",
    "website_area": "discuss"
  },
  {
    "id": "53e13659-f58b-4f8c-8c1a-3cafdd954522",
    "url": "https://discuss.elastic.co/t/exposing-kibana-with-istio-virtualservice-need-some-extra-steps/185412",
    "title": "Exposing kibana with istio virtualservice need some extra steps",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "mohamamd",
    "date": "June 13, 2019, 1:04pm June 12, 2019, 6:43pm June 13, 2019, 7:05am June 18, 2019, 12:22pm July 16, 2019, 12:22pm",
    "body": "I have deployed elasticsearch and Kibana using the ECK operator as explained in https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html When I port-forward to Kibana service everything works fine. But if I expose the service using Istio virtualservice I see the login page only but nothing works even I cannot login to Kibana. The problem is when Kibana runs behind a proxy there is some problem with the base path. By default Kibana base path is \" /app/kibana\". So in my Kibana manifest, I tried to set the basepath as environment variable as below apiVersion: kibana.k8s.elastic.co/v1alpha1 kind: Kibana metadata: name: my-kibana namespace: elastic-system spec: version: 7.1.0 nodeCount: 2 elasticsearchRef: name: myelasticsearch podTemplate: metadata: labels: app: my-kibana spec: containers: - name: kibana resources: limits: memory: 1Gi cpu: 1 env: - name: SERVER_BASEPATH value: / But it seems the \"SERVER_BASEPATH\" env variable I tried to set in the pod template gets ignored by \"Kibana\" object. My istio virtualservice is as below apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: kibana namespace: elastic-system spec: hosts: - \"kibana.mydomain.com\" gateways: - istio-system/https-gateway - istio-system/http-to-https-redirect http: #- match: # - uri: # prefix: \"/app/kibana\" # rewrite: # uri: \"/\" # route: # - destination: # host: my-kibana # port: # number: 5601 - route: - destination: host: my-kibana port: number: 5601 Any idea how to expose Kibana using istio when ECK operator is used? NOTE: Please ignore any kind name mismatch as i put custom name here and Kibana works completely fine when I do kubectl portforward",
    "website_area": "discuss"
  },
  {
    "id": "a03ee838-75ff-42f7-ae93-8cec0de55e24",
    "url": "https://discuss.elastic.co/t/k8s-operator-with-custom-es-docker-image/186229",
    "title": "K8s operator with custom ES docker image",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "gboanea",
    "date": "June 18, 2019, 11:00am June 18, 2019, 12:02pm June 18, 2019, 12:20pm",
    "body": "Hi, Is is possible to use a custom Elasticsearch docker image with the K8s operator? Thank you, Georgeta",
    "website_area": "discuss"
  },
  {
    "id": "b79af5b1-1078-4bbd-85e9-4dc19b14639d",
    "url": "https://discuss.elastic.co/t/quickstart-example-aws-persistent-storage/185212",
    "title": "Quickstart Example - AWS Persistent Storage",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tomseddon",
    "date": "June 11, 2019, 2:11pm June 12, 2019, 9:12am June 17, 2019, 12:39pm",
    "body": "Hi, I've been trying out the example on my kops K8s setup on AWS (not EKS). I can get the first example to work but when I move on to trying to allocate a PersistentVolumeClaim I run into difficulties. The example given uses gcePersistentDisk, so I try to use the equivalent AWS PVC and can't seem to get it to work. The following give an error in the operator logs of DNS-1123 subdomain must consist of lower case alphanumeric characters: AWSElasticBlockStore awsElasticBlockStore Following the error message, I try: awselasticblockstore Which results in pods being created but a describe on them shows: pod has unbound immediate PersistentVolumeClaims What is the correct way to provision persistent storage on AWS? Thanks, Tom",
    "website_area": "discuss"
  },
  {
    "id": "05d7ad6d-20a1-4c68-9abb-370bfaaf53f8",
    "url": "https://discuss.elastic.co/t/filebeat-installed-as-daemon-set-in-ibm-private-cloud-kubernetes-cluster-dont-send-logs-to-elastic-cloud-instance/184406",
    "title": "Filebeat installed as Daemon Set in IBM Private Cloud Kubernetes Cluster don't send logs to Elastic Cloud instance",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "l.rava",
    "date": "June 5, 2019, 3:09pm June 5, 2019, 3:15pm June 16, 2019, 8:47pm",
    "body": "Hi I installed as Daemon Set metricBeat and fileBeat into my infrastracture: IBM Private Cloud kubernetes cluster - 2 nodes - v.1.13 following this guide. MetricBeat send data without problem. Filebeat don't send data. here logs in debug for filebeat: INFO instance/beat.go:571 Home path: [/usr/share/filebeat] Config path: [/usr/share/filebeat] Data path: [/usr/share/filebeat/data] Logs path: [/usr/share/filebeat/logs] DEBUG [beat] instance/beat.go:623 Beat metadata path: /usr/share/filebeat/data/meta.json INFO instance/beat.go:579 Beat ID: 8498bd87-16c3-42a3-b5e0-4ca23133a429 INFO [index-management.ilm] ilm/ilm.go:129 Policy name: filebeat-7.1.1 DEBUG [filters] add_cloud_metadata/add_cloud_metadata.go:164 add_cloud_metadata: starting to fetch metadata, timeout=3s DEBUG [filters] add_cloud_metadata/add_cloud_metadata.go:196 add_cloud_metadata: received disposition for qcloud after 339.455729ms. result=[provider:qcloud, error=failed requesting qcloud metadata: Get http://metadata.tencentyun.com/meta-data/instance-id: dial tcp: lookup metadata.tencentyun.com on 172.21.0.10:53: no such host, metadata={}] DEBUG [filters] add_cloud_metadata/add_cloud_metadata.go:196 add_cloud_metadata: received disposition for openstack after 3.000222149s. result=[provider:openstack, error=failed requesting openstack metadata: Get http://169.254.169.254/2009-04-04/meta-data/hostname: dial tcp 169.254.169.254:80: i/o timeout, metadata={}] DEBUG [filters] add_cloud_metadata/add_cloud_metadata.go:203 add_cloud_metadata: timed-out waiting for all responses DEBUG [filters] add_cloud_metadata/add_cloud_metadata.go:167 add_cloud_metadata: fetchMetadata ran for 3.000347097s INFO add_cloud_metadata/add_cloud_metadata.go:346 add_cloud_metadata: hosting provider type not detected. DEBUG [processors] processors/processor.go:66 Processors: add_cloud_metadata=null DEBUG [seccomp] seccomp/seccomp.go:109 Loading syscall filter {\"seccomp_filter\": {\"no_new_privs\":true,\"flag\":\"tsync\",\"policy\":{\"default_action\":\"errno\",\"syscalls\":[{\"names\":[\"accept\",\"accept4\",...,\"writev\"],\"action\":\"allow\"}]}}} INFO [seccomp] seccomp/seccomp.go:116 Syscall filter successfully installed INFO [beat] instance/beat.go:827 Beat info {\"system_info\": {\"beat\": {\"path\": {\"config\": \"/usr/share/filebeat\", \"data\": \"/usr/share/filebeat/data\", \"home\": \"/usr/share/filebeat\", \"logs\": \"/usr/share/filebeat/logs\"}, \"type\": \"filebeat\", \"uuid\": \"8498bd87-16c3-42a3-b5e0-4ca23133a429\"}}} INFO [beat] instance/beat.go:836 Build info {\"system_info\": {\"build\": {\"commit\": \"3358d9a5a09e3c6709a2d3aaafde628ea34e8419\", \"libbeat\": \"7.1.1\", \"time\": \"2019-05-23T13:21:33.000Z\", \"version\": \"7.1.1\"}}} INFO [beat] instance/beat.go:839 Go runtime info {\"system_info\": {\"go\": {\"os\":\"linux\",\"arch\":\"amd64\",\"max_procs\":4,\"version\":\"go1.11.5\"}}} INFO [beat] instance/beat.go:843 Host info {\"system_info\": {\"host\": {\"architecture\":\"x86_64\",\"boot_time\":\"2019-04-26T14:06:48Z\",\"containerized\":false,\"name\":\"kube-fra02-cr7a*************49fc787c-w5.cloud.ibm\",\"ip\":[\"127.0.0.1/8\",\"172.20.0.1/32\",\"::1/128\",\"10.XX.225.43/26\",\"fe80::467:43ff:fec8:f89c/64\",\"158.XX.138.101/28\",\"158.XX.145.222/32\",\"fe80::xxx:72ff:fe53:6022/64\",\"127.0.0.10/31\",\"fe80::bcaa:c3ff:fe81:c38/64\",\"172.30.7.64/32\"}}} INFO [beat] instance/beat.go:872 Process info {\"system_info\": {\"process\": {\"capabilities\": {\"inheritable\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"permitted\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"effective\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"bounding\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"ambient\":null}, \"cwd\": \"/usr/share/filebeat\", \"exe\": \"/usr/share/filebeat/filebeat\", \"name\": \"filebeat\", \"pid\": 1, \"ppid\": 0, \"seccomp\": {\"mode\":\"filter\"}, \"start_time\": \"2019-06-05T12:35:27.460Z\"}}} INFO instance/beat.go:280 Setup Beat: filebeat; Version: 7.1.1 DEBUG [beat] instance/beat.go:301 Initializing output plugins INFO [index-management] idxmgmt/std.go:165 Set output.elasticsearch.index to 'filebeat-7.1.1' as ILM is enabled. INFO elasticsearch/client.go:165 Elasticsearch url: https://51fxxxxxxxxxxxxf3c92201363c.europe-west1.gcp.cloud.es.io:443 DEBUG [publisher] pipeline/consumer.go:137 start pipeline event consumer INFO [publisher] pipeline/module.go:97 Beat name: kube-fra02-cr7a4aeac0xxxxxxx40c8c9799d049fc787c-w5.cloud.ibm INFO instance/beat.go:391 filebeat start running. DEBUG [test] registrar/migrate.go:159 isFile(/usr/share/filebeat/data/registry) -> false DEBUG [test] registrar/migrate.go:159 isFile() -> false DEBUG [test] registrar/migrate.go:152 isDir(/usr/share/filebeat/data/registry/filebeat) -> true DEBUG [test] registrar/migrate.go:159 isFile(/usr/share/filebeat/data/registry/filebeat/meta.json) -> true INFO [monitoring] log/log.go:117 Starting metrics logging every 30s DEBUG [registrar] registrar/migrate.go:51 Registry type '0' found DEBUG [registrar] registrar/registrar.go:125 Registry file set to: /usr/share/filebeat/data/registry/filebeat/data.json INFO registrar/registrar.go:145 Loading registrar data from /usr/share/filebeat/data/registry/filebeat/data.json INFO registrar/registrar.go:152 States Loaded from registrar: 0 INFO crawler/crawler.go:72 Loading Inputs: 0 INFO crawler/crawler.go:106 Loading and starting Inputs completed. Enabled inputs: 0 WARN [cfgwarn] kubernetes/kubernetes.go:55 BETA: The kubernetes autodiscover is beta INFO kubernetes/util.go:86 kubernetes: Using pod name kube-fra02-cr7a4aeacxxxxx799d049fc787c-w5.cloud.ibm and namespace kube-system to discover kubernetes node INFO cfgfile/reload.go:150 Config reloader started DEBUG [registrar] registrar/registrar.go:278 Starting Registrar ERROR kubernetes/util.go:90 kubernetes: Querying for pod failed with error: kubernetes api: Failure 404 pods \"kube-fra02-cr7a4aeac036c7440c8c9799d049fc787c-w5.cloud.ibm\" not found",
    "website_area": "discuss"
  },
  {
    "id": "037a02f1-dd60-4ccd-8bd6-7390e75d23ee",
    "url": "https://discuss.elastic.co/t/issue-elasticsearch-pods-are-running-into-crashloopbackoff-because-of-inject-process-manager-container/185382",
    "title": "Issue: Elasticsearch pods are running into crashloopbackoff because of inject-process-manager container",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Batchu",
    "date": "June 12, 2019, 10:18am June 13, 2019, 2:26am June 13, 2019, 6:19am June 13, 2019, 6:20am June 13, 2019, 6:22am June 13, 2019, 6:24am",
    "body": "github.com/elastic/cloud-on-k8s Issue: Elasticsearch pods are running into crashloopbackoff because of inject-process-manager container opened by aakarshit-batchu on 2019-06-12 Bug Report What did you do? I just freshly installed an Elastic stack on Kubernetes in following tutorial: https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html So I have an elasticsearch cluster... What did you do? I just freshly installed an Elastic stack on Kubernetes in following tutorial: https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html So I have an elasticsearch cluster of three nodes (test phase;)) + an instance of Kibana that connects to it. Everything seems to work because everything is in a \"green\" state: [root@chnkubmtr36 es-operator]# kubectl get elasticsearch NAME HEALTH NODES VERSION PHASE AGE quickstart green 3 7.1.0 Operational 26m But all the elasticsearch-pods are in the crashloopbackoff state. [root@chnkubmtr36 es-operator]# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES quickstart-es-55mbfz28n8 0/1 Init:CrashLoopBackOff 2 28m 10.233.66.163 chnkubnode38 <none> <none> quickstart-es-d789sfck4d 0/1 Init:CrashLoopBackOff 2 28m 10.233.65.56 chnkubnode37 <none> <none> quickstart-es-g8dx797tw5 0/1 Init:CrashLoopBackOff 1 28m 10.233.64.129 chnkubmtr36 <none> <none> What did you expect to see? All the pods in running state without any errors. What did you see instead? Under which circumstances? Elasticsearch pods in crashloopbackoff and when described the elasticsearch pods, we found out that inject-process-manager container is going to crashloopbackoff. and we are not able to view its logs as well. So what are functionalities of inject-process-manager container? and on what all conditions does it fail? Environment Vmware Vms CentOS Linux release 7.6.1810 (Core) Kubernetes - 1.13.5 (on premise setup) Docker - 18.09.1 $ kubectl version - 1.13.5",
    "website_area": "discuss"
  },
  {
    "id": "c2a42ae3-26cc-42f7-a10d-1575f575fb76",
    "url": "https://discuss.elastic.co/t/using-a-custom-docker-image-for-elastic-cloud-on-kubernetes/184241",
    "title": "Using a custom docker image for Elastic Cloud on Kubernetes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "rutomo",
    "date": "June 4, 2019, 6:09pm June 5, 2019, 7:32am June 5, 2019, 12:21pm June 5, 2019, 12:26pm June 5, 2019, 1:45pm June 5, 2019, 1:59pm June 5, 2019, 3:08pm",
    "body": "Hi, I'm looking into or doing some research on Elastic Cloud on Kubernetes and went through the Quickstart page but I couldn't find a way to use ECK with a custom ES docker image hosted on docker hub. Is there a way to do so?",
    "website_area": "discuss"
  },
  {
    "id": "a9874632-4ebd-49ad-a0bc-d0665441e752",
    "url": "https://discuss.elastic.co/t/about-the-hadoop-and-elasticsearch-category/229",
    "title": "About the Hadoop and Elasticsearch category",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Leslie_Hawthorn",
    "date": "May 5, 2015, 3:06pm May 6, 2015, 5:42am May 14, 2015, 6:52am May 14, 2015, 6:52am",
    "body": "Questions about Elasticsearch and all things Hadoop (Map/Reduce, Hive, Pig, Cascading, Spark and friends)",
    "website_area": "discuss"
  },
  {
    "id": "4c6cb6a5-a72b-4320-a698-e71b38c4330e",
    "url": "https://discuss.elastic.co/t/spark-2-4-to-elasticsearch-prevent-data-loss-during-dataproc-nodes-decommissioning/216076",
    "title": "Spark 2.4 to Elasticsearch : prevent data loss during dataproc nodes decommissioning?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "fredrouvier",
    "date": "January 22, 2020, 2:04pm",
    "body": "My technical task is to synchronize data from GCS (Google Cloud Storage) to our Elasticsearch cluster. We use Apache Spark 2.4 with the Elastic Hadoop connector on a Google dataproc cluster (autoscaling enabled). During the execution, if the dataproc cluster downscaled, all tasks on the decommissioned node are lost and the processed data on this node are never pushed to elastic. This problem does not exist when I save to GCS or HDFS for example. How to make resilient this task even when nodes are decommissioned ? An extract of the stacktrace : Lost task 50.0 in stage 2.3 (TID 427, xxxxxxx-sw-vrb7.c.xxxxxxx, executor 43): FetchFailed(BlockManagerId(30, xxxxxxx-w-23.c.xxxxxxx, 7337, None), shuffleId=0, mapId=26, reduceId=170, message=org.apache.spark.shuffle.FetchFailedException: Failed to connect to xxxxxxx-w-23.c.xxxxxxx:7337 Caused by: java.net.UnknownHostException: xxxxxxx-w-23.c.xxxxxxx Task 50.0 in stage 2.3 (TID 427) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded). Thanks. Fred",
    "website_area": "discuss"
  },
  {
    "id": "a0e82bdf-5bb5-409c-8545-d38c30ad558a",
    "url": "https://discuss.elastic.co/t/spark-sql-not-reading-all-the-columns-from-index/215518",
    "title": "Spark sql not reading all the columns from index",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "raghu2",
    "date": "January 17, 2020, 8:27pm",
    "body": "I am using pyspark 1.6.1 using elasticsearch-spark-13_2.10-7.5.1.jar to read data from ES 5.6.8 running on AWS Es service. I am able to use \"es.read.field.include\" to extract only the columns we need or also register the index as temp table and select only the columns we need. The source index mapping is not auto updated and any columns that are not in the index mapping are not available to extract from spark. How do we read all the columns from ES using spark. I tried to pass es.query = {\"query\": {\"match_all\": {}} }\" as well but still it uses the index mapping for the schema. Is there a way i can extract the whole index and create a data frame on that data.",
    "website_area": "discuss"
  },
  {
    "id": "27bdbde7-4859-4a77-bcc0-bdcc8c4ae229",
    "url": "https://discuss.elastic.co/t/elastic-spark-errorhandler/215170",
    "title": "Elastic Spark ErrorHandler",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "itssujan",
    "date": "January 15, 2020, 4:14pm",
    "body": "Hello, Im using elasticsearch-hadoop to index our dataframe via spark sink. I have used the configurations as mentioned in https://www.elastic.co/guide/en/elasticsearch/hadoop/current/errorhandlers.html to configure a custom ErrorHandler. But I fail to get the errorhandler activated when an error occurs. Doesnt it work when we use or.elasticsearch.spark classes for processing?",
    "website_area": "discuss"
  },
  {
    "id": "8ce27bd6-7174-4633-8d48-fcffa35d010d",
    "url": "https://discuss.elastic.co/t/spark-structured-streaming-json-serialization/214661",
    "title": "Spark Structured Streaming JSON Serialization",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "execdd17",
    "date": "January 13, 2020, 1:49pm",
    "body": "Hey Everyone, I hope this question hasn't been asked before, I looked around quite a bit before posting. If I missed it, sorry! I'm trying to use the org.elasticsearch.spark.sql structured streaming sink for ES, and am running into some challenges when my column contains a string literal of valid JSON. I have one column called combined. When the documents are written to ES they look like this: \"_source\" : { \"combined\" : \"\"\"{\"sparkTimestamp\":\"2020-01-08T15:42:24.890Z\",\"@timestamp\":\"2020-01-08T15:39:40.583Z\",\"@metadata\":{\"beat\":\"winlogbeat\",\"type\":\"_doc\",\"version\":\"7.4.2\",\"topic\":\"ingest-winlogbeat\"},\"host\":{\"hostname\":\"A HOSTNAME\"................ truncated for brevity As you can see, it's assuming that I have a normal string, and is nice enough to escape it for me too. What I am really after though, is for the JSON string within the combined column to be the _source. It looks like this is possible with the older RDD approach, but I didn't see any way to make it work within the context of this sink. Is this possible? If there is another way, what are the message delivery guarantees? If you're wondering why I'm doing this, then I'm happy to briefly explain. I'm using winlogbeat as a source, and while it is ECS compliant, a vast number of fields are within a custom extension. In addition, depending on the event, the fields will change. If I try to account for this in a spark managed(SQL) table, it's going to be very difficult because of the sparsity and sheer number of fields. To alleviate this I parsed out core ECS fields and retained the original JSON from winlogbeat. Now that the raw table has been established, I want to index the original JSON into ES. This is effectively the combined column. Any insight would be appreciated. Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "173063b4-a712-499a-a4d4-1c0cbd951334",
    "url": "https://discuss.elastic.co/t/how-to-add-truststore-to-classpath-in-hive/212959",
    "title": "How to add truststore to classpath in Hive",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "oranciog",
    "date": "December 24, 2019, 1:31pm January 6, 2020, 5:57pm",
    "body": "Hi, According to documentation, es.net.ssl.truststore.location is trust store location (typically a URL, without a prefix it is interpreted as a classpath entry) My question is how to add a truststore in a hive cli job classpath. I tried the following solutions, none is working: ADD JAR truststore.jks - hive cli command (According to hive cli docs, ADD JAR resources are also added to the Java classpath.) (I've loaded with ADD JAR a jar and is loaded ok). hive.aux.jars.path - setting in hive-site.xml (I've set this to a hdfs location, where I put all the jars needed for the job and the truststore.jks. All jars were loaded correctly, but truststore.jks was not found in classpath, according to es-hadoop plugin). Also, all the examples I've found were with file://, but none of them were showing how to load this file from classpath.",
    "website_area": "discuss"
  },
  {
    "id": "aa7c3200-7aa8-486b-9c5f-9bc011105b54",
    "url": "https://discuss.elastic.co/t/unable-to-index-documents-into-elastic-cloud-hosted-managed-service-on-gcp-using-spark/212135",
    "title": "Unable to index documents into Elastic Cloud hosted managed service on GCP using Spark",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "devender2601",
    "date": "December 17, 2019, 6:18pm December 17, 2019, 10:50am December 17, 2019, 8:51pm December 18, 2019, 1:03pm December 18, 2019, 6:57pm January 6, 2020, 5:47pm",
    "body": "Hi, I am trying to insert documents in the index after reading documents from GCP storage. My Elasticsearch flavour on GCP is elastic cloud service hosted by elastic. for this, I am using version 7.5.0. and my Gradle dependencies look like the below:- dependencies { compile group: 'org.apache.spark', name: 'spark-core_2.11', version: '2.3.4' compile group: 'org.apache.spark', name: 'spark-sql_2.11', version: '2.3.4' compile group: 'org.elasticsearch', name: 'elasticsearch-hadoop', version: '7.5.0' compile group: 'org.elasticsearch',name:'elasticsearch',version:'7.5.0' } In code, I am passing username and auth using spark.es.net.http.auth.user & spark.es.net.http.auth.pass property. So, with the same code and dependency, if I just change the hostname to localhost and port no. to 9200 which is different for elastic cloud service, my code is able to successfully insert the documents in the index in the local elastic cluster. but while trying with elastic cloud hostname and 9243 port, I am gettting the weird error Exception in thread \"main\" java.lang.NoClassDefFoundError: org/elasticsearch/spark/sql/api/java/JavaEsSparkSQL at com.cortex.spark.gcpSparktoES.main(gcpSparktoES.java:42) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:890) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:192) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:217) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) Caused by: java.lang.ClassNotFoundException: org.elasticsearch.spark.sql.api.java.JavaEsSparkSQL at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:418) at java.lang.ClassLoader.loadClass(ClassLoader.java:351) Can you please help me with the solution for it. For your information , same error is coming when i am submitting the job on dataproc cluster on GCP.",
    "website_area": "discuss"
  },
  {
    "id": "ccc3102f-eeeb-4c9f-9796-c4230434a0cd",
    "url": "https://discuss.elastic.co/t/spark-to-aws-elasticsearch-service/212246",
    "title": "Spark to AWS ElasticSearch Service",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "yarch",
    "date": "December 18, 2019, 3:56am January 6, 2020, 5:43pm",
    "body": "I am running spark on my local machine. I have Elastic Search up and running in AWS-ElasticSearch service. I am trying to follow the documentation specified here: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html Version of Elasticsearch-spark that I am using is, <dependency> <groupId>org.elasticsearch</groupId> <artifactId>elasticsearch-spark-20_2.10</artifactId> <version>7.5.0</version> </dependency> This is how my SparkConf looks like: SparkConf conf = new SparkConf().setMaster(\"local[*]\").setAppName(properties.getProperty(\"app.name\")) .set(\"es.nodes\", \"search-**********.us-west-1.es.amazonaws.com\") .set(\"es.port\",\"443\") .set(\"es.http.timeout\", \"5m\") .set(\"es.nodes.wan.only\", \"true\"); # Call the method to send logs to ES, assume stringResults to a JavaDStream<Map<String, Object>> object ElasticSearchManager.sendToEs(stringResults); This is how I am trying to store the data in ElasticSearch import static org.elasticsearch.spark.streaming.api.java.JavaEsSparkStreaming.saveToEs; public class ElasticSearchManager { public static void sendToEs(JavaDStream<Map<String, Object>> javaDStream) { ZonedDateTime dateTime = LocalDateTime.now().atZone(ZoneId.systemDefault()); saveToEs(javaDStream, dateTime.format(DateTimeFormatter.ofPattern(\"YYYY-MM-dd\"))); } } This is the error I get org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:340) at org.elasticsearch.spark.rdd.EsSpark$.doSaveToEs(EsSpark.scala:104) at org.elasticsearch.spark.streaming.EsSparkStreaming$anonfun$doSaveToEs$1.apply(EsSparkStreaming.scala:71) at org.elasticsearch.spark.streaming.EsSparkStreaming$anonfun$doSaveToEs$1.apply(EsSparkStreaming.scala:71) at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628) at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628) at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416) at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at scala.util.Try$.apply(Try.scala:213) at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39) at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62) at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: [GET] on [] failed; server[search-************.us-west-1.es.amazonaws.com:443] returned [400|Bad Request:] at org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:477) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:434) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:428) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:388) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:392) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:168) at org.elasticsearch.hadoop.rest.RestClient.mainInfo(RestClient.java:745) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:330) ... 19 more I tried to debug as to what's the issue. And, this is what I found in package org.elasticsearch.hadoop.rest.RestClient.java in line 745 Map<String, Object> result = get(\"\", null); Not sure why they would set the URI in the get method to empty string. Now I am struck at this point and don't have a good path forward. Any help would be appreciated.",
    "website_area": "discuss"
  },
  {
    "id": "0e439d57-2a9c-4865-8144-e70a2b21346b",
    "url": "https://discuss.elastic.co/t/essparksql-savetoes-circuitbreakingexception-parent-data-too-large-data-for-transport-request/212372",
    "title": "EsSparkSQL.saveToEs() CircuitBreakingException: [parent] Data too large, data for [<transport_request>]",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Enrique_Garcia_Garci",
    "date": "December 18, 2019, 4:45pm January 15, 2020, 4:45pm",
    "body": "I'm trying to get a single line JSON datasource of around some 26M records, apply some logic (two filters and then a \"select\" to get the desired results -6 fields at all-) and save them in ES... so far so good. Tha problem seems to be the EsSparkSQL.saveToEs() which always raises the circuit breaker exception. If this connector is responsible of doing whatever it does (I don't really know why this connector takes that large amount of tasks/jobs to save already \"formated\" data, except it is not JSON) and then save to ES why this exception is raised? Shouldn't it be smart enough to check the max data size and flush the bulk before that limit is exceed? This is the exact exception (bailing out): org.elasticsearch.hadoop.rest.EsHadoopRemoteException: circuit_breaking_exception: [parent] Data too large, data for [<transport_request>] would be [259268254/247.2mb], which is larger than the limit of [254332108/242.5mb], real usage: [258560280/246.5mb], new bytes reserved: [707974/691.3kb], usages [request=0/0b, fielddata=38332/37.4kb, in_flight_requests=1395602/1.3mb, accounting=5724890/5.4mb] I've also tried to set the breaker limit up to 99% but with same result. KO. String payload = \"{\\\"persistent\\\" : {\\\"indices.breaker.total.limit\\\" : \\\"99%\\\"}}\"; StringRequestEntity requestEntity = new StringRequestEntity(payload, \"application/json\", \"UTF-8\"); PutMethod putMethod = new PutMethod(host + CLUSTER_SETTINGS_ENDPOINT); putMethod.setRequestEntity(requestEntity); int statusCode = httpClient.executeMethod(putMethod); If answer is no, how can I fix it? Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "31c1b23f-99fc-4cf7-8503-54ccf987302d",
    "url": "https://discuss.elastic.co/t/unable-to-index-the-document-through-es-hadoop-spark-in-local-mode-it-is-working-but-from-cluster-it-is-not-working/211309",
    "title": "Unable to index the document through ES-Hadoop(Spark) : In local mode it is working ,but from cluster it is not working",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Srinivas2",
    "date": "December 11, 2019, 7:53am December 17, 2019, 9:08pm January 14, 2020, 9:08pm",
    "body": "Hello Everyone, Need help on below issue. I am indexing a nested JSON through ES-Hadoop , but it is failing with the below error org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages): org.elasticsearch.hadoop.rest.EsHadoopRemoteException: mapper_parsing_exception: failed to parse;org.elasticsearch.hadoop.rest.EsHadoopRemoteException: not_x_content_exception: Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes {\"index\":{}} I am using below code to write the document into AWS-ES. In below code nonModifiedDataFrame always have only one record. documentpath have one Json file(size is around 100 MB) and I have to add indexid and indextimestamp as two columns. If I use sparkContext.textFile(documentpath) to create RDD[String] I am unable to add two more columns in the json , so I took dataframe approach and cleaning some information in Json y using Method replaceDotsWithUnderScore. def indexThroughSpark(spark:SparkSession,indexid:String,documnetPath:String,indexName:String ,indextimestamp:String)={ import spark.implicits._ val nonModifiedDataFrame= spark.read.json(documnetPath) .withColumn(\"indexid\",lit(indexid)) .withColumn(\"indextimestamp\",lit(indextimestamp)) val convertedString:RDD[String] = nonModifiedDataFrame.toJSON.rdd val replacedString = convertedString.map{ line => ModifyKeysForDots().replaceDotsWithUnderScore(line) } val cfg = Map( (\"es.resource\",\"indexfor_spark/_doc\") ) EsSpark.saveJsonToEs(replacedString,cfg) } If I execute the above code in my local environment i.e. master is local it is working fine and I am able to see the no of documents , but if I run the same code in cluster and master as Yarn it is failing . Thanks In advance.",
    "website_area": "discuss"
  },
  {
    "id": "5c879240-48aa-415a-9739-3019ec45e756",
    "url": "https://discuss.elastic.co/t/example-of-spark-eshadoop-query-counting-distinct-host-hostname/211369",
    "title": "Example of Spark/EShadoop query counting distinct host.hostname",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Saas_Cloud_Security",
    "date": "December 10, 2019, 7:34pm January 7, 2020, 7:34pm",
    "body": "I would like to count the # of distinct host.hostname in a 50G Elasticsearch index. Do you have an example that I model after?",
    "website_area": "discuss"
  },
  {
    "id": "1635fe89-c886-4598-8f2f-9909cbda21ac",
    "url": "https://discuss.elastic.co/t/hive-3-1-1-es-hadoop-connector/205992",
    "title": "Hive 3.1.1 ES-Hadoop Connector",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Loo_Ying_Ting",
    "date": "October 31, 2019, 7:34am November 14, 2019, 8:49pm November 22, 2019, 9:49am November 22, 2019, 10:25am December 2, 2019, 6:06pm December 5, 2019, 10:47am January 2, 2020, 10:48am",
    "body": "Continuing the discussion from Support for HDFS 3.1.1 and Hive 3.1: My Hive version is 3.1.1 Hi I keep getting error Error: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:175) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1911) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:169) Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) ... 9 more Caused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 more Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) ... 17 more Caused by: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:137) ... 22 more Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 7.3.1 at org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:623) at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.initializeOp(VectorFileSinkOperator.java:84) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:573) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:525) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:386) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:573) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:525) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:386) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.initializeMapOperator(VectorMapOperator.java:591) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:116) ... 22 more Caused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 7.3.1 at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:82) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:314) at org.elasticsearch.hadoop.hive.EsSerDe.initialize(EsSerDe.java:87) at org.elasticsearch.hadoop.hive.EsSerDe.initialize(EsSerDe.java:102) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:532) ... 32 more 19/10/31 15:22:41 INFO impl.YarnClientImpl: Killed application application_1571499978896_0102 FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask when I try to insert data. Any idea the reasons? Is Hive 3.1.1 supported yet?",
    "website_area": "discuss"
  },
  {
    "id": "0d19d119-a742-43f4-b70d-18be544164c0",
    "url": "https://discuss.elastic.co/t/enable-compression-while-indexing-data-using-hive-elastic-connector/210265",
    "title": "Enable compression while indexing data using Hive/Elastic connector",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "sachinjose",
    "date": "December 3, 2019, 1:49am December 4, 2019, 3:01pm December 4, 2019, 6:44pm January 1, 2020, 6:43pm",
    "body": "Hi Team, We are using Hive/Elasticsearch(connector) for batch indexing data to ES, Can some one pls share the configs in the indexer side to enable compression(gzip/deflate) for the data in transit(bulk write) in order to reduce the network consumption during indexing time. I am able to test enabling decompress data in the ES side. Thanks, Sachin",
    "website_area": "discuss"
  },
  {
    "id": "d9272b8f-a929-4571-b0d7-d8d1c21b82ab",
    "url": "https://discuss.elastic.co/t/loading-pyspark-dataframe-into-elasticsearch-keep-getting-illegal-argument-error/208348",
    "title": "Loading pySpark DataFrame into Elasticsearch - keep getting illegal argument error",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "dkajtoch",
    "date": "November 18, 2019, 4:07pm December 2, 2019, 6:04pm December 30, 2019, 6:04pm",
    "body": "I am trying to send data stored in pySpark Dataframe directly into Elasticsearch. I found some code snippets online on how to do this and basically it looks like this: # df - DataFrame df.write.format(\"org.elasticsearch.spark.sql\") \\ .option(\"es.nodes\", \"ip_address\") \\ .option(\"es.resources\", \"test\") .save() but whatever I type in option results in IllegalArgumentException. I am running scala 2.11.0, spark 2.3.4 on Google Cloud. My elasticsearch machine is on a separate machine ver. 7.4.1 I download jar elasticsearch-spark-20_2.11-7.4.1.jar and placed it in spark jars directory (it previously worked that way with mongo and google cloud storage). How to properly send pyspark DataFrame into Elasticsearch?",
    "website_area": "discuss"
  },
  {
    "id": "ae1c46d2-2bab-41a5-9b0b-67537024d8bb",
    "url": "https://discuss.elastic.co/t/hive-read-es-data-slow/208670",
    "title": "Hive read es data slow",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "yousanghz",
    "date": "November 20, 2019, 10:07am November 21, 2019, 5:40am November 21, 2019, 1:38pm November 22, 2019, 1:42am November 22, 2019, 7:51am December 20, 2019, 7:58am",
    "body": "hive version 1.2.1 es version 5.5.0 hadoop-elasticsearch-5.5.0.jar es.index: data_monthly 10 shard this is hsql CREATE EXTERNAL TABLE es_test5( id string, uid string, wb_name string, platform string, comment_count int, fetch_time timestamp, play_count int, favorite_count int, repost_count int, monthly_net_inc_favorite_count int , monthly_net_inc_play_count int, monthly_net_inc_comment_count int, release_time timestamp ) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES( 'es.nodes' = '192.168.17.111, 192.168.17.121', 'es.index.auto.create' = 'false', 'es.resource' = 'data_monthly', 'es.read.metadata' = 'true', 'es.mapping.names' = 'id:_metadata._id, uid:UID'); read data from es into hive , image.png773231 3.19 KB why 10 shard but 2 map ? why slow please help me , Thank",
    "website_area": "discuss"
  },
  {
    "id": "b872d48a-e75f-4ba7-8828-2619b5a30a5b",
    "url": "https://discuss.elastic.co/t/saving-dataframe-to-elasticsearch/204451",
    "title": "Saving dataframe to ElasticSearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Hugh_McBride",
    "date": "October 21, 2019, 11:32am November 14, 2019, 7:23pm November 15, 2019, 11:15am December 13, 2019, 11:15am",
    "body": "I am on a new project that is saving spark dataframes to elastic search using dataFrame.saveToEsI(.... ) There are multi terabytes of Data (> 10) that are being saved to a rather small cluster (5 node ) , And it is taking on the order of a week to load, Is this method of saving data to elastic search suitable for this volume of data, (I am guessing not ) or would it be better to save the dataframes to json ( the spark/hadoop cluster is much much bigger ) first and then bulkload",
    "website_area": "discuss"
  },
  {
    "id": "ff679720-f637-4664-8e89-b399073da287",
    "url": "https://discuss.elastic.co/t/hadoop-mapr-and-elk/204912",
    "title": "Hadoop/Mapr and ELK",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "lcui_dxc",
    "date": "October 23, 2019, 4:35pm November 14, 2019, 8:47pm December 12, 2019, 8:47pm",
    "body": "Hello there: We have a request to use ELK to access/fetch some log files stored on a Mapr cluster file system (HDFS or MFS), but don't know how and where to setup the ES for Hadoop... Any suggestions on how to fetch (like filebeat does) log files on HDFS and send to Logstash (or Elasticsearch nodes)? Thank you very much Li",
    "website_area": "discuss"
  },
  {
    "id": "cb1a4e58-4b88-4d84-8c03-3bff3f30007a",
    "url": "https://discuss.elastic.co/t/spark-string-to-elasticsearch-geo-point/207902",
    "title": "Spark string to elasticsearch geo_point",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "ParthM",
    "date": "November 14, 2019, 1:28pm December 12, 2019, 1:28pm",
    "body": "Hello, I am new to ES so I don't know much about it. I have a spark streaming DataFrame which has a field \"location\" and it has the latitude and longitude of a place and I want to show the data into kibana Map. The issue is I don't know how. I've tried creating index first and then dump data into the index gives me error of rejected mapping. Tell me how to do so....",
    "website_area": "discuss"
  },
  {
    "id": "a7fac5d4-3fa9-4e60-9280-72ca24cb6a60",
    "url": "https://discuss.elastic.co/t/use-cases-for-es-hadoop/203317",
    "title": "Use cases for es-hadoop",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Devendra_Kumar",
    "date": "October 12, 2019, 12:26pm October 16, 2019, 8:37pm October 23, 2019, 8:58pm November 20, 2019, 8:58pm",
    "body": "I want to know complete use cases for es-hadoop. I want to understand in what ways we can utilise es-hadoop.",
    "website_area": "discuss"
  },
  {
    "id": "d5f136db-215a-4b26-81c3-2311e22821bc",
    "url": "https://discuss.elastic.co/t/hadoop-connection-with-elasticsearch/203065",
    "title": "Hadoop connection with Elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Vishnu_mk",
    "date": "October 11, 2019, 12:13pm October 16, 2019, 8:32pm November 13, 2019, 8:32pm",
    "body": "Hi, While trying to connect to elasticsearch through hive, getting below exception ERROR : Job Submission failed with exception 'org.elasticsearch.hadoop.EsHadoopIllegalArgumentException(Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only')' org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:344) at org.elasticsearch.hadoop.mr.EsOutputFormat.init(EsOutputFormat.java:262) at org.elasticsearch.hadoop.mr.EsOutputFormat.checkOutputSpecs(EsOutputFormat.java:253) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.checkOutputSpecs(FileSinkOperator.java:1087) at org.apache.hadoop.hive.ql.io.HiveOutputFormatImpl.checkOutputSpecs(HiveOutputFormatImpl.java:67) at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:272) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1304) at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:578) at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:573) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920) at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:573) at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:564) at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:428) at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:142) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1978) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1691) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1423) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1207) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1202) at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237) at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88) at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920) at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[127.0.0.1:9200]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:152) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:403) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:367) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:371) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:166) at org.elasticsearch.hadoop.rest.RestClient.mainInfo(RestClient.java:692) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:334) ... 40 more ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask INFO : Completed executing command(queryId=hive_20191010211212_5c6fbe43-b34e-49be-8ea3- 42ff5f305978); Time taken: 0.103 seconds Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask (state=08S01,code=1) Using hadoop-elasticsearch connector version - 6.7.0 Elasticsearch - 6.7.0 Also I dont understand why its trying to connect to 127.0.0.1 when i created external table on CREATE EXTERNAL TABLE test.artist (name STRING) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.node' = '10.34.9.64' , 'es.resource' = 'test/logs', 'es.index.auto.create' = 'yes'); Need some urgent help. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "d82058f1-7f8a-457b-88f1-cd51bf7e179e",
    "url": "https://discuss.elastic.co/t/elasticsearch-hadoop-connection/202501",
    "title": "Elasticsearch Hadoop Connection",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Vishnu_mk",
    "date": "October 7, 2019, 12:38pm October 16, 2019, 8:31pm November 13, 2019, 8:31pm",
    "body": "Hi, I am trying to add hive table into Elasticsearch. Have added the jar in hive-site.xml file. CDH 5.11 Hadoop 2.6 Hive 1.1.0 elasticsearch-hadoop-7.3.2.jar Elasticsearch 7.3.2 I am able to successfully create table using below command CREATE EXTERNAL TABLE elkdummy ( name STRING, id INT) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.nodes' = '192.168.4.23', 'es.resource' = 'hive-elastic/log', 'es.query' = '?q=*'); But when I try to insert data using below command INSERT INTO TABLE elkdummy VALUES ('elastic',1); I get below error Caused by: java.lang.ClassNotFoundException: org.elasticsearch.hadoop.hive.EsHiveInputFormat at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136) ... 49 more I want to understand why the create query is getting executed but the insert query is not?",
    "website_area": "discuss"
  },
  {
    "id": "d4cc8d15-4667-4b8f-9e03-00f2b5d693dd",
    "url": "https://discuss.elastic.co/t/correct-settings-for-es-nodes-wan-only/58743",
    "title": "Correct settings for \"es.nodes.wan.only\"",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "darthapple",
    "date": "August 23, 2016, 8:59pm August 24, 2016, 3:51pm October 12, 2016, 10:41pm January 31, 2017, 4:29am May 10, 2017, 5:32pm June 29, 2017, 8:56pm July 6, 2017, 7:39am November 16, 2017, 10:35pm October 24, 2018, 12:14pm January 9, 2019, 5:05pm September 19, 2019, 10:03am September 25, 2019, 3:55am October 16, 2019, 8:29pm October 16, 2019, 8:29pm",
    "body": "Hello, I am trying to run a spark job to load data from emr to ES cluster hosted by elastic.co. [ cluster ID \"665e60\" ] Following is code snippet i am using to test this. import java.io.PrintStream import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.elasticsearch.spark._ val conf = new SparkConf() conf.set(\"spark.es.nodes\",\"ssssss.us-east-1.aws.found.io\") conf.set(\"spark.es.port\",\"9243\") conf.set(\"spark.es.nodes.discovery\",\"ture\") conf.set(\"spark.es.nodes.client.only\",\"false\") conf.set(\"spark.es.nodes.wan.only\",\"false\")`indent preformatted text by 4 spaces` conf.set(\"spark.es.net.http.auth.user\",\"sssss\") conf.set(\"spark.es.net.http.auth.pass\",\"lololol\") val sc = new SparkContext(conf) // print(conf.toDebugString) val numbers = Map(\"one\" -> 1, \"two\" -> 2, \"three\" -> 3) val airports = Map(\"arrival\" -> \"Otopeni\", \"SFO\" -> \"San Fran\") sc.makeRDD(Seq(numbers, airports)).saveToEs(\"spark/docs\") Which results in the following error. > org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' > at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:196) > at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:379) > at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:40) > at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:84) > at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:84) > at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) > at org.apache.spark.scheduler.Task.run(Task.scala:89) > at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) > at java.lang.Thread.run(Thread.java:745) Can someone please suggest what config am i missing here ?",
    "website_area": "discuss"
  },
  {
    "id": "1b84d90c-54eb-4bb7-a489-1041b95e1170",
    "url": "https://discuss.elastic.co/t/hive-real-time-or-near-real-time-sync-with-es/202103",
    "title": "Hive real time or near real time sync with ES",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "bharat1",
    "date": "October 3, 2019, 7:16am October 3, 2019, 3:23pm October 3, 2019, 3:33pm October 4, 2019, 2:29pm October 16, 2019, 8:24pm November 13, 2019, 8:24pm",
    "body": "Dear All, Have a scenario, I am successfully able to bring Hive table data to ES. At the same time when it comes to sync latest updated row data from same Hive table then there is a manual work required to get the updated table row data in Hive to bring it in ES under same index. So is there any settings or parameters that we need to add/modify that will constantly look/watch for changes happening in Hive table and sync those with ES without manual intervention? Because when there are several processing/algorithms run on Big Data it's hard to manually keep track of updated data in Hive DB/Tables Any pointers will be helpful",
    "website_area": "discuss"
  },
  {
    "id": "b315e12a-3b13-4dc7-8db9-38aa2780168d",
    "url": "https://discuss.elastic.co/t/eshadoopnonodesleftexception-all-nodes-failed-on-spark-savetoes/202161",
    "title": "EsHadoopNoNodesLeftException-all nodes failed On Spark.SaveToES",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Ramakrishnan_Venkata",
    "date": "October 3, 2019, 12:18pm October 16, 2019, 8:19pm November 13, 2019, 8:26pm",
    "body": "Hi, I Get \"EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed\" When i do a df = spark.sql(select * from a table)and do a df.saveToES(indexName+\"/docs\") I have 200 ORC files and with average of 145mb (raw data) = ~29GB of serialized and compressed ORC raw data. I see 200 tasks in SPARK UI during the above code. And the last task gets failed with the above exception. I infer from this ticket: Similar Issue That i need to reduce the bulk SIZE. MY QUESTION: How to determine the bulk size during dataframe.saveToEs() during runtime? Is there a formula based on No of executors, Cores, Memory etc..? How to reduce the bulk size? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "74f94e46-239a-4985-9001-4920fbcea6cb",
    "url": "https://discuss.elastic.co/t/apache-spark-to-query-elasticsearch-https-and-basic-authentication/197027",
    "title": "Apache Spark to query Elasticsearch (https and basic authentication)",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Darshan_Parab",
    "date": "August 28, 2019, 4:52am September 6, 2019, 4:12pm September 25, 2019, 11:57am October 23, 2019, 11:57am",
    "body": "Use case: Query secure Elasticsearch cluster (https and basic authentication enabled) using Apache Spark(pyspark and spark-submit) What I tried: start pyspark as follows: ./bin/pyspark --jars ./jars/elasticsearch-hadoop-7.2.0.jar --files /opt/ssl/jkeystore/elastic --driver-class-path /opt/ssl/jkeystore/elastic --conf \"spark.executor.extraJavaOptions=-Djavax.net.ssl.trustStore=elastic\" --conf \"spark.execurot.extraJavaOptions=-Djavax.net.ssl.trustStorePassword=xxxxxx\" Query Elasticsearch as follows: df = spark.read.format(\"org.elasticsearch.spark.sql\").option(\"es.nodes\",\"https://elasticsearch:9200\").option(\"es.resource\",\"index/_doc\").option(\"es.read.field.as.array.include\",\"tags\").option(\"es.net.http.auth.user\",\"user\").option(\"es.net.http.auth.pass\",\"password\").option(\"es.net.ssl\",\"true\").load() I'm getting error as below: Caused by: org.elasticsearch.hadoop.rest.EsHadoopTransportException: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target Apparently it looks like spark is unable to understand truststore settings. Elasticsearch Hadoop doesn't have any options to add certificates to keystore file using secure settings. How do I configure it correctly so that I can talk to Elasticsearch?",
    "website_area": "discuss"
  },
  {
    "id": "b429b743-79e0-46ca-ba8c-9c7074dfa6d4",
    "url": "https://discuss.elastic.co/t/exactly-once-guarantee-for-spark-structured-streaming/200746",
    "title": "Exactly-once guarantee for Spark Structured Streaming",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "danielyahn",
    "date": "September 23, 2019, 7:55pm September 23, 2019, 8:26pm September 23, 2019, 9:07pm October 21, 2019, 9:07pm",
    "body": "The following blog that @james.baiera wrote says, Users of ES-Hadoop can specify one of their fields to be used as the documents ID, and Elasticsearch manages ID based writes consistently. We cant know ahead of time what your documents IDs are, so its up to each user to ensure their streaming data contains an ID of some sort. Are there some best practices on how to generate the ID for time series data and to optimize for ingest? I'm starting by introducing UUID v4, but seeing that there are better implementation. Such as one and two. I already understand that using auto-generated ID will skip duplicate check, thus saving lookup cost. Here I'm looking for an ID generation scheme for exactly-once guarantee and ingest performance.",
    "website_area": "discuss"
  },
  {
    "id": "d5b20829-1e8a-479f-b04a-09dcd4e43648",
    "url": "https://discuss.elastic.co/t/unable-to-connect-elasticsearch-with-spark/199543",
    "title": "Unable to connect Elasticsearch with Spark",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Manel_Serrano",
    "date": "September 15, 2019, 4:26pm October 13, 2019, 4:26pm",
    "body": "Hi all, I get this error when I try to connect Elasticsearch (running in a cluster) with Spark (running locally): 19/09/14 20:17:32 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection timed out: connect 19/09/14 20:17:32 INFO HttpMethodDirector: Retrying request 19/09/14 20:17:53 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection timed out: connect 19/09/14 20:17:53 INFO HttpMethodDirector: Retrying request 19/09/14 20:18:14 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection timed out: connect 19/09/14 20:18:14 INFO HttpMethodDirector: Retrying request 19/09/14 20:18:35 ERROR NetworkClient: Node [xx.xx.xx.10:7474] failed (Connection timed out: connect); selected next node [xx.xx.xx.11:7474] 19/09/14 20:18:56 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection timed out: connect 19/09/14 20:18:56 INFO HttpMethodDirector: Retrying request 19/09/14 20:21:23 ERROR NetworkClient: Node [xx.xx.xx.12:7474] failed (Connection timed out: connect); no other nodes left - aborting... 19/09/14 20:21:23 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1) org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[xx.xx.xx.10:7474, xx.xx.xx.11:7474, xx.xx.xx.12:7474]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:149) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:461) at org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:469) at org.elasticsearch.hadoop.rest.RestClient.exists(RestClient.java:565) at org.elasticsearch.hadoop.rest.RestClient.indexExists(RestClient.java:560) at org.elasticsearch.hadoop.rest.RestClient.touch(RestClient.java:571) at org.elasticsearch.hadoop.rest.RestRepository.touch(RestRepository.java:418) at org.elasticsearch.hadoop.rest.RestService.initSingleIndex(RestService.java:609) at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:597) at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58) at org.elasticsearch.spark.rdd.EsSpark$anonfun$doSaveToEs$1.apply(EsSpark.scala:107) at org.elasticsearch.spark.rdd.EsSpark$anonfun$doSaveToEs$1.apply(EsSpark.scala:107) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source) 19/09/14 20:21:23 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[xx.xx.xx.10:7474, xx.xx.xx.11:7474, xx.xx.xx.12:7474]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:149) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:461) at org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:469) at org.elasticsearch.hadoop.rest.RestClient.exists(RestClient.java:565) at org.elasticsearch.hadoop.rest.RestClient.indexExists(RestClient.java:560) at org.elasticsearch.hadoop.rest.RestClient.touch(RestClient.java:571) at org.elasticsearch.hadoop.rest.RestRepository.touch(RestRepository.java:418) at org.elasticsearch.hadoop.rest.RestService.initSingleIndex(RestService.java:609) at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:597) at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58) at org.elasticsearch.spark.rdd.EsSpark$anonfun$doSaveToEs$1.apply(EsSpark.scala:107) at org.elasticsearch.spark.rdd.EsSpark$anonfun$doSaveToEs$1.apply(EsSpark.scala:107) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source) The code looks like: SparkConf conf = new SparkConf() .setAppName(\"ES-Spark\") .(\"local[*]\") .set(\"es.nodes\", \"cluster-xxxx.xx.xx.xx\") .set(\"es.port\", \"50467\") .set(\"es.resource\", \"recommendations_2/recommendation\") .set(\"es.resource.read\", \"products-20190912/product\") .set(\"es.resource.write\", \"recommendations_2/recommendation\") ; JavaSparkContext ctx = new JavaSparkContext(conf); JavaStreamingContext jsc = new JavaStreamingContext(ctx, new Duration(10000)); JavaInputDStream<ConsumerRecord<String, String>> kafkaStream = Utils.getKafkaStream(jsc); JavaDStream<String> allInfo = kafkaStream.map(f -> f.value()); JavaEsSparkStreaming.saveToEs(allInfo, \"recommendations_2/recommendation\"); jsc.start(); jsc.awaitTermination(); It seems it reaches the cluster but it cannot connect back... The elasticsearch-hadoop version used is: <dependency> <groupId>org.elasticsearch</groupId> <artifactId>elasticsearch-spark-20_2.10</artifactId> <version>5.6.2</version> </dependency> When I try to connect via the Transport Client everything is alright and I am able to make it. I already checked the elasticsearch.yml file and the http.port property is set correctly to the right port.",
    "website_area": "discuss"
  },
  {
    "id": "b305e09d-207d-44c8-9d35-ebd1a5991c50",
    "url": "https://discuss.elastic.co/t/difference-between-index-refresh-interval-vs-es-batch-write-refresh/197950",
    "title": "Difference between `index.refresh_interval` vs `es.batch.write.refresh`",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "danielyahn",
    "date": "September 4, 2019, 1:59pm September 6, 2019, 5:29pm September 6, 2019, 5:29pm September 12, 2019, 6:55pm September 12, 2019, 9:30pm October 10, 2019, 9:31pm",
    "body": "I see two similar settings one in ES-Hadoop and the other in index configuration. Is it recommended to set index.refresh_interval to -1, when es.batch.write.refresh=true? When running Spark job, how often does refresh API get called? Is it per Spark job, stage or task? Or something smaller? I see the following code. Is my understanding correct that Structured Streaming uses EsRDDWriter and the scope of writer (which calls refresh API when closing) is a Spark task? Is the refresh API call per index (or per shard)? When multiple tasks finish around the same time, is it possible that the code above is calling refresh API multiple times at the same time? Could it cause performance issue on ES side? Is there a known performance issue with ES-Hadoop's refresh call when there's a large parallelism? I'm running into the following problem. I'm seeing \"Read\" timeouts with Flush API calls when ingesting to ES using SPARK with the default setting es.batch.write.refresh=true And it doesn't seem to relate to tuning Bulk request size because when I set es.batch.write.refresh=false and index.refresh_interval='30s' I don't get the timeout. What's the best practice for configuring refresh when writing a Spark ingest job? 19/09/03 20:37:50 TRACE NetworkClient: Caught exception while performing request [<redacted>:9200][/_refresh] - falling back to the next node in line... java.net.SocketTimeoutException: Read timed out at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:170) at java.net.SocketInputStream.read(SocketInputStream.java:141) at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) at sun.security.ssl.InputRecord.read(InputRecord.java:503) at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) at java.io.BufferedInputStream.read(BufferedInputStream.java:265) at org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:78) at org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:106) at org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1116) at org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1973) at org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1735) at org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1098) at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:398) at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171) at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397) at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323) at org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.execute(CommonsHttpTransport.java:489) at org.elasticsearch.hadoop.rest.pooling.TransportPool$LeasedTransport.execute(TransportPool.java:235) at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:115) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:398) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:362) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:366) at org.elasticsearch.hadoop.rest.RestClient.refresh(RestClient.java:267) at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:550) at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:219) at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:121) at org.elasticsearch.spark.rdd.EsRDDWriter$$anonfun$write$1.apply(EsRDDWriter.scala:60) at org.elasticsearch.spark.rdd.EsRDDWriter$$anonfun$write$1.apply(EsRDDWriter.scala:60) at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:118) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:118) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:131) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:129) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:129) at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:117) at org.apache.spark.scheduler.Task.run(Task.scala:125) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 19/09/03 20:37:50 ERROR NetworkClient: Node [<redacted>:9200] failed (Read timed out); selected next node [<redacted>:9200]",
    "website_area": "discuss"
  },
  {
    "id": "9f5ef696-5d76-4c89-bd10-609b58b450fd",
    "url": "https://discuss.elastic.co/t/bulk-request-monitoring/198464",
    "title": "Bulk Request Monitoring",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "danielyahn",
    "date": "September 6, 2019, 6:07pm September 12, 2019, 6:56pm September 12, 2019, 9:23pm October 10, 2019, 9:23pm",
    "body": "According to the doc, it's important to monitor bulk requests to tune ES-Hadoop ingest. I think it will be beneficial to set up monitoring around the requests for production operation as well. I've tried few different routes to gather the metrics: Security audit log and filter successful authentication for bulk request. From Kibana's monitoring plugin, and go to index's advanced view, and use Request Rate and Request Time visualization. Enable debugging on org.elasticsearch.hadoop.rest.bulk It looks like option 3 has the richest information, but it's still not enough to answer questions like: What's the average bulk request response time? How many bulk request is es-hadoop job sending per second? How many rejection is happening? As a workaround, I implemented a simple log parser that can correlate request created event and request response event. Is there a easier way to gather and consume these metrics? Through Kibana's monitoring plugin? Expose these metrics as custom Spark metrics?",
    "website_area": "discuss"
  },
  {
    "id": "83d6a278-4af6-4cc8-9c5a-bdc928f594e4",
    "url": "https://discuss.elastic.co/t/elasticsearch-support-for-spark-2-4-2-with-scala-2-12/197810",
    "title": "Elasticsearch support for spark 2.4.2 with scala 2.12",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Gokul_Raj",
    "date": "September 3, 2019, 10:57am September 17, 2019, 3:04am October 4, 2019, 4:27pm",
    "body": "I'm not able to find any ES 6.7.1 supporting jar for spark 2.4.2 with scala 2.12 In maven repo only scala 2.11 and 2.10 is supported for the jar <dependency> <groupId>org.elasticsearch</groupId> <artifactId>elasticsearch-spark-20_2.11</artifactId> <version>6.7.1</version> </dependency> For my application we are using spark 2.4.2 which supports only scala 2.12 version. Following is the error shown when I try to run with \"elasticsearch-spark-20_2.11\" jar image.png1366366 17.4 KB Is there any alternative to resolve this situation as downgrade is not an option? o",
    "website_area": "discuss"
  },
  {
    "id": "3f4fe972-b434-45d8-bbc6-4b2ac8b495bf",
    "url": "https://discuss.elastic.co/t/es-hadoop-with-tls/197417",
    "title": "Es-hadoop with TLS",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "mstarensier",
    "date": "August 29, 2019, 8:20pm September 6, 2019, 4:22pm October 4, 2019, 4:22pm",
    "body": "Has anybody been able to use es-hadoop connector with an Es cluster where TLS is enabled? If so, what configuration params did you use, and what kind of certificate? I'm trying to connect to a 7.e ES cluster with xpack security enabled and TLS enabled. I'm using a CA and instance cert generated by the es certutil. verification mode = certificate, so all es nodes are using the same cert. The cert is in PKCS12 format. The es nodes all communicate with each other. But I haven't beeen able to get es-hadoop to work using tls. In spark conf, I enabled es ssl, specified keystore and truststore, using the same pkcs12 keystore that I'm using on the es nodes. I also specified PKCS12 as the keystore type. But es rejects requests from es-hadoop. Error [o.e.t.TcpTransport ] [test-es-data-node1] exception caught on transport layer [Netty4TcpChannel{localAddress=0.0.0.0/0.0.0 .0:9300, remoteAddress=/10.0.1.81:44991}], closing connection io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: Received fatal alert: internal_error Is the certificate I'm using on the es nodes not valid for use by es-hadoop? Is there some alternate configuration required? Any other avenues to try?",
    "website_area": "discuss"
  },
  {
    "id": "dfffb765-ddca-4811-b004-1f1cd8fa2ec6",
    "url": "https://discuss.elastic.co/t/trying-to-insert-data-into-the-external-table/197368",
    "title": "Trying to insert data into the external table",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "varunakuraju",
    "date": "August 29, 2019, 6:56pm September 6, 2019, 4:18pm October 4, 2019, 4:18pm",
    "body": "Hi, Using hadoop and elasticsearch on cloud created external table but getting some errors while inserting the data into the external table set hive.execution.engine=mr hive> insert overwrite table vgsale5es select * from vgsale5; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. Query ID = saivarunakuraju_20190829180559_6bd94dfb-f0fa-4c58-a4f6-963a3e10deb3 Total jobs = 1 Launching Job 1 out of 1 Number of reduce tasks is set to 0 since there's no reduce operator Starting Job = job_1567078294077_0011, Tracking URL = http://hadoopcluster-m:8088/proxy/application_1567078294077_0011/ Kill Command = /usr/lib/hadoop/bin/hadoop job -kill job_1567078294077_0011 Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0 2019-08-29 18:06:13,379 Stage-3 map = 0%, reduce = 0% 2019-08-29 18:07:13,827 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 2.02 sec 2019-08-29 18:08:14,062 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 2.02 sec 2019-08-29 18:09:14,304 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.57 sec 2019-08-29 18:10:14,527 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.57 sec 2019-08-29 18:11:14,669 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.97 sec 2019-08-29 18:12:14,784 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.97 sec 2019-08-29 18:13:14,840 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.97 sec 2019-08-29 18:14:14,973 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.97 sec 2019-08-29 18:14:48,571 Stage-3 map = 100%, reduce = 0% MapReduce Total cumulative CPU time: 1 seconds 970 msec Ended Job = job_1567078294077_0011 with errors Error during job, obtaining debugging information... Examining task ID: task_1567078294077_0011_m_000000 (and more) from job job_1567078294077_0011 Task with the most failures(4): ----- Task ID: task_1567078294077_0011_m_000000 URL: http://hadoopcluster-m:8088/taskdetails.jsp?jobid=job_1567078294077_0011&tipid=task_1567078294077_0011_m_000000 ----- Diagnostic Messages for this Task: Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"rank\":\"rank\",\"name\":\"name\",\"platform\":\"platform\",\"year\":\"year\",\"genre\":\"genre\",\"publisher\":\"publisher\",\"na_sales\":\"na_sales\",\"eu_sales\":\"eu_ales\",\"jp_sales\":\"jp_sales\",\"other_sales\":\"other_sales\",\"global_sales\":\"global_sales\"} at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:169) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:177) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:171) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"rank\":\"rank\",\"name\":\"name\",\"platform\":\"platform\",\"year\":\"year\",\"genre\":\"genre\",\"publisher\":\"publisher\",\"na_sales\":\"na_sales\",\"eu_sales\":\"eu_ales\",\"jp_sales\":\"jp_sales\",\"other_sales\":\"other_sales\",\"global_sales\":\"global_sales\"} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:160) ... 8 more Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.168.0.3:9200]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:152) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:424) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:388) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:392) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:168) at org.elasticsearch.hadoop.rest.request.GetAliasesRequestBuilder.execute(GetAliasesRequestBuilder.java:68) at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:622) at org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.init(EsOutputFormat.java:175) at org.elasticsearch.hadoop.hive.EsHiveOutputFormat$EsHiveRecordWriter.write(EsHiveOutputFormat.java:59) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:762) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897) at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:547) ... 9 more FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask MapReduce Jobs Launched: Stage-Stage-3: Map: 1 Cumulative CPU: 1.97 sec HDFS Read: 0 HDFS Write: 0 FAIL Total MapReduce CPU Time Spent: 1 seconds 970 msec thanks",
    "website_area": "discuss"
  },
  {
    "id": "3383845b-afb3-482b-a3bd-4013d6b61701",
    "url": "https://discuss.elastic.co/t/unable-to-use-es-hadoop-with-tls-enabled-on-my-es-cluster/197225",
    "title": "Unable to use es-hadoop with TLS enabled on my ES cluster",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "mstarensier",
    "date": "August 28, 2019, 8:55pm September 6, 2019, 4:16pm October 4, 2019, 4:16pm",
    "body": "I'm having trouble configuring TLS for es-hadoop. Running 7.2 Elasticsearch in a 3 node cluster (1 data, 1 master, 1 coordinating which hosts kibana). Running Spark on an EMR cluster in the same vpn. Before enabling xpack security, was able to connect to ES from Spark using both es-hadoop and Rest Client. Configured TLS with CA and single signing cert, generated by certutil, used in all es and spark nodes. The nodes all ES nodes communicate with each other correctly. But in spark, the es-hadoop connector can no longer connect to the ES cluster. (The rest client running in the spark job can connect using basic auth). For experimentation, we are running this job from a zeppelin notebook spark interpreter. I am using the same certificate in the spark cluster nodes which I am using in the ES cluster nodes. spark es config properties are: es.net.http.auth.pass : xxxxxx es.net.http.auth.user : xxxxxx es.net.ssl : true es.net.ssl.keystore.location : file:///home/hadoop/ria/certs/test-es-node-cert.p12 es.net.ssl.trustore.location : file:///home/hadoop/ria/certs/test-es-node-cert.p12 es.nodes : 10.0.1.179 es.port : 9300 Rest client works fine, connector fails. In the spark logs on failure, message begins: %text org.elasticsearch.hadoop.EsHadoopIllegalArgumentExc eption: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:340) at org.elasticsearch.hadoop.rest.RestService.findPartitions(RestService.java:220) at in es logs, stack trace is exception caught on transport layer [Netty4TcpChannel{localAddress=0.0.0.0/0.0.0.0:9300, remoteAddress=/10.0.1.81:51709}], closing connection io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: Received fatal alert: internal_error at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:472) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1408) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:682) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:582) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:536) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:906) [netty-common-4.1.35.Final.jar:4.1.35.Final] at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.35.Final.jar:4.1.35.Final] at java.lang.Thread.run(Thread.java:835) [?:?] Caused by: javax.net.ssl.SSLException: Received fatal alert: internal_error at sun.security.ssl.Alert.createSSLException(Alert.java:133) ~[?:?] at sun.security.ssl.Alert.createSSLException(Alert.java:117) ~[?:?] at sun.security.ssl.TransportContext.fatal(TransportContext.java:307) ~[?:?] at sun.security.ssl.Alert$AlertConsumer.consume(Alert.java:285) ~[?:?] at sun.security.ssl.TransportContext.dispatch(TransportContext.java:180) ~[?:?] at sun.security.ssl.SSLTransport.decode(SSLTransport.java:164) ~[?:?] at sun.security.ssl.SSLEngineImpl.decode(SSLEngineImpl.java:681) ~[?:?] at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:636) ~[?:?] at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:454) ~[?:?] at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:433) ~[?:?] at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:634) ~[?:?] at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:295) ~[netty-handler-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1332) ~[netty-handler-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1227) ~[netty-handler-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1274) ~[netty-handler-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:441) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] ... 16 more [2019-08-28T14:12:16,592][WARN ][o.e.t.TcpTransport ] [test-es-data-node1] exception caught on transport layer [Netty4TcpChannel{localAddress=0.0.0.0/0.0.0.0:9300, remoteAddress=/10.0.1.81:39683}], closing connection io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: Received fatal alert: internal_error at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:472) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at 4 elided.... Are there some other additional or different configurations required? Do I need a different certificate to act as the es-hadoop keystore? Any ideas would be helpful.",
    "website_area": "discuss"
  },
  {
    "id": "53093958-5202-43b4-9f58-9322ca98ff29",
    "url": "https://discuss.elastic.co/t/can-i-search-data-on-hadoop-using-elasticsearch/197780",
    "title": "Can i search data on hadoop using elasticsearch?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Vinit_Pillai",
    "date": "September 3, 2019, 8:03am September 3, 2019, 8:15am September 3, 2019, 9:29am October 1, 2019, 9:29am",
    "body": "I am a noob in the es-Hadoop category. I have tried to read all the documents but am still confused regarding how es-hadoop works and do I need it. The question is simple. Can I search for data on Hadoop using elasticsearch? Can I create weekly indexes in elasticsearch which saves in Hadoop? Will I be able to search through these indexes using elasticsearch?",
    "website_area": "discuss"
  },
  {
    "id": "b9822617-c10b-4c18-8133-ecdf87def120",
    "url": "https://discuss.elastic.co/t/how-to-specify-number-of-shards-when-putting-data-into-es-using-elasticsearch-hadoop-sdk/197001",
    "title": "How to specify number_of_shards when putting data into ES using elasticsearch-hadoop SDK",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "yuecong",
    "date": "August 27, 2019, 8:40pm August 27, 2019, 9:06pm August 27, 2019, 11:09pm September 24, 2019, 11:09pm",
    "body": "From this page for the elasticsearch hadoop SDK, I can not find how to configure number_of_shards when creating one index automatically like .option(\"es.resource.write\", \"log-{dateHour}\") In this doc, it menthioned it can use index setting API, but number_of_shards can only be set during index creation. https://www.elastic.co/guide/en/elasticsearch/reference/7.3/indices-create-index.html#create-index-settings I tried .option(\"es.index.number_of_shards\", 10) .option(\"es.index.refresh_interval\", 10) but it does not looks like work, becuase when I query the index setting, the number_of_shards for the one created is still 1. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "751ef688-d761-4cb1-a78e-0564be210d13",
    "url": "https://discuss.elastic.co/t/getting-error-while-processing-nested-document-org-elasticsearch-hadoop-mr-writablearraywritable-cannot-be-cast-to-org-apache-hadoop-io-mapwritable/196359",
    "title": "Getting error while processing Nested Document :org.elasticsearch.hadoop.mr.WritableArrayWritable cannot be cast to org.apache.hadoop.io.MapWritable",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Srinivas2",
    "date": "August 23, 2019, 6:37pm August 22, 2019, 4:00pm August 22, 2019, 4:03pm August 22, 2019, 4:11pm August 23, 2019, 5:04pm August 23, 2019, 6:45pm August 27, 2019, 1:20pm September 24, 2019, 1:20pm",
    "body": "Hi Team, I am creating an external table on a nested JSON document looks like below. { \"rootcol1\": \"val1\", \"rootcol2\": \"vla2\", \"rootcol3\": \"val2\", \"rootcol4\": \"val3\", \"rootcol5\": [ { \"childcol1\": \"vla5\", \"childcol2\":[ {\"innercol1\":\"innervalue1\"}, {\"innercol2\":\"innervalue2}\" ] } ] } Below is the create table statement . create external table elasticserach_pool_59(rootcol1 STRING,rootcol2 STRING,rootcol3 STRING,childcol1 STRING) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES( 'es.nodes'='Endpoint Name', 'es.port'='port', 'es.resource'='indexname' , 'es.nodes.wan.only' = 'true', 'es.nodes.discover'='true', 'es.mapping.names'='rootcol1 :rootcol1 ,rootcol2 :rootcol2 ,rootcol3:rootcol3,childcol1:rootcol5.childcol1', 'es.read.field.as.array.exclude'='rootcol5.*' ); I am getting the error when I execute select statement. :org.elasticsearch.hadoop.mr.WritableArrayWritable cannot be cast to org.apache.hadoop.io.MapWritable I tried with exclude option as well , but getting same exception. If I am not including the child columns ,I able to see the Data without any issue. Please assist me on this issue. Thanks in advance.",
    "website_area": "discuss"
  },
  {
    "id": "594bf0a0-86b1-4d62-ab6a-6059cae156d9",
    "url": "https://discuss.elastic.co/t/connecting-hadoop-and-elasticsearch/196170",
    "title": "Connecting hadoop and elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "varunakuraju",
    "date": "August 21, 2019, 5:32pm August 23, 2019, 4:59pm August 26, 2019, 8:21pm September 20, 2019, 7:50pm",
    "body": "Hi all, I am new to Elasticsearch and kibana I want establish two-way connection between hdfs and elasticsearch. I used Hive external tables but its not working. I am not finding a way , steps i have done installed elasticsearch and kibana for hadoop, i created hadoop cluster in google cloud and then i connected to terminal through ssh key. I download es-hadoop(https://www.elastic.co/downloads/hadoop). loaded data into hadoop and then created tables in hive. In hadoop, added elasticsearch-hadoop-hive-7.3.0.jar to class path and trying to create the external tables to create indexes in elasticsearch but couldn't do it. Is there any command to verify the connectivity between hadoop and elasticsearch. Can anyone please suggest me which method is better and how to implement. Thank you",
    "website_area": "discuss"
  },
  {
    "id": "e41f63ac-6dff-421a-88c5-e1a119b01846",
    "url": "https://discuss.elastic.co/t/eshadoopillegalstateexception-when-using-ranges/195733",
    "title": "EsHadoopIllegalStateException when using ranges",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "markoutso",
    "date": "August 19, 2019, 12:57pm August 23, 2019, 4:38pm August 26, 2019, 9:29am September 23, 2019, 6:54am",
    "body": "Hello, I have the following index and data: PUT range_index { \"settings\": { \"number_of_shards\": 2 }, \"mappings\": { \"_doc\": { \"properties\": { \"expected_attendees\": { \"type\": \"integer_range\" }, \"time_frame\": { \"type\": \"date_range\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" } } } } } PUT range_index/_doc/1?refresh { \"expected_attendees\" : { \"gte\" : 10, \"lte\" : 20 }, \"time_frame\" : { \"gte\" : \"2015-10-31 12:00:00\", \"lte\" : \"2015-11-01\" } } and I wrote a simple spark application to display the data: package com.es_range import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.elasticsearch.hadoop.cfg.ConfigurationOptions object Main { def main(args: Array[String]): Unit = { val sparkConf = new SparkConf() .setAppName(\"test\") .set(ConfigurationOptions.ES_NODES, \"localhost\") .set(ConfigurationOptions.ES_PORT, \"9200\") .set(ConfigurationOptions.ES_INDEX_READ_MISSING_AS_EMPTY, \"true\") sparkConf.setMaster(\"local[*]\") sparkConf.set(\"spark.driver.host\", \"localhost\") val spark = SparkSession.builder .appName(\"test\") .config(sparkConf) .enableHiveSupport() .getOrCreate() val esDataFrame = spark.sqlContext.read .format(\"org.elasticsearch.spark.sql\") .load(\"range_index/_doc\") esDataFrame.show(10) } } When I try to run it I get the following error: org.elasticsearch.hadoop.rest.EsHadoopParsingException: org.elasticsearch.hadoop.EsHadoopIllegalStateException: Field '_' not found; typically this occurs with arrays which are not mapped as single value I don't understand what is going on there and why the data cannot be loaded into the dataframe. Could someone shed some light? Is this maybe a bug?",
    "website_area": "discuss"
  },
  {
    "id": "bf0beee9-ee66-4674-bef0-2f38934276d3",
    "url": "https://discuss.elastic.co/t/data-ingestion-into-elasticsearch-from-spark-structured-streaming/196075",
    "title": "Data ingestion into ElasticSearch from Spark Structured Streaming",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "SandeepReddy",
    "date": "August 21, 2019, 9:43am August 23, 2019, 4:39pm August 24, 2019, 4:36pm September 21, 2019, 4:36pm",
    "body": "Hi, I have created dataset/dataframe using the watermark and window function and writing the output to ElasticSearch is not working. However dataset/dataframe created without watermark and window inserts data into ElasticSearch. Please find the code snippet. val df = dsLog1.withWatermark(\"time\",\"3 minutes\").groupBy(window(col(\"time\"),\"3 minutes\",\"1 minute\")) .agg(count(col(\"column_name\"))) df.writeStream .outputMode(\"append\") .format(\"org.elasticsearch.spark.sql\") .option(\"es.nodes\", \"localhost\") .option(\"es.port\", \"9200\") //.option(\"checkpointLocation\", \"/tmp\") .option(\"es.resource\",\"test4/_doc\") .start() Thanks in Advance for the help.",
    "website_area": "discuss"
  },
  {
    "id": "ddf62062-9fce-4c7b-87c3-46a78179900e",
    "url": "https://discuss.elastic.co/t/verify-connection-between-hdfs-and-elasticsearch/196351",
    "title": "Verify connection between hdfs and elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "varunakuraju",
    "date": "August 22, 2019, 2:45pm August 23, 2019, 5:01pm September 20, 2019, 5:05pm",
    "body": "Hi, How to verify the connectivity between the hdfs and elasticsearch. Thanks",
    "website_area": "discuss"
  },
  {
    "id": "4d410715-bbb7-4413-babf-36606a732ad9",
    "url": "https://discuss.elastic.co/t/added-elasticsearch-and-hadoop-and-while-creating-external-table-getting-error/195938",
    "title": "Added elasticsearch and hadoop and while creating external table getting error",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "varunakuraju",
    "date": "August 20, 2019, 2:57pm August 23, 2019, 4:34pm September 20, 2019, 4:34pm",
    "body": "created table vgsale in hive after added es-hadoop connector successful and trying to create external table Added [file:///home/saivarunakuraju/elasticsearch-hadoop-hive-7.3.0.jar] to class path Added resources: [file:///home/saivarunakuraju/elasticsearch-hadoop-hive-7.3.0.jar] hive> CREATE EXTERNAL TABLE vgsales_es( > rank int, > Name string, > Platform string, > year int, > Genre string, > Publisher string, > nasales int, > eusales int, > jpsales int, > othersales int, > Globalsales int > ) > stored by 'org.elasticsearch.hadoop.hive.ESStorageHandler' > TBLPROPERTIES ('es.resource' = vgsale/vgsals, > 'es.nodes'='localhost'); MismatchedTokenException(24!=352) at org.antlr.runtime.BaseRecognizer.recoverFromMismatchedToken(BaseRecognizer.java:617) at org.antlr.runtime.BaseRecognizer.match(BaseRecognizer.java:115) at org.apache.hadoop.hive.ql.parse.HiveParser.keyValueProperty(HiveParser.java:28199) at org.apache.hadoop.hive.ql.parse.HiveParser.tablePropertiesList(HiveParser.java:27995) at org.apache.hadoop.hive.ql.parse.HiveParser.tableProperties(HiveParser.java:27867) at org.apache.hadoop.hive.ql.parse.HiveParser.tablePropertiesPrefixed(HiveParser.java:27804) at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:6338) at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:3808) at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2382) at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1333) at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:208) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:468) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:244) at org.apache.hadoop.util.RunJar.main(RunJar.java:158) FAILED: ParseException line 15:16 cannot recognize input near 'es' '.' 'resource' in table properties list hive>",
    "website_area": "discuss"
  },
  {
    "id": "e95d1736-4d55-4c57-b237-051ff90f9ada",
    "url": "https://discuss.elastic.co/t/hive-not-working-with-elasticsearch/192894",
    "title": "Hive not working with Elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "bharat1",
    "date": "July 30, 2019, 12:27pm September 6, 2019, 12:18pm August 20, 2019, 4:56am August 20, 2019, 8:44pm August 22, 2019, 8:40pm September 19, 2019, 8:40pm",
    "body": "I am trying to bring Hive table data to Elasticsearch but it's failing to get this through, any help will be appreciated. The error I get is \"Unable to find class: org.elasticsearch.hadoop.hive.EsHiveInputFormat\" and \"FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask\" The hive engine is set to mr Logging initialized using configuration in file:/etc/hive/2.6.0.3-8/0/hive-log4j.properties OK Time taken: 1.118 seconds Query ID = hive_20190725220921_fd962234-e1a2-41af-89f3-b7e4cc27e86d Total jobs = 1 Launching Job 1 out of 1 Number of reduce tasks is set to 0 since there's no reduce operator Starting Job = job_1563949354091_0016, Tracking URL = http://hadoop-node.test.com:8088/proxy/application_1563949354091_0016/ Kill Command = /usr/hdp/2.6.0.3-8/hadoop/bin/hadoop job -kill job_1563949354091_0016 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0 2019-07-25 22:09:28,184 Stage-1 map = 0%, reduce = 0% 2019-07-25 22:09:41,582 Stage-1 map = 100%, reduce = 0% Ended Job = job_1563949354091_0016 with errors Error during job, obtaining debugging information... Examining task ID: task_1563949354091_0016_m_000000 (and more) from job job_1563949354091_0016 Task with the most failures(4): ----- Task ID: task_1563949354091_0016_m_000000 URL: http://hadoop-node.test.com:8088/taskdetails.jsp?jobid=job_1563949354091_0016&tipid=task_1563949354091_0016_m_000000 ----- Diagnostic Messages for this Task: Error: java.lang.RuntimeException: Failed to load plan: hdfs://hadoop-node.test.com:8020/tmp/hive/hive/d6d4079b-4813-4e2f-97be-5ea3acea7efa/hive_2019-07-25_22-09-21_665_114610607148402302-1/-mr-10002/268762c6-18a3-467a-936e-7cab06dd1f1c/map.xml: org.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: org.elasticsearch.hadoop.hive.EsHiveInputFormat Serialization trace: inputFileFormatClass (org.apache.hadoop.hive.ql.plan.TableDesc) tableInfo (org.apache.hadoop.hive.ql.plan.FileSinkDesc) conf (org.apache.hadoop.hive.ql.exec.FileSinkOperator) childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator) childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator) aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork) removing log lines due to character limitations . . at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164) Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: org.elasticsearch.hadoop.hive.EsHiveInputFormat Serialization trace: inputFileFormatClass (org.apache.hadoop.hive.ql.plan.TableDesc) tableInfo (org.apache.hadoop.hive.ql.plan.FileSinkDesc) conf (org.apache.hadoop.hive.ql.exec.FileSinkOperator) childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator) childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator) aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:238) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:226) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:745) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672) at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:1182) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:1069) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:1083) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:439) ... 13 more Caused by: java.lang.ClassNotFoundException: org.elasticsearch.hadoop.hive.EsHiveInputFormat at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136) ... 49 more Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 Container exited with a non-zero exit code 143 FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask MapReduce Jobs Launched: Stage-Stage-1: Map: 1 HDFS Read: 0 HDFS Write: 0 FAIL Total MapReduce CPU Time Spent: 0 msec",
    "website_area": "discuss"
  },
  {
    "id": "1e68f8df-37f7-4cbc-960d-7bf9babdb526",
    "url": "https://discuss.elastic.co/t/date-format-issue-when-passing-data-from-spark-to-elasticsearch/195704",
    "title": "Date format issue when passing data from spark to ElasticSearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Yoav_Ben_Moha",
    "date": "August 20, 2019, 7:06am August 19, 2019, 9:36am August 19, 2019, 7:30pm August 20, 2019, 6:05pm September 17, 2019, 6:04pm",
    "body": "I have successfuly uploaded data from spark into elasticsearch 7. The date format i have in the csv that i am reading are: yyyy-MM-dd+HH:mm:ss.SSS val cdr=spark.read.option(\"delimiter\", \";\") .option(\"timestampFormat\",\"yyyy-MM-dd+HH:mm:ss.SSS\") .schema(customSchema) .csv(\"/data/staging/abc\") Index created in elasticsearch didnt recocnized those values as date. It was automaically maped by ES as: \"ING_CALL_ANSWER_TIME\": { \"type\": \"long\" Does the above format is not a valid date in ES ? Thanks",
    "website_area": "discuss"
  },
  {
    "id": "b88d8d76-b95c-4dcd-89e4-d423a9b4588f",
    "url": "https://discuss.elastic.co/t/too-many-slices-lead-to-bloat-of-thread-pool-queue-and-degradation-of-performance/189833",
    "title": "Too many slices lead to bloat of thread pool queue and degradation of performance",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "lonlylocly",
    "date": "July 10, 2019, 5:03pm July 23, 2019, 11:22am August 14, 2019, 7:02pm August 16, 2019, 1:51pm September 13, 2019, 1:51pm",
    "body": "Hello dear sirs, We are using .esJsonRDD() to copy an index from an Elasticsearch cluster to Spark cluster, and lately we started to observe that this copy is taking 10-14 hours instead of 2-3. Here's what is going on. Cluster and index setup We have an Elasticsearch 6.8 cluster on AWS EC2 with 3 machines, 2 cores/14 Gb RAM each (i3.large). Elasticsearch is configured to use 7Gb of memory. The index that we copy contains 54 million documents, which correspond to 8 shards ~20 Gb each. Performance metrics CPU of one of the nodes is through the roof: 2019-07-10_18-00-58.png1474317 69.2 KB So is the Load Average. Also we notice that thread pool queue is at size 60 and stays like this: 2019-07-10_18-05-26.png735321 18.5 KB And search query latency is also very big on that instance: 2019-07-10_18-06-06.png747287 20.5 KB Logs We enabled logging of slow queries, and here's what we saw: [2019-07-10T14:44:50,844][WARN ][index.search.slowlog.query] [AT1mr_C] [index-XXX][1] took[1.3s], took_millis[1327], total_hits[100044], types[person], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], source[{\"size\":50,\"query\":{\"match_all\":{\"boost\":1.0}},\"sort\":[{\"_doc\":{\"order\":\"asc\"}}],\"slice\":{\"field\":\"_id\",\"id\":63,\"max\":68}}], id[], [2019-07-10T14:44:51,909][WARN ][index.search.slowlog.query] [AT1mr_C] [index-XXX][1] took[1.5s], took_millis[1570], total_hits[100006], types[person], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], source[{\"size\":50,\"query\":{\"match_all\":{\"boost\":1.0}},\"sort\":[{\"_doc\":{\"order\":\"asc\"}}],\"slice\":{\"field\":\"_id\",\"id\":32,\"max\":68}}], id[], [2019-07-10T14:44:52,468][WARN ][index.search.slowlog.query] [AT1mr_C] [index-XXX][1] took[1.5s], took_millis[1539], total_hits[100337], types[person], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], source[{\"size\":50,\"query\":{\"match_all\":{\"boost\":1.0}},\"sort\":[{\"_doc\":{\"order\":\"asc\"}}],\"slice\":{\"field\":\"_id\",\"id\":59,\"max\":68}}], id[], [2019-07-10T14:44:52,526][WARN ][index.search.slowlog.query] [AT1mr_C] [index-XXX][4] took[1s], took_millis[1034], total_hits[100538], types[person], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], source[{\"size\":50,\"query\":{\"match_all\":{\"boost\":1.0}},\"sort\":[{\"_doc\":{\"order\":\"asc\"}}],\"slice\":{\"field\":\"_id\",\"id\":11,\"max\":68}}], id[], Basically, there are 68 parallel scrolls. Knowing that the thread pool size is 60 it is safe to say that most of the time those slices are waiting, not improving the speed of reading. (I should say that we actually have 2 such indexes and do 2 esJsonRDD() in parallel.) What I have seen from my own experiments with Elasticsearch performance under load, when thread pool becomes of non-zero size performance of that node degrades much faster, like in this case, leaving an impression that management of the thread pool consumes a lof of CPU and requires a lot of context switching (hence high load average). An obvious fix would be to set the amount of parallel scroll requests (slices) so the throughput of a single node is enough. I couldn't find any way to configure this slice number. The closest thing seems to be the es.input.max.docs.per.partition but I am not sure if it is going to work. My question is: How can we prevent Elasticsearch from getting clogged with too many parallel scroll slices, executed by esJsonRDD? Or better, what would be the most effective way to dump contents of an Elasticsearch index to Spark? Any advice or suggestion is welcome! Thank you! P.S. I was considering to file a bug in elasticsearch-hadoop since this slice-vs-thread pool size thing didn't seem right, but I was also not sure this description will be enough to narrow down the problem, so I decided to create a post here in hope that ES devs will see it and help me reason about the problem. P.P.S. I have just checked, we use elasticsearch-hadoop=6.6.0 but I think little minor version difference should not impact. We use 8 spark slave machines.",
    "website_area": "discuss"
  },
  {
    "id": "1c65645d-b597-4280-8152-4857bb173926",
    "url": "https://discuss.elastic.co/t/hadoop-why-does-the-default-value-of-es-batch-write-refresh-is-true/191955",
    "title": "[hadoop] Why does the default value of es.batch.write.refresh is true?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "f9865",
    "date": "July 24, 2019, 6:22am August 14, 2019, 7:14pm August 15, 2019, 3:56am August 15, 2019, 1:57pm August 16, 2019, 3:45am September 13, 2019, 3:45am",
    "body": "elasticsearch-hadoop document link says es.batch.write.refresh (default true) Whether to invoke an index refresh or not after a bulk update has been completed. Note this is called only after the entire write (meaning multiple bulk updates) have been executed. However, the official document of ES about refresh says that The Index, Update, Delete, and Bulk APIs support setting refresh to control when changes made by this request are made visible to search. This (set refresh=true of the bulk request) should ONLY be done after careful thought and verification that it does not lead to poor performance, both from an indexing and a search standpoint. true creates less efficient indexes constructs (tiny segments) that must later be merged into more efficient index constructs (larger segments). Meaning that the cost of true is paid at index time to create the tiny segment, at search time to search the tiny segment, and at merge time to make the larger segments. The confliction between their documents is confusing. Can anybody help explain more about the reason that elastic-hadoop use true as the default value?",
    "website_area": "discuss"
  },
  {
    "id": "e07ccaf2-52c6-401b-b577-de3c17b87d6a",
    "url": "https://discuss.elastic.co/t/how-to-create-an-external-table-on-es-document-uisng-spark-sql-es-hadoop-6-7-0-and-es-6-7-0/195228",
    "title": "How to create an external Table on ES document uisng Spark SQL(ES-Hadoop 6.7.0 and ES 6.7.0)",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Srinivas2",
    "date": "August 14, 2019, 3:23pm August 14, 2019, 7:27pm August 15, 2019, 6:39pm September 12, 2019, 6:39pm",
    "body": "Hi Team, Requesting you help. Is it possible to create a table using spark-sql like mentioned here (https://docs.databricks.com/spark/latest/data-sources/elasticsearch.html [Use SQL to access Elasticsearch index]) I am able to create external table successfully on top of ES document using ES-hive using ESStorage Handler , however I am unable to create a table using spark-sql synstax ,and using below command. create table elasticsearch_poc using org.elasticsearch.spark.sql options('resource'='indexname', 'nodes'= 'node name', 'es.nodes.wan.only'='true', 'es.port'='portnumber' ); Below command is using to create table(successfully) using HIVE. create external table elasticserach_test(col1 STRING,col2 STRING) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES( 'es.nodes'='nodeaddress', 'es.port'='portnumber', 'es.resource'='resourcenamet' , 'es.nodes.wan.only' = 'true', 'es.nodes.discover'='true', 'es.mapping.names'='col1:col1,col2:col2' ) Regards, Srini",
    "website_area": "discuss"
  },
  {
    "id": "5ed24fd0-0880-4ea1-a008-337471f0407b",
    "url": "https://discuss.elastic.co/t/es-ece-2-2-hadoop-connectivity/194461",
    "title": "ES(ECE 2.2) - HADOOP Connectivity",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "rijash",
    "date": "August 8, 2019, 3:08pm August 14, 2019, 7:20pm August 14, 2019, 8:58pm September 11, 2019, 8:58pm",
    "body": "Hi, I am getting the below error while trying to ingest data to ES (ECE) from hadoop (HIVE) query. =============== 2019-08-08 13:55:24,957 INFO [main]: httpclient.HttpMethodDirector (HttpMethodDirector.java:executeWithRetry(444)) - Retrying request 2019-08-08 13:55:24,958 ERROR [main]: rest.NetworkClient (NetworkClient.java:execute(147)) - Node [d985e2b66da74bd1860a09a9347c3506.elkeu.ondemand.com:443] failed (java.net.ConnectException: Connection refused (Connection refused)); no other nodes left - aborting... 2019-08-08 13:55:24,959 ERROR [main]: CliDriver (SessionState.java:printError(1089)) - Failed with exception java.io.IOException:org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' java.io.IOException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:520) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:427) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1773) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:237) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:169) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:380) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:740) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:685) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:233) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) Caused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:340) at org.elasticsearch.hadoop.hive.HiveUtils.init(HiveUtils.java:197) at org.elasticsearch.hadoop.hive.EsHiveInputFormat.getSplits(EsHiveInputFormat.java:112) at org.elasticsearch.hadoop.hive.EsHiveInputFormat.getSplits(EsHiveInputFormat.java:51) at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:371) at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:303) at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:458) ... 15 more Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[d985e2b66da74bd1860a09a9347c3506.elkeu.ondemand.com:443]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:152) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:424) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:388) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:392) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:168) at org.elasticsearch.hadoop.rest.RestClient.mainInfo(RestClient.java:735) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:330) ... 21 more 2019-08-08 13:55:24,959 INFO [main]: exec.TableScanOperator (Operator.java:close(616)) - Closing operator TS[0] 2019-08-08 13:55:24,959 INFO [main]: exec.SelectOperator (Operator.java:close(616)) - Closing operator SEL[1] 2019-08-08 13:55:24,959 INFO [main]: exec.ListSinkOperator (Operator.java:close(616)) - Closing operator OP[3] 2019-08-08 13:55:24,963 INFO [main]: CliDriver (SessionState.java:printInfo(1066)) - Time taken: 0.051 seconds 2019-08-08 13:55:24,963 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(149)) - 2019-08-08 13:55:24,963 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(177)) - </PERFLOG method=releaseLocks start=1565272524963 end=1565272524963 duration=0 from=org.apache.hadoop.hive.ql.Driver> Below configuration is working fine for ES as independent component but failing in ECE. ==================== 'es.index.read.missing.as.empty'='true', 'es.mapping.date.rich'='false', 'es.mapping.names'='date:@timestamp', 'es.net.http.auth.pass'='<es_cluster_pwd>', 'es.net.http.auth.user'='<es_cluster_id>', 'es.net.ssl'='true', 'es.net.ssl.cert.allow.self.signed'='true', 'es.net.ssl.keystore.location'='file:///etc/ssl/certs/1.jks', 'es.net.ssl.keystore.pass'='<jks_pwd>', 'es.net.ssl.keystore.type'='jks', 'es.net.ssl.protocol'='SSL', 'es.nodes.wan.only'='true', 'es.nodes'='d985e2b66da74bd1860a09a9347c3506.elkeu.ondemand.com:443', 'es.query'='?q=*', 'es.resource'='users/user-events'",
    "website_area": "discuss"
  },
  {
    "id": "5e440ddb-02c2-4402-a158-8fc3652963f1",
    "url": "https://discuss.elastic.co/t/caused-by-org-apache-spark-sparkexception-data-of-type-java-util-gregoriancalendar-cannot-be-used/194471",
    "title": "Caused by: org.apache.spark.SparkException: Data of type java.util.GregorianCalendar cannot be used",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Mayukh",
    "date": "August 9, 2019, 5:34am August 14, 2019, 7:25pm September 11, 2019, 7:25pm",
    "body": "Hi, I am trying to write a PySpark RDD into Elasticsearch, however I am getting the GregorianCalendar error. Any suggestions or help would be appreciated. es_fault_df = sqlContext.sql(\"select * from fault_search_all limit 10\") es_fault_rdd = es_fault_df.rdd.map(lambda item: ('key', {'objid': item['objid'],'customer_name': item['customer_name'] ,'veh_hdr':item['veh_hdr'],'road_number':item['road_number'],'fleet_name':item['fleet_name'],'model_desc':item['model_desc'],'fault_code':item['fault_code'],'sub_id':item['sub_id'],'fault_desc':item['fault_desc'],'occur_date':item['occur_date'],'fault_reset_date':item['fault_reset_date'],'fault_origin':item['fault_origin'],'record_type':item['record_type'],'gps_latitude':item['gps_latitude'],'gps_longitude':item['gps_longitude'],'offboard_load_date':item['offboard_load_date'],'loco_speed':item['loco_speed'],'engine_speed':item['engine_speed'],'notch':item['notch'],'direction':item['direction'],'hp':item['hp'],'water_temp':item['water_temp'],'oil_temp':item['oil_temp'],'mode_call':item['mode_call'],'loco_state_desc':item['loco_state_desc'],'software_subid':item['software_subid']})) es_fault_rdd.saveAsNewAPIHadoopFile(path='-',outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",keyClass=\"org.apache.hadoop.io.NullWritable\", valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=es_conf) Logs: Caused by: org.apache.spark.SparkException: Task failed while writing rows at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.org$apache$spark$internal$io$SparkHadoopMapReduceWriter$$executeTask(SparkHadoopMapReduceWriter.scala:178) at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:89) at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:88) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ... 1 more Caused by: org.apache.spark.SparkException: Data of type java.util.GregorianCalendar cannot be used at org.apache.spark.api.python.JavaToWritableConverter.org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable(PythonHadoopUtil.scala:141) at org.apache.spark.api.python.JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$1.apply(PythonHadoopUtil.scala:134) at org.apache.spark.api.python.JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$1.apply(PythonHadoopUtil.scala:133) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.apache.spark.api.python.JavaToWritableConverter.org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable(PythonHadoopUtil.scala:133) at org.apache.spark.api.python.JavaToWritableConverter.convert(PythonHadoopUtil.scala:148) at org.apache.spark.api.python.JavaToWritableConverter.convert(PythonHadoopUtil.scala:115) at org.apache.spark.api.python.PythonHadoopUtil$$anonfun$convertRDD$1.apply(PythonHadoopUtil.scala:181) at org.apache.spark.api.python.PythonHadoopUtil$$anonfun$convertRDD$1.apply(PythonHadoopUtil.scala:181) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$4.apply(SparkHadoopMapReduceWriter.scala:147) at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$4.apply(SparkHadoopMapReduceWriter.scala:144) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371) at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.org$apache$spark$internal$io$SparkHadoopMapReduceWriter$$executeTask(SparkHadoopMapReduceWriter.scala:159) ... 8 more",
    "website_area": "discuss"
  },
  {
    "id": "73c274e8-6993-4719-8e86-292fc815b080",
    "url": "https://discuss.elastic.co/t/cannot-detect-es-version-typically-this-happens-if-the-network-elasticsearch-cluster-is-not-accessible-hive-es-nodes-wan-only/191713",
    "title": "Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible (HIVE) es.nodes.wan.only",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Marcos_Peralta",
    "date": "July 22, 2019, 9:45pm August 14, 2019, 7:24pm September 11, 2019, 7:24pm",
    "body": "I'm currently trying to execute only a \"SELECT * FROM table\" from Hive to ElasticSearch after create my external table. I'm using cloudera CDH 6.0.1. I've already added the elasticsearch-hadoop-hive-7.1.1 jar to my hive path. I have ElasticSearch 7.1.1 The cloudera stack and elastic running in deferents server but in the same network . CREATE EXTERNAL TABLE ctrl_rater_resumen_lla_es ( fecha_registro string, direccion string, linea_b_codigo_prestadora string, linea_b_tipo_numero string, es_roaming string, call_duration string, linea_b_routing_number string, minutos string, fecha string ) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES ( 'es.resource' = 'ctrl_rater_resumen_lla/hb', 'es.node' = 'http://10.129.x.xxx', 'es.port' = '9200', 'es.index.auto.create' = 'true', 'es.index.read.missing.as.empty' = 'true', 'es.nodes.discovery'='true', 'es.net.ssl'='false' 'es.nodes.client.only'='false', 'es.nodes.wan.only' = 'true' 'es.net.http.auth.user'='xxxxx', 'es.net.http.auth.pass' = 'xxxxx' ); The create was successfully SELECT * FROM ctrl_rater_resumen_lla_es; But the select fail, error: Bad status for request TFetchResultsReq(fetchType=0, operationHandle=TOperationHandle(hasResultSet=True, modifiedRowCount=None, operationType=0, operationId=THandleIdentifier(secret='\\xbaYG*\\xd4wI\\xc0\\xb8\\xf6\\x94Q\\xa3\\xa4IY', guid='\\xff\\xca\\xdb\\xb5\\x040E\\x0e\\x8eE\\xe4\\xf7?t\\x1b\\x01')), orientation=4, maxRows=100): TFetchResultsResp(status=TStatus(errorCode=0, errorMessage=\"java.io.IOException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'\", sqlState=None, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:java.io.IOException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only':25:24\", 'org.apache.hive.service.cli.operation.SQLOperation:getNextRowSet:SQLOperation.java:492', 'org.apache.hive.service.cli.operation.OperationManager:getOperationNextRowSet:OperationManager.java:297', 'org.apache.hive.service.cli.session.HiveSessionImpl:fetchResults:HiveSessionImpl.java:852', 'sun.reflect.GeneratedMethodAccessor24:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1726', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy38:fetchResults::-1', 'org.apache.hive.service.cli.CLIService:fetchResults:CLIService.java:505', 'org.apache.hive.service.cli.thrift.ThriftCLIService:FetchResults:ThriftCLIService.java:702', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults:getResult:TCLIService.java:1717', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults:getResult:TCLIService.java:1702', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', \"*java.io.IOException:org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only':29:4\", 'org.apache.hadoop.hive.ql.exec.FetchOperator:getNextRow:FetchOperator.java:521', 'org.apache.hadoop.hive.ql.exec.FetchOperator:pushRow:FetchOperator.java:428', 'org.apache.hadoop.hive.ql.exec.FetchTask:fetch:FetchTask.java:146', 'org.apache.hadoop.hive.ql.Driver:getResults:Driver.java:2196', 'org.apache.hive.service.cli.operation.SQLOperation:getNextRowSet:SQLOperation.java:487', \"*org.elasticsearch.hadoop.EsHadoopIllegalArgumentException:Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only':35:6\", 'org.elasticsearch.hadoop.rest.InitializationUtils:discoverClusterInfo:InitializationUtils.java:340', 'org.elasticsearch.hadoop.hive.HiveUtils:init:HiveUtils.java:197', 'org.elasticsearch.hadoop.hive.EsHiveInputFormat:getSplits:EsHiveInputFormat.java:112', 'org.elasticsearch.hadoop.hive.EsHiveInputFormat:getSplits:EsHiveInputFormat.java:51', 'org.apache.hadoop.hive.ql.exec.FetchOperator:getNextSplits:FetchOperator.java:372', 'org.apache.hadoop.hive.ql.exec.FetchOperator:getRecordReader:FetchOperator.java:304', 'org.apache.hadoop.hive.ql.exec.FetchOperator:getNextRow:FetchOperator.java:459', '*org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException:Connection error (check network and/or proxy settings)- all nodes failed; tried [[localhost:9200]] :41:6', 'org.elasticsearch.hadoop.rest.NetworkClient:execute:NetworkClient.java:152', 'org.elasticsearch.hadoop.rest.RestClient:execute:RestClient.java:424', 'org.elasticsearch.hadoop.rest.RestClient:execute:RestClient.java:388', 'org.elasticsearch.hadoop.rest.RestClient:execute:RestClient.java:392', 'org.elasticsearch.hadoop.rest.RestClient:get:RestClient.java:168', 'org.elasticsearch.hadoop.rest.RestClient:mainInfo:RestClient.java:735', 'org.elasticsearch.hadoop.rest.InitializationUtils:discoverClusterInfo:InitializationUtils.java:330'], statusCode=3), results=None, hasMoreRows=None) The cloudera stack and elastic running in deferents server but in the same network .",
    "website_area": "discuss"
  },
  {
    "id": "6a38862d-a93d-42c7-8479-4f9dd334a350",
    "url": "https://discuss.elastic.co/t/integration-of-hadoop-specifically-hdfs-files-with-elk-stack/192596",
    "title": "Integration of hadoop (specifically HDFS files) with ELK stack",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Yukti_Agrawal",
    "date": "July 29, 2019, 3:01am August 14, 2019, 7:07pm September 11, 2019, 7:07pm",
    "body": "I am trying to integrate hadoop with ELK stack. My use case is \" i have to get a data from a file present in HDFS path (AVRO format) and show the contents on kibana dashboard\" Anybody is having any article with step by step process?",
    "website_area": "discuss"
  },
  {
    "id": "3a024b40-9865-4823-8d8a-fcc141a1d3dd",
    "url": "https://discuss.elastic.co/t/classcastexception-with-nested-array-in-hive-over-elasticsearch/191344",
    "title": "ClassCastException with nested array in hive over elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "varun_r",
    "date": "July 19, 2019, 8:21am August 16, 2019, 8:28am",
    "body": "I am using elasticsearch-hadoop-hive-7.2.0.jar to use elasticsearch tables on hive. However couldn't get it work with nested column. CREATE EXTERNAL TABLE test2019197 (fault_details struct<faults:array<struct<Details:string, Resolution:string>>>) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.nodes'='11111.us-central1.gcp.cloud.es.io:9243', 'es.mapping.names' = 'fault_details:fault_details', 'es.net.ssl'='true', 'es.resource' = 'test2019197', 'es.nodes.wan.only' = 'true', 'es.index.read.missing.as.empty'='false'); For this particular column my es data looks like: \"fault_details\" : { \"faults\" : [ { \"Resolution\" : \"Bright\", \"Details\" : \"Reject\" }, { \"Resolution\" : \"Reduce Brightness\", \"Details\" : \"Reject\" } ] }, I tried with multiple mapping/without map with string column but got the classcast exception which should come as the datatype mismatches on both sides. But can some have a look over the above ; as now i am using the similar datatype as of es. error: Failed with exception java.io.IOException:java.lang.ClassCastException: org.elasticsearch.hadoop.mr.WritableArrayWritable cannot be cast to org.apache.hadoop.io.MapWritable TIA",
    "website_area": "discuss"
  },
  {
    "id": "0f649e4a-19af-48c7-b87b-bca734881e84",
    "url": "https://discuss.elastic.co/t/unable-to-create-hdfs-repository-for-elastic-snapshot-creation/190992",
    "title": "Unable to create hdfs repository for elastic snapshot creation",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "raja_augstin",
    "date": "July 24, 2019, 1:07pm August 14, 2019, 12:58pm",
    "body": "I wants to store the elastic index into hdfs. And I'm using following version elastic version: 5.6.3 snapshot : 2.8.1 when we use following command to create hdfs type repository we facing error. can someone help on this issues PUT _snapshot/my_hdfs_repository { \"type\": \"hdfs\", \"settings\": { \"uri\": \"hdfs://xxxxxxxx:12345/\", \"path\": \"/dev/test/snapshot\", \"security.principal\": \"abc@TEST-xyv.COM\" } } we receive following error : { \"error\" : { \"root_cause\" : [ { \"type\" : \"repository_exception\", \"reason\" : \"[my_hdfs_repository] failed to create repository\" } ], \"type\" : \"repository_exception\", \"reason\" : \"[my_hdfs_repository] failed to create repository\", \"caused_by\" : { \"type\" : \"unchecked_i_o_exception\", \"reason\" : \"Cannot create HDFS repository for uri [hdfs://yyyyyyy:12345/ \"caused_by\" : { \"type\" : \"i_o_exception\", \"reason\" : \"Failed on local exception: java.io.IOException: org.apache.h s : local host is: \\\"xxxxxxxx.server.net/xx.yyyy.zz.qq\\\"; destination host is \"caused_by\" : { \"type\" : \"i_o_exception\", \"reason\" : \"**org.apache.hadoop.ipc.RpcException**: RPC response exceeds m \"caused_by\" : { \"type\" : \"rpc_exception\", \"reason\" : \"RPC response exceeds maximum data length\" } } } } }, \"status\" : 500 }",
    "website_area": "discuss"
  },
  {
    "id": "07a67e75-6e4f-4445-9e01-f91a0fec8e63",
    "url": "https://discuss.elastic.co/t/how-to-avoid-data-too-large-error-for-org-apache-spark-sql-streaming-streamingqueryexception/190604",
    "title": "How to avoid Data too large error for org.apache.spark.sql.streaming.StreamingQueryException",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "yuecong",
    "date": "July 17, 2019, 5:58am July 15, 2019, 8:10pm July 17, 2019, 5:57am August 14, 2019, 5:57am",
    "body": "I am using spark streaming to dump data from Kafka to ES and I got the following errors. org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 6 in stage 4888.0 failed 4 times, most recent failure: Lost task 6.3 in stage 4888.0 (TID 58565, 10.139.64.27, executor 256): org.apache.spark.util.TaskCompletionListenerException: org.elasticsearch.hadoop.rest.EsHadoopRemoteException: circuit_breaking_exception: [parent] Data too large, data for [<http_request>] would be [6104859024/5.6gb], which is larger than the limit of [6103767449/5.6gb], real usage: [6104859024/5.6gb], new bytes reserved: [0/0b] Could someone suggest how I can adjust some parameters to avoid this? Here is my es configurations to write to ES. val esURL = \"xxxx\" serviceLogDfForES.writeStream .outputMode(\"append\") .format(\"org.elasticsearch.spark.sql\") .option(\"es.nodes.wan.only\",\"true\") .option(\"es.port\",\"9200\") .option(\"es.net.http.auth.user\", \"xxx\") .option(\"es.net.http.auth.pass\", \"xxx\") .option(\"checkpointLocation\", \"/mnt/xxxx/_checkpoint1\") .option(\"es.net.ssl\",\"true\") .option(\"es.net.ssl.cert.allow.self.signed\", \"true\") .option(\"es.mapping.date.rich\", \"true\") .option(\"es.nodes\", esURL) .option(\"es.resource.write\", \"service-log-{date}\") .start().awaitTermination()",
    "website_area": "discuss"
  },
  {
    "id": "707a96fb-ea99-4b08-b59e-e6928a1dc3eb",
    "url": "https://discuss.elastic.co/t/spark-structured-streaming-timeseries-indices/189869",
    "title": "Spark Structured Streaming timeseries indices",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "danielyahn",
    "date": "July 11, 2019, 1:47pm July 11, 2019, 3:12pm July 11, 2019, 7:09pm August 8, 2019, 7:24pm",
    "body": "I have a follow up question to [Spark Structured streaming] Elasticsearch sink index name James said, \"As christian has mentioned above, ES-Hadoop will not create an index name using the current time, but rather will let you give a field name on your document to use in the index name creation. This field can contain a timestamp, which is usually what users want when they are using time based indices.\" What is the field name that ES-Hadoop (or ES possibly) looking for index name creation? And how do we configure which field to use? Do we have a related documentation or example? In this example, https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html#spark-sql-streaming-write-scala , wouldn't it create a spark index?",
    "website_area": "discuss"
  },
  {
    "id": "9e1dc4d4-1af0-48d5-91d8-fc5cfafd8911",
    "url": "https://discuss.elastic.co/t/how-to-authenticate-with-x-509-pem-certificates-and-pkcs-8-keys/189063",
    "title": "How to authenticate with X.509 PEM certificates and PKCS #8 keys",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "",
    "date": "July 5, 2019, 9:42am August 2, 2019, 9:42am",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "aa6aae18-dd04-4058-97f0-6845e19bbae8",
    "url": "https://discuss.elastic.co/t/elasticsearch-spark-connection-for-structured-streaming/188225",
    "title": "Elasticsearch spark connection for Structured Streaming",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Gokul_Raj",
    "date": "July 1, 2019, 9:11am July 3, 2019, 7:22am July 31, 2019, 7:33am",
    "body": "I'm trying to make a connection to elasticsearch from my spark program. My elasticsearch host is https and found no connection property for that. We are using spark structred streaming Java API and the connection details are as follows, SparkSession spark = SparkSession.builder() .config(ConfigurationOptions.ES_NET_HTTP_AUTH_USER, \"username\") .config(ConfigurationOptions.ES_NET_HTTP_AUTH_PASS, \"password\") .config(ConfigurationOptions.ES_NODES, \"my_host_url\") .config(ConfigurationOptions.ES_PORT, \"9200\") .config(ConfigurationOptions.ES_NET_SSL_TRUST_STORE_LOCATION,\"C:\\\\certs\\\\elastic\\\\truststore.jks\") .config(ConfigurationOptions.ES_NET_SSL_TRUST_STORE_PASS,\"my_password\") .config(ConfigurationOptions.ES_NET_SSL_KEYSTORE_TYPE,\"jks\") .master(\"local[2]\") .appName(\"spark_elastic\").getOrCreate(); spark.conf().set(\"spark.sql.shuffle.partitions\",2); spark.conf().set(\"spark.default.parallelism\",2); And I'm getting the following error 19/07/01 12:26:00 INFO HttpMethodDirector: I/O exception (org.apache.commons.httpclient.NoHttpResponseException) caught when processing request: The server 10.xx.xxx.xxx failed to respond 19/07/01 12:26:00 INFO HttpMethodDirector: Retrying request 19/07/01 12:26:00 ERROR NetworkClient: Node [10.xx.xxx.xxx:9200] failed (The server 10.xx.xxx.xxx failed to respond); no other nodes left - aborting... 19/07/01 12:26:00 ERROR StpMain: Error org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:344) Probably it's because it tries to initiate connection by http protocol but in my case I need https connection and not sure how to configure that",
    "website_area": "discuss"
  },
  {
    "id": "3c08a2f8-9c56-465e-8f6e-5f42dd21b069",
    "url": "https://discuss.elastic.co/t/es-spark-connector-circuit-breaker-error/187358",
    "title": "ES Spark Connector - Circuit Breaker error",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Daniel_Solow",
    "date": "June 25, 2019, 2:02pm July 2, 2019, 5:13pm July 2, 2019, 5:25pm July 30, 2019, 5:25pm",
    "body": "I'm using ES 7.1.1 and Spark 2.4.2. The ES cluster is on Google Kubernetes Engine and the Spark cluster is on Google Dataproc. Big jobs are failing with the following error, often several hours into the job: 19/06/25 08:46:15 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.0 in stage 2.0 (TID 556, cluster.name, executor 3): org.apache.spark.util.TaskCompletionListenerException: org.elasticsearch.hadoop.rest.EsHadoopRemoteException: circuit_breaking_exception: [parent] Data too large, data for [<http_request>] would be [5127135952/4.7gb], which is larger than the limit of [5067151769/4.7gb], real usage: [5127135952/4.7gb], new bytes reserved: [0/0b] It then prints the batch request, which is very large. Any ideas on how to prevent this kind of error? It looks to me like ES is not keeping up with the rate of requests, so memory usage is increasing until requests are rejected. In this case, it would be nice if Spark slowed down. It looks like retries are enabled, but the back-off time doesn't appear to increase. Any tips on resolving this problem?",
    "website_area": "discuss"
  },
  {
    "id": "0fa14ecf-5184-4915-928e-caeb3aed7491",
    "url": "https://discuss.elastic.co/t/support-for-aggregations-in-spark/182251",
    "title": "Support for Aggregations in Spark",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "angelazhao",
    "date": "May 22, 2019, 2:36pm June 19, 2019, 2:36pm June 24, 2019, 3:14am",
    "body": "Is it now possible to perform ES aggregations using Spark? An earlier post from March 2016 says the functionality isn't available yet but is on the roadmap. ES Aggregations in Spark Hadoop and Elasticsearch Hello everyone, based on discussion about ES use cases I was wondering whether there is any way how Spark could benefit from ES aggregations and convert them to Dataframe. F.e something like: val esConf = ... val esQuery = \"\"\"{\"agg\" : {\"my_agg\" : {\"terms\" : {\"field\": \"field_A\"} } } }\"\"\" val jsonResult = client.search(esQuery, esConf) val transformer = ... val df = jsonResult.toDF(transformer) val result = df.filter(...).join(otherDf) .... A) Is there any plan to support something similar in E Can you please give me an update on its status? Thanks!",
    "website_area": "discuss"
  },
  {
    "id": "e159e596-c152-47e7-8127-07c86a68a18d",
    "url": "https://discuss.elastic.co/t/es-hadoop-spark-6-6-securable-keystore/184634",
    "title": "ES+Hadoop+Spark 6.6 securable keystore",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Miguel_Oliveira",
    "date": "June 6, 2019, 4:59pm June 17, 2019, 9:34am June 24, 2019, 3:06am July 22, 2019, 3:07am",
    "body": "Hello all, Following the instructions on https://www.elastic.co/guide/en/elasticsearch/hadoop/6.6/security.html#keystore, we have successfully created a keystore. So far we added to it the key=\"es.net.http.auth.pass\" with a value corresponding to the password. Now we were instancing our Spark configuration like this: val conf = new SparkConf().setAppName(\"ReadFromES\") .set(\"es.nodes\", \"xxx.xxx.xxx.xxx\") .set(\"es.port\", \"9200\") .set(\"es.net.ssl\", \"true\") .set(\"es.net.ssl.truststore.location\", \"file:///etc/spark/conf/truststore.jks\") .set(\"es.net.ssl.truststore.pass\", \"truststore_password\") .set(\"es.keystore.location\", \"file:///etc/spark/conf/esh.keystore\") .set(\"es.nodes.wan.only\", \"true\") .set(\"es.net.http.auth.user\", \"user_name\") Running it like this, it does find both truststore and keystore (we deployed them on master and worker nodes). Our question now is: how can we recover the password stored on the keystore being passed, so we can set the configuration key \"es.net.http.auth.pass\" with the value coming from the keystore? Is there any naming convention? Do we set that item in a different way? Thank you. Best regards, Miguel",
    "website_area": "discuss"
  },
  {
    "id": "84876de4-8186-4030-bd37-acdd2c8c8a34",
    "url": "https://discuss.elastic.co/t/loading-csv-hdfs-to-es/184992",
    "title": "Loading csv hdfs to ES",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "genesis",
    "date": "June 12, 2019, 1:04pm June 24, 2019, 2:56am July 22, 2019, 2:56am",
    "body": "I am trying to load cvs files that are in HDFS into ElasticSearch, can someone please guide me the best way to accomplish this task hdfs cvs files: /user/xxxxx/mpc.db/ ES: http:elasticsearch.pulse.prod:9200",
    "website_area": "discuss"
  },
  {
    "id": "4eb89620-a4be-47f8-ae48-b238779ddedd",
    "url": "https://discuss.elastic.co/t/hive-es-issue/183659",
    "title": "Hive ES issue",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "manoj_kumar3",
    "date": "May 31, 2019, 6:07am June 24, 2019, 2:55am July 22, 2019, 2:55am",
    "body": "Anyone of you please help me in solving this problem. I'm trying to create a hive table on elasticsearch 6.2 where ssl is enabled.but not able to create the table, getting \"Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only\" issue. I have added elasticsearch 6.2.2 handler and required jks file to the terminal and passed below parameters in table properties. but getting the wan only issues. I'm not understanding where it went wrong. I'm able to create the elastic search index from shell using the elastic user id and password by passing the certificates. CREATE EXTERNAL TABLE if not exists hive_on_elastic( id1 string, anotherId string) COMMENT 'This is external table on ElasticSearch' STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES ( \"es.index.auto.create\"=\"false\", \"es.nodes\"=\"Node Name\", \"es.port\"=\"9200\", \"es.resource\"=\"index/type\", \"es.net.http.auth.user\"=\"elsticUser\", \"es.batch.size.bytes\"=\"10mb\", \"es.net.http.auth.pass\"=\"elasticPassword\", \"es.net.ssl.protocol\"=\"SSL\", \"es.net.ssl\"=\"true\", \"es.net.ssl.truststore.location\"=\"fileName.jks\", \"es.net.ssl.truststore.pass\"=\"jksPassword\", \"es.batch.size.entries\"=\"0\", \"es.mapping.names\"=\"id1:id\", \"es.mapping.id\"=\"anotherUniqueId\");",
    "website_area": "discuss"
  },
  {
    "id": "29af7850-8697-410b-b319-db5fab0e878c",
    "url": "https://discuss.elastic.co/t/curl-works-but-hive-cant-connect-to-es/186760",
    "title": "Curl works but Hive can't connect to ES",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "A_Felgueiras",
    "date": "June 20, 2019, 8:22pm June 24, 2019, 3:08am July 22, 2019, 1:51am",
    "body": "Hi, From an Hadoop cluster I can request my index using curl, like this : curl -u user:pwd -X GET http://mynode:9020/myindex, it works, but when I try to request using Hive (Beeline) I have the famous exception \"Cannot detect ES version\". My external table has been created using ES handler like this : CREATE EXTERNAL TABLE json (data STRING) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.resource' = 'myindex', 'es.input.json` = 'yes', 'es.nodes' = 'XXXX', 'es.port' = '9020'); and a select request on this table returns the ES version error. My ES instance and Hive instance are on the same LAN, ping, telnet and curl are ok. I tried to set 'es.nodes.wan.only' to true but same result. In verbose mode on Beeline there is nothing more I can use to resolve this, only a \"org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: null\". Thanks for your help",
    "website_area": "discuss"
  },
  {
    "id": "b3522d21-0f14-4e7f-b242-2ce21e4e537d",
    "url": "https://discuss.elastic.co/t/spark-structured-streaming-elasticsearch-integration-issue/185366",
    "title": "Spark structured streaming Elasticsearch integration issue",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Manishvsaraf",
    "date": "June 12, 2019, 12:46pm June 24, 2019, 2:57am July 11, 2019, 4:34pm",
    "body": "I am writing a Spark structured streaming application in which data processed with Spark needs be sink'ed to elastic search. This is my development environment. Hadoop 2.6.0-cdh5.16.1 Spark version 2.3.0.cloudera4 elasticsearch 6.8.0 I ran spark-shell as spark2-shell --jars /tmp/elasticsearch-hadoop-2.3.2/dist/elasticsearch-hadoop-2.3.2.jar import org.apache.spark.SparkContext import org.apache.spark.SparkConf import org.apache.spark.sql.functions._ import org.apache.spark.sql.SparkSession import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, TimestampType}; import java.util.Calendar import org.apache.spark.sql.SparkSession import org.elasticsearch.spark.sql import sys.process._ val checkPointDir = \"/tmp/rt/checkpoint/\" val spark = SparkSession.builder .config(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\") .config(\"fs.s3n.awsAccessKeyId\",\"aaabbb\") .config(\"fs.s3n.awsSecretAccessKey\",\"aaabbbccc\") .config(\"spark.sql.streaming.checkpointLocation\",s\"$checkPointDir\") .config(\"es.index.auto.create\", \"true\").getOrCreate() import spark.implicits._ val requestSchema = new StructType().add(\"log_type\", StringType).add(\"time_stamp\", StringType).add(\"host_name\", StringType).add(\"data_center\", StringType).add(\"build\", StringType).add(\"ip_trace\", StringType).add(\"client_ip\", StringType).add(\"protocol\", StringType).add(\"latency\", StringType).add(\"status\", StringType).add(\"response_size\", StringType).add(\"request_id\", StringType).add(\"user_id\", StringType).add(\"pageview_id\", StringType).add(\"impression_id\", StringType).add(\"source_impression_id\", StringType).add(\"rnd\", StringType).add(\"publisher_id\", StringType).add(\"site_id\", StringType).add(\"zone_id\", StringType).add(\"slot_id\", StringType).add(\"tile\", StringType).add(\"content_id\", StringType).add(\"post_id\", StringType).add(\"postgroup_id\", StringType).add(\"brand_id\", StringType).add(\"provider_id\", StringType).add(\"geo_country\", StringType).add(\"geo_region\", StringType).add(\"geo_city\", StringType).add(\"geo_zip_code\", StringType).add(\"geo_area_code\", StringType).add(\"geo_dma_code\", StringType).add(\"browser_group\", StringType).add(\"page_url\", StringType).add(\"document_referer\", StringType).add(\"user_agent\", StringType).add(\"cookies\", StringType).add(\"kvs\", StringType).add(\"notes\", StringType).add(\"request\", StringType) val requestDF = spark.readStream.option(\"delimiter\", \"\\t\").format(\"com.databricks.spark.csv\").schema(requestSchema).load(\"s3n://aa/logs/cc.com/r/year=\" + Calendar.getInstance().get(Calendar.YEAR) + \"/month=\" + \"%02d\".format(Calendar.getInstance().get(Calendar.MONTH)+1) + \"/day=\" + \"%02d\".format(Calendar.getInstance().get(Calendar.DAY_OF_MONTH)) + \"/hour=\" + \"%02d\".format(Calendar.getInstance().get(Calendar.HOUR_OF_DAY)) + \"/*.log\") requestDF.writeStream.format(\"org.elasticsearch.spark.sql\").option(\"es.resource\", \"rt_request/doc\").option(\"es.nodes\", \"localhost\").outputMode(\"Append\").start() I have tried following two ways to sink the data in the DataSet to ES. 1.ds.writeStream().format(\"org.elasticsearch.spark.sql\").start(\"spark/orders\"); 2.ds.writeStream().format(\"es\").start(\"rt_request/doc\"); In both cases I am getting the following error: Caused by: java.lang.UnsupportedOperationException: Data source es does not support streamed writing java.lang.UnsupportedOperationException: Data source org.elasticsearch.spark.sql does not support streamed writing at org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:320) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:293) ... 56 elided",
    "website_area": "discuss"
  },
  {
    "id": "b15856ad-ff49-4488-935d-1fe097b51b27",
    "url": "https://discuss.elastic.co/t/how-to-use-mr-to-import-nested-data-into-es/182114",
    "title": "How to use MR to import nested data into ES",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "zhentao_wang",
    "date": "May 22, 2019, 2:13am June 19, 2019, 2:13am",
    "body": "My Trying public class Hadoop2ElasticsearchWithJSON extends Configured implements Tool { public static void main(String args) throws Exception { int res = ToolRunner.run(new Configuration(), new Hadoop2ElasticsearchWithJSON(), args); System.exit(res); } /** Map-reduce program for importing Tweet User exampler data. Files are named as: User.csv. Format: nodeId,name 1,jack */ public int run(String args) throws Exception { Configuration conf = this.getConf();conf.setBoolean(\"mapred.map.tasks.speculative.execution\", false); conf.setBoolean(\"mapred.reduce.tasks.speculative.execution\", false); conf.set(\"es.nodes\", \"\"); // conf.set(\"es.nodes\",\"127.0.0.1:9200\"); conf.set(\"es.resource\", \"hadoop-person/hadoop-person\"); conf.set(\"es.output.json\", \"true\");Job job = Job.getInstance(conf, \"es-hadoop-example\"); job.setJarByClass(Hadoop2ElasticsearchWithJSON.class); job.setMapperClass(Hadoop2ElasticsearchMapper.class); job.setOutputFormatClass(EsOutputFormat.class); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(MapWritable.class);FileInputFormat.addInputPath(job, new Path(\"tinkerpop-modern-2.json\")); FileOutputFormat.setOutputPath(job, new Path(\"output\"));job.waitForCompletion(true); return 0; } private static class Hadoop2ElasticsearchMapper extends Mapper<Object, Text, NullWritable, MapWritable> { public void map(Object key, Text value, Context context) throws IOException, InterruptedException { JSONObject vertex = JSONObject.parseObject(value.toString()); JSONObject properties = vertex.getJSONObject(\"properties\"); JSONArray nest = new JSONArray(); for (String fieldName : properties.keySet()) { JSONArray jsonArray = properties.getJSONArray(fieldName); for (Object o : jsonArray) { JSONObject property = JSONObject.parseObject(o.toString()); JSONObject jsonObject = new JSONObject(); jsonObject.put(\"prop\", fieldName); jsonObject.put(\"val\", property.getString(\"value\")); nest.add(jsonObject); } } MapWritable doc = new MapWritable(); doc.put(new Text(\"ontology\"), new JsonWritable(nest)); context.write(NullWritable.get(), doc); } } } JsonWritable /** Writable representing a JSON object. */ public class JsonWritable implements Writable { private static final Gson GSON = new Gson(); private JSONArray jsonArray; /** Creates an empty {@code JsonWritable}. */ public JsonWritable() { jsonArray = new JSONArray(); } public JsonWritable(JSONArray jsonArray) { this.jsonArray = jsonArray; } /** Deserializes a {@code JsonWritable} object. * @param in source for raw byte representation */ @override public void readFields(DataInput in) throws IOException { int cnt = in.readInt(); byte buf = new byte[cnt]; in.readFully(buf); jsonArray = JSONArray.parseArray(new String(buf, \"UTF-8\")); } /** Serializes this object. * @param out where to write the raw byte representation */ @override public void write(DataOutput out) throws IOException { byte buf = GSON.toJson(jsonArray).getBytes(); out.writeInt(buf.length); out.write(buf); } /** Returns the serialized representation of this object as a byte array. * @return byte array representing the serialized representation of this object @throws IOException */ public byte serialize() throws IOException { ByteArrayOutputStream bytesOut = new ByteArrayOutputStream(); DataOutputStream dataOut = new DataOutputStream(bytesOut); write(dataOut); return bytesOut.toByteArray(); } public JSONArray getJsonArray() { return jsonArray; } @override public String toString() { return getJsonArray().toString(); } public static JsonWritable create(DataInput in) throws IOException { JsonWritable json = new JsonWritable(); json.readFields(in); return json; } public static JsonWritable create(byte bytes) throws IOException { return create(new DataInputStream(new ByteArrayInputStream(bytes))); } }",
    "website_area": "discuss"
  },
  {
    "id": "917e4d64-0859-47f0-be45-202cf597bb5f",
    "url": "https://discuss.elastic.co/t/release-of-7-1-0-elasticsearch-spark-20/181523",
    "title": "Release of 7.1.0 elasticsearch-spark-20",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "yogiraj99",
    "date": "May 17, 2019, 6:53am May 20, 2019, 4:45pm June 17, 2019, 4:45pm",
    "body": "Hi Team, When do you plan to release the elasticsearch-spark-20_2.11 7.1.0 version. We are looking for a fix on #1276.",
    "website_area": "discuss"
  },
  {
    "id": "0f1bb3d4-7e6d-43ba-83e5-0142c9e85818",
    "url": "https://discuss.elastic.co/t/es-hive-integration-error-cannot-detect-es-version-typically-this-happens-if-the-network-elasticsearch-cluster-is-not-accessible-or-when-targeting-a-wan-cloud-instance-without-the-proper-setting-es-nodes-wan-only/181416",
    "title": "ES-hive integration error- Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Bhanu_Pratap_Singh",
    "date": "May 16, 2019, 2:11pm June 13, 2019, 1:56pm",
    "body": "Hi, I am trying to push data from a Hive(1.2.1000.2.6.3.0-235) table over to ElasticSearch (\"version\": : \"6.6.1\") in the secured environment. I have tried numerous combinations of properties for the TBL_PROPERTIES of the \"CREATE EXTERNAL TABLE\" but nothing works. Created codac_booktest via kibana with normal mapping Added jar: add jar /usr/hdp/current/hive-webhcat/share/elasticsearch-hadoop-6.6.1.jar;(local file system) add jar /tmp/es_truststore_ut/commons-httpclient-3.0.jar;(local file system) CREATE EXTERNAL TABLE bookstest ( bookId STRING, author STRING, publisher STRING, name STRING) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.nodes' = 'infnt010112', 'es.port' = '9200','es.net.ssl'='true','es.net.ssl.protocol'='SSL', 'es.nodes.wan.only'= 'TRUE', 'es.net.ssl.keystore.location'='file:///tmp/es_truststore_ut/infyn010143_keystore.jks','es.net.ssl.keystore.pass'='','es.net.ssl.truststore.location'='file:///tmp/es_truststore_ut/codac_ut_truststore','es.net.ssl.truststore.pass'='','es.nodes.discovery' = 'false', 'es.net.ssl.keystore.type'='JKS','es.net.http.auth.user'='username', 'es.net.http.auth.pass'='********', 'es.resource' = 'codac_booktest/books'); FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'",
    "website_area": "discuss"
  },
  {
    "id": "6623ef43-a099-4731-86f1-35b0e2dd17ad",
    "url": "https://discuss.elastic.co/t/elasticsearch-spark-sql-queries-return-null/179777",
    "title": "Elasticsearch.spark.sql queries return null",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "niloo-sh",
    "date": "May 6, 2019, 1:20pm June 3, 2019, 1:10pm",
    "body": "I connect to remote elastic search. when I load my index data metadata column has information but all of other fields that are in \"doc\" column are null. spark 2.4.0 elasticsearch-hadoop-7.0.1.jar output: schema: root |-- doc: struct (nullable = true) | |-- screen_name: string (nullable = true) |-- _metadata: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) +----+-------------------------------------------------------------------------------+ |doc |_metadata | +----+-------------------------------------------------------------------------------+ |null|[_index -> ab_fa, _type -> doc, _id -> 152515152, _score -> 16.521849]| +----+-------------------------------------------------------------------------------+",
    "website_area": "discuss"
  },
  {
    "id": "4948ca30-a624-40b2-af26-4dbbb981680c",
    "url": "https://discuss.elastic.co/t/essparksql-savetoes-join-type-urgent/178828",
    "title": "EsSparkSQL.saveToEs join type(urgent)",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "001kulou",
    "date": "April 29, 2019, 3:18am May 27, 2019, 3:18am",
    "body": "I want to import a sub-document of join type, but the official document does not give an example. The sub-document needs to set name and parame and specify roting. \"my_join_field\": { \"name\": \"answer\", \"parent\": \"1\" } How to set these two parameters? Can you give an example of EsSparkSQL. saveToEs?",
    "website_area": "discuss"
  },
  {
    "id": "ecc056c5-dead-4967-834a-b6ea2e3548bc",
    "url": "https://discuss.elastic.co/t/spark-import-es-join-type/177574",
    "title": "Spark import es join type",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "001kulou",
    "date": "April 19, 2019, 7:08am May 17, 2019, 7:03am",
    "body": "es6.X version URLhttps://cloud.tencent.com/developer/article/1106756 This is a join type mapping \"mappings\":{ \"business1\":{ \"properties\":{ \"my_join_field\":{ \"type\":\"join\", \"relations\":{ \"company\":[ \"shareholder\", \"mainmember\" ] }}}}} Import parent document to set my_join_field value to company But importing a child document requires specifying the name of routing and my_join_field and the parent of the parent document. For example 1PUT my_join_index/_doc/3?routing=1&refresh 2{ 3 \"text\": \"This is an answer\", 4 \"my_join_field\": { 5 \"name\": \"answer\", 6 \"parent\": \"1\" 7 } I want to use Scala to write import subdocument codeBut when I write my_join_field, I don't know what to write. My example var mp = Map(\"name\"->\"answer\",\"parent\"->\"1\") var js= Json(DefaultFormats).write(mp) val numbers = Map(\"my_join_field\" -> js ,\"companyKeyNoM\" -> \"123132321321\", \"companyNames\" -> \"\",\"Plaintiff\"->\"\",\"Defendant\"->\"\",\"Court\"->\"\",\"Type\"->\"\",\"Content\"->\"20160114760\",\"Date\"->\"2017-10-12\") val mapRDD = sc.makeRDD(Seq(numbers)) mapRDD.saveToEs(\"business-data1/business1\") but throw Exception: failed to parse;org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: unknown join name [{\"name\":\"1231231\",\"parent\":\"12313123\"}] for field [my_join_field] Please tell me how to write the value of this field in scala code when importing join type Thanks",
    "website_area": "discuss"
  },
  {
    "id": "81b7b7dd-cd1c-4145-8f55-d2e66fb1b7d5",
    "url": "https://discuss.elastic.co/t/spark-es-issue/177025",
    "title": "SPARK-ES issue",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "allami",
    "date": "April 16, 2019, 8:24am May 14, 2019, 8:24am",
    "body": "Hello, i've this error while using \"org.elasticsearch\" % \"elasticsearch-spark-20_2.11\" % \"6.3.0\" could you please help me resolve that: scala.MatchError: Buffer(00205215) (of class scala.collection.convert.Wrappers$JListWrapper)",
    "website_area": "discuss"
  },
  {
    "id": "39f62db5-3cfb-4cdb-82a9-52044360288b",
    "url": "https://discuss.elastic.co/t/restoring-hdfs-snapshots/176901",
    "title": "Restoring HDFS Snapshots",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "paul1243",
    "date": "April 15, 2019, 12:22pm May 13, 2019, 12:22pm",
    "body": "Hi, I have took the ES snapshots with HDFS repository. I'd like to restore the snapshots in a different Elastic Stack Cluster. Could someone let me know the steps of doing it. Thanks !",
    "website_area": "discuss"
  },
  {
    "id": "51624790-e954-409a-99b2-fdb50a9a5847",
    "url": "https://discuss.elastic.co/t/inconsistent-behaviors-between-different-runs-of-es-spark-connector/174649",
    "title": "Inconsistent Behaviors between Different Runs of ES-Spark Connector",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "qixinwm2010",
    "date": "March 30, 2019, 9:30pm April 5, 2019, 8:49pm April 13, 2019, 7:43pm May 10, 2019, 12:22am",
    "body": "Hello Community, We are doing ES indexing using the ES-SPark Connector (6.6.1) in our daily spark job. Here is our API call we make in our spark code df.write .format(\"org.elasticsearch.spark.sql\") .option(\"es.resource\", \"indexA_staging/indexA_main\") .option(\"es.write.operation\", \"upsert\") .option(\"es.mapping.id\",\"conformedid\") .option(\"es.batch.write.refresh\",\"false\") .option(\"es.index.auto.create\",\"no\") .option(\"es.nodes\", \"10.7.17.250:9200\") .option(\"es.batch.size.bytes\", \"2mb\") .option(\"es.batch.size.entries\", \"2000\") .option(\"es.write.operation\", \"upsert\") .mode(\"append\") .save(\"indexA_staging/indexA_main\") After the first run of our daily job on each day, we sometimes find that a few records in ES are partially indexed. When we detect this kind of issue, we rerun the spark daily job and find they can be fully indexed then. Here is an example we get today: After the first run of our daily spark job today, we have 85 records out of 12 million with the following fields partially indexed. But there are 234 fields in total { \"_index\": \"indexA\", \"_type\": \"indexA_main\", \"_id\": \"indexa|6a90e8b8-8c8f-4842-b3a3-8d3da67eaca9\", \"_version\": 1, \"found\": true, \"_source\": { \"conformedid\": \"indexa|6a90e8b8-8c8f-4842-b3a3-8d3da67eaca9\", \"secondid\": \"DAD506F4-9563-42CD-BCC2-BB8E74B96A45\", \"updated\": \"2018-05-29T04:59:38.660Z\", \"soid\": 7057358, \"creationdate\": \"2018-05-29T11:33:04.730Z\", \"path\": \"dummy text\", \"labels\": \"dummy text\", \"isupdated\": false, \"status\": \"dummy text\", \"response\": false, \"pubilish date\": \"dummy text\" } } After the second run of our same daily job today, we have all records (about 12 million) with the following fields fully indexed there are 234 fields in total { \"_index\": \"indexA\", \"_type\": \"indexA_main\", \"_id\": \"indexa|6a90e8b8-8c8f-4842-b3a3-8d3da67eaca9\", \"_version\": 1, \"found\": true, \"_source\": { \"conformedid\": \"indexa|6a90e8b8-8c8f-4842-b3a3-8d3da67eaca9\", \"secondid\": \"DAD506F4-9563-42CD-BCC2-BB8E74B96A45\", \"updated\": \"2018-05-29T04:59:38.660Z\", \"soid\": 7057358, \"creationdate\": \"2018-05-29T11:33:04.730Z\", \"path\": \"dummy text\", \"labels\": \"dummy text\", \"isupdated\": false, \"status\": \"dummy text\", \"response\": false, \"pubilish date\": \"dummy text\", \"fieldA\": \"dummy filedA value\", \"filedB \": \"dummy filedB value\", \"filedC \": \"dummy filedC value\", \"filedD \": \"dummy filedD value\", \"filedE \": \"dummy filedE value\", \"filedF \": \"dummy filedF value\", \"filedG \": \"dummy filedG value\" ... total 234 fields } } Both runs succeeded I wonder, whether our batches are getting truncated or not. Do we misconfigure anything? How should we check for errors in the ES logs and Spark logs?",
    "website_area": "discuss"
  },
  {
    "id": "31ee019e-1f62-431a-b3ab-695d6c241a21",
    "url": "https://discuss.elastic.co/t/duplicate-in-dataset-while-reading-from-elasticsearch-index-with-spark/176379",
    "title": "Duplicate in Dataset while reading from elasticsearch index with SPARK",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "pverma0312",
    "date": "April 11, 2019, 10:32am May 9, 2019, 10:33am",
    "body": "Hi, I am trying to read an unstructured index from es and loading it to RDD of SPARK, sometimes in RDD the complete data is getting duplicated, did somebody faced the similar issue, any recommendation. Thanks, Prashant Verma",
    "website_area": "discuss"
  },
  {
    "id": "e6e80199-084b-4746-81c0-50217bbfbea2",
    "url": "https://discuss.elastic.co/t/repository-verification-exception-when-creating-hdfs-repository/175757",
    "title": "Repository verification exception when creating HDFS Repository",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "paul1243",
    "date": "April 15, 2019, 12:22pm April 9, 2019, 9:00am May 7, 2019, 9:00am",
    "body": "Hi, I couldn't able to create the HDFS repository on my elasticsearch cluster. Here is the request and response: Could someone please help me how do I resolve this issue. Thanks ! PUT _snapshot/my_staging_hdfs { \"type\": \"hdfs\", \"settings\": { \"uri\": \"hdfs://173.20.158.196:9000\", \"path\": \"elasticsearch/repositories/my_staging_hdfs\", \"conf.dfs.client.read.shortcircuit\": \"true\", \"conf.dfs.domain.socket.path\": \"true\", \"compress\": true } } Response: { \"error\": { \"root_cause\": [ { \"type\": \"repository_verification_exception\", \"reason\": \"[my_staging_hdfs] [[2NDinVP7TYitBlXUdN8GkQ, 'RemoteTransportException[[node-1][173.20.159.94:9300][internal:admin/repository/verify]]; nested: RepositoryVerificationException[[my_staging_hdfs] a file written by master to the store [hdfs://173.20.158.196:9000/user/Admin-002$/elasticsearch/repositories/my_staging_hdfs] cannot be accessed on the node [{node-1}{2NDinVP7TYitBlXUdN8GkQ}{DzjkHnVvR5OBPautd-z8Yw}{173.20.159.94}{173.20.159.94:9300}]. This might indicate that the store [hdfs://173.20.158.196:9000/user/Admin-002$/elasticsearch/repositories/my_staging_hdfs] is not shared between this node and the master node or that permissions on the store don't allow reading files written by the master node];']]\" } ], \"type\": \"repository_verification_exception\", \"reason\": \"[my_staging_hdfs] [[2NDinVP7TYitBlXUdN8GkQ, 'RemoteTransportException[[node-1][173.20.159.94:9300][internal:admin/repository/verify]]; nested: RepositoryVerificationException[[my_staging_hdfs] a file written by master to the store [hdfs://173.20.158.196:9000/user/Admin-002$/elasticsearch/repositories/my_staging_hdfs] cannot be accessed on the node [{node-1}{2NDinVP7TYitBlXUdN8GkQ}{DzjkHnVvR5OBPautd-z8Yw}{173.20.159.94}{173.20.159.94:9300}]. This might indicate that the store [hdfs://173.20.158.196:9000/user/Admin-002$/elasticsearch/repositories/my_staging_hdfs] is not shared between this node and the master node or that permissions on the store don't allow reading files written by the master node];']]\" }, \"status\": 500 }",
    "website_area": "discuss"
  },
  {
    "id": "c007c681-f121-4ed9-940a-bd73524884da",
    "url": "https://discuss.elastic.co/t/query-hdfs-data-using-es-and-kibana/175787",
    "title": "Query HDFS data using ES and Kibana",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "leekk8",
    "date": "April 8, 2019, 8:10am April 8, 2019, 8:14am April 9, 2019, 1:54am April 9, 2019, 7:51am May 7, 2019, 7:50am",
    "body": "Hi, I would like to setup a Kibana GUI to query some log files in HDFS. So, I'm looking at the Hadoop-ES connector if this can help to meet my requirement. However, there are few doubts I have now. Do I need to store all these log files into ElasticSearch in order to query using Kibana? Then, I will have double copy of log files in HDFS and ElasticSearch. How Hadoop-ES connector to connect to HDFS? I refer to the documents, it only mentions about connecting to Hive, Pig, Spark, Storm, etc, but no HDFS. Where should I install this Hadoop-ES connector? In ES cluster, or Hadoop cluster? I have gone through the installation guide, however, it didn't state the installation steps clearly. Anyone can help? Thank you. Regards, KK",
    "website_area": "discuss"
  },
  {
    "id": "0e21498a-c1df-4c86-bd73-ef1ab1bad7bb",
    "url": "https://discuss.elastic.co/t/how-to-parallelize-es-load-operation-in-spark-using-the-connector-lib/174618",
    "title": "How to parallelize ES load operation in Spark using the connector lib?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "traam",
    "date": "March 29, 2019, 11:27pm April 5, 2019, 8:41pm April 8, 2019, 10:59pm April 8, 2019, 11:05pm April 8, 2019, 11:10pm May 6, 2019, 11:10pm",
    "body": "I have a three node Spark cluster (8 cores and 16GB RAM each) in standalone mode. I am using Elasticsearch-hadoop connector to read an ES index. The index is really huge with over 100M documents and has 5 shards and 2 replications. When I create a dataframe using Spark, I want this load operation to be parallelized but I see that it is handled only in driver and not on the executors. This single operation alone takes over 8 hours to load. How can I optimize this and let all the worker nodes to load the data in parallel? I submit the job using 16 executors each with 1 core and 2GB memory and driver with 4GB memory. df = sqlContext.read.format(\"org.elasticsearch.spark.sql\").option(\"es.nodes\", es_ip).load(es_index)",
    "website_area": "discuss"
  },
  {
    "id": "b76a331f-29e4-4d0d-9e93-9f67b1fcc316",
    "url": "https://discuss.elastic.co/t/dynamic-indexing-in-spark-structured-streamin/174717",
    "title": "Dynamic indexing in spark structured streamin",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Fahad_Khan",
    "date": "April 1, 2019, 6:30am April 5, 2019, 9:31pm May 3, 2019, 9:31pm",
    "body": "I'm using spark structured streaming to read so many files recursively and writing them into elasticsearch, I want to know is there a way to create index at runtime according to the file name spark reading from folder. suppose spark read file from client1 dir so it should index it in elasticsearch as client1. I've read this https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html#cfg-multi-writes, they did quite easily but the problem is I'm using spark structured streaming, in which there is no SavetoEs method instead there is WriteStream method, What i want to do is that i want to read files recursively from different directories and when i'll write them into elasticsearch i want to index them in elasticsearch as i-e file:/home/usr/client/message/single so it should be indexed like \"client-message-single\" in elasticsearh based on file path. ``` file_path= file://home/usr/client/email/message/singlemesage file_split_path= file_path.split('/') index = str(file_split_path[-4] + '-' + file_split_path[-3] + '-' + file_split_path[-2]) myjson['doc-index'] =index //adding key value in my json df=myjson.writeStream.option(\"es.resource.write\",\"{doc-index}/default\").outputMode(\"\"append\").format(\"org.elasticsearch.spark.sql\").start() df.awaitTermination() ``` while using above technique I'm getting following error org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot find match for {doc-index}/default at org.elasticsearch.hadoop.serialization.bulk.BulkEntryWriter.writeBulkEntry(BulkEntryWriter.java:136) at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:170) at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:74) at org.elasticsearch.spark.sql.streaming.EsStreamQueryWriter.run(EsStreamQueryWriter.scala:41) at org.elasticsearch.spark.sql.streaming.EsSparkSqlStreamingSink$$anonfun$addBatch$2$$anonfun$2.apply(EsSparkSqlStreamingSink.scala:52) at org.elasticsearch.spark.sql.streaming.EsSparkSqlStreamingSink$$anonfun$addBatch$2$$anonfun$2.apply(EsSparkSqlStreamingSink.scala:51) there is \"doc-index\" : \"client-message-singlemessage\" in json which read by spark stuctured streaming.",
    "website_area": "discuss"
  },
  {
    "id": "06b1b12d-6924-4b26-b008-0f7b97b4afc2",
    "url": "https://discuss.elastic.co/t/elasticsearch-spark-eshadoopillegalstateexception-field-position/171576",
    "title": "Elasticsearch-spark - EsHadoopIllegalStateException - field position",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "bytemedwb",
    "date": "March 22, 2019, 8:38pm March 22, 2019, 8:40pm March 29, 2019, 11:18pm April 5, 2019, 8:45pm May 3, 2019, 8:45pm",
    "body": "Hello, I have a question the is bother me. I have used the Elastic Spark to create a dataframe for one of my indexes. The schema prints fine (see below). However whenever I try to do anything with the dataframe I get the exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 108, localhost, executor driver): org.elasticsearch.hadoop.EsHadoopIllegalStateException: Position for 'oedocumentrecordset.md5sum' not found in row; typically this is caused by a mapping inconsistency The field mentioned varies. If I just try to do a count I get a different name. This index works perfectly find in Kibana and access via curl commands and via other code. I found a bug listed on GitHub about this but it is marked as fixed. Same issue if I use SPARK SQL to read the dataframe. Here is my code snippet: val esdf = spark.read.format(\"es\").options(esParams).load(\"coalesce-oedocument\") esdf.createOrReplaceTempView(\"esdocs\") esdf.printSchema() esdf.groupBy(\"oedocumentrecordset.documentsource\").count().show() Here is the schema and accompanying stack trace. root |-- coalesceentity: struct (nullable = true) | |-- datecreated: timestamp (nullable = true) | |-- entityid: string (nullable = true) | |-- entityidtype: string (nullable = true) | |-- lastmodified: timestamp (nullable = true) | |-- name: string (nullable = true) | |-- objectkey: string (nullable = true) | |-- source: string (nullable = true) | |-- status: string (nullable = true) | |-- title: string (nullable = true) | |-- version: string (nullable = true) |-- oedocumentrecordset: struct (nullable = true) | |-- categories: string (nullable = true) | |-- content: string (nullable = true) | |-- contentlength: long (nullable = true) | |-- datasource: string (nullable = true) | |-- dateingested: timestamp (nullable = true) | |-- documentdate: timestamp (nullable = true) | |-- documentlastmodifieddate: timestamp (nullable = true) | |-- documentsource: string (nullable = true) | |-- documenttitle: string (nullable = true) | |-- documenttype: string (nullable = true) | |-- issimulation: boolean (nullable = true) | |-- md5sum: string (nullable = true) | |-- ner_date: string (nullable = true) | |-- ner_location: string (nullable = true) | |-- ner_money: string (nullable = true) | |-- ner_organization: string (nullable = true) | |-- ner_percent: string (nullable = true) | |-- ner_person: string (nullable = true) | |-- ner_time: string (nullable = true) | |-- ontologyreference: string (nullable = true) | |-- pmesiipteconomic: float (nullable = true) | |-- pmesiiptinformation: float (nullable = true) | |-- pmesiiptinfrastructure: float (nullable = true) | |-- pmesiiptmilitary: float (nullable = true) | |-- pmesiiptphysicalenvironment: float (nullable = true) | |-- pmesiiptpolitical: float (nullable = true) | |-- pmesiiptsocial: float (nullable = true) | |-- pmesiipttime: float (nullable = true) | |-- sourceuri: string (nullable = true) | |-- tags: string (nullable = true) | |-- wordcount: long (nullable = true) org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 108, localhost, executor driver): org.elasticsearch.hadoop.EsHadoopIllegalStateException: Position for 'oedocumentrecordset.md5sum' not found in row; typically this is caused by a mapping inconsistency at org.elasticsearch.spark.sql.RowValueReader$class.addToBuffer(RowValueReader.scala:60) at org.elasticsearch.spark.sql.ScalaRowValueReader.addToBuffer(ScalaEsRowValueReader.scala:32) at org.elasticsearch.spark.sql.ScalaRowValueReader.addToMap(ScalaEsRowValueReader.scala:118) at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:810) at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:700) at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466) at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391) at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286) at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259) at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365) at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92) at org.elasticsearch.spark.rdd.AbstractEsRDDIterator.hasNext(AbstractEsRDDIterator.scala:61) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)",
    "website_area": "discuss"
  },
  {
    "id": "268c9d75-89f3-43cc-afe5-31b62db7c57d",
    "url": "https://discuss.elastic.co/t/hdfs-repository-connectionrefused/173750",
    "title": "Hdfs repository ConnectionRefused",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "hyhong",
    "date": "March 25, 2019, 12:50pm April 5, 2019, 8:39pm May 3, 2019, 8:39pm",
    "body": "When I create hdfs repository,throws a exception.As follows:",
    "website_area": "discuss"
  },
  {
    "id": "35a9d9fe-2e5e-48fa-91bc-062e0f790168",
    "url": "https://discuss.elastic.co/t/what-is-the-best-way-to-collect-yarn-application-logs-from-hdfs/173641",
    "title": "What is the best way to collect yarn application logs from hdfs?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Drahkar",
    "date": "March 24, 2019, 2:09pm April 5, 2019, 8:37pm May 3, 2019, 8:37pm",
    "body": "We have a dynamic infrastructure for Hadoop, which means the yarn application logs only exist for a limited period of time. Currently we are using Splunk HadoopConnect to ingest those logs as a live feed into Splunk. However this requires the installation of the full Splunk server on the cluster to accomplish, which is not only resource intensive, but not the most efficient thing to do every time we spin up a dynamic Hadoop cluster. Does Elasticsearch, Logstash, etc have an alternative to HadoopConnect that could be used to collect the Yarn application logs out of HDFS and feed them into the ELK stack?",
    "website_area": "discuss"
  },
  {
    "id": "90f0e6ab-f2ee-472c-ba72-80c943900140",
    "url": "https://discuss.elastic.co/t/installing-elasticsearch-6-7-0-on-cdh-5-8-2-cdh-5-13-1/175088",
    "title": "Installing Elasticsearch 6.7.0 on CDH 5.8.2 / CDH 5.13.1",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "mohammedayub44",
    "date": "April 3, 2019, 2:46am April 4, 2019, 2:28am May 2, 2019, 2:37am",
    "body": "Hi, I'm trying to install the elastic service on CDH platform(5.8.2 for now). I have been successful in installing it on just one node using the defaults (i.e. it runs on 127.0.0.1:9200 and writes data to locally to linux system). I also successfully tested this Elasticsearch-Hadoop Example on localhost and it runs perfectly fine. However running on localhost does not help me too much, I want it to be setup like a cluster on all my CDH cluster nodes (I have 9 of them) and its reads/writes data to my CDH datanodes instead where data disks are mounted as below I see that there are variables in the config/elasticsearch.yml that I can change but I'm not sure about which one exactly to modify. Are these the below ones - path.data: /path/to/data network.host: 0.0.0.0 discovery.zen.ping.unicast.hosts: [\"0.0.0.0\", \"::0\"] What values do I set them to and are there any other variables in addition to this I need to set ? I did look into the Requirements section of the Installation document, it just shows to run it on localhost, it doesn't help you much on the settings the variables on Platforms Cloudera/Hortonworks/Mapr etc. I'm really new to this elasticsearch world and would appreciate any help on this. I would be happy to provide you with more details if necessary. Thanks ! Mohammed Ayub",
    "website_area": "discuss"
  },
  {
    "id": "5e019d7b-f15b-41d6-a997-e65641860179",
    "url": "https://discuss.elastic.co/t/all-data-from-elasticspark-sql-queries-returns-as-null/173560",
    "title": "All data from Elasticspark sql queries returns as null",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "bytemedwb",
    "date": "March 22, 2019, 9:12pm April 2, 2019, 9:13am April 2, 2019, 11:33am April 30, 2019, 11:33am",
    "body": "So having gotten further by doing a build of the 8.0.0-SNAPSHOT from source, I now have a new issue. I am able to create data frames and print schemas. However, every query returns null for every field. Here is my code and the schemas: val dsParams = Map( \"es.nodes.wan.only\" -> \"true\", \"es.port\" -> \"9201\", \"es.net.ssl\" -> \"true\", \"es.net.ssl.cert.allow.self.signed\" -> \"true\", \"es.net.ssl.truststore.location\" -> \"file:///etc/pki/java/truststore.jks\", \"es.net.ssl.truststore.type\" -> \"JKS\", \"es.net.ssl.truststore.pass\"-> \"changeit\", \"es.net.ssl.keystore.location\" -> \"file:///etc/pki/java/keystore.jks\", \"es.net.ssl.keystore.type\" -> \"jks\", \"es.net.ssl.keystore.pass\"-> \"changeit\", \"es.clustername\" -> \"BDP_oedl.dev.oedl.tradoc.army.mil\", \"es.nodes\" -> \"oedevnode06.dev.oedl.tradoc.army.mil\" ) // Create DataFrame using the \"es\" format val docFrame = spark.read.format(\"es\").options(dsParams).load(\"coalesce-oedocument\") docFrame.createOrReplaceTempView(\"esdocview\") // Create DataFrame using the \"es\" format val eventFrame = spark.read.format(\"es\").options(dsParams).load(\"coalesce-oeevent\") eventFrame.createOrReplaceTempView(\"eseventview\") docFrame.printSchema() eventFrame.printSchema() val esdocs = spark.sql(\"select recordset.coalesceentity.objectkey from esdocview limit 20\") esdocs.show() val esevents = spark.sql(\"select recordset.coalesceentity.objectkey from eseventview limit 20\") esdocs.show() root |-- recordset: struct (nullable = true) | |-- coalesceentity: struct (nullable = true) | | |-- datecreated: timestamp (nullable = true) | | |-- entityid: string (nullable = true) | | |-- entityidtype: string (nullable = true) | | |-- lastmodified: timestamp (nullable = true) | | |-- name: string (nullable = true) | | |-- objectkey: string (nullable = true) | | |-- source: string (nullable = true) | | |-- status: string (nullable = true) | | |-- title: string (nullable = true) | | |-- version: string (nullable = true) | |-- oedocumentrecordset: struct (nullable = true) | | |-- categories: string (nullable = true) | | |-- content: string (nullable = true) | | |-- contentlength: long (nullable = true) | | |-- datasource: string (nullable = true) | | |-- dateingested: timestamp (nullable = true) | | |-- documentdate: timestamp (nullable = true) | | |-- documentlastmodifieddate: timestamp (nullable = true) | | |-- documentsource: string (nullable = true) | | |-- documenttitle: string (nullable = true) | | |-- documenttype: string (nullable = true) | | |-- issimulation: boolean (nullable = true) | | |-- md5sum: string (nullable = true) | | |-- ner_date: string (nullable = true) | | |-- ner_location: string (nullable = true) | | |-- ner_money: string (nullable = true) | | |-- ner_organization: string (nullable = true) | | |-- ner_percent: string (nullable = true) | | |-- ner_person: string (nullable = true) | | |-- ner_time: string (nullable = true) | | |-- ontologyreference: string (nullable = true) | | |-- pmesiipteconomic: float (nullable = true) | | |-- pmesiiptinformation: float (nullable = true) | | |-- pmesiiptinfrastructure: float (nullable = true) | | |-- pmesiiptmilitary: float (nullable = true) | | |-- pmesiiptphysicalenvironment: float (nullable = true) | | |-- pmesiiptpolitical: float (nullable = true) | | |-- pmesiiptsocial: float (nullable = true) | | |-- pmesiipttime: float (nullable = true) | | |-- sourceuri: string (nullable = true) | | |-- tags: string (nullable = true) | | |-- wordcount: long (nullable = true) root |-- recordset: struct (nullable = true) | |-- coalesceentity: struct (nullable = true) | | |-- datecreated: timestamp (nullable = true) | | |-- entityid: string (nullable = true) | | |-- entityidtype: string (nullable = true) | | |-- lastmodified: timestamp (nullable = true) | | |-- name: string (nullable = true) | | |-- objectkey: string (nullable = true) | | |-- source: string (nullable = true) | | |-- status: string (nullable = true) | | |-- title: string (nullable = true) | | |-- version: string (nullable = true) | |-- eventrecordset: struct (nullable = true) | | |-- actiongeoadm1code: string (nullable = true) | | |-- actiongeoadm2code: string (nullable = true) | | |-- actiongeocountrycode: string (nullable = true) | | |-- actiongeofeatureid: string (nullable = true) | | |-- actiongeofullname: string (nullable = true) | | |-- actiongeolocation: struct (nullable = true) | | | |-- lat: double (nullable = true) | | | |-- lon: double (nullable = true) | | |-- actiongeotype: integer (nullable = true) | | |-- avgtone: float (nullable = true) | | |-- datasource: string (nullable = true) | | |-- dateadded: integer (nullable = true) | | |-- datetime: timestamp (nullable = true) | | |-- day: integer (nullable = true) | | |-- eventbasecode: string (nullable = true) | | |-- eventcode: string (nullable = true) | | |-- eventrootcode: string (nullable = true) | | |-- fractiondate: float (nullable = true) | | |-- globaleventid: integer (nullable = true) | | |-- goldsteinscale: float (nullable = true) | | |-- isrootevent: integer (nullable = true) | | |-- issimulation: boolean (nullable = true) | | |-- monthyear: integer (nullable = true) | | |-- numarticles: integer (nullable = true) | | |-- nummentions: integer (nullable = true) | | |-- numsources: integer (nullable = true) | | |-- ontologyreference: string (nullable = true) | | |-- pmesiipteconomic: float (nullable = true) | | |-- pmesiiptinformation: float (nullable = true) | | |-- pmesiiptinfrastructure: float (nullable = true) | | |-- pmesiiptmilitary: float (nullable = true) | | |-- pmesiiptphysicalenvironment: float (nullable = true) | | |-- pmesiiptpolitical: float (nullable = true) | | |-- pmesiiptsocial: float (nullable = true) | | |-- pmesiipttime: float (nullable = true) | | |-- quadclass: integer (nullable = true) | | |-- sourceurl: string (nullable = true) | | |-- tags: string (nullable = true) | | |-- year: integer (nullable = true) Both queries return nulls. I am using Elastic 5.4.3 . All works fine in Kibana and java.",
    "website_area": "discuss"
  },
  {
    "id": "dece792d-b1c6-407a-b60c-1822c91f7abf",
    "url": "https://discuss.elastic.co/t/is-hadoop-the-right-solution-for-distributed-computation-with-es/174971",
    "title": "Is Hadoop the right solution for distributed computation with ES?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "nicom",
    "date": "April 2, 2019, 11:23am April 30, 2019, 11:22am",
    "body": "I have a elasticsearch cluster (300Go) with 2 nodes (I know.. 3 is min). We have a java program that use the java Rest API to fetch all the data from ES cluster and execute custom clustering algorithms. To avoid network bottleneck between ES cluster and the program we currently execute the program locally into one of the Elasticsearch node, which is super fast. furthermore we do it in parallel: by cutting the data in N distinct partitions, and fetching + clustering the data in parallel. (NOTE: clustering is not trivial as it seems. a priori the goal is not to change our custom clustering method here) This solution is simple and works well for real time processing. However when we need to reprocess all the data (when we improved the clustering algorithm for instance) we have the following issue: this ES node is suffering (90% CPU) we have some rare case of Out of Memory, but still we need to fix that. this approach is not scalable/distributed to fully use the distribution of computation though the partitions. I am a newbie in Hadoop and wonder whether using Hadoop would solve such pb. my main concern is how to sync data between ES cluster and the distributed processing platform /how to avoid 400 Go download. how to limit/avoid big change to our clustering java program. Thanks !",
    "website_area": "discuss"
  },
  {
    "id": "c6606d14-8184-4c48-9a0b-ded1e67f6127",
    "url": "https://discuss.elastic.co/t/elasticsearch-hive-datatype-cast-issues-errors-while-query-the-data-through-hive/170251",
    "title": "Elasticsearch - Hive datatype cast issues - Errors while query the data through HIVE",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "ksivaus",
    "date": "February 27, 2019, 9:58pm March 27, 2019, 9:58pm",
    "body": "Hi All, I am currently working with Elasticsearch - Hive integration . I have created the index and loaded the data successfully, with the below mapping... curl -XPOST localhost:9200/sales_v3 -d '{ \"settings\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 1 }, \"mappings\" : { \"fact\" : { \"_source\" : { \"enabled\" : false }, \"properties\" : { \"PROD_ID\" : { \"type\" : \"integer\" }, \"CUST_ID\" : { \"type\" : \"integer\" }, \"TIME_ID\" : { \"type\" : \"date\", \"format\" : \"yyyy-MM-dd\" }, \"CHANNEL_ID\" : { \"type\" : \"text\" }, \"PROMO_ID\" : { \"type\" : \"integer\" }, \"QUANTITY_SOLD\" : { \"type\" : \"text\" }, \"AMOUNT_SOLD\" : { \"type\" : \"double\" }, \"UNIT_COST\" : { \"type\" : \"double\" }, \"UNIT_PRICE\" : { \"type\" : \"double\" } } } } }' my data in json file is as below - head -6 /tmp/salesv3_newe.json { \"index\" : { \"_index\" : \"sales_v3\", \"_type\" : \"fact\", \"_id\" : \"1\"} } { \"prod_id\": 13, \"cust_id\": 987, \"time_id\": \"1998-01-10\", \"channel_id\": 3, \"promo_id\": 999, \"quantity_sold\": 1, \"amount_sold\": 1232.16, \"unit_cost\": 783.03, \"unit_price\": \"1232.16\" } { \"index\" : { \"_index\" : \"sales_v3\", \"_type\" : \"fact\", \"_id\" : \"2\"} } { \"prod_id\": 13, \"cust_id\": 1660, \"time_id\": \"1998-01-10\", \"channel_id\": 3, \"promo_id\": 999, \"quantity_sold\": 1, \"amount_sold\": 1232.16, \"unit_cost\": 783.03, \"unit_price\": \"1232.16\" } { \"index\" : { \"_index\" : \"sales_v3\", \"_type\" : \"fact\", \"_id\" : \"3\"} } { \"prod_id\": 13, \"cust_id\": 1762, \"time_id\": \"1998-01-10\", \"channel_id\": 3, \"promo_id\": 999, \"quantity_sold\": 1, \"amount_sold\": 1232.16, \"unit_cost\": 783.03, \"unit_price\": \"1232.16\" } Later I have created my Hive external table as below - CREATE EXTERNAL TABLE sales ( prod_id INT, cust_id INT, time_id TIMESTAMP, channel_id STRING, promo_id INT, quantity_sold STRING, amount_sold DOUBLE, unit_cost DOUBLE, unit_price DOUBLE) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.nodes' = 'localhost:9200', 'es.resource' = 'sales_v3/fact', 'es.query' = '?q=*'); I am using the Elasticsearch 6.2.4 and the same elasticsearch-hadoop jar ls -l /usr/lib/hive/lib/elastic* -rwxr-xr-x 1 root root 890273 Feb 25 12:34 /usr/lib/hive/lib/elasticsearch-hadoop-6.2.4.jar When I query the data - this is error message I see. How would I resolve this. Please help and Thanks in advance. > select * from sales; OK Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.LongWritable cannot be cast to org.apache.hadoop.io.Text Time taken: 0.473 seconds Thanks ksivaus",
    "website_area": "discuss"
  },
  {
    "id": "7d0533e3-a106-4158-ab10-3c57ff1dfbcb",
    "url": "https://discuss.elastic.co/t/client-only-routing-specified-but-no-client-nodes-with-http-enabled-were-found-in-the-cluster/167466",
    "title": "Client-only routing specified but no client nodes with HTTP-enabled were found in the cluster",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "saklar",
    "date": "February 7, 2019, 2:12pm February 11, 2019, 9:45pm February 12, 2019, 11:46am February 15, 2019, 4:59pm March 15, 2019, 4:59pm",
    "body": "H, We have elasticsearch 6.5.4 version and Cloudera 5.4.7-1 version. We installed elasticsearch-hadoop-2.1.1.jar to connect between hdfs and elasticsearch. Then the hive table below were created to fetch the data from elasticsearch. CREATE EXTERNAL TABLE TEST_ES_TBL ( st string) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.nodes' = '10.51.29.105', 'es.port' = '9200', 'es.resource' = 'test/docs', 'es.mapping.names' = 'st: str', 'es.index.auto.create'='true'); But it gives the error like that; java.io.IOException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Client-only routing specified but no client nodes with HTTP-enabled were found in the cluster... We also set the \"es.nodes.client.only\" = \"true\" but nothing happens. Can you help us to solve the problem, please? What is the point we miss? Any help would be appreciated. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "3da2fa44-8978-4cff-a6ad-98e651d14875",
    "url": "https://discuss.elastic.co/t/how-es-schema-is-determined-while-reading-using-hadoop/167694",
    "title": "How ES schema is determined while reading using hadoop",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Sonny_Heer",
    "date": "February 9, 2019, 12:11am February 11, 2019, 9:54pm February 11, 2019, 11:06pm March 11, 2019, 11:06pm",
    "body": "Env: using this jar: elasticsearch-spark-13_2.10-5.1.1.jar sqlContext.read.format(\"org.elasticsearch.spark.sql\").option(\"es.nodes\",es_url).option(\"es.port\", \"443\").option(\"es.nodes.wan.only\", \"true\").option(\"es.net.ssl\", \"true\").option(\"es.read.field.as.array.include\",array_with_comma).option(\"es.mapping.date.rich\",\"false\").option(\"es.read.field.exclude\", exclude_with_comma).option(\"es.read.field.include\", \"\").option(\"pushdown\", \"true\").load(es_index) not passing any args except for exclude fields in which case we exclude a couple from top level. The problem we have is missing fields in the dataframe.printSchema()... Does it use _mapping to figure out the schema or sampling? I didn't find any docs on this. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "1a13846f-395b-4384-961c-bef8144fe744",
    "url": "https://discuss.elastic.co/t/cascading-integration-removed-in-7-0-0/167934",
    "title": "Cascading integration removed in 7.0.0",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "james.baiera",
    "date": "February 11, 2019, 9:58pm February 11, 2019, 9:58pm February 11, 2019, 9:58pm",
    "body": "Please be advised that as of ES-Hadoop version 6.6.0 the Cascading integration in ES-Hadoop has been deprecated, and has been removed from the project starting in version 7.0.0. For more information about this deprecation and removal, please see the related Github issue.",
    "website_area": "discuss"
  },
  {
    "id": "5249cb8f-1d71-46c0-bc10-56b2f5223b3e",
    "url": "https://discuss.elastic.co/t/found-duplicate-column-s-in-the-data-schema-need-help-on-how-to-load-such-index-data-into-spark-dataframe/167517",
    "title": "Found duplicate column(s) in the data schema, Need help on how to load such index data into Spark Dataframe",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Yasmeenc",
    "date": "February 7, 2019, 7:25pm February 11, 2019, 9:50pm March 11, 2019, 9:50pm",
    "body": "Hi Team, I am trying to read data from elasticsearch index and write into a spark dataframe, but the index has same field name with different cases(upper/lower case) below is the mapping, and the error I am getting is pyspark.sql.utils.AnalysisException: u'Found duplicate column(s) in the data schema: providercolumn;' Can you please help on how do I deal with this scenario { INDEXNAME: { \"mappings\": { Type: { \"properties\": { \"Providercolumn: { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"providercolumn: { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }",
    "website_area": "discuss"
  },
  {
    "id": "69e24a5a-0ccc-4358-8636-e965119ccfb0",
    "url": "https://discuss.elastic.co/t/problem-with-retrieving-data-from-elasticsearch-by-spark/166790",
    "title": "Problem with retrieving data from Elasticsearch by Spark",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Yasmeenc",
    "date": "February 2, 2019, 3:16am February 11, 2019, 9:42pm March 11, 2019, 9:43pm",
    "body": "Hi Team, I'm quite new to ES and ES-Hadoop. The code for pulling the data out from ES is like below: es_read_conf = { \"es.nodes\" : \"\", \"es.port\" : \"80\", \"es.resource\" : 'temprollover/rollover', \"es.input.json\": \"yes\" } es_rdd = sc.newAPIHadoopRDD( inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\", keyClass=\"org.apache.hadoop.io.NullWritable\", valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=es_read_conf) I am trying to retrieve data from Elasticsearch and trying to convert into RDD and getting the below error, Can you please help Traceback (most recent call last): File \"/home/hadoop/rdd-spark.py\", line 21, in conf=es_read_conf) File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 751, in newAPIHadoopRDD File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in call File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD. : org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: No data nodes with HTTP-enabled available at org.elasticsearch.hadoop.rest.InitializationUtils.filterNonDataNodesIfNeeded(InitializationUtils.java:159) at org.elasticsearch.hadoop.rest.RestService.findPartitions(RestService.java:223) at org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:412)",
    "website_area": "discuss"
  },
  {
    "id": "25e9c0c0-e2e5-4878-a5aa-4a39a1a88dc4",
    "url": "https://discuss.elastic.co/t/cannot-connect-to-elasticsearch-from-hive-using-aws-instance-and-docker-container/165645",
    "title": "Cannot connect to elasticsearch from hive using Aws instance and Docker Container",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "sanjayap",
    "date": "January 24, 2019, 7:06pm February 11, 2019, 9:40pm March 11, 2019, 9:46pm",
    "body": "Hi, I am running 2 different docker containers on AWS instance. One for HDP sandbox and other for elasticsearch. Both the containers are up and running on the respective port (i.e., HDP on 8080 and elasticsearch on 9400). Have also added the elastic-hadoop jar. But when i am trying to read data from elasticsearch , i am getting the below error Blockquote CREATE EXTERNAL TABLE hd2 (users String, message String) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.resource'='hdp/json', 'es.nodes'='5.21.33.44', 'es.port'='9400',\"es.index.auto.create\"=\"TRUE\", 'es.nodes.wan.only'='TRUE','es.query'='?q=users:kimchy'); Blockquote 06:04:30,489 ERROR [60f09da6-50ba-40d1-8def-069859df4d29 HiveServer2-Handler-Pool: Thread-71]: rest.NetworkClient (:()) - Node [127.0.0.1:9400] failed (Connection refused (Connection refused)); no other nodes left - aborting... 2019-01-24T06:04:30,489 INFO [60f09da6-50ba-40d1-8def-069859df4d29 HiveServer2-Handler-Pool: Thread-71]: conf.HiveConf (HiveConf.java:getLogIdVar(5080)) - Using the default value passed in for log id: 60f09da6-50ba-40d1-8def-069859df4d29 2019-01-24T06:04:30,489 INFO [60f09da6-50ba-40d1-8def-069859df4d29 HiveServer2-Handler-Pool: Thread-71]: session.SessionState (:()) - Resetting thread name to HiveServer2-Handler-Pool: Thread-71 2019-01-24T06:04:30,493 INFO [HiveServer2-Handler-Pool: Thread-71]: conf.HiveConf (HiveConf.java:getLogIdVar(5080)) - Using the default value passed in for log id: 60f09da6-50ba-40d1-8def-069859df4d29 2019-01-24T06:04:30,494 INFO [60f09da6-50ba-40d1-8def-069859df4d29 HiveServer2-Handler-Pool: Thread-71]: conf.HiveConf (HiveConf.java:getLogIdVar(5080)) - Using the default value passed in for log id: 60f09da6-50ba-40d1-8def-069859df4d29 2019-01-24T06:04:30,489 WARN [HiveServer2-Handler-Pool: Thread-71]: thrift.ThriftCLIService (:()) - Error fetching results: org.apache.hive.service.cli.HiveSQLException: java.io.IOException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:467) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:328) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:910) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_191] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_191] at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_191] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_191] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?] at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at com.sun.proxy.$Proxy66.fetchResults(Unknown Source) ~[?:?] at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:564) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:786) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1837) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1822) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_191] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_191] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191] org.apache.hive.service.cli.HiveSQLException: java.io.IOException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' But i am not sure why it is trying to connect into 127.0.0.0:9400 even though i have sepecified my ip in hive command Any help would be greatly appreciated @james.baiera any suggestions ?",
    "website_area": "discuss"
  },
  {
    "id": "f8eb6bec-548b-4e35-813b-d713eb41fa90",
    "url": "https://discuss.elastic.co/t/repository-hdfs-with-kerberos-failed/166491",
    "title": "Repository-hdfs with kerberos failed",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "",
    "date": "January 31, 2019, 8:06am February 11, 2019, 9:36pm March 11, 2019, 9:36pm",
    "body": "",
    "website_area": "discuss"
  },
  {
    "id": "849a88e7-f2f7-4d5b-970c-dc65e6120b41",
    "url": "https://discuss.elastic.co/t/how-to-ingest-parquet-files-into-elasticsearch/167426",
    "title": "How to ingest Parquet files into Elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "shivakumar_kasha",
    "date": "February 8, 2019, 12:46pm March 7, 2019, 10:50am",
    "body": "Hi, I've parquet files. I want to ingest them into Elasticsearch. Please respond with the possible solutions. I've tried with elasticsearch_loader ( https://github.com/moshe/elasticsearch_loader ) but unable to write the data into elasticsearch. elasticsearch_loader : Load parquet to elasticsearch using elasticsearch_loader elasticsearch_loader --index incidents --type incident parquet file1.parquet Thanks in advance!! --Shivakumar",
    "website_area": "discuss"
  },
  {
    "id": "507d1d8d-fb32-43f8-bd93-f54152a31923",
    "url": "https://discuss.elastic.co/t/how-to-combine-latitude-and-longitude-columns-into-one-location-column-and-make-its-type-to-geo-point-in-elasticsearch-and-hive-integration-elasticsearch-hive-integration/167595",
    "title": "How to combine Latitude and Longitude columns into one Location Column and make it's type to geo_point in Elasticsearch and Hive Integration - Elasticsearch & Hive Integration",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "shivakumar_kasha",
    "date": "February 8, 2019, 11:21am March 8, 2019, 9:16am",
    "body": "Hi Everyone, I'm trying to integrate ES-Hadoop plugin to connect Hive and Elasticsearch. I'm able to connect the Hive and Elasticsearch. The data i'm getting in My elasticsearch index is having lat and long columns separately but i need them to be combined to one geo_point type column so that i can plot geo maps. I tried in the following way: \"LocationLatitude_Tile\" : {\"type\" : \"geo_point\",\"copy_to\":\"Location\"}, \"LocationLongitude_Tile\" : {\"type\" : \"geo_point\",\"copy_to\":\"Location\"}, \"Location\": {\"type\" : \"geo_point\" } But it is not working. Lat and Lon values are not copying to Location field. And the other issue is : I've a Hive table integrated with elasticsearch and i loaded the data into the table. Then I'm able to see the data in Elasticsearch but not in Hive after loading the data into hive using insert command. Could anyone please help me out on these issues. Thanks in advance !! --Shivakumar",
    "website_area": "discuss"
  },
  {
    "id": "34a57f98-2e2e-4451-884d-1284d997ef2c",
    "url": "https://discuss.elastic.co/t/unable-to-create-index-and-insert-data/166286",
    "title": "Unable to create index and insert data?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Abdul_Gaffar_Shaikh",
    "date": "January 31, 2019, 10:14am February 8, 2019, 9:18am March 8, 2019, 9:18am",
    "body": "hey Guys , I am trying to index data from hive into elasticsearch , however when i ran the below query , there is no error and index is not created in elasticsearch. create external table es_names_text (employeeid int,firstname string,title string,state string,laptop string) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.resource' = 'es_names_text/employee', 'es.nodes'='localhost','es.port'='9200','es.index.auto.create' = 'true') ; It is a single node cluster Also if i try to insert data into this , below snippet is show indefinitely Total jobs = 1 Launching Job 1 out of 1 Number of reduce tasks is set to 0 since there's no reduce operator Also i have not created index explicitly on elasticsearch. please let me know how to create it and insert data without waiting for mr job to execute. edit note:- elasticsearch and hadoop and hive are on the same machine but are on installed using different user id's. Hadoop and hive in login A and elasticsearch on login B. does this affect hive connecting to elasticsearch ?",
    "website_area": "discuss"
  },
  {
    "id": "54447eaa-13f9-4160-bd61-0ee880158d89",
    "url": "https://discuss.elastic.co/t/how-do-i-connect-pyspark-to-elasticsearch-with-ssl-and-verify-certs-set-to-false-ask-question/167412",
    "title": "How do I connect PySpark to Elasticsearch with SSL and verify certs set to False? Ask Question",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Brett_Anderson",
    "date": "February 7, 2019, 9:58am February 7, 2019, 10:01am February 7, 2019, 10:02am February 7, 2019, 10:02am February 7, 2019, 10:38am March 7, 2019, 10:35am",
    "body": "Previously I have successfully connected to an Elasticsearch cluster directly from Python with the following code: ssl_context = create_ssl_context() ssl_context.check_hostname = False ssl_context.verify_mode = ssl.CERT_NONE es = Elasticsearch( ES_HOST, http_auth=(ES_USERNAME, ES_PASSWORD), scheme=\"https\", port=ES_PORT, use_ssl=True, verify_certs=False, ssl_context=ssl_context, ca_certs=False ) Now I'm trying to connect to the same cluster using the Pyspark to Elasticsearch connector. Spark has been setup with version 2.4.0 and Hadoop 2.7. I'm using elasticsearch-hadoop-6.1.1 to connect the two. I use the following configuration to connect PySpark with ES: es_live_conf = { \"es.nodes\" : ES_HOST, \"es.port\" : ES_PORT, \"es.resource\" : 'testindex/testdoc', \"es.net.http.auth.user\" : ES_USERNAME, \"es.net.http.auth.pass\" : ES_PASSWORD, \"es.net.ssl\":\"true\", \"es.nodes.resolve.hostname\": \"false\", \"es.net.ssl.cert.allow.self.signed\": \"true\" } Then this code to activate the connection: sc = SparkContext(appName=\"PythonSparkStreaming\") sc.setLogLevel(\"WARN\") es_rdd = sc.newAPIHadoopRDD( inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\", keyClass=\"org.apache.hadoop.io.NullWritable\", valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=es_live_conf) I then get an error as it tries each node. I've added the error as a comment since I exceeded the character limit when it was part of this post. I should note that the same code using a configuration that connects with a local ES cluster works successfully. So the error is purely to do with the SSL connection to the remote cluster. I'm able to access the cluster from the client environment with the command curl --insecure --user ES_USER:ES_PASS -XGET 'https://ES_HOST:ES_PORT/' - so it seems that the client should be able to connect. How do I setup the Spark ES connection with the correct ssl details? I've tried removing the es.net.ssl.cert.allow.self.signed and es.nodes.resolve.hostname properties in the configuration but still receive the same error.",
    "website_area": "discuss"
  },
  {
    "id": "cf7134d8-24cf-4640-9baf-a6a30f284e7a",
    "url": "https://discuss.elastic.co/t/elasticsearch-for-apache-hadoop-doesnt-work/163971",
    "title": "Elasticsearch for Apache Hadoop doesn't work",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Arthur_Luz",
    "date": "January 12, 2019, 3:20am January 22, 2019, 6:28pm January 22, 2019, 8:58pm January 23, 2019, 12:36am January 23, 2019, 11:18am January 23, 2019, 11:33am January 23, 2019, 1:52pm January 31, 2019, 10:11am February 25, 2019, 9:08am",
    "body": "Hello, guys. I need some help with Elasticsearch for Apache Hadoop connector. I have on my environment a hortonworks hdp cluster and with hive. I need to create a external table for elasticsearch to move data from my hdfs to elasticsearch. Ok, I followed elasticsearch documentation and I used Add jar command to import jars. Ok. it's fine. But, when I try to create external table I receive this error: Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoClassDefFoundError org/apache/commons/httpclient/Credentials) (state=08S01,code=1) I'm using the version 3.1 of hdp cluster. What I need to do to make this work? can you help me? Best regards!",
    "website_area": "discuss"
  },
  {
    "id": "617a458a-6c60-43ee-ac0a-4d236329f891",
    "url": "https://discuss.elastic.co/t/elastic-spark-deprecated-search-type-scan-showing-up-in-post-url-from-spark-2-4-and-returning-0-hits/166383",
    "title": "Elastic - spark : deprecated search_type=scan showing up in post url from spark 2.4 and returning 0 hits",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "johnmarshall",
    "date": "January 30, 2019, 3:48pm January 30, 2019, 3:26pm February 27, 2019, 3:26pm",
    "body": "Hi Team Am trying to fetch data from elastic server into spark 2.4 environment using \"org.elasticsearch.spark.sql\" connector, but even though i got all the configs right, there is no data received on the spark engine Using the below config to connect to Elastic from spark On analyzing with fiddler found that the request created had a search_type=scan param in the query string, which i guess is no longer supported http://NodeURL/index/type/_search?search_type=scan&scroll=5m&size=50&preference=_shards%3A3%3B_local On opening this on the browser didn't receive any data, but the same query without the search_type param worked as expected. Am wondering when the above param is no longer supported, why is it being appended in the request url by the elasticsearch-spark connector Can someone please help me figure out a proper solution Spark - 2.4.0 scala - 2.11 elasticsearch-spark-20_2.11 (6.6.0) Thanks John",
    "website_area": "discuss"
  },
  {
    "id": "4a1b2ffa-f34e-4304-8056-18112ef441d2",
    "url": "https://discuss.elastic.co/t/error-unsupported-unknown-elasticsearch-version-6-x-x/164846",
    "title": "Error: Unsupported/Unknown Elasticsearch version 6.X.X",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "spagano",
    "date": "January 22, 2019, 1:52am January 22, 2019, 12:27am February 19, 2019, 12:08am",
    "body": "Hi, I saw the same problem on too many places. I have Hortonworks 2.6.4, Hadoop 2.7.3.2.6.4.0-91, Hive 1.2.1000.2.6.4.0-91, and, on the same host, I have installed Elasticsearch 6.4.3. Then, I do the following by command line: wget -P /tmp https://artifacts.elastic.co/downloads/elasticsearch-hadoop/elasticsearch-hadoop-6.4.3.zip; ok unzip /tmp/elasticsearch-hadoop-6.4.3.zip -d /tmp; ok hdfs dfs -copyFromLocal /tmp/elasticsearch-hadoop-6.4.3/dist/elasticsearch-hadoop-6.4.3.jar /tmp ok On beeline o hive from dbeaver I do this: ADD JAR hdfs:///tmp/elasticsearch-hadoop-6.4.3.jar; LIST JAR; ok CREATE EXTERNAL TABLE artists ( id BIGINT, name STRING, links STRUCT<url:STRING, picture:STRING>) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.nodes' = 'localhost' , 'es.resource' = 'radio/artists'); ok select * from artists; wrong! I GET \"java.io.IOException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.4.3\" Why?? my ES is 6.4.3 and my ES-Hadoop 6.4.3 I saw the java code from elasticsearch-hadoop project and found this: For InicializationUtils.java if (!(esVersion.startsWith(\"1.\") || esVersion.startsWith(\"2.\"))) { throw new EsHadoopIllegalArgumentException(\"Unsupported/Unknown Elasticsearch version \" + esVersion); } This is a validation that my ES has to be 1.x or 2.x???? What is wrong here?? Thanks in advance!!! Sergio.",
    "website_area": "discuss"
  },
  {
    "id": "0b16b9b7-83ac-4626-b8d2-1938f417af2d",
    "url": "https://discuss.elastic.co/t/support-for-hadoop-3-0/164661",
    "title": "Support for hadoop 3.0",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Saul_Tawil",
    "date": "January 17, 2019, 3:06pm February 14, 2019, 3:06pm",
    "body": "When can we expect support for hadoop 3.0",
    "website_area": "discuss"
  },
  {
    "id": "19d2654a-c305-4f95-9b4d-66a29e2485ce",
    "url": "https://discuss.elastic.co/t/while-executing-the-spark-job-in-cluster-mode-cannot-detect-es-version-hosted-by-aws/164570",
    "title": "While executing the spark Job in cluster mode Cannot detect ES version hosted by AWS",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "vinodanumarla",
    "date": "January 17, 2019, 5:26am January 17, 2019, 5:26am February 14, 2019, 5:26am",
    "body": "While I am executing a spark job to load data from Json to ES cluster hosted by AWS using cluster mode with the below mentioned configuration settings getting error as Error :- \"py4j.protocol.Py4JJavaError: An error occurred while calling o2519.save. : org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' \" Configuration settings : - DataFrame.write.format(\"org.elasticsearch.spark.sql\") .option(\"es.nodes.wan.only\", \"true\") .option(\"es.port\", \"443\") .option(\"es.net.ssl\", \"true\") .option(\"es.net.ssl.cert.allow.self.signed\", \"true\") .option(\"es.nodes\", \"ssssss.us-east-2.es.amazonaws.com\") .option(\"es.mapping.id\", \"doc_id\") .mode(\"append\") .save(\"index/type\") Though I mentioned \"es.nodes.wan.only\", \"true\" it is getting ignored in execution time. Any help would be appreciated.",
    "website_area": "discuss"
  },
  {
    "id": "745d6ca4-7a96-4ed2-be08-af7924049927",
    "url": "https://discuss.elastic.co/t/when-is-useful-to-use-hadoop-with-elasticsearch/164531",
    "title": "When is useful to use Hadoop with elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "elk2",
    "date": "January 16, 2019, 8:13pm February 13, 2019, 8:13pm",
    "body": "I don't understand when is useful to use Hadoop with elasticsearch. Hadoop and Kafka can live together with elasticsearch?",
    "website_area": "discuss"
  },
  {
    "id": "37d37126-78fd-4146-8954-56ba0597f910",
    "url": "https://discuss.elastic.co/t/could-not-export-table-on-to-elasticsearch/163595",
    "title": "Could not export table on to ElasticSearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "nara",
    "date": "January 9, 2019, 5:28pm February 6, 2019, 5:28pm",
    "body": "Hi, I am using Spark 2.2.0, Hadoop distribution:Amazon 2.7.3, Hail 0.2. I am trying to export a table on to ElasticSearch and I am experiencing this error. (Note: Both EMR and ES are hosted on Amazon) >>> mt=l.export_elasticsearch(ht,host='https://xxxxxxxxx.us-east-1.es.amazonaws.com',port=80,index='singlevcf',index_type='variant',block_size=1000,config=None,verbose=True) Config Map(es.nodes -> https://xxxxxxxxxxxx.us-east-1.es.amazonaws.com, es.port -> 80, es.batch.size.entries -> 1000, es.index.auto.create -> true) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"<decorator-gen-998>\", line 2, in export_elasticsearch File \"/opt/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py\", line 560, in wrapper File \"/opt/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py\", line 2052, in export_elasticsearch File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__ File \"/opt/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py\", line 224, in deco hail.utils.java.FatalError: SSLException: Unrecognized SSL message, plaintext connection? Java stack trace: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:327) at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:97) at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:83) at org.elasticsearch.spark.sql.package$SparkDataFrameFunctions.saveToEs(package.scala:49) at is.hail.io.ElasticsearchConnector$.export(ElasticsearchConnector.scala:47) at is.hail.io.ElasticsearchConnector$.export(ElasticsearchConnector.scala:21) at is.hail.io.ElasticsearchConnector.export(ElasticsearchConnector.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:280) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:214) at java.lang.Thread.run(Thread.java:748) org.elasticsearch.hadoop.rest.EsHadoopTransportException: javax.net.ssl.SSLException: Unrecognized SSL message, plaintext connection? at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:124) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:380) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:344) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:348) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:158) at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:574) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:320) at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:97) at org.elasticsearch.spark.sql.EsSparkSQL$.saveToEs(EsSparkSQL.scala:83) at org.elasticsearch.spark.sql.package$SparkDataFrameFunctions.saveToEs(package.scala:49) at is.hail.io.ElasticsearchConnector$.export(ElasticsearchConnector.scala:47) at is.hail.io.ElasticsearchConnector$.export(ElasticsearchConnector.scala:21) at is.hail.io.ElasticsearchConnector.export(ElasticsearchConnector.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:280) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:214) at java.lang.Thread.run(Thread.java:748) javax.net.ssl.SSLException: Unrecognized SSL message, plaintext connection? at sun.security.ssl.InputRecord.handleUnknownRecord(InputRecord.java:710) at sun.security.ssl.InputRecord.read(InputRecord.java:527) at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:983) at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1385) at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:757) at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:123) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) at org.apache.commons.httpclient.HttpConnection.flushRequestOutputStream(HttpConnection.java:828) at org.apache.commons.httpclient.HttpMethodBase.writeRequest(HttpMethodBase.java:2116) at org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1096) at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:398) at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171) at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397) at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323) at org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.execute(CommonsHttpTransport.java:478) at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:112) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:380) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:344) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:348) Hail version: 0.2.7-e08cc2a17c4a Error summary: SSLException: Unrecognized SSL message, plaintext connection? I can curl the ElasticSearch url from the cluster though. Any help appreciated. Thanks.",
    "website_area": "discuss"
  },
  {
    "id": "a676fcf7-355f-4ada-8cf1-9860a8cd7121",
    "url": "https://discuss.elastic.co/t/insert-into-externe-table-hive/163482",
    "title": "Insert into externe table Hive",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "anass",
    "date": "January 9, 2019, 9:08am February 6, 2019, 9:06am",
    "body": "Hi, i want to insert into externe table Hive but i have this error. Error: java.lang.NoClassDefFoundError: org/elasticsearch/hadoop/mr/EsOutputFormat$EsOutputCommitter ADD JAR hdfs:/user/lib/elastic/commons-httpclient-3.1.jar; ADD JAR hdfs:/user/lib/elastic/elasticsearch-hadoop-hive-2.3.3.jar; CREATE EXTERNAL TABLE IF NOT EXISTS test ( id_indicateur string , date_indicateur timestamp, DQ_cons_abo_date_reference date , DQ_cons_type_consentement string, DQ_cons_source string , DQ_cons_etat string , DQ_abo_kafka_statut string , DQ_abo_code_retour string , DQ_abo_interpretation_etat string , DQ_cons_abo_nb_bp_prm bigint , DQ_cons_abo_nb_bp_prm_cumul bigint ) COMMENT 'Table externe pour export vers ELK' STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.resource'='{ELASTIC_INDEX_NAME_VALUE}/{TYPE_ABO_VALUE}','es.index.auto.create'='false','es.nodes'='${NODES_ELASTICSEARCH_VALUE}','es.nodes.discovery'='false','es.nodes.client.only'='fa lse','es.nodes.wan.only'='true'); INSERT INTO TABLE test SELECT 'A' AS id_indicateur, CURRENT_TIMESTAMP as date_indicateur, date_sub(CURRENT_DATE(),1) AS DQ_cons_abo_date_reference, B AS dq_cons_type_consentement, t_iddemandeur AS DQ_cons_source, 'ACCEPTE' AS DQ_cons_etat, C AS abo_kfk_statut, D AS abo_dem_code_retour, 'ABO_ACCEPTE' AS DQ_abo_interpretation_etat, count(distinct (case when TO_DATE(date_de_modification) = date_sub(CURRENT_DATE(),1) then CONCAT(bp,pdl) else null end)) as DQ_cons_abo_nb_bp_prm , count(distinct CONCAT(bp,pdl)) AS DQ_cons_abo_nb_bp_prm_cumul FROM test2 GROUP BY codeobjetconsentement, t_iddemandeur, abo_kfk_statut, abo_dem_code_retour ;",
    "website_area": "discuss"
  },
  {
    "id": "b0624dac-04d4-437d-a1fd-7d3c87ef26d4",
    "url": "https://discuss.elastic.co/t/writing-dataframe-to-elasticsearch-using-scala/162228",
    "title": "Writing Dataframe to Elasticsearch using scala",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Ram18",
    "date": "December 27, 2018, 12:23pm January 2, 2019, 4:44pm January 4, 2019, 7:27am January 7, 2019, 7:29am February 4, 2019, 7:30am",
    "body": "Hi, How to map the fields in the dataframe using scala code to load data into ES. For example - string to int or string to date field or string to array. please help me to achieve the same.",
    "website_area": "discuss"
  },
  {
    "id": "67692d1b-f651-4f07-ace2-19ff473dd89d",
    "url": "https://discuss.elastic.co/t/spark-task-are-failing-no-search-context-found-for-id/163085",
    "title": "Spark task are failing: No search context found for id",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Satendrakumar",
    "date": "January 6, 2019, 3:46pm February 3, 2019, 3:43pm",
    "body": "Spark tasks are failing because of \"No search context found for id\". I tried a couple of options like spark.es.input.max.docs.per.partition 250 spark.es.scroll.size 100 spark.es.batch.size.bytes 32mb But tasks are still failing. we are using: Spark version: 2.3.1, Elasticsearh Cluster version: 6.5.4(Elastic.co hosted service 30*2 GB deployment), elasticsearch-spark version: 6.5.4. org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: org.elasticsearch.hadoop.rest.EsHadoopRemoteException: search_context_missing_exception: No search context found for id [4872076] {\"scroll_id\":\"DXF1ZXJ5QW5kRmV0Y2gBAAAAAABKV4wWRTJHQW9CdEZRTHVqMldEWnQxTUJnQQ==\"} at org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:443) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:400) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:382) at org.elasticsearch.hadoop.rest.RestClient.scroll(RestClient.java:458) at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:323) at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:115) at org.elasticsearch.spark.rdd.AbstractEsRDDIterator.hasNext(AbstractEsRDDIterator.scala:61) at scala.collection.Iterator$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$anon$14.hasNext(Iterator.scala:533) at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216) at org.apache.spark.storage.BlockManager$anonfun$doPutIterator$1.apply(BlockManager.scala:1092) at org.apache.spark.storage.BlockManager$anonfun$doPutIterator$1.apply(BlockManager.scala:1083) at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018) at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083) at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809) at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:347) at org.apache.spark.rdd.RDD.iterator(RDD.scala:298) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:42) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:336) at org.apache.spark.rdd.RDD.iterator(RDD.scala:300) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:112) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:384) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)",
    "website_area": "discuss"
  },
  {
    "id": "f0831976-d9dc-4b15-b961-9ae129797bba",
    "url": "https://discuss.elastic.co/t/cannot-initialize-ssl-expected-to-find-keystore-file-at/161006",
    "title": "Cannot initialize SSL - Expected to find keystore file at",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "crawdaddy18",
    "date": "December 15, 2018, 5:49pm January 2, 2019, 4:22pm January 3, 2019, 6:31pm January 31, 2019, 6:31pm",
    "body": "Experts, I am trying to push data from a Hive table over to ElasticSearch. There are numerous examples on the Internet of how to do this, but there are very few on how to do it when security is enabled on ElasticSearch. I have tried numerous combinations of properties for the TBL_PROPERTIES portion of the \"CREATE EXTERNAL TABLE\" but nothing works. Here is my current attempt: create external table sample_07_es (code string, description string, total_emp int, salary int) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.resource' = 'job/jobs','es.nodes'='mapr04.wired.carnoustie','es.index.auto.create' = 'true','es.net.ssl'='true','es.net.ssl.truststore.location'='/opt/mapr/elasticsearch/elasticsearch-6.2.3/etc/elasticsearch/keystores','es.net.http.auth.user'='admin','es.net.http.auth.pass'='admin'); And here is the error message in the Hive logs: Caused by: org.elasticsearch.hadoop.EsHadoopIllegalStateException: Cannot initialize SSL - Expected to find keystore file at [/opt/mapr/elasticsearch/elasticsearch-6.2.3/etc/elasticsearch/keystores] but was unable to. Make sure that it is available on the classpath, or if not, that you have specified a valid URI. at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.createSSLContext(SSLSocketFactory.java:173) at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.getSSLContext(SSLSocketFactory.java:158) at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.createSocket(SSLSocketFactory.java:127) at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:706) at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:386) at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170) at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396) at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324) at org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.execute(CommonsHttpTransport.java:478) So, the error is around keystores, which are not my strongest area. I don't know what keystore Hive is looking for, but the error is coming from the ElasticSearch method createSSLContext, so my guess was that I needed to pass the ElastiicSearch keystores. If I knew what keystore it was looking for, I could better troubleshoot the issue. I don't know if the keystore path should be a path on the local machine or a keystore path in the MapR FS. My next step is to start reading through the code, I \"think\" this is the code that is being called: KeyStore truststore = KeyStore.getInstance(\"jks\"); try (InputStream is = Files.newInputStream(keyStorePath)) { truststore.load(is, keyStorePass.toCharArray()); } SSLContextBuilder sslBuilder = SSLContexts.custom() .loadTrustMaterial(truststore, null); final SSLContext sslContext = sslBuilder.build(); RestClientBuilder builder = RestClient.builder( new HttpHost(\"localhost\", 9200, \"https\")) .setHttpClientConfigCallback(new HttpClientConfigCallback() { @Override public HttpAsyncClientBuilder customizeHttpClient( HttpAsyncClientBuilder httpClientBuilder) { return httpClientBuilder.setSSLContext(sslContext); } }); Welcome any thoughts or ideas!",
    "website_area": "discuss"
  },
  {
    "id": "0a47d598-76b7-48b4-95e2-89624caf3031",
    "url": "https://discuss.elastic.co/t/hardware-recommendations-for-shipping-iis-logs/162739",
    "title": "Hardware Recommendations for Shipping IIS Logs",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Machina_X_Machina",
    "date": "January 3, 2019, 5:01am January 31, 2019, 5:01am",
    "body": "Hi there. I'm going to deploy new IIS Log monitoring system with elasticsearch + Hadoop. The Elasticsearch Cluster would be for storing Microsoft IIS Log. And Hadoop would backup the expired elasticsearch indices(over 180 days). IIS Log Size is 5.3 TB per a month. So I think that 32 TB SSD Volume Size is appropriate for me because I should store total 3 months logs. _But, Following some articles and some experiments which is done by myself, If over 90 % of total disk is used, System automatically alert low disk and lock some kibana monitoring indices. _ To avoid those lack of available disk situation, Now I'm carefully calculating required disk spec, so I want to know how much percentage of spare disk would be need ?",
    "website_area": "discuss"
  },
  {
    "id": "16199931-e262-4b38-8a6e-2cfaf8c6085d",
    "url": "https://discuss.elastic.co/t/how-do-i-build-results-dinamically-from-a-dataframe-apache-spark/162366",
    "title": "How do I build results dinamically from a Dataframe? (Apache Spark)",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "yeikel",
    "date": "December 30, 2018, 5:24am December 30, 2018, 4:33am January 2, 2019, 4:48pm January 30, 2019, 4:49pm",
    "body": "I have a Dataframe containing a list of cities as following : val cities = sc.parallelize(Seq(\"New York\")).toDF() Now , for each city , I would like to query Elastic and build a set of results similar to the following logic : val cities = sc.parallelize(Seq(\"New York\")).toDF() cities.foreach(r => { val city = r.getString(0) val dfs = sqlContext.esDF(\"cities/docs\", \"?q=\" + city) //returns a DataFrame which triggers the exception }) Problem is that Spark does not allow nested operations that return dataframes. What options do I have to iterate a dataframe and get the results?",
    "website_area": "discuss"
  },
  {
    "id": "992a0dde-7e42-4c0b-994e-ef35f4825ca9",
    "url": "https://discuss.elastic.co/t/unable-to-insert-data-into-es-through-spark-submit-works-with-pyspark/159261",
    "title": "Unable to insert data into ES through spark-submit - works with pyspark",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "rohitgcs",
    "date": "December 4, 2018, 12:30am December 12, 2018, 8:02pm December 13, 2018, 5:36am December 20, 2018, 6:36am January 2, 2019, 4:40pm January 30, 2019, 4:40pm",
    "body": "This is the line I use: .write.options(**es_write_conf).mode(\"append\").format(\"org.elasticsearch.spark.sql\").save(esPath) When I run it in a shell launching it with pyspark -jars /usr/lib/spark/jars/elasticsearch-hadoop-5.6.9.jar it runs perfectly. But when I tried to run it through spark-submit -jars /usr/lib/spark/jars/elasticsearch-hadoop-5.6.9.jar I get a ClassNotFoundException. What would I need to do to figure out why it doesn't work on spark-submit?",
    "website_area": "discuss"
  },
  {
    "id": "7de3eb90-0aba-4be1-bf52-f0f7536cc8dc",
    "url": "https://discuss.elastic.co/t/indexing-json-with-nested-fields/161941",
    "title": "Indexing JSON with nested fields",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Buntu_Dev",
    "date": "December 23, 2018, 7:29am January 2, 2019, 4:38pm January 30, 2019, 4:47pm",
    "body": "I've a nested json data with nested fields that I want to extract and construct a Scala Map. Heres the sample JSON: \"nested_field\": [ { \"airport\": \"sfo\", \"score\": 1.0 }, { \"airport\": \"phx\", \"score\": 1.0 }, { \"airport\": \"sjc\", \"score\": 1.0 } ] I want to use saveToES() and construct a Scala Map to index the field into ES index with mapping as below: \"nested_field\": { \"properties\": { \"score\": { \"type\": \"double\" }, \"airport\": { \"type\": \"keyword\", \"ignore_above\": 1024 } } } The json file is read into the dataframe using spark.read.json(\"example.json\"). Whats the right way to construct the Scala Map in this case? Thanks for any help!",
    "website_area": "discuss"
  },
  {
    "id": "9b347c06-1af8-432d-b75f-5f7a0df7b7ff",
    "url": "https://discuss.elastic.co/t/unable-to-find-prebuilt-jar-for-spark-1-6/161865",
    "title": "Unable to find prebuilt jar for Spark 1.6",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "enricoP",
    "date": "December 21, 2018, 3:06pm January 3, 2019, 3:43pm January 30, 2019, 4:31pm",
    "body": "Hi, I'm trying to use elasticsearch-hadoop in Spark 1.6 (right now It's the only avaible version in our CDH cluster) and I'm having issues with java.lang.ClassNotFoundException using the main jar elasticsearch-hadoop-6.5.4.jar. From the documentation I'm supposed to use a separate jar elasticsearch-spark-1.x-<version>.jar, but I can't find it. In the zip from the official download page there are these jars: elasticsearch-hadoop-6.5.4.jar elasticsearch-hadoop-cascading-6.5.4.jar elasticsearch-hadoop-hive-6.5.4.jar elasticsearch-hadoop-mr-6.5.4.jar elasticsearch-hadoop-pig-6.5.4.jar elasticsearch-spark-20_2.11-6.5.4.jar elasticsearch-storm-6.5.4.jar The spark jar is the one for Spark 2.x... Thanks, Enrico",
    "website_area": "discuss"
  },
  {
    "id": "fef40ef2-45f8-4872-91bf-82704e075d98",
    "url": "https://discuss.elastic.co/t/routing-is-missing-for-join-field-relation/162502",
    "title": "[routing] is missing for join field [relation]",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "IlyaG",
    "date": "December 31, 2018, 10:32am December 31, 2018, 12:53pm January 28, 2019, 12:53pm",
    "body": "Process: I receive data from spark as Dataset. Then it is mapped to MyObject (according to the ES mapping) using map() function. Result is converted to RDD and passed to JavaEsSpark.saveToEs(). public class MyObject implements Serializable { //some fields private MyRelation relation; } public class MyRelation implements Serializable { private String parent; private String name = \"child\"; public MyRelation(String parent) { this.parent = parent; } } This is how data is inserted: JavaEsSpark.saveToEs( rdd, \"myIndex/myType, ImmutableMap.of( ConfigurationOptions.ES_MAPPING_JOIN, \"relation\"))); Relation from the mapping: \"relation\": { \"type\": \"join\", \"eager_global_ordinals\": true, \"relations\": { \"parent\": \"child\" } }, And, finally, the error: Previous handler messages: Non retryable code [400] encountered. org.elasticsearch.hadoop.rest.EsHadoopRemoteException: mapper_parsing_exception: failed to parse at org.elasticsearch.hadoop.rest.ErrorExtractor.extractErrorWithCause(ErrorExtractor.java:30) at org.elasticsearch.hadoop.rest.ErrorExtractor.extractError(ErrorExtractor.java:60) at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.tryFlush(BulkProcessor.java:245) at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:499) at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:541) at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:219) at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:121) at org.elasticsearch.spark.rdd.EsRDDWriter$$anonfun$write$1.apply(EsRDDWriter.scala:60) at org.elasticsearch.spark.rdd.EsRDDWriter$$anonfun$write$1.apply(EsRDDWriter.scala:60) at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:118) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:118) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:131) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:129) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:129) at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:117) at org.apache.spark.scheduler.Task.run(Task.scala:125) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: org.elasticsearch.hadoop.rest.EsHadoopRemoteException: illegal_argument_exception: [routing] is missing for join field [relation] at org.elasticsearch.hadoop.rest.ErrorExtractor.extractErrorWithCause(ErrorExtractor.java:30) at org.elasticsearch.hadoop.rest.ErrorExtractor.extractErrorWithCause(ErrorExtractor.java:39) Before inserting children, i insert a parent, and it is written to es successfully. Adding a ConfigurationOptions.ES_MAPPING_ROUTING together with the existing document didn't help. Same documents without relation are written successfully Any help would very appreciated!",
    "website_area": "discuss"
  },
  {
    "id": "632958a2-c01b-44a8-910a-55800b8d3f5f",
    "url": "https://discuss.elastic.co/t/hive-export-data-to-es-throw-a-exception-eshadoopinvalidrequest-returned-405-method-not-allowed/162392",
    "title": "Hive export data to es , throw a exception \"EsHadoopInvalidRequest returned [405|Method Not Allowed:]\"",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "4e90e4215b3ebadd7128",
    "date": "December 29, 2018, 7:06am December 29, 2018, 10:03am January 26, 2019, 3:35am",
    "body": "Specific error message: java.io.IOException: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: [HEAD] on [index/type] failed; server[eshost:port] returned [405|Method Not Allowed:] Hope to help",
    "website_area": "discuss"
  },
  {
    "id": "bfb27953-e14c-47b1-ada5-6fd9d02b7fea",
    "url": "https://discuss.elastic.co/t/unclear-usage-of-es-read-field-as-array-exclude/159959",
    "title": "Unclear usage of es.read.field.as.array.exclude",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "markoutso",
    "date": "December 9, 2018, 10:41am December 12, 2018, 8:27pm December 14, 2018, 4:54pm January 11, 2019, 4:22pm",
    "body": "Hello, I am trying to use the setting es.read.field.as.array.exclude From the name of it, I understand that it must produce the opposite result of es.read.field.as.array.include. es.read.field.as.array.include transforms the type a field to array so it seems reasonable the exclude would transform the type of a field from an array to its element type. The documentation states that: es.read.field.as.array.exclude Fields/properties that should be considered as arrays/lists I think that this might just be a copy-paste error. I made an experiment and created an index with the following mapping { \"test_index\": { \"mappings\": { \"test_type\": { \"dynamic\": \"strict\", \"_all\": { \"enabled\": false }, \"properties\": { \"outer\": { \"type\": \"nested\", \"properties\": { \"inner1\": { \"type\": \"boolean\" }, \"inner2\": { \"type\": \"keyword\" } } } } } } } } Nested fields are transformed by default to arrays but in my case the field is just a struct. Loading the data with the option set(ConfigurationOptions.ES_READ_FIELD_AS_ARRAY_EXCLUDE, \"outer\") gives me the following schema: root |-- outer: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- inner1: boolean (nullable = true) | | |-- inner2: string (nullable = true) As you can see the the field outer has the type of array despite the fact that it was part of the exclusions. Am I missing something? How does ConfigurationOptions.ES_READ_FIELD_AS_ARRAY_EXCLUDE work? I' m using spark 2.3.2, elasticsearch 6.5.0 and elasticsearch-hadoop 6.5.0",
    "website_area": "discuss"
  },
  {
    "id": "70832a5e-6aee-4d0a-90a5-fac2ca40e9d4",
    "url": "https://discuss.elastic.co/t/monitor-yarn-application-jobs/160443",
    "title": "Monitor YARN Application jobs",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "amillion8",
    "date": "December 11, 2018, 9:44pm December 12, 2018, 8:31pm December 12, 2018, 9:47pm January 9, 2019, 9:47pm",
    "body": "Has anyone done YARN application jobs monitoring with Elastic? If so, how did you do that? Thanks, Alex",
    "website_area": "discuss"
  },
  {
    "id": "4009618f-41de-4188-851b-42270b3df921",
    "url": "https://discuss.elastic.co/t/elasticsearch-sql-and-spark-rdd-sql/158387",
    "title": "Elasticsearch SQL and Spark RDD/SQL",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "xzhou",
    "date": "November 27, 2018, 5:09pm December 12, 2018, 8:25pm December 12, 2018, 8:58pm January 9, 2019, 8:58pm",
    "body": "Hi, Since Elasticsearch supports SQL (JDBC) interface in v6.5, could Spark (RDD or SQL) read data via JDBC? if it is feasible, how to deal with security on both sides (Elasticsearch, Spark job)? the env is currently configured to use user/password over SSL for both logstash and kibana. Thanks in advance! xzhou",
    "website_area": "discuss"
  },
  {
    "id": "d2ac38ab-ee8e-43a8-a21a-c76aa11631cd",
    "url": "https://discuss.elastic.co/t/elasticsearch-hadoop-and-index-max-result-window/158653",
    "title": "Elasticsearch-hadoop and index.max_result_window",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "xzhou",
    "date": "November 28, 2018, 9:26pm December 12, 2018, 8:00pm December 12, 2018, 8:49pm January 9, 2019, 8:50pm",
    "body": "Hi All, I think elasticsearch-hadoop uses REST API to retrieve data. does \"index.max_result_window\", which is 10k by default, take effect in batch job mode? my spark job needs to retrieve and analyze all data in this case. the index could have 1M docs. Thanks, xzhou",
    "website_area": "discuss"
  },
  {
    "id": "8b819c1b-41cf-4031-b845-adc97e55da5a",
    "url": "https://discuss.elastic.co/t/support-for-hdfs-3-1-1-and-hive-3-1/159198",
    "title": "Support for HDFS 3.1.1 and Hive 3.1",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Muj33b",
    "date": "December 3, 2018, 2:41pm December 12, 2018, 8:01pm January 9, 2019, 8:05pm",
    "body": "Hi, Which Es-Hadoop connector supports Hdfs 3.1.1 and Hive 3.1? I tried with 6.1.1 and getting problem with timestamp types. Thanks M",
    "website_area": "discuss"
  },
  {
    "id": "ace69614-13ce-4ba3-9385-02a47a9d1523",
    "url": "https://discuss.elastic.co/t/pyspark-read-data-from-elasticsearch-cluster-on-emr/158454",
    "title": "Pyspark - read data from elasticsearch cluster on EMR",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "ruxiz",
    "date": "November 28, 2018, 8:22pm November 28, 2018, 4:47am November 28, 2018, 5:40pm December 12, 2018, 7:48pm January 9, 2019, 7:48pm",
    "body": "I am trying to read data from elasticsearch from pyspark. I was using the elasticsearch-hadoop api in Spark. The es cluster sits on aws emr, which requires credential to sign in. My script is as below: from pyspark import SparkContext, SparkConf sc.stop() conf = SparkConf().setAppName(\"ESTest\") sc = SparkContext(conf=conf) es_read_conf = { \"es.host\" : \"vhost\", \"es.nodes\" : \"node\", \"es.port\" : \"443\", #\"es.query\": '{ \"query\": { \"match_all\": {} } }', #\"es.input.json\": \"true\", \"es.net.https.auth.user\": \"aws_access_key\", \"es.net.https.auth.pass\": \"aws_secret_key\", \"es.net.ssl\": \"true\", \"es.resource\" : \"index/type\", \"es.nodes.wan.only\": \"true\" } es_rdd = sc.newAPIHadoopRDD( inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\", keyClass=\"org.apache.hadoop.io.NullWritable\", valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=es_read_conf) Pyspark keeps throwing error: py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD. : org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: [HEAD] on [index] failed; servernode:443] returned [403|Forbidden:]",
    "website_area": "discuss"
  },
  {
    "id": "d254b463-1294-4060-86c0-e6757514bba5",
    "url": "https://discuss.elastic.co/t/cannot-handle-type-timestampwritablev2-exception-when-creating-index-with-es-hadoop-connector/158220",
    "title": "Cannot handle type TimestampWritableV2 Exception when creating index with ES Hadoop connector",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Muj33b",
    "date": "November 27, 2018, 9:06am December 12, 2018, 7:42pm January 9, 2019, 7:42pm",
    "body": "I am using 6.2.2 and getting the following exception when trying to create an index from Hive table with ES Hadoop connector I am using elasticsearch-hadoop-6.2.4.jar I can't What is the work around? ERROR : FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1543081145832_0094_2_00, diagnostics=[Task failed, taskId=task_1543081145832_0094_2_00_000004, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1543081145832_0094_2_00_000004_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:101) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:76) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:419) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 16 more Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:973) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:92) ... 19 more Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: Cannot handle type [class org.apache.hadoop.hive.serde2.io.TimestampWritableV2] within type [class org.elasticsearch.hadoop.hive.HiveType], instance [2017-01-09 00:00:00] within instance [org.elasticsearch.hadoop.hive.HiveType@1d054054] using writer [org.elasticsearch.hadoop.hive.HiveValueWriter@1de171d0] at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:63) at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71) at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58) at org.elasticsearch.hadoop.hive.EsSerDe.serialize(EsSerDe.java:163) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:957) at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.process(VectorFileSinkOperator.java:106) at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:965) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:938) at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:158) at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:965) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:938) at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:136) at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:965) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:938) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:125) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:889) ... 20 more",
    "website_area": "discuss"
  },
  {
    "id": "c9a10e86-f493-4fc5-9c45-e522f8573648",
    "url": "https://discuss.elastic.co/t/spark-structured-streaming-elasticsearch-sink-index-name/152449",
    "title": "[Spark Structured streaming] Elasticsearch sink index name",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "VincentL",
    "date": "October 15, 2018, 9:00am October 15, 2018, 9:07am October 15, 2018, 9:51am October 23, 2018, 3:08pm October 29, 2018, 9:36am October 31, 2018, 8:43am October 31, 2018, 8:43am October 31, 2018, 8:42am December 19, 2018, 10:48am",
    "body": "Hello, I'm currently working with Spark Structured streaming, reading data from Kafka, and writing the output to an Elasticsearch sink. Everything is working fine, except that I would like the index name to be generated based on the current date/time, as it can be done in Logstash for instance. Is there any way to do it ? Below is my code for reference (pySpark), latest try : def generateIndexName(): return(\"es_spark_\" + datetime.now().strftime(\"%Y_%m_%d__%H_%M\")) query = kafka_stream.writeStream \\ .outputMode(\"append\") \\ .queryName(\"writing_to_es\") \\ .format(\"org.elasticsearch.spark.sql\") \\ .option(\"checkpointLocation\", \"C:/TEMP/\") \\ .option(\"es.resource\", generateIndexName() + \"/es_spark\") \\ .option(\"es.nodes\", \"192.168.1.1:9200\") \\ .start() query.awaitTermination() The index name is generated when the code is first executed, and then the index name is never modified. Thanks in advance if you have any idea how to figure this out. Regards. Vincent.",
    "website_area": "discuss"
  },
  {
    "id": "876a26f0-3a31-4fac-87bb-7ed1bf54e0c9",
    "url": "https://discuss.elastic.co/t/use-elasticsearchs-geo-shape-in-pyspark/157550",
    "title": "Use elasticSearch's geo_shape in PySpark",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Vianney_Bailleux",
    "date": "November 20, 2018, 12:52pm December 18, 2018, 12:53pm",
    "body": "currently I have a program that uses elasticSearch's data with coordinates in Geo_shape and I can't find the way to create the mapping under Spark / PySpark someone will have already encountered this problem? thank you!",
    "website_area": "discuss"
  },
  {
    "id": "5ed83cf9-f848-4f13-823b-c295fbbbc7c2",
    "url": "https://discuss.elastic.co/t/error-with-mapping-arraytype-in-pyspark-not-found-typically-this-occurs-with-arrays-which-are-not-mapped-as-single-value/157499",
    "title": "Error with mapping ArrayType in pySpark \"not found; typically this occurs with arrays which are not mapped as single value\"",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Vianney_Bailleux",
    "date": "November 20, 2018, 9:02am November 20, 2018, 12:49pm December 18, 2018, 12:49pm",
    "body": "in database's elasticSearch I have this data: { \"titre\": \"Formation ElasticSearch\", \"sous-titre\": \"Mon sous titre\", \"formateurs\": [ { \"prenom\": \"Martin\", \"nom\": \"Legros\" } ], \"jours\": 3, \"url\": \"http://test.fr\" } formateurs is a array of person. here we have one person. and I do this mapping on pySpark: person= StructType([ StructField(\"nom\", StringType()), StructField(\"prenom\", StringType()), ]) schema= StructType([ StructField(\"titre\", StringType()), StructField(\"sous-titre\", StringType()), StructField(\"jours\", LongType()), StructField(\"url\", StringType()), StructField(\"formateurs\", ArrayType(person)), ]) parcel= sqlContext.read.format(\"org.elasticsearch.spark.sql\").schema(schema).load(\"zenika\") parcel.printSchema() parcel.show(1) I get this schema: |-- titre: string (nullable = true) |-- sous-titre: string (nullable = true) |-- jours: long (nullable = true) |-- url: string (nullable = true) |-- formateurs: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- nom: string (nullable = true) | | |-- prenom: string (nullable = true) in this example there are no errors but if i add one formateurs, i have one errors. example: { \"titre\": \"Formation ElasticSearch\", \"sous-titre\": \"Mon sous titre\", \"formateurs\": [ { \"prenom\": \"Martin\", \"nom\": \"Legros\" }, { \"prenom\": \"Marc\", \"nom\": \"Duchien\" } ], \"jours\": 3, \"url\": \"http://test.fr\" } and I get this error: Caused by: org.elasticsearch.hadoop.EsHadoopIllegalStateException: Field 'formateurs.nom' not found; typically this occurs with arrays which are not mapped as single value at org.elasticsearch.spark.sql.RowValueReader$class.rowColumns(RowValueReader.scala:51) at org.elasticsearch.spark.sql.ScalaRowValueReader.rowColumns(ScalaEsRowValueReader.scala:32) at org.elasticsearch.spark.sql.ScalaRowValueReader.createMap(ScalaEsRowValueReader.scala:69) at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:968) at org.elasticsearch.hadoop.serialization.ScrollReader.readListItem(ScrollReader.java:875) at org.elasticsearch.hadoop.serialization.ScrollReader.list(ScrollReader.java:927) at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:833) at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:1004) at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:846) at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:602) at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:426) ... 27 more would you be able to explain me how to make ArrayType because I did not find a tutorial with complex schema. thank you so much.",
    "website_area": "discuss"
  },
  {
    "id": "d200a9dd-546b-4890-9d1d-bbafaaa6cc57",
    "url": "https://discuss.elastic.co/t/high-level-rest-client-in-hadoop-env/156082",
    "title": "High level REST client in Hadoop env",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Georgi_Ivanov",
    "date": "November 9, 2018, 4:03pm December 7, 2018, 4:03pm",
    "body": "Hi, I am having a hard time integrating the High-level-REST (6.4) client into a MapReduce job Looks like a Jar dependency problem. I tried to shade - no luck at all. The problem seems to be in the httpcore libs from Apache. What i find strange is that the exception is thrown in in org.apache class, and i have shaded all org.apache.http to hidden.org.apache.http Any help is appreciated. Hadoop version : Hadoop 2.6.0-cdh5.7.1 ES version : 6.4 Here is the stacktrace: 2018-11-09 16:54:10,086 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.NoSuchFieldError: INSTANCE at org.apache.http.impl.nio.codecs.DefaultHttpRequestWriterFactory.&lt;init&gt;(DefaultHttpRequestWriterFactory.java:53) at org.apache.http.impl.nio.codecs.DefaultHttpRequestWriterFactory.&lt;init&gt;(DefaultHttpRequestWriterFactory.java:57) at org.apache.http.impl.nio.codecs.DefaultHttpRequestWriterFactory.&lt;clinit&gt;(DefaultHttpRequestWriterFactory.java:47) at org.apache.http.impl.nio.conn.ManagedNHttpClientConnectionFactory.&lt;init&gt;(ManagedNHttpClientConnectionFactory.java:75) at org.apache.http.impl.nio.conn.ManagedNHttpClientConnectionFactory.&lt;init&gt;(ManagedNHttpClientConnectionFactory.java:83) at org.apache.http.impl.nio.conn.ManagedNHttpClientConnectionFactory.&lt;clinit&gt;(ManagedNHttpClientConnectionFactory.java:64) at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager$InternalConnectionFactory.&lt;init&gt;(PoolingNHttpClientConnectionManager.java:553) at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.&lt;init&gt;(PoolingNHttpClientConnectionManager.java:163) at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.&lt;init&gt;(PoolingNHttpClientConnectionManager.java:147) at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.&lt;init&gt;(PoolingNHttpClientConnectionManager.java:119) at org.apache.http.impl.nio.client.HttpAsyncClientBuilder.build(HttpAsyncClientBuilder.java:668) at org.elasticsearch.client.RestClientBuilder$2.run(RestClientBuilder.java:230) at org.elasticsearch.client.RestClientBuilder$2.run(RestClientBuilder.java:227) at java.security.AccessController.doPrivileged(Native Method) at org.elasticsearch.client.RestClientBuilder.createHttpClient(RestClientBuilder.java:227) at org.elasticsearch.client.RestClientBuilder.access$000(RestClientBuilder.java:42) at org.elasticsearch.client.RestClientBuilder$1.run(RestClientBuilder.java:198) at org.elasticsearch.client.RestClientBuilder$1.run(RestClientBuilder.java:195) at java.security.AccessController.doPrivileged(Native Method) at org.elasticsearch.client.RestClientBuilder.build(RestClientBuilder.java:195) at org.elasticsearch.client.RestHighLevelClient.&lt;init&gt;(RestHighLevelClient.java:221) at org.elasticsearch.client.RestHighLevelClient.&lt;init&gt;(RestHighLevelClient.java:213) at com.vesseltracker.hadoop.mapreduce.ReduceTemplate.getElasticSearchClient(ReduceTemplate.java:156) at com.vesseltracker.hadoop.mapreduce.ReduceTemplate.setup(ReduceTemplate.java:122) at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:168) at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
    "website_area": "discuss"
  },
  {
    "id": "93c4dfe3-3075-4f1e-9c1c-e64a956ed4f2",
    "url": "https://discuss.elastic.co/t/hadoop-es-structured-streaming-write-sink-partition-issue/155550",
    "title": "Hadoop-ES Structured Streaming-write Sink Partition Issue",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "grifith",
    "date": "November 6, 2018, 1:11pm December 4, 2018, 1:11pm",
    "body": "Hello, We have a structured streaming application which write the data from Spark to ES.The flow is follows- Kinesis Stream( 8 shards)---->(Databricks Library for Kinesis-readstream)---->Spark structured Stream(transformation) ----->(es hadoop library 6.2.1-writestream)--->ES When we use the es-hadoop library for the ES write Sink, we have seen that the data frame repartition is not happening.When we try the write stream with console ,the actual shuffle is happening . The number of tasks in the stage1 of the processing is always equal to the kinesis stream shard number.We have tried different ES configurations and its not working as expected. Code flow is as below:- ##Read stream(data bricks Library for Kinesis)####################### var result = spark.readStream .format(\"kinesis\") .option(\"streamName\", conf.aws.kinesisStream) .option(\"region\", \"eu-west-1\") .option(\"awsAccessKey\", conf.aws.accessKey) .option(\"awsSecretKey\", conf.aws.secretKey) .option(\"initialPosition\", conf.aws.kinesisPosition) .option(\"maxRecordsPerFetch\", conf.databricks.maxRecordsPerFetch) .option(\"fetchBufferSize\", conf.databricks.fetchBufferSize) .option(\"maxFetchDuration\", conf.databricks.maxFetchDuration) .option(\"maxFetchRate\", conf.databricks.maxFetchRate) .option(\"minFetchPeriod\", conf.databricks.minFetchPeriod) .option(\"shardFetchInterval\", conf.databricks.shardFetchInterval) .option(\"shardsPerTask\", conf.databricks.shardsPerTask) ######repartition after the stream read####################### val mStream = stream .repartition(conf.spark.inPartitions.toInt) ###Data Transformations############# ####Write Sink############################### eventsStream .writeStream .outputMode(OutputMode.Append) //Only mode for ES .format(\"org.elasticsearch.spark.sql\") //es .option(\"es.nodes\", conf.es.nodes) .option(\"es.nodes.wan.only\", conf.es.wanOnly) .queryName(\"name\") .option(\"checkpointLocation\", streamCheckPointLoc) .option(\"es.batch.size.entries\", conf.es.es_batch_size_entries) .option(\"es.batch.size.bytes\", conf.es.es_batch_size_bytes) .option(\"es.batch.write.refresh\", conf.es.es_batch_write_refresh) .option(\"es.input.max.docs.per.partition\", conf.es.es_max_docs_per_partition) .option(\"es.batch.write.retry.count\", conf.es.es_batch_write_retry_count) .start(conf.es.index) Could you please let us know why the repartition is not happening when we use the es-hadoop library? for other built-in output sinks the repartitioning and shuffle operation is happening.",
    "website_area": "discuss"
  },
  {
    "id": "cdcc2e1a-2a3c-4d9d-bc95-15956b3a718f",
    "url": "https://discuss.elastic.co/t/reading-data-from-elasticsearch-with-es-hadoop/154339",
    "title": "Reading data from elasticsearch with ES-Hadoop",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "alizadehei",
    "date": "October 31, 2018, 5:08am October 29, 2018, 2:19pm November 26, 2018, 2:26pm",
    "body": "i want to retrieve Elasticsearch Data with ES-Hadoop. i use the following code to retrieve Elastic data but get Java.io.IOException: JobConf conf=new JobConf(); conf.set(\"es.nodes\",\"10.25.4.12:9200\"); conf.set(\"es.resource\",\"university/docs\"); conf.set(\"es.query\",\"?q=*\"); conf.setInputFormat(EsInputFormat.class); conf.setMapOutputKeyClass(Text.class); conf.setOutputValueClass(MapWritable.class); FileOutputFormat.setOutputPath(conf,new Path(hdfs://10.25.4.12/user)); JobClient.runJob(conf);",
    "website_area": "discuss"
  },
  {
    "id": "a75daa86-3045-45c0-a6fe-d2ca5a749851",
    "url": "https://discuss.elastic.co/t/hdfs-repository-and-name-nodes-setting/151242",
    "title": "HDFS Repository and Name Nodes setting",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "ajay_bh111",
    "date": "October 5, 2018, 3:36pm October 23, 2018, 3:05pm October 23, 2018, 6:26pm October 25, 2018, 2:55pm October 25, 2018, 8:34pm October 26, 2018, 3:01pm November 23, 2018, 2:45pm",
    "body": "I could create and do snapshot /restore successfully with hdfs repo using kerbros authentication. However in the config If I use namenode setting as defined in hadoop hdfs-site.xml file:, it fails with error {\"error\":{\"root_cause\":[{\"type\":\"invocation_target_exception\",\"reason\":\"invocation_target_exception: null\"}],\"type\":\"repository_exception\",\"reason\":\"[flowtest_hdfs_repository] cannot create blob store\",\"caused_by\":{\"type\":\"runtime_exception\",\"reason\":\"runtime_exception: java.lang.reflect.InvocationTargetException\",\"caused_by\":{\"type\":\"invocation_target_exception\",\"reason\":\"invocation_target_exception: null\",\"caused_by\":{\"type\":\"illegal_argument_exception\",\"reason\":\"java.net.UnknownHostException: hadoop.log.labs\",\"caused_by\":{\"type\":\"i_o_exception\",\"reason\":\"hadoop.log.labs\"}}}}},\"status\":500} If URI is replaced with actual active node name address m1.hadoop.log.labs:8020, It works fine. Question is how to specify namenodes in the URI or what other config is needed so that name node automatically use active name node as URI. In the config setup : \"uri\": \"hdfs://hadoop.log.labs:8020/\" , \"conf.dfs.namenode.rpc-address.hadoop.log.labs.nn1\": \"m1.hadoop.log.labs:8020\", \"conf.dfs.namenode.rpc-address.hadoop.log.labs.nn2\": \"m2.hadoop.log.labs:8020\",",
    "website_area": "discuss"
  },
  {
    "id": "95fdee3d-8b3f-4f18-b892-da75af04cce7",
    "url": "https://discuss.elastic.co/t/understanding-es-input-max-docs-per-partition-configuration/154045",
    "title": "Understanding \"es.input.max.docs.per.partition\" configuration",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Swapnil_Nawale",
    "date": "October 25, 2018, 5:19pm October 26, 2018, 3:10am October 26, 2018, 3:10am November 22, 2018, 8:42pm",
    "body": "I am using elasticsearch-hadoop 5.6.2 to read from the elasticsearch 5.6.1 indices and aliases with Spark 2.3. The ES indices are configured to have 5 primary and 1 replica shards. In my case, the RDD returned by JavaEsSpark.esRDD() is heavily imbalanced in terms of the data contained by each partition. Below is an example of the # of documents per RDD partition : partition number: 0, document count: 0 partition number: 1, document count: 0 partition number: 2, document count: 0 partition number: 3, document count: 355326 partition number: 4, document count: 355518 partition number: 5, document count: 0 partition number: 6, document count: 0 partition number: 7, document count: 0 partition number: 8, document count: 0 partition number: 9, document count: 355436 partition number: 10, document count: 356470 partition number: 11, document count: 0 partition number: 12, document count: 356520 partition number: 13, document count: 0 partition number: 14, document count: 0 Only 5 partitions (which seem to correspond to 5 ES shards) have the data and other partitions are empty. I am using the default value (100000) for es.input.max.docs.per.partition config. And my understanding for this config was that with the default value, no RDD partition should have more than 100000 documents. So I am wondering if my understanding is not correct for this config or this is a bug?",
    "website_area": "discuss"
  },
  {
    "id": "e5e51688-40c7-4fbd-9cdf-7319b1f12dc5",
    "url": "https://discuss.elastic.co/t/pyspark-seeing-a-connectiontimedout-to-a-local-elasticsearch/153879",
    "title": "PySpark - Seeing a 'ConnectionTimedOut' to a local ElasticSearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "initialv",
    "date": "October 25, 2018, 12:07pm October 25, 2018, 5:04pm October 25, 2018, 8:36pm November 22, 2018, 8:36pm",
    "body": "I have a simple elasticsearch query that I want to receive that count of. On my local mac environment, the below setup works as expected. On an Ubuntu machine, I see ERROR NetworkClient:144 - Node [10.0.0.1:9200] failed (also odd, because the configuration points to 127.0.0.1). Environment that does not work: OS: Ubuntu Python: 2.7.10 Spark: 2.3.1 and 2.3.2 ElasticSearch: 5.6.5 ES Hadoop Jar: elasticsearch-spark-20_2.11-5.6.5.jar Environment that does work: OS: mac Python: 2.7.10 Spark: 2.2.1, 2.3.1, and 2.3.2 ElasticSearch: 5.6.3 ES Hadoop Jar: elasticsearch-spark-20_2.11-5.6.3.jar PySpark is run from the interactive shell via path/to/pyspark \\ --jars /path/to/elasticsearch-spark-20_2.11-5.6.5.jar --master local The code I run is as follows: # I put the timeout to 10 seconds to see if I could get the error quicker, but it appears to still wait the full 1 minute (which is the default as per the docs) config = { 'es.nodes': '127.0.0.1:9200', 'es.scroll.size': '9000', 'es.resource': 'mydata/data', 'es.query': '{\"query\": {\"match_all\": {}}}', 'es.http.timeout': '10s'} dataframe = sqlContext.read.format(\"org.elasticsearch.spark.sql\").options(**config).load() dataframe.count() # waits for about a minute then shows the below error, then retries with 127.0.0.1 and errors out again # ERROR NetworkClient:144 - Node [10.0.0.1:9200] failed (Connection timed out (Connection timed out)); selected next node [127.0.0.1:9200]",
    "website_area": "discuss"
  },
  {
    "id": "565531e1-e410-497d-b0d7-f1f0da86be04",
    "url": "https://discuss.elastic.co/t/cannot-load-class-org-elasticsearch-spark-sql-sparksqlcompatibilitylevel/152517",
    "title": "Cannot load class org.elasticsearch.spark.sql.SparkSQLCompatibilityLevel",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "SKing",
    "date": "October 15, 2018, 2:44pm October 23, 2018, 3:10pm October 24, 2018, 8:26am November 21, 2018, 8:26am",
    "body": "I am running a Java Spark application. I can run the application from inside the intellij IDE. However, if I try to run it using the application built as a jar I get the following error. Caused by: org.elasticsearch.hadoop.EsHadoopIllegalStateException: Cannot load class [org.elasticsearch.spark.sql.SparkSQLCompatibilityLevel] Using spark-core_2.10 v 1.6.3 elasticsearch-spark-13_2.10 v 5.6.8 The problem seems identical to the one encountered in this thread. Class org.elasticsearch.spark.sql.SparkSQLCompatibilityLevel not found Hadoop and Elasticsearch Playing with es4Hadoop 5.2.1 and Spark, I built a Java application with Maven deps: <dependencies> <dependency> <groupId>org.apache.spark</groupId> <artifactId>spark-core_2.10</artifactId> <version>1.6.2</version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql_2.10 --> <dependency> <groupId>org.apache.spark</groupId> <artifactId>spark-sql_2.10</artifactId> <version>1.6.2</version> </dependency> However, no mention is given of how to fix this.",
    "website_area": "discuss"
  },
  {
    "id": "506f0014-a168-4798-88aa-826de0e195b9",
    "url": "https://discuss.elastic.co/t/spark-read-from-elasticsearch-and-different-indexes-the-speed-of-reading-is-five-times-worse/151765",
    "title": "Spark read from elasticsearch and different indexes, the speed of reading is five times worse",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "stupidsky",
    "date": "October 10, 2018, 6:55am November 7, 2018, 6:55am",
    "body": "spark read from elasticsearch and different indexes, the speed of reading is five times worse. spark 2.2.0,elasticsearch 6.0.0. image.png139661 6.21 KB 1 task contains about one hundred thousand data. The two data of indexs are about the same size.",
    "website_area": "discuss"
  },
  {
    "id": "3b417641-d872-4604-b991-fd47a8c8521e",
    "url": "https://discuss.elastic.co/t/org-elasticsearch-hadoop-rest-eshadoopnonodesleftexception-connection-error-check-network-and-or-proxy-settings-all-nodes-failed-tried-172-24-191-234-9010-172-24-191-231-9010-172-24-191-233-9010/150957",
    "title": "org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[172.24.191.234:9010, 172.24.191.231:9010, 172.24.191.233:9010]]",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "bpaul",
    "date": "October 4, 2018, 3:11am October 4, 2018, 7:04pm October 5, 2018, 6:57am November 2, 2018, 6:57am",
    "body": "Hello guys, I am trying to export data from hadoop into elastic search. The Map reduce job is getting failed and the daeomon of elastic search is automatically shutting down.I have configured 4 nodes to form an elastic search cluster. Below given are the configuration setting of one node . The configuration parameters are same of other 3 nodes as well. elasticsearch.yml node.name: ES_SLAVE2 node.master: false node.data: true path.data: /das/elasticsearch-2.3.4/data bootstrap.memory_lock: false xpack.security.enabled: false bootstrap.system_call_filter: false action.auto_create_index: .security,.monitoring*,.watches,.triggered_watches,.watcher-history* indices.query.bool.max_clause_count: 10240 network.host: 172.24.191.233 http.port: 9010 Guys. Let me know if you need any other input. Kindly help",
    "website_area": "discuss"
  },
  {
    "id": "e3a8d8f7-d952-4e05-9513-092893ea52c5",
    "url": "https://discuss.elastic.co/t/hadoop-error-cannot-detect-es-version-typically-this-happens-if-the-network-elasticsearch-cluster-is-not-accessible-or-when-targeting-a-wan-cloud-instance-without-the-proper-setting-es-nodes-wan-only/150833",
    "title": "Hadoop Error : Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "bpaul",
    "date": "October 3, 2018, 7:11am October 4, 2018, 7:04pm October 4, 2018, 7:04pm",
    "body": "I have been trying to load data from hadoop cluster into Elastic search index. I had started the MapReduce job for executing the operation but it failed throwing the above given error. It throwed the above given error and the mapreduce operation stopped. The daemon on the Elastic search deamon got killed automatically. Kindly help. Expecting your cooperation. Thankyou",
    "website_area": "discuss"
  },
  {
    "id": "4ad5894d-efad-409f-8ad8-5e06c8d326e4",
    "url": "https://discuss.elastic.co/t/warn-scalarowvaluereader-field-hits-hits-score-is-backed-by-an-array-but-the-associated-spark-schema-does-not-reflect-this/150248",
    "title": "WARN ScalaRowValueReader: Field 'hits.hits._score' is backed by an array but the associated Spark Schema does not reflect this",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Madhur_Chopra",
    "date": "September 27, 2018, 7:56pm October 25, 2018, 7:55pm",
    "body": "i am trying to access _id field from metadata @udf('string') def get_id(metadata): return metadata['_id'] def get_index(server,index_name,query): df = sqlContext.read.format(\"org.elasticsearch.spark.sql\")\\ .option(\"es.nodes\",server)\\ .option(\"es.query\",query)\\ .option(\"es.read.metadata\",\"true\")\\ .option(\"es.read.metadata.field\",\"metadata\")\\ .option(\"es.read.field.empty.as.null\",\"yes\")\\ .option(\"es.index.read.missing.as.empty\",\"yes\")\\ .option(\"es.read.field.validate.presence\",\"ignore\")\\ .load(index_name) return df.withColumn('Id',get_id(df.metadata)) query = \"\"\"{\"query\":{ \"match_all\": {}}}\"\"\" The strange part is when i call \"get_index()\" and on returned dataframe, I do .select('Id') i see this warning, but when i select all the fields, i do not see this warning. i have tried adding es.read.field.as.array.include hits.hits._score but the warning doesnt seem to go away. Thanks in advance for help",
    "website_area": "discuss"
  },
  {
    "id": "a4d36d52-5886-4dd5-87d2-688a3eae9c00",
    "url": "https://discuss.elastic.co/t/nested-fields-upsert-with-spark-not-working/146633",
    "title": "Nested fields upsert with Spark not working",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "cluengo",
    "date": "August 30, 2018, 7:30am September 4, 2018, 5:31pm September 4, 2018, 6:08pm September 5, 2018, 8:11am October 3, 2018, 8:11am",
    "body": "Hi, I already asked this a few months ago, but since the answer was that it would be fixed in a following release, I wanted to know whether I'm doing something wrong, or if it's still not fixed. I'm trying to update a nested field using Spark and a scripted update. The code I'm using is: update_params = \"new_samples: samples\" update_script = \"ctx._source.samples += new_samples\" es_conf = { \"es.mapping.id\": \"id\", \"es.mapping.exclude\": \"id\", \"es.write.operation\": \"upsert\", \"es.update.script.params\": update_params, \"es.update.script.inline\": update_script } result.write.format(\"org.elasticsearch.spark.sql\").options(**es_conf).option(\"es.nodes\",configuration[\"elasticsearch\"][\"host\"]).option(\"es.port\",configuration[\"elasticsearch\"][\"port\"] ).save(configuration[\"elasticsearch\"][\"index_name\"]+\"/\"+configuration[\"version\"],mode='append') And the schema of the field is: |-- samples: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- gq: integer (nullable = true) | | |-- dp: integer (nullable = true) | | |-- gt: string (nullable = true) | | |-- adBug: array (nullable = true) | | | |-- element: integer (containsNull = true) | | |-- ad: double (nullable = true) | | |-- sample: string (nullable = true) And I get the following error: py4j.protocol.Py4JJavaError: An error occurred while calling o83.save. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.ClassCastException: scala.collection.mutable.WrappedArray$ofRef cannot be cast to scala.Tuple2 Thanks so much!",
    "website_area": "discuss"
  },
  {
    "id": "f44aed54-50a3-4c57-b6a3-7b7a8a9a0e41",
    "url": "https://discuss.elastic.co/t/hive-integration-which-jar-do-we-need/145650",
    "title": "Hive integration - Which jar do we need?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Divit_Sharma",
    "date": "August 23, 2018, 5:34am September 4, 2018, 4:35pm September 4, 2018, 5:49pm October 2, 2018, 5:49pm",
    "body": "I have downloaded ES hadoop jar. I need to fetch data from Hive tables to ES. There are two jar ES-hadoop and ES-hadoop-hive. Which one to utilise to connect and fetch data from Hive?",
    "website_area": "discuss"
  },
  {
    "id": "d8af63eb-ed5b-4a03-be73-39beaa11a0ab",
    "url": "https://discuss.elastic.co/t/read-field-include-with-nested-field-doesnot-really-work/147180",
    "title": "Read.field.include with nested field doesnot really work",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "ebuildy",
    "date": "September 4, 2018, 9:50am September 4, 2018, 5:30pm October 2, 2018, 2:33pm",
    "body": "CREATE TEMPORARY TABLE trips USING org.elasticsearch.spark.sql OPTIONS ( nodes \"....\", port \"80\", resource \"events-2018.09.03/events\", read.field.include \"client.browser.os\" ) select client.browser.os from trips limit 10 This is working fine, but, the generated elasticsearch query is: { \"slice\": { \"id\": 20, \"max\": 21 }, \"query\": { \"match_all\": {} }, \"_source\": [\"client\"] } Why _source is not : client.browser.os ?",
    "website_area": "discuss"
  },
  {
    "id": "b56efc7f-fe78-4e6a-bbd4-14c33a248108",
    "url": "https://discuss.elastic.co/t/esjsonrdd-only-returns-a-single-field-rather-than-all-fields/147216",
    "title": "esJsonRDD only returns a single field rather than all fields",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "SKing",
    "date": "September 4, 2018, 1:32pm October 2, 2018, 1:32pm",
    "body": "This topic is similar to the one found here. - esRdd not returning script field values. Like the poster of the linked message. My query {\"query\": {\"bool\" : {\"must_not\": {\"term\": {\"request\": \"configuration.log.message\"}},\"must\":[{\"exists\":{\"field\":\"total_execution_time_ms\"}}]}}} When run using Dev Tools in kibana console will return all the fields in the elastic search entries. However, when run using JavaESSpark.esJsonRdd( , <index_name>, {\"query\": {\"bool\" : {\"must_not\": {\"term\": {\"request\": \"configuration.log.message\"}},\"must\":[{\"exists\":{\"field\":\"total_execution_time_ms\"}}]}}}) only one field, the session-id, is return. Aside: From this Question - Passing variables to es-query, I no longer specified specific fields and just looked for the elastic search entry (with all fields) with a view to filtering out the fields I wanted via a transform function. Previously the query had been {\"_source\":[\"total_execution_time_ms\",\"templating_time_ms\", \"responseServices_time_ms\", \"requestServices_time_ms\", \"substitutions_time_ms\", \"session-id\", \"time_of_log\"],\"query\": {\"bool\" : {\"must_not\": {\"term\": {\"request\": \"configuration.log.message\"}},\"must\":[{\"exists\":{\"field\":\"total_execution_time_ms\"}}]}}} But with the same problem (only session-id name-value being returned) Thank you",
    "website_area": "discuss"
  },
  {
    "id": "ee444912-5314-4eeb-9ede-8ddac2a4039b",
    "url": "https://discuss.elastic.co/t/where-to-find-java-doc-for-javaesspark/146511",
    "title": "Where to find Java Doc for JavaEsSpark",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "SKing",
    "date": "August 30, 2018, 5:25am September 26, 2018, 10:31am",
    "body": "Can't find the link for Java Docs for JavaEsSpark. Can someone post it please?",
    "website_area": "discuss"
  },
  {
    "id": "67396b4f-04f5-407b-bc45-294b7f686dbb",
    "url": "https://discuss.elastic.co/t/how-to-scroll-through-an-elasticsearch-index-using-elasticsearch-spark/144618",
    "title": "How to scroll through an Elasticsearch index using elasticsearch-spark?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Swapnil_Nawale",
    "date": "August 16, 2018, 4:55am August 17, 2018, 8:05pm August 21, 2018, 5:48pm August 21, 2018, 8:21pm August 21, 2018, 6:24pm September 18, 2018, 6:24pm",
    "body": "With the Java Client.prepareSearch() and Client.prepareSearchScroll() APIs, we can query an Elasticsearch index using the scrolls as mentioned in the documentation. With these APIs, we can select only a specific number of hits per request by setting SearchRequestBuilder.setSize() . The SearchResponse provides the scroll Id, which is then used in the subsequent request. How can one use elasticsearch-spark to implement a similar functionality ? All JavaEsSpark.esRDD() methods return JavaPairRDD , which would contain all hits. Is there a way to request only a specific number of hits per request and then continue scrolling with further request? I found the configuration es.scroll.size , which seems equivalent to SearchRequestBuilder.setSize() but I am not sure how to use it and how the scroll ids would be used in the context of elasticsearch-spark?",
    "website_area": "discuss"
  },
  {
    "id": "f241703a-00ce-441c-b998-43b3e6ed5150",
    "url": "https://discuss.elastic.co/t/hive-to-elastic-eshadoopnonodesleftexception/144375",
    "title": "Hive to elastic :- EsHadoopNoNodesLeftException",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Amandeep_Singh1",
    "date": "August 17, 2018, 3:09pm August 17, 2018, 8:14pm September 14, 2018, 8:14pm",
    "body": "Trying to connect HIve to elastic added elasticsearch-hadoop-6.3.2.jar elasticsearch-hadoop-6.3.2-javadoc.jar elasticsearch-hadoop-6.3.2-sources.jar to/usr/lib/hive/auxlib The query for creating the table. CREATE EXTERNAL TABLE prediction ( opid bigint, egmcogsspread double, egmnrpred double, egmshippred double, modelnum int, bclgid int, eventdate string) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.resource' = 'prediction/document', 'es.nodes' = '10.228.164.125', 'es.mapping.id' = opid, es.index.auto.create' = 'true','es.nodes.discovery' = 'true','es.nodes.client.only' = 'false, ); desc tblegm opid bigin egmcogspred double egmnrpred double egmshippred double modelnum int bclgid int eventdate string query to pass data: INSERT OVERWRITE table prediction select * from tblegm where eventdate=\"2018-07-05\" limit 100; Error: Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":2662420715,\"_col1\":25.42,\"_col2\":34.05,\"_col3\":7.9464,\"_col4\":null,\"_col5\":1}} at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265) at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":2662420715,\"_col1\":25.42,\"_col2\":34.05,\"_col3\":7.9464,\"_col4\":null,\"_col5\":1}} at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253) ... 7 more Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.228.164.125:9200]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:149) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:380) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:344) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:348) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:158) at org.elasticsearch.hadoop.rest.RestClient.getHttpNodes(RestClient.java:115) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverNodesIfNeeded(InitializationUtils.java:92) at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:580) at org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.init(EsOutputFormat.java:173) at org.elasticsearch.hadoop.hive.EsHiveOutputFormat$EsHiveRecordWriter.write(EsHiveOutputFormat.java:58) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:717) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(LimitOperator.java:51) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84) at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244) ... 7 more Just to add: there are 5 mapping jobs and 1 reducing job. curl http://10.228.164.125:9200/ { \"name\" : \"elasticclientc2n1.dev.bo1.csnzoo.com\", \"cluster_name\" : \"logslabbo1\", \"cluster_uuid\" : \"i741QymvTjSLghDqhjQ1rQ\", \"version\" : { \"number\" : \"6.2.3\", \"build_hash\" : \"c59ff00\", \"build_date\" : \"2018-03-13T10:06:29.741383Z\", \"build_snapshot\" : false, \"lucene_version\" : \"7.2.1\", \"minimum_wire_compatibility_version\" : \"5.6.0\", \"minimum_index_compatibility_version\" : \"5.0.0\" }, \"tagline\" : \"You Know, for Search\" } alternative table properties tried setting node to elasticclientc2n1.dev.bo1.csnzoo.com TBLPROPERTIES('es.resource'='prediction/document', 'es.nodes'= 'elasticclientc2n1.dev.bo1.csnzoo.com', 'es.mapping.id'= 'opid', 'es.index.auto.create'='true', 'es.nodes.wan.only'='true');",
    "website_area": "discuss"
  },
  {
    "id": "829fcd6b-66d0-4a59-a4c2-135e65dbd217",
    "url": "https://discuss.elastic.co/t/not-able-to-read-from-elasticsearch-from-spark/144851",
    "title": "Not able to read from elasticsearch from spark?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "sheryl",
    "date": "August 17, 2018, 9:51am August 17, 2018, 8:08pm September 14, 2018, 8:08pm",
    "body": "using logstash i have ingested file into elastic search with index/type .. car/soldcars now for practising i want to read that csv file from spark using scala code. ;val spark= SparkSession.builder.master(\"local\").appName(\"ReadFromES\") .config(\"es.nodes\", \"localhost\").config(\"es.port\",9200).config(\"es.resoruces\",\"car/soldcars\") .config(\"es.index.auto.create\",\"true\").getOrCreate() val es_df=spark.read.format(\"org.elasticsearch.spark.sql\") } what to do next ?",
    "website_area": "discuss"
  },
  {
    "id": "2b312119-24a4-42fa-a49e-0a68752c8139",
    "url": "https://discuss.elastic.co/t/not-able-to-detect-es-version/142340",
    "title": "Not able to detect ES version",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "sheryl",
    "date": "July 31, 2018, 10:39am August 14, 2018, 3:55am September 11, 2018, 3:56am",
    "body": "Exception in thread \"main\" org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' i am dumping dataframe to elastic search but i am not able to do it . df.write.format(\"org.elasticsearch.spark.sql\") .option(\"es.nodes.wan.only\",\"true\") .option(\"es.port\",\"9200\") .option(\"es.net.ssl\",\"true\") .option(\"es.nodes\",\"localhost\") .mode(\"append\") .option(\"es.nodes.client.only\", \"false\") .save(\"C:\\elasticsearch-6.3.1\")",
    "website_area": "discuss"
  },
  {
    "id": "f17e644a-fe93-49a8-ba45-9d9d63465095",
    "url": "https://discuss.elastic.co/t/how-to-save-spark-scala-dataframe-to-elastic-search/142315",
    "title": "How to save spark scala dataframe to elastic search",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "sheryl",
    "date": "July 31, 2018, 9:07am August 14, 2018, 3:45am August 14, 2018, 3:46am",
    "body": "i am using this df to save into elasticsearch but it showing error --- ERROR NetworkClient: Node [192.168.0.1:9200] failed (Connection timed out: connect); no other nodes left - aborting... -- df.write.format(\"org.elasticsearch.spark.sql\").option(\"es.nodes.wan.only\",\"true\") .option(\"es.port\",\"9200\") .option(\"es.net.ssl\",\"true\") .option(\"es.nodes\",\"192.168.0.1\") .mode(\"append\") .option(\"es.nodes.client.only\", \"false\") .save(\"C:\\elasticsearch-6.3.1\") i m running this on windows but not working .",
    "website_area": "discuss"
  },
  {
    "id": "9952eea4-c208-4914-8ae3-e1d1da66384f",
    "url": "https://discuss.elastic.co/t/error-in-adding-nested-objects-using-spark-on-es-index/143694",
    "title": "Error in adding Nested objects using Spark on Es-Index",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "TANISHQ_BATRA",
    "date": "August 9, 2018, 12:56pm September 6, 2018, 12:56pm",
    "body": "Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be cast to scala.Tuple2 Details: Es version-6.3 Incoming object example { \"note\": { \"a\":\"1\", \"b':\"2\" } } //Es properties used: .option(\"es.mapping.id\", \"note.a\") .option(\"es.write.operation\", \"upsert\") .option(ConfigurationOptions.ES_UPDATE_SCRIPT_INLINE, \"ctx._source.notefield.add(params.param1)\") .option(ConfigurationOptions.ES_UPDATE_SCRIPT_PARAMS, \"param1:note\") .option(\"es.write.operation\", \"upsert\") //the \"notefield\" is declared as nested in mapping at ES. Any suggestions",
    "website_area": "discuss"
  },
  {
    "id": "53dbcd54-9d34-483d-b189-8a80ca61d1ab",
    "url": "https://discuss.elastic.co/t/elastic-hadoop-es-port-setting-of-1/143598",
    "title": "Elastic/Hadoop es.port setting -of 1",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "mhohara",
    "date": "August 8, 2018, 9:18pm September 5, 2018, 9:18pm",
    "body": "I was working on connecting an AWS EMR to an AWS Managed Elastic cluster. I'd noticed that I could connect to the managed cluster using a curl, but that the curl could not use a port number. Putting either :80 or :9200 would work, the curl request would just hang. As EMR uses 9200 for a default, this also seemed to prevent the EMR from hitting the cluster - a request to it (which worked when sent to our own elastic cluster using 9200) would just hang when configured to hit the managed cluster, whether setting es.port to 80, 8080 or 9200. I happened to try setting the es.port to -1, and this (apparently) worked as the EMR job completed successfully and the managed cluster did show the indices. So I'm able to proceed with using managed clusters if I like. However, this behavior isn't documented, which makes me concerned that it may change. I'm interested whether this behavior was accidental, and/or whether it could be documented and hence become 'official'.",
    "website_area": "discuss"
  },
  {
    "id": "b9024e5b-33a4-48c7-b538-d99a36aef1f8",
    "url": "https://discuss.elastic.co/t/hadoop-and-es-connectivity/139534",
    "title": "Hadoop and ES connectivity",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "impu.vasudev",
    "date": "July 11, 2018, 10:18am July 25, 2018, 2:49pm July 26, 2018, 8:03am July 26, 2018, 2:36pm July 26, 2018, 2:39pm July 26, 2018, 3:30pm August 3, 2018, 10:54am August 31, 2018, 10:54am",
    "body": "Hi all, ES is installed in one of my node and i am running a trail version of ES but i am unable to connect it to Hadoop. My question is can we establish connectivity between ES and Hadoop in trail version.",
    "website_area": "discuss"
  },
  {
    "id": "87fbf3e6-0bd9-4394-b689-eb03d6df8b22",
    "url": "https://discuss.elastic.co/t/save-json-file-to-elasticsearch-using-spark/141168",
    "title": "Save json file to elasticsearch using spark?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Aymen_Rahal",
    "date": "July 23, 2018, 12:14pm July 25, 2018, 3:00pm July 29, 2018, 8:20pm August 26, 2018, 8:20pm",
    "body": "I am trying to save a json file to Elasticsearch, I am getting this error (Exception in thread \"main\" java.lang.AbstractMethodError) I am using \"spark 2.3.0\" and \"scala 2.11.6\" please find the code Below: // import org.apache.spark.sql.SQLContext import org.elasticsearch.spark.sql._ import org.apache.spark.{SparkConf, SparkContext} object OrangetoES { def main(args: Array[String]) { val conf = new SparkConf().setAppName(\"OrangetoES\").setMaster(\"local[*]\") val sc = new SparkContext(conf) conf.set(\"es.index.auto.create\", \"true\") val sqlc = new SQLContext(sc) val df = sqlc.read.json(\"../Orange.json\") df.saveToEs(\"orangetoes/people\") } } // help me to solve this problem",
    "website_area": "discuss"
  },
  {
    "id": "93e104df-6254-4bbb-8fcb-65095dc56174",
    "url": "https://discuss.elastic.co/t/hive-external-table-performance-issue/141965",
    "title": "Hive external table performance issue",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Serge_Lashdev",
    "date": "July 27, 2018, 4:32pm July 27, 2018, 4:37pm July 27, 2018, 4:48pm August 24, 2018, 4:48pm",
    "body": "Hi there, I created an external table in hive to query indices in ES whith the following statement. CREATE EXTERNAL TABLE svaadclog ( bytexfr BIGINT, httpquery STRING, httpreferer STRING, httpresultcode INT, ipsrc VARCHAR(15), loghost STRING, targetname STRING, time TIMESTAMP ) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES( 'es.resource' = 'svaadclog*/adclog', 'es.query' = '{\"query\": { \"range\": { \"time\": { \"gte\": \"13/04/2018-19:55:00\", \"lte\": \"13/04/2018-20:05:00\", \"format\": \"dd/MM/yyyy-HH:mm:ss\" } } }}' ); A simple select is much slower (460 secs) than the kibana query (< 1sec) and the java processes are CPU intensive. IS there any guideline to improve performance of reading ES whith hive external tables ? ES stack is the latest , hadoop is 3.1.0",
    "website_area": "discuss"
  },
  {
    "id": "50ca5984-af72-4f47-b827-a99906fc5e41",
    "url": "https://discuss.elastic.co/t/spark-sql-query-for-empty-string/141588",
    "title": "Spark sql query for empty string",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "wangqinghuan",
    "date": "July 25, 2018, 1:36pm July 25, 2018, 3:07pm July 27, 2018, 10:39am July 27, 2018, 3:02pm August 24, 2018, 3:02pm",
    "body": "I am using spark sql to query for an empty string . Documents are returned exactly when I use elasticsearch dsl as following: user/_search { \"query\": { \"term\": { \"type\": \"\" } } } But in spark sql, I written a sql like \"select * from user where type = '' \", It does not return any results. how to query for empty string in spark sql ?",
    "website_area": "discuss"
  }
]